
#include "convolution.h"



extern __attribute__((aligned(64))) unsigned short int f_vector[16]; //initialized by 'prepare_for_division()' routine

//this is for 9x9 case
const unsigned char reminder_msk_9x9[36][32] __attribute__((aligned(64))) ={//this lookup table is used in the loop reminder
		{255,255,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0},
		{255,255,255,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0},
		{255,255,255,255,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0},
		{255,255,255,255,255,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0},
		{255,255,255,255,255,255,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0},
		{255,255,255,255,255,255,255,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0},
		{255,255,255,255,255,255,255,255,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0},
		{255,255,255,255,255,255,255,255,255,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,0,0,0,0,0,0,0,0,0,0,0},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,0,0,0,0,0,0,0,0,0,0},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,0,0,0,0,0,0,0,0,0},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,0,0,0,0,0,0,0,0},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,0,0,0,0,0,0,0},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,0,0,0,0,0,0},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,0,0,0,0,0},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,0,0,0,0},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,0,0,0},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,0,0},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,0},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255},

} ;


//this is for 7x7 case
const unsigned char reminder_msk_7x7[35][32] __attribute__((aligned(64))) ={//this lookup table is used in the loop reminder
		{255,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0},
		{255,255,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0},
		{255,255,255,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0},
		{255,255,255,255,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0},
		{255,255,255,255,255,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0},
		{255,255,255,255,255,255,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0},
		{255,255,255,255,255,255,255,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0},
		{255,255,255,255,255,255,255,255,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0},
		{255,255,255,255,255,255,255,255,255,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,0,0,0,0,0,0,0,0,0,0,0},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,0,0,0,0,0,0,0,0,0,0},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,0,0,0,0,0,0,0,0,0},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,0,0,0,0,0,0,0,0},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,0,0,0,0,0,0,0},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,0,0,0,0,0,0},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,0,0,0,0,0},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,0,0,0,0},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,0,0,0},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,0,0},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,0},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255},
} ;


//this is for 5x5 case
extern const unsigned char reminder_msk1[31][32] __attribute__((aligned(64)));
extern const unsigned char reminder_msk2[31][32] __attribute__((aligned(64))) ;

extern const unsigned char reminder_msk1_3x3[31][32] __attribute__((aligned(64))) ;







void convolution_optimized_3x3_16(unsigned char **frame1,unsigned char **filt,const unsigned int M, const unsigned int N, const unsigned short int divisor, signed char **filter){

const signed char f00=filter[0][0];
const signed char f01=filter[0][1];
const signed char f02=filter[0][2];

const signed char f10=filter[1][0];
const signed char f11=filter[1][1];
const signed char f12=filter[1][2];

const signed char f20=filter[2][0];
const signed char f21=filter[2][1];
const signed char f22=filter[2][2];

	const __m256i c0=_mm256_set_epi8(0,f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00);
	const __m256i c1=_mm256_set_epi8(0,f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10);
	const __m256i c2=_mm256_set_epi8(0,f22,f21,f20,0,f22,f21,f20,0,f22,f21,f20,0,f22,f21,f20,0,f22,f21,f20,0,f22,f21,f20,0,f22,f21,f20,0,f22,f21,f20);


	const __m256i c0_sh1=_mm256_set_epi8(f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00,0);
	const __m256i c1_sh1=_mm256_set_epi8(f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10,0);
	const __m256i c2_sh1=_mm256_set_epi8(f22,f21,f20,0,f22,f21,f20,0,f22,f21,f20,0,f22,f21,f20,0,f22,f21,f20,0,f22,f21,f20,0,f22,f21,f20,0,f22,f21,f20,0);


	const __m256i c0_sh2=_mm256_set_epi8(0,0,0,f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00,0,0);
	const __m256i c1_sh2=_mm256_set_epi8(0,0,0,f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10,0,0);
	const __m256i c2_sh2=_mm256_set_epi8(0,0,0,f22,f21,f20,0,f22,f21,f20,0,f22,f21,f20,0,f22,f21,f20,0,f22,f21,f20,0,f22,f21,f20,0,f22,f21,f20,0,0);


	const __m256i c0_sh3=_mm256_set_epi8(0,0,f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00,0,0,0);
	const __m256i c1_sh3=_mm256_set_epi8(0,0,f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10,0,0,0);
	const __m256i c2_sh3=_mm256_set_epi8(0,0,f22,f21,f20,0,f22,f21,f20,0,f22,f21,f20,0,f22,f21,f20,0,f22,f21,f20,0,f22,f21,f20,0,f22,f21,f20,0,0,0);

	const __m256i mask3  = _mm256_set_epi16(0,0,0,0,0,0,0,65535,0,0,0,0,0,0,0,0);
	const __m256i mask_prelude  = _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0);

	const __m256i output_mask      = _mm256_set_epi16(0,65535,0,65535,0,65535,0,65535,0,65535,0,65535,0,65535,0,65535);
	const __m256i output_mask_sh1  = _mm256_set_epi16(0,0,65535,0,65535,0,65535,0,65535,0,65535,0,65535,0,65535,0);




	const unsigned int REMINDER_ITERATIONS =  (M-((((M-32)/30)*30)+30)); //M-(last_col_value+30)
	//printf("\n%d",REMINDER_ITERATIONS);


	const unsigned int division_case=prepare_for_division(divisor); //determine which is the division case (A, B or C)
	const __m256i f = _mm256_load_si256( (__m256i *) &f_vector[0]);// initialize the division vector

//printf("\n%d %d",division_case,b);


#pragma omp parallel
{

unsigned int row,col;
register __m256i r0,r1,r2,m0,m1,m2,m4;
//__m256i output_row0_even,output_row1_even,output_row0_odd,output_row1_odd; //these are for processing row #0 and #1 only.
__m256i output_even,output_odd;
//__m256i m0_prelude_row0,m0_prelude_row1;


/*---------------------- Gaussian Blur ---------------------------------*/

    #pragma omp for schedule(static)
	for (row = 1; row < N-1; row++) {

	if (row==1){//special case compute filt[0:1][:]

		for (col = 0; col <= M-32; col+=30){

		  if (col==0){

				//load the 5 rows
				r0=_mm256_loadu_si256( (__m256i *) &frame1[0][0]);
				r1=_mm256_loadu_si256( (__m256i *) &frame1[1][0]);
				r2=_mm256_loadu_si256( (__m256i *) &frame1[2][0]);


		  			//START - extra code needed for prelude
		  					m0=_mm256_slli_si256(r0,1);//shift 1 elements left - equivalent to filling with one zero inthe beginning
		  					m1=_mm256_slli_si256(r1,1);//shift 1 elements left - equivalent to filling with one zero inthe beginning
		  					m2=_mm256_slli_si256(r2,1);//shift 1 elements left - equivalent to filling with one zero inthe beginning

		  					//preserve the element being lost because of the shift above
		  					r0=_mm256_and_si256(r0,mask_prelude);
		  					r0=_mm256_permute2f128_si256(r0,r0,1);
		  					r0=_mm256_srli_si256(r0,15);//shift 15 elements
		  					r0=_mm256_add_epi16(m0,r0);

		  					r1=_mm256_and_si256(r1,mask_prelude);
		  					r1=_mm256_permute2f128_si256(r1,r1,1);
		  					r1=_mm256_srli_si256(r1,15);//shift 15 elements
		  					r1=_mm256_add_epi16(m1,r1);

		  					r2=_mm256_and_si256(r2,mask_prelude);
		  					r2=_mm256_permute2f128_si256(r2,r2,1);
		  					r2=_mm256_srli_si256(r2,15);//shift 15 elements
		  					r2=_mm256_add_epi16(m2,r2);

		  			//END - extra code needed for prelude
		  		}
		  else {
				//load the 5 rows
				r0=_mm256_loadu_si256( (__m256i *) &frame1[0][col-1]);
				r1=_mm256_loadu_si256( (__m256i *) &frame1[1][col-1]);
				r2=_mm256_loadu_si256( (__m256i *) &frame1[2][col-1]);
		  }

		//col iteration computes output pixels of    1,5,9,13,17,21,25,29
		// col+1 iteration computes output pixels of 2,6,10,14,18,22,26,30
		// col+2 iteration computes output pixels of 3,7,11,15,19,23,27
		// col+3 iteration computes output pixels of 4,8,12,16,20,24,28
		//afterwards, col becomes 30 and repeat the above process

		 //row==1
		//1st col iteration

		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,c0);
		m1=_mm256_maddubs_epi16(r1,c1);
		m2=_mm256_maddubs_epi16(r2,c2);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m0=_mm256_add_epi16(m0,m2);

		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others. later on they will be stored into 0,4,8,12,16,20,24,28 positions (8bit register).
		output_even=_mm256_and_si256(m2,output_mask);


		//2nd col iteration
		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,c0_sh1);
		m1=_mm256_maddubs_epi16(r1,c1_sh1);
		m2=_mm256_maddubs_epi16(r2,c2_sh1);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m0=_mm256_add_epi16(m0,m2);

		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);


		//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others.
		output_odd = _mm256_and_si256(m2,output_mask);


       //3rd col iteration
		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,c0_sh2);
		m1=_mm256_maddubs_epi16(r1,c1_sh2);
		m2=_mm256_maddubs_epi16(r2,c2_sh2);


		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m0=_mm256_add_epi16(m0,m2);


		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		m4=_mm256_and_si256(m0,mask3);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
		m2=_mm256_add_epi16(m2,m4);

		//m2 has 16 16bit values now. the results I need are in positions 1,3,5,7,9,11,13. keep only those, discard others
		m2 = _mm256_and_si256(m2,output_mask_sh1);
		output_even = _mm256_add_epi16(output_even,m2);


		//4th col iteration

		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,c0_sh3);
		m1=_mm256_maddubs_epi16(r1,c1_sh3);
		m2=_mm256_maddubs_epi16(r2,c2_sh3);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m0=_mm256_add_epi16(m0,m2);


		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		m4=_mm256_and_si256(m0,mask3);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
		m2=_mm256_add_epi16(m2,m4);

		//m2 has 16 16bit values now. the results I need are in positions 1,3,5,7,9,11,13. keep only those, discard others
		m2 = _mm256_and_si256(m2,output_mask_sh1);
		output_odd = _mm256_add_epi16(output_odd,m2);


		//now division follows
		output_even=division(division_case,output_even,f);
		output_odd=division(division_case,output_odd,f);

		//shift odd 1 position and add to even
		output_odd = _mm256_slli_si256(output_odd,1);
		output_even = _mm256_add_epi8(output_even,output_odd);


		_mm256_storeu_si256( (__m256i *) &filt[1][col],output_even );

		 //row==0
		//1st col iteration

		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,c1);
		m1=_mm256_maddubs_epi16(r1,c2);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);

		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others. later on they will be stored into 0,4,8,12,16,20,24,28 positions (8bit register).
		output_even=_mm256_and_si256(m2,output_mask);


		//2nd col iteration
		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,c1_sh1);
		m1=_mm256_maddubs_epi16(r1,c2_sh1);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);

		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);


		//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others.
		output_odd = _mm256_and_si256(m2,output_mask);


      //3rd col iteration
		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,c1_sh2);
		m1=_mm256_maddubs_epi16(r1,c2_sh2);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);


		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		m4=_mm256_and_si256(m0,mask3);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
		m2=_mm256_add_epi16(m2,m4);

		//m2 has 16 16bit values now. the results I need are in positions 1,3,5,7,9,11,13. keep only those, discard others
		m2 = _mm256_and_si256(m2,output_mask_sh1);
		output_even = _mm256_add_epi16(output_even,m2);


		//4th col iteration

		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,c1_sh3);
		m1=_mm256_maddubs_epi16(r1,c2_sh3);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);

		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		m4=_mm256_and_si256(m0,mask3);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
		m2=_mm256_add_epi16(m2,m4);

		//m2 has 16 16bit values now. the results I need are in positions 1,3,5,7,9,11,13. keep only those, discard others
		m2 = _mm256_and_si256(m2,output_mask_sh1);
		output_odd = _mm256_add_epi16(output_odd,m2);


		//now division follows
		output_even=division(division_case,output_even,f);
		output_odd=division(division_case,output_odd,f);

		//shift odd 1 position and add to even
		output_odd = _mm256_slli_si256(output_odd,1);
		output_even = _mm256_add_epi8(output_even,output_odd);


		_mm256_storeu_si256( (__m256i *) &filt[0][col],output_even );
		}
	conv_loop_reminder_3x3_first_values(frame1,filt,M,N,col,REMINDER_ITERATIONS,division_case,divisor,filter,c0,c1,c2,c0_sh1,c1_sh1,c2_sh1,c0_sh2,c1_sh2,c2_sh2,c0_sh3,c1_sh3,c2_sh3,f);

	}

	else if (row==N-2){//special case compute filt[N-2:N-1][:]
		for (col = 0; col <= M-32; col+=30){

		  if (col==0){

				//load the 5 rows
				r0=_mm256_loadu_si256( (__m256i *) &frame1[N-3][0]);
				r1=_mm256_loadu_si256( (__m256i *) &frame1[N-2][0]);
				r2=_mm256_loadu_si256( (__m256i *) &frame1[N-1][0]);


		  			//START - extra code needed for prelude
		  					m0=_mm256_slli_si256(r0,1);//shift 1 elements left - equivalent to filling with one zero inthe beginning
		  					m1=_mm256_slli_si256(r1,1);//shift 1 elements left - equivalent to filling with one zero inthe beginning
		  					m2=_mm256_slli_si256(r2,1);//shift 1 elements left - equivalent to filling with one zero inthe beginning

		  					//preserve the element being lost because of the shift above
		  					r0=_mm256_and_si256(r0,mask_prelude);
		  					r0=_mm256_permute2f128_si256(r0,r0,1);
		  					r0=_mm256_srli_si256(r0,15);//shift 15 elements
		  					r0=_mm256_add_epi16(m0,r0);

		  					r1=_mm256_and_si256(r1,mask_prelude);
		  					r1=_mm256_permute2f128_si256(r1,r1,1);
		  					r1=_mm256_srli_si256(r1,15);//shift 15 elements
		  					r1=_mm256_add_epi16(m1,r1);

		  					r2=_mm256_and_si256(r2,mask_prelude);
		  					r2=_mm256_permute2f128_si256(r2,r2,1);
		  					r2=_mm256_srli_si256(r2,15);//shift 15 elements
		  					r2=_mm256_add_epi16(m2,r2);

		  			//END - extra code needed for prelude
		  		}
		  else {
				//load the 5 rows
				r0=_mm256_loadu_si256( (__m256i *) &frame1[N-3][col-1]);
				r1=_mm256_loadu_si256( (__m256i *) &frame1[N-2][col-1]);
				r2=_mm256_loadu_si256( (__m256i *) &frame1[N-1][col-1]);
		  }

		//col iteration computes output pixels of    1,5,9,13,17,21,25,29
		// col+1 iteration computes output pixels of 2,6,10,14,18,22,26,30
		// col+2 iteration computes output pixels of 3,7,11,15,19,23,27
		// col+3 iteration computes output pixels of 4,8,12,16,20,24,28
		//afterwards, col becomes 30 and repeat the above process

		 //row==N-2
		//1st col iteration

		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,c0);
		m1=_mm256_maddubs_epi16(r1,c1);
		m2=_mm256_maddubs_epi16(r2,c2);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m0=_mm256_add_epi16(m0,m2);

		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others. later on they will be stored into 0,4,8,12,16,20,24,28 positions (8bit register).
		output_even=_mm256_and_si256(m2,output_mask);


		//2nd col iteration
		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,c0_sh1);
		m1=_mm256_maddubs_epi16(r1,c1_sh1);
		m2=_mm256_maddubs_epi16(r2,c2_sh1);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m0=_mm256_add_epi16(m0,m2);

		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);


		//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others.
		output_odd = _mm256_and_si256(m2,output_mask);


       //3rd col iteration
		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,c0_sh2);
		m1=_mm256_maddubs_epi16(r1,c1_sh2);
		m2=_mm256_maddubs_epi16(r2,c2_sh2);


		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m0=_mm256_add_epi16(m0,m2);


		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		m4=_mm256_and_si256(m0,mask3);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
		m2=_mm256_add_epi16(m2,m4);

		//m2 has 16 16bit values now. the results I need are in positions 1,3,5,7,9,11,13. keep only those, discard others
		m2 = _mm256_and_si256(m2,output_mask_sh1);
		output_even = _mm256_add_epi16(output_even,m2);


		//4th col iteration

		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,c0_sh3);
		m1=_mm256_maddubs_epi16(r1,c1_sh3);
		m2=_mm256_maddubs_epi16(r2,c2_sh3);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m0=_mm256_add_epi16(m0,m2);


		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		m4=_mm256_and_si256(m0,mask3);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
		m2=_mm256_add_epi16(m2,m4);

		//m2 has 16 16bit values now. the results I need are in positions 1,3,5,7,9,11,13. keep only those, discard others
		m2 = _mm256_and_si256(m2,output_mask_sh1);
		output_odd = _mm256_add_epi16(output_odd,m2);


		//now division follows
		output_even=division(division_case,output_even,f);
		output_odd=division(division_case,output_odd,f);

		//shift odd 1 position and add to even
		output_odd = _mm256_slli_si256(output_odd,1);
		output_even = _mm256_add_epi8(output_even,output_odd);


		_mm256_storeu_si256( (__m256i *) &filt[N-2][col],output_even );

		 //row==N-1
		//1st col iteration

		//multiply with the mask
		m1=_mm256_maddubs_epi16(r1,c0);
		m2=_mm256_maddubs_epi16(r2,c1);

		//vertical add
		m0=_mm256_add_epi16(m1,m2);

		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others. later on they will be stored into 0,4,8,12,16,20,24,28 positions (8bit register).
		output_even=_mm256_and_si256(m2,output_mask);


		//2nd col iteration
		//multiply with the mask
		m1=_mm256_maddubs_epi16(r1,c0_sh1);
		m2=_mm256_maddubs_epi16(r2,c1_sh1);

		//vertical add
		m0=_mm256_add_epi16(m1,m2);

		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);


		//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others.
		output_odd = _mm256_and_si256(m2,output_mask);


       //3rd col iteration
		//multiply with the mask
		m1=_mm256_maddubs_epi16(r1,c0_sh2);
		m2=_mm256_maddubs_epi16(r2,c1_sh2);


		//vertical add
		m0=_mm256_add_epi16(m1,m2);


		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		m4=_mm256_and_si256(m0,mask3);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
		m2=_mm256_add_epi16(m2,m4);

		//m2 has 16 16bit values now. the results I need are in positions 1,3,5,7,9,11,13. keep only those, discard others
		m2 = _mm256_and_si256(m2,output_mask_sh1);
		output_even = _mm256_add_epi16(output_even,m2);


		//4th col iteration

		//multiply with the mask
		m1=_mm256_maddubs_epi16(r1,c0_sh3);
		m2=_mm256_maddubs_epi16(r2,c1_sh3);

		//vertical add
		m0=_mm256_add_epi16(m1,m2);

		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		m4=_mm256_and_si256(m0,mask3);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
		m2=_mm256_add_epi16(m2,m4);

		//m2 has 16 16bit values now. the results I need are in positions 1,3,5,7,9,11,13. keep only those, discard others
		m2 = _mm256_and_si256(m2,output_mask_sh1);
		output_odd = _mm256_add_epi16(output_odd,m2);


		//now division follows
		output_even=division(division_case,output_even,f);
		output_odd=division(division_case,output_odd,f);

		//shift odd 1 position and add to even
		output_odd = _mm256_slli_si256(output_odd,1);
		output_even = _mm256_add_epi8(output_even,output_odd);


		_mm256_storeu_si256( (__m256i *) &filt[N-1][col],output_even );
		}
	conv_loop_reminder_3x3_last_values(frame1,filt,M,N,col,REMINDER_ITERATIONS,division_case,divisor,filter,c0,c1,c2,c0_sh1,c1_sh1,c2_sh1,c0_sh2,c1_sh2,c2_sh2,c0_sh3,c1_sh3,c2_sh3,f);

	}

	else {//main loop

	  for (col = 0; col <= M-32; col+=30){

		  if (col==0){

				//load the 5 rows
				r0=_mm256_loadu_si256( (__m256i *) &frame1[row-1][0]);
				r1=_mm256_loadu_si256( (__m256i *) &frame1[row][0]);
				r2=_mm256_loadu_si256( (__m256i *) &frame1[row+1][0]);


		  			//START - extra code needed for prelude
		  					m0=_mm256_slli_si256(r0,1);//shift 1 elements left - equivalent to filling with one zero inthe beginning
		  					m1=_mm256_slli_si256(r1,1);//shift 1 elements left - equivalent to filling with one zero inthe beginning
		  					m2=_mm256_slli_si256(r2,1);//shift 1 elements left - equivalent to filling with one zero inthe beginning

		  					//preserve the element being lost because of the shift above
		  					r0=_mm256_and_si256(r0,mask_prelude);
		  					r0=_mm256_permute2f128_si256(r0,r0,1);
		  					r0=_mm256_srli_si256(r0,15);//shift 15 elements
		  					r0=_mm256_add_epi16(m0,r0);

		  					r1=_mm256_and_si256(r1,mask_prelude);
		  					r1=_mm256_permute2f128_si256(r1,r1,1);
		  					r1=_mm256_srli_si256(r1,15);//shift 15 elements
		  					r1=_mm256_add_epi16(m1,r1);

		  					r2=_mm256_and_si256(r2,mask_prelude);
		  					r2=_mm256_permute2f128_si256(r2,r2,1);
		  					r2=_mm256_srli_si256(r2,15);//shift 15 elements
		  					r2=_mm256_add_epi16(m2,r2);

		  			//END - extra code needed for prelude
		  		}
		  else {
				//load the 5 rows
				r0=_mm256_loadu_si256( (__m256i *) &frame1[row-1][col-1]);
				r1=_mm256_loadu_si256( (__m256i *) &frame1[row][col-1]);
				r2=_mm256_loadu_si256( (__m256i *) &frame1[row+1][col-1]);
		  }

		//col iteration computes output pixels of    1,5,9,13,17,21,25,29
		// col+1 iteration computes output pixels of 2,6,10,14,18,22,26,30
		// col+2 iteration computes output pixels of 3,7,11,15,19,23,27
		// col+3 iteration computes output pixels of 4,8,12,16,20,24,28
		//afterwards, col becomes 30 and repeat the above process

		//1st col iteration

		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,c0);
		m1=_mm256_maddubs_epi16(r1,c1);
		m2=_mm256_maddubs_epi16(r2,c2);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m0=_mm256_add_epi16(m0,m2);

		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others. later on they will be stored into 0,4,8,12,16,20,24,28 positions (8bit register).
		output_even=_mm256_and_si256(m2,output_mask);


		//2nd col iteration
		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,c0_sh1);
		m1=_mm256_maddubs_epi16(r1,c1_sh1);
		m2=_mm256_maddubs_epi16(r2,c2_sh1);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m0=_mm256_add_epi16(m0,m2);

		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);


		//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others.
		output_odd = _mm256_and_si256(m2,output_mask);


         //3rd col iteration
		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,c0_sh2);
		m1=_mm256_maddubs_epi16(r1,c1_sh2);
		m2=_mm256_maddubs_epi16(r2,c2_sh2);


		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m0=_mm256_add_epi16(m0,m2);


		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		m4=_mm256_and_si256(m0,mask3);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
		m2=_mm256_add_epi16(m2,m4);

		//m2 has 16 16bit values now. the results I need are in positions 1,3,5,7,9,11,13. keep only those, discard others
		m2 = _mm256_and_si256(m2,output_mask_sh1);
		output_even = _mm256_add_epi16(output_even,m2);


		//4th col iteration

		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,c0_sh3);
		m1=_mm256_maddubs_epi16(r1,c1_sh3);
		m2=_mm256_maddubs_epi16(r2,c2_sh3);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m0=_mm256_add_epi16(m0,m2);


		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		m4=_mm256_and_si256(m0,mask3);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
		m2=_mm256_add_epi16(m2,m4);

		//m2 has 16 16bit values now. the results I need are in positions 1,3,5,7,9,11,13. keep only those, discard others
		m2 = _mm256_and_si256(m2,output_mask_sh1);
		output_odd = _mm256_add_epi16(output_odd,m2);


		//now division follows
		output_even=division(division_case,output_even,f);
		output_odd=division(division_case,output_odd,f);

		//shift odd 1 position and add to even
		output_odd = _mm256_slli_si256(output_odd,1);
		output_even = _mm256_add_epi8(output_even,output_odd);


		_mm256_storeu_si256( (__m256i *) &filt[row][col],output_even );


		}
	  conv_loop_reminder_3x3(frame1,filt,M,N,row,col,REMINDER_ITERATIONS,division_case,divisor,filter,c0,c1,c2,c0_sh1,c1_sh1,c2_sh1,c0_sh2,c1_sh2,c2_sh2,c0_sh3,c1_sh3,c2_sh3,f);

	}

}

}//end of parallel


}




int conv_loop_reminder_3x3(unsigned char **frame1,unsigned char **filt,const unsigned int M, const unsigned int N,const unsigned int row, const unsigned int col,const unsigned int REMINDER_ITERATIONS,const unsigned int division_case,const unsigned short int divisor,signed char **filter,const __m256i c0,const __m256i c1,const __m256i c2,const __m256i c0_sh1,const __m256i c1_sh1,const __m256i c2_sh1,const __m256i c0_sh2,const __m256i c1_sh2,const __m256i c2_sh2, const __m256i c0_sh3,const __m256i c1_sh3,const __m256i c2_sh3,const __m256i f){

register	__m256i r0,r1,r2,m0,m1,m2,m4,output_even,output_odd;


const __m256i mask3  = _mm256_set_epi16(0,0,0,0,0,0,0,65535,0,0,0,0,0,0,0,0);

const __m256i output_mask      = _mm256_set_epi16(0,65535,0,65535,0,65535,0,65535,0,65535,0,65535,0,65535,0,65535);
const __m256i output_mask_sh1  = _mm256_set_epi16(0,0,65535,0,65535,0,65535,0,65535,0,65535,0,65535,0,65535,0);

 __m256i reminder_mask1;

if (REMINDER_ITERATIONS == 0){
	return 0; //no loop reminder is needed
}

	reminder_mask1=_mm256_load_si256( (__m256i *) &reminder_msk1_3x3[REMINDER_ITERATIONS-1][0]);
	//reminder_mask2=_mm256_load_si256( (__m256i *) &reminder_msk2[REMINDER_ITERATIONS-1][0]);


	 //1st col iteration
	//load the 5 rows
	r0=_mm256_loadu_si256( (__m256i *) &frame1[row-1][col-1]);
	r1=_mm256_loadu_si256( (__m256i *) &frame1[row][col-1]);
	r2=_mm256_loadu_si256( (__m256i *) &frame1[row+1][col-1]);


	/*
	 * The above load operations load outside of the array bounds; these elements are filled with zeros just after
	 * Furthermore, I add one extra zero in the end to compute col=N-1. The last value of col is N-1, not N
	 */

	//AND r0-r4 with reminder_mask
	r0=_mm256_and_si256(r0,reminder_mask1);
	r1=_mm256_and_si256(r1,reminder_mask1);
	r2=_mm256_and_si256(r2,reminder_mask1);


	//col iteration computes output pixels of    1,5,9,13,17,21,25,29
	// col+1 iteration computes output pixels of 2,6,10,14,18,22,26,30
	// col+2 iteration computes output pixels of 3,7,11,15,19,23,27
	// col+3 iteration computes output pixels of 4,8,12,16,20,24,28
	//afterwards, col becomes 30 and repeat the above process

	//1st col iteration

	//multiply with the mask
	m0=_mm256_maddubs_epi16(r0,c0);
	m1=_mm256_maddubs_epi16(r1,c1);
	m2=_mm256_maddubs_epi16(r2,c2);

	//vertical add
	m0=_mm256_add_epi16(m0,m1);
	m0=_mm256_add_epi16(m0,m2);

	//hozizontal additions
	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);

	//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others. later on they will be stored into 0,4,8,12,16,20,24,28 positions (8bit register).
	output_even=_mm256_and_si256(m2,output_mask);


	//2nd col iteration
	//multiply with the mask
	m0=_mm256_maddubs_epi16(r0,c0_sh1);
	m1=_mm256_maddubs_epi16(r1,c1_sh1);
	m2=_mm256_maddubs_epi16(r2,c2_sh1);

	//vertical add
	m0=_mm256_add_epi16(m0,m1);
	m0=_mm256_add_epi16(m0,m2);

	//hozizontal additions
	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);


	//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others.
	output_odd = _mm256_and_si256(m2,output_mask);


     //3rd col iteration
	//multiply with the mask
	m0=_mm256_maddubs_epi16(r0,c0_sh2);
	m1=_mm256_maddubs_epi16(r1,c1_sh2);
	m2=_mm256_maddubs_epi16(r2,c2_sh2);


	//vertical add
	m0=_mm256_add_epi16(m0,m1);
	m0=_mm256_add_epi16(m0,m2);


	//hozizontal additions
	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);

	m4=_mm256_and_si256(m0,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
	m2=_mm256_add_epi16(m2,m4);

	//m2 has 16 16bit values now. the results I need are in positions 1,3,5,7,9,11,13. keep only those, discard others
	m2 = _mm256_and_si256(m2,output_mask_sh1);
	output_even = _mm256_add_epi16(output_even,m2);


	//4th col iteration

	//multiply with the mask
	m0=_mm256_maddubs_epi16(r0,c0_sh3);
	m1=_mm256_maddubs_epi16(r1,c1_sh3);
	m2=_mm256_maddubs_epi16(r2,c2_sh3);

	//vertical add
	m0=_mm256_add_epi16(m0,m1);
	m0=_mm256_add_epi16(m0,m2);


	//hozizontal additions
	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);

	m4=_mm256_and_si256(m0,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
	m2=_mm256_add_epi16(m2,m4);

	//m2 has 16 16bit values now. the results I need are in positions 1,3,5,7,9,11,13. keep only those, discard others
	m2 = _mm256_and_si256(m2,output_mask_sh1);
	output_odd = _mm256_add_epi16(output_odd,m2);


	//now division follows
	output_even=division(division_case,output_even,f);
	output_odd=division(division_case,output_odd,f);

	//shift odd 1 position and add to even
	output_odd = _mm256_slli_si256(output_odd,1);
	output_even = _mm256_add_epi8(output_even,output_odd);


 	switch (REMINDER_ITERATIONS){
	case 1:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
	  break;
	case 2:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		  break;
	case 3:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		  break;
	case 4:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output_even,3);
		  break;
	case 5:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output_even,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output_even,4);
		  break;
	case 6:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output_even,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output_even,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output_even,5);
		  break;
	case 7:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output_even,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output_even,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output_even,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output_even,6);
		  break;
	case 8:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output_even,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output_even,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output_even,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output_even,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output_even,7);
		  break;
	case 9:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output_even,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output_even,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output_even,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output_even,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output_even,7);
		filt[row][col+8] = (unsigned char) _mm256_extract_epi8(output_even,8);
		  break;
	case 10:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output_even,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output_even,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output_even,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output_even,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output_even,7);
		filt[row][col+8] = (unsigned char) _mm256_extract_epi8(output_even,8);
		filt[row][col+9] = (unsigned char) _mm256_extract_epi8(output_even,9);
		  break;
	case 11:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output_even,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output_even,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output_even,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output_even,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output_even,7);
		filt[row][col+8] = (unsigned char) _mm256_extract_epi8(output_even,8);
		filt[row][col+9] = (unsigned char) _mm256_extract_epi8(output_even,9);
		filt[row][col+10] = (unsigned char) _mm256_extract_epi8(output_even,10);
		  break;
	case 12:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output_even,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output_even,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output_even,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output_even,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output_even,7);
		filt[row][col+8] = (unsigned char) _mm256_extract_epi8(output_even,8);
		filt[row][col+9] = (unsigned char) _mm256_extract_epi8(output_even,9);
		filt[row][col+10] = (unsigned char) _mm256_extract_epi8(output_even,10);
		filt[row][col+11] = (unsigned char) _mm256_extract_epi8(output_even,11);
		  break;
	case 13:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output_even,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output_even,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output_even,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output_even,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output_even,7);
		filt[row][col+8] = (unsigned char) _mm256_extract_epi8(output_even,8);
		filt[row][col+9] = (unsigned char) _mm256_extract_epi8(output_even,9);
		filt[row][col+10] = (unsigned char) _mm256_extract_epi8(output_even,10);
		filt[row][col+11] = (unsigned char) _mm256_extract_epi8(output_even,11);
		filt[row][col+12] = (unsigned char) _mm256_extract_epi8(output_even,12);
		  break;
	case 14:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output_even,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output_even,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output_even,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output_even,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output_even,7);
		filt[row][col+8] = (unsigned char) _mm256_extract_epi8(output_even,8);
		filt[row][col+9] = (unsigned char) _mm256_extract_epi8(output_even,9);
		filt[row][col+10] = (unsigned char) _mm256_extract_epi8(output_even,10);
		filt[row][col+11] = (unsigned char) _mm256_extract_epi8(output_even,11);
		filt[row][col+12] = (unsigned char) _mm256_extract_epi8(output_even,12);
		filt[row][col+13] = (unsigned char) _mm256_extract_epi8(output_even,13);
		  break;
	case 15:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output_even,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output_even,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output_even,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output_even,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output_even,7);
		filt[row][col+8] = (unsigned char) _mm256_extract_epi8(output_even,8);
		filt[row][col+9] = (unsigned char) _mm256_extract_epi8(output_even,9);
		filt[row][col+10] = (unsigned char) _mm256_extract_epi8(output_even,10);
		filt[row][col+11] = (unsigned char) _mm256_extract_epi8(output_even,11);
		filt[row][col+12] = (unsigned char) _mm256_extract_epi8(output_even,12);
		filt[row][col+13] = (unsigned char) _mm256_extract_epi8(output_even,13);
		filt[row][col+14] = (unsigned char) _mm256_extract_epi8(output_even,14);
		  break;
	case 16:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
		  break;
	case 17:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
		  break;
	case 18:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
		  break;
	case 19:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
		  break;
	case 20:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output_even,19);
		  break;
	case 21:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output_even,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output_even,20);
		  break;
	case 22:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output_even,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output_even,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output_even,21);
		break;
	case 23:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output_even,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output_even,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output_even,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output_even,22);
		  break;
	case 24:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output_even,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output_even,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output_even,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output_even,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output_even,23);
		  break;
	case 25:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output_even,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output_even,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output_even,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output_even,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output_even,23);
	filt[row][col+24] = (unsigned char) _mm256_extract_epi8(output_even,24);
		  break;
	case 26:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output_even,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output_even,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output_even,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output_even,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output_even,23);
	filt[row][col+24] = (unsigned char) _mm256_extract_epi8(output_even,24);
	filt[row][col+25] = (unsigned char) _mm256_extract_epi8(output_even,25);
		  break;
	case 27:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output_even,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output_even,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output_even,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output_even,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output_even,23);
	filt[row][col+24] = (unsigned char) _mm256_extract_epi8(output_even,24);
	filt[row][col+25] = (unsigned char) _mm256_extract_epi8(output_even,25);
	filt[row][col+26] = (unsigned char) _mm256_extract_epi8(output_even,26);
		  break;
	case 28:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output_even,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output_even,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output_even,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output_even,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output_even,23);
	filt[row][col+24] = (unsigned char) _mm256_extract_epi8(output_even,24);
	filt[row][col+25] = (unsigned char) _mm256_extract_epi8(output_even,25);
	filt[row][col+26] = (unsigned char) _mm256_extract_epi8(output_even,26);
	filt[row][col+27] = (unsigned char) _mm256_extract_epi8(output_even,27);
		  break;
	case 29:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output_even,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output_even,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output_even,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output_even,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output_even,23);
	filt[row][col+24] = (unsigned char) _mm256_extract_epi8(output_even,24);
	filt[row][col+25] = (unsigned char) _mm256_extract_epi8(output_even,25);
	filt[row][col+26] = (unsigned char) _mm256_extract_epi8(output_even,26);
	filt[row][col+27] = (unsigned char) _mm256_extract_epi8(output_even,27);
	filt[row][col+28] = (unsigned char) _mm256_extract_epi8(output_even,28);
		  break;
	case 30:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output_even,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output_even,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output_even,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output_even,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output_even,23);
	filt[row][col+24] = (unsigned char) _mm256_extract_epi8(output_even,24);
	filt[row][col+25] = (unsigned char) _mm256_extract_epi8(output_even,25);
	filt[row][col+26] = (unsigned char) _mm256_extract_epi8(output_even,26);
	filt[row][col+27] = (unsigned char) _mm256_extract_epi8(output_even,27);
	filt[row][col+28] = (unsigned char) _mm256_extract_epi8(output_even,28);

	//the filt[row][col+29] is computed unvectorized
	int newPixel = 0;
	newPixel += frame1[row-1][M-1-1] * filter[0][0];
	newPixel += frame1[row-1][M-1] * filter[0][1];

	newPixel += frame1[row][M-1-1] * filter[1][0];
	newPixel += frame1[row][M-1] * filter[1][1];

	newPixel += frame1[row+1][M-1-1] * filter[2][0];
	newPixel += frame1[row+1][M-1] * filter[2][1];

	filt[row][M-1] = (unsigned char) (newPixel / divisor);

		  break;

	case 31:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output_even,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output_even,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output_even,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output_even,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output_even,23);
	filt[row][col+24] = (unsigned char) _mm256_extract_epi8(output_even,24);
	filt[row][col+25] = (unsigned char) _mm256_extract_epi8(output_even,25);
	filt[row][col+26] = (unsigned char) _mm256_extract_epi8(output_even,26);
	filt[row][col+27] = (unsigned char) _mm256_extract_epi8(output_even,27);
	filt[row][col+28] = (unsigned char) _mm256_extract_epi8(output_even,28);

	//the filt[row][col+29/30] are computed unvectorized
	newPixel = 0;
	newPixel += frame1[row-1][M-2-1] * filter[0][0];
	newPixel += frame1[row-1][M-2] * filter[0][1];
	newPixel += frame1[row-1][M-2+1] * filter[0][2];


	newPixel += frame1[row][M-2-1] * filter[1][0];
	newPixel += frame1[row][M-2] * filter[1][1];
	newPixel += frame1[row][M-2+1] * filter[1][2];


	newPixel += frame1[row+1][M-2-1] * filter[2][0];
	newPixel += frame1[row+1][M-2] * filter[2][1];
	newPixel += frame1[row+1][M-2+1] * filter[2][2];

	filt[row][M-2] = (unsigned char) (newPixel / divisor);

	newPixel = 0;
	newPixel += frame1[row-1][M-1-1] * filter[0][0];
	newPixel += frame1[row-1][M-1] * filter[0][1];

	newPixel += frame1[row][M-1-1] * filter[1][0];
	newPixel += frame1[row][M-1] * filter[1][1];

	newPixel += frame1[row+1][M-1-1] * filter[2][0];
	newPixel += frame1[row+1][M-1] * filter[2][1];

	filt[row][M-1] = (unsigned char) (newPixel / divisor);
		  break;
	default:
		printf("\nsomething went wrong");
		return -1;
 	}

return 0;

}




int conv_loop_reminder_3x3_first_values(unsigned char **frame1,unsigned char **filt,const unsigned int M, const unsigned int N, const unsigned int col,const unsigned int REMINDER_ITERATIONS,const unsigned int division_case,const unsigned short int divisor,signed char **filter,const __m256i c0,const __m256i c1,const __m256i c2,const __m256i c0_sh1,const __m256i c1_sh1,const __m256i c2_sh1,const __m256i c0_sh2,const __m256i c1_sh2,const __m256i c2_sh2, const __m256i c0_sh3,const __m256i c1_sh3,const __m256i c2_sh3,const __m256i f){

register	__m256i r0,r1,r2,m0,m1,m2,m4,output_even,output_odd,output_row0,output_row1;
int newPixel;
unsigned int i;
const __m256i mask3  = _mm256_set_epi16(0,0,0,0,0,0,0,65535,0,0,0,0,0,0,0,0);

const __m256i output_mask      = _mm256_set_epi16(0,65535,0,65535,0,65535,0,65535,0,65535,0,65535,0,65535,0,65535);
const __m256i output_mask_sh1  = _mm256_set_epi16(0,0,65535,0,65535,0,65535,0,65535,0,65535,0,65535,0,65535,0);

 __m256i reminder_mask1;

if (REMINDER_ITERATIONS == 0){
	return 0; //no loop reminder is needed
}

	reminder_mask1=_mm256_load_si256( (__m256i *) &reminder_msk1_3x3[REMINDER_ITERATIONS-1][0]);
	//reminder_mask2=_mm256_load_si256( (__m256i *) &reminder_msk2[REMINDER_ITERATIONS-1][0]);


	 //1st col iteration
	//load the 5 rows
	r0=_mm256_loadu_si256( (__m256i *) &frame1[0][col-1]);
	r1=_mm256_loadu_si256( (__m256i *) &frame1[1][col-1]);
	r2=_mm256_loadu_si256( (__m256i *) &frame1[2][col-1]);


	/*
	 * The above load operations load outside of the array bounds; these elements are filled with zeros just after
	 * Furthermore, I add one extra zero in the end to compute col=N-1. The last value of col is N-1, not N
	 */

	//AND r0-r4 with reminder_mask
	r0=_mm256_and_si256(r0,reminder_mask1);
	r1=_mm256_and_si256(r1,reminder_mask1);
	r2=_mm256_and_si256(r2,reminder_mask1);


	//col iteration computes output pixels of    1,5,9,13,17,21,25,29
	// col+1 iteration computes output pixels of 2,6,10,14,18,22,26,30
	// col+2 iteration computes output pixels of 3,7,11,15,19,23,27
	// col+3 iteration computes output pixels of 4,8,12,16,20,24,28
	//afterwards, col becomes 30 and repeat the above process

	//1st col iteration

	//multiply with the mask
	m0=_mm256_maddubs_epi16(r0,c0);
	m1=_mm256_maddubs_epi16(r1,c1);
	m2=_mm256_maddubs_epi16(r2,c2);

	//vertical add
	m0=_mm256_add_epi16(m0,m1);
	m0=_mm256_add_epi16(m0,m2);

	//hozizontal additions
	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);

	//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others. later on they will be stored into 0,4,8,12,16,20,24,28 positions (8bit register).
	output_even=_mm256_and_si256(m2,output_mask);


	//2nd col iteration
	//multiply with the mask
	m0=_mm256_maddubs_epi16(r0,c0_sh1);
	m1=_mm256_maddubs_epi16(r1,c1_sh1);
	m2=_mm256_maddubs_epi16(r2,c2_sh1);

	//vertical add
	m0=_mm256_add_epi16(m0,m1);
	m0=_mm256_add_epi16(m0,m2);

	//hozizontal additions
	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);


	//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others.
	output_odd = _mm256_and_si256(m2,output_mask);


     //3rd col iteration
	//multiply with the mask
	m0=_mm256_maddubs_epi16(r0,c0_sh2);
	m1=_mm256_maddubs_epi16(r1,c1_sh2);
	m2=_mm256_maddubs_epi16(r2,c2_sh2);


	//vertical add
	m0=_mm256_add_epi16(m0,m1);
	m0=_mm256_add_epi16(m0,m2);


	//hozizontal additions
	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);

	m4=_mm256_and_si256(m0,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
	m2=_mm256_add_epi16(m2,m4);

	//m2 has 16 16bit values now. the results I need are in positions 1,3,5,7,9,11,13. keep only those, discard others
	m2 = _mm256_and_si256(m2,output_mask_sh1);
	output_even = _mm256_add_epi16(output_even,m2);


	//4th col iteration

	//multiply with the mask
	m0=_mm256_maddubs_epi16(r0,c0_sh3);
	m1=_mm256_maddubs_epi16(r1,c1_sh3);
	m2=_mm256_maddubs_epi16(r2,c2_sh3);

	//vertical add
	m0=_mm256_add_epi16(m0,m1);
	m0=_mm256_add_epi16(m0,m2);


	//hozizontal additions
	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);

	m4=_mm256_and_si256(m0,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
	m2=_mm256_add_epi16(m2,m4);

	//m2 has 16 16bit values now. the results I need are in positions 1,3,5,7,9,11,13. keep only those, discard others
	m2 = _mm256_and_si256(m2,output_mask_sh1);
	output_odd = _mm256_add_epi16(output_odd,m2);


	//now division follows
	output_even=division(division_case,output_even,f);
	output_odd=division(division_case,output_odd,f);

	//shift odd 1 position and add to even
	output_odd = _mm256_slli_si256(output_odd,1);
	output_even = _mm256_add_epi8(output_even,output_odd);

	output_row1=output_even;

	 //row==0
	//1st col iteration

	//multiply with the mask
	m0=_mm256_maddubs_epi16(r0,c1);
	m1=_mm256_maddubs_epi16(r1,c2);

	//vertical add
	m0=_mm256_add_epi16(m0,m1);

	//hozizontal additions
	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);

	//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others. later on they will be stored into 0,4,8,12,16,20,24,28 positions (8bit register).
	output_even=_mm256_and_si256(m2,output_mask);


	//2nd col iteration
	//multiply with the mask
	m0=_mm256_maddubs_epi16(r0,c1_sh1);
	m1=_mm256_maddubs_epi16(r1,c2_sh1);

	//vertical add
	m0=_mm256_add_epi16(m0,m1);

	//hozizontal additions
	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);


	//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others.
	output_odd = _mm256_and_si256(m2,output_mask);


 //3rd col iteration
	//multiply with the mask
	m0=_mm256_maddubs_epi16(r0,c1_sh2);
	m1=_mm256_maddubs_epi16(r1,c2_sh2);

	//vertical add
	m0=_mm256_add_epi16(m0,m1);


	//hozizontal additions
	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);

	m4=_mm256_and_si256(m0,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
	m2=_mm256_add_epi16(m2,m4);

	//m2 has 16 16bit values now. the results I need are in positions 1,3,5,7,9,11,13. keep only those, discard others
	m2 = _mm256_and_si256(m2,output_mask_sh1);
	output_even = _mm256_add_epi16(output_even,m2);


	//4th col iteration

	//multiply with the mask
	m0=_mm256_maddubs_epi16(r0,c1_sh3);
	m1=_mm256_maddubs_epi16(r1,c2_sh3);

	//vertical add
	m0=_mm256_add_epi16(m0,m1);

	//hozizontal additions
	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);

	m4=_mm256_and_si256(m0,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
	m2=_mm256_add_epi16(m2,m4);

	//m2 has 16 16bit values now. the results I need are in positions 1,3,5,7,9,11,13. keep only those, discard others
	m2 = _mm256_and_si256(m2,output_mask_sh1);
	output_odd = _mm256_add_epi16(output_odd,m2);


	//now division follows
	output_even=division(division_case,output_even,f);
	output_odd=division(division_case,output_odd,f);

	//shift odd 1 position and add to even
	output_odd = _mm256_slli_si256(output_odd,1);
	output_even = _mm256_add_epi8(output_even,output_odd);

	output_row0=output_even;

 	switch (REMINDER_ITERATIONS){
	case 1:
		filt[1][col] = (unsigned char) _mm256_extract_epi8(output_row1,0);
		filt[0][col] = (unsigned char) _mm256_extract_epi8(output_row0,0);

	  break;
	case 2:
		for (i=0;i<=1;i++){
			filt[1][col+i] = (unsigned char) _mm256_extract_epi8(output_row1,i);
			filt[0][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 3:
		for (i=0;i<=2;i++){
			filt[1][col+i] = (unsigned char) _mm256_extract_epi8(output_row1,i);
			filt[0][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 4:
		for (i=0;i<=3;i++){
			filt[1][col+i] = (unsigned char) _mm256_extract_epi8(output_row1,i);
			filt[0][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 5:
		for (i=0;i<=4;i++){
			filt[1][col+i] = (unsigned char) _mm256_extract_epi8(output_row1,i);
			filt[0][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 6:
		for (i=0;i<=5;i++){
			filt[1][col+i] = (unsigned char) _mm256_extract_epi8(output_row1,i);
			filt[0][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 7:
		for (i=0;i<=6;i++){
			filt[1][col+i] = (unsigned char) _mm256_extract_epi8(output_row1,i);
			filt[0][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 8:
		for (i=0;i<=7;i++){
			filt[1][col+i] = (unsigned char) _mm256_extract_epi8(output_row1,i);
			filt[0][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 9:
		for (i=0;i<=8;i++){
			filt[1][col+i] = (unsigned char) _mm256_extract_epi8(output_row1,i);
			filt[0][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 10:
		for (i=0;i<=9;i++){
			filt[1][col+i] = (unsigned char) _mm256_extract_epi8(output_row1,i);
			filt[0][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 11:
		for (i=0;i<=10;i++){
			filt[1][col+i] = (unsigned char) _mm256_extract_epi8(output_row1,i);
			filt[0][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 12:
		for (i=0;i<=11;i++){
			filt[1][col+i] = (unsigned char) _mm256_extract_epi8(output_row1,i);
			filt[0][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 13:
		for (i=0;i<=12;i++){
			filt[1][col+i] = (unsigned char) _mm256_extract_epi8(output_row1,i);
			filt[0][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 14:
		for (i=0;i<=13;i++){
			filt[1][col+i] = (unsigned char) _mm256_extract_epi8(output_row1,i);
			filt[0][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 15:
		for (i=0;i<=14;i++){
			filt[1][col+i] = (unsigned char) _mm256_extract_epi8(output_row1,i);
			filt[0][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 16:
  	_mm_storeu_si128( (__m128i *) &filt[1][col],_mm256_extractf128_si256(output_row1, 0)); //store low 128bit - 16pixels
  	_mm_storeu_si128( (__m128i *) &filt[0][col],_mm256_extractf128_si256(output_row0, 0)); //store low 128bit - 16pixels

		  break;
	case 17:
  	_mm_storeu_si128( (__m128i *) &filt[1][col],_mm256_extractf128_si256(output_row1, 0)); //store low 128bit - 16pixels
  	_mm_storeu_si128( (__m128i *) &filt[0][col],_mm256_extractf128_si256(output_row0, 0)); //store low 128bit - 16pixels
	filt[1][col+16] = (unsigned char) _mm256_extract_epi8(output_row1,16);
	filt[0][col+16] = (unsigned char) _mm256_extract_epi8(output_row0,16);
		  break;
	case 18:
  	_mm_storeu_si128( (__m128i *) &filt[1][col],_mm256_extractf128_si256(output_row1, 0)); //store low 128bit - 16pixels
  	_mm_storeu_si128( (__m128i *) &filt[0][col],_mm256_extractf128_si256(output_row0, 0)); //store low 128bit - 16pixels

	for (i=16;i<=17;i++){
	 filt[1][col+i] = (unsigned char) _mm256_extract_epi8(output_row1,i);
	 filt[0][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
	}
		  break;
	case 19:
	  	_mm_storeu_si128( (__m128i *) &filt[1][col],_mm256_extractf128_si256(output_row1, 0)); //store low 128bit - 16pixels
	  	_mm_storeu_si128( (__m128i *) &filt[0][col],_mm256_extractf128_si256(output_row0, 0)); //store low 128bit - 16pixels

		for (i=16;i<=18;i++){
		 filt[1][col+i] = (unsigned char) _mm256_extract_epi8(output_row1,i);
		 filt[0][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 20:
	  	_mm_storeu_si128( (__m128i *) &filt[1][col],_mm256_extractf128_si256(output_row1, 0)); //store low 128bit - 16pixels
	  	_mm_storeu_si128( (__m128i *) &filt[0][col],_mm256_extractf128_si256(output_row0, 0)); //store low 128bit - 16pixels

		for (i=16;i<=19;i++){
		 filt[1][col+i] = (unsigned char) _mm256_extract_epi8(output_row1,i);
		 filt[0][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 21:
	  	_mm_storeu_si128( (__m128i *) &filt[1][col],_mm256_extractf128_si256(output_row1, 0)); //store low 128bit - 16pixels
	  	_mm_storeu_si128( (__m128i *) &filt[0][col],_mm256_extractf128_si256(output_row0, 0)); //store low 128bit - 16pixels

		for (i=16;i<=20;i++){
		 filt[1][col+i] = (unsigned char) _mm256_extract_epi8(output_row1,i);
		 filt[0][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 22:
	  	_mm_storeu_si128( (__m128i *) &filt[1][col],_mm256_extractf128_si256(output_row1, 0)); //store low 128bit - 16pixels
	  	_mm_storeu_si128( (__m128i *) &filt[0][col],_mm256_extractf128_si256(output_row0, 0)); //store low 128bit - 16pixels

		for (i=16;i<=21;i++){
		 filt[1][col+i] = (unsigned char) _mm256_extract_epi8(output_row1,i);
		 filt[0][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		break;
	case 23:
	  	_mm_storeu_si128( (__m128i *) &filt[1][col],_mm256_extractf128_si256(output_row1, 0)); //store low 128bit - 16pixels
	  	_mm_storeu_si128( (__m128i *) &filt[0][col],_mm256_extractf128_si256(output_row0, 0)); //store low 128bit - 16pixels

		for (i=16;i<=22;i++){
		 filt[1][col+i] = (unsigned char) _mm256_extract_epi8(output_row1,i);
		 filt[0][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 24:
	  	_mm_storeu_si128( (__m128i *) &filt[1][col],_mm256_extractf128_si256(output_row1, 0)); //store low 128bit - 16pixels
	  	_mm_storeu_si128( (__m128i *) &filt[0][col],_mm256_extractf128_si256(output_row0, 0)); //store low 128bit - 16pixels

		for (i=16;i<=23;i++){
		 filt[1][col+i] = (unsigned char) _mm256_extract_epi8(output_row1,i);
		 filt[0][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 25:
	  	_mm_storeu_si128( (__m128i *) &filt[1][col],_mm256_extractf128_si256(output_row1, 0)); //store low 128bit - 16pixels
	  	_mm_storeu_si128( (__m128i *) &filt[0][col],_mm256_extractf128_si256(output_row0, 0)); //store low 128bit - 16pixels

		for (i=16;i<=24;i++){
		 filt[1][col+i] = (unsigned char) _mm256_extract_epi8(output_row1,i);
		 filt[0][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 26:
	  	_mm_storeu_si128( (__m128i *) &filt[1][col],_mm256_extractf128_si256(output_row1, 0)); //store low 128bit - 16pixels
	  	_mm_storeu_si128( (__m128i *) &filt[0][col],_mm256_extractf128_si256(output_row0, 0)); //store low 128bit - 16pixels

		for (i=16;i<=25;i++){
		 filt[1][col+i] = (unsigned char) _mm256_extract_epi8(output_row1,i);
		 filt[0][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}

		  break;
	case 27:
	  	_mm_storeu_si128( (__m128i *) &filt[1][col],_mm256_extractf128_si256(output_row1, 0)); //store low 128bit - 16pixels
	  	_mm_storeu_si128( (__m128i *) &filt[0][col],_mm256_extractf128_si256(output_row0, 0)); //store low 128bit - 16pixels

		for (i=16;i<=26;i++){
		 filt[1][col+i] = (unsigned char) _mm256_extract_epi8(output_row1,i);
		 filt[0][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}

		  break;
	case 28:
	  	_mm_storeu_si128( (__m128i *) &filt[1][col],_mm256_extractf128_si256(output_row1, 0)); //store low 128bit - 16pixels
	  	_mm_storeu_si128( (__m128i *) &filt[0][col],_mm256_extractf128_si256(output_row0, 0)); //store low 128bit - 16pixels

		for (i=16;i<=27;i++){
		 filt[1][col+i] = (unsigned char) _mm256_extract_epi8(output_row1,i);
		 filt[0][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 29:
	  	_mm_storeu_si128( (__m128i *) &filt[1][col],_mm256_extractf128_si256(output_row1, 0)); //store low 128bit - 16pixels
	  	_mm_storeu_si128( (__m128i *) &filt[0][col],_mm256_extractf128_si256(output_row0, 0)); //store low 128bit - 16pixels

		filt[1][col+16] = (unsigned char) _mm256_extract_epi8(output_row1,16);
		filt[1][col+17] = (unsigned char) _mm256_extract_epi8(output_row1,17);
		filt[1][col+18] = (unsigned char) _mm256_extract_epi8(output_row1,18);
		filt[1][col+19] = (unsigned char) _mm256_extract_epi8(output_row1,19);
		filt[1][col+20] = (unsigned char) _mm256_extract_epi8(output_row1,20);
		filt[1][col+21] = (unsigned char) _mm256_extract_epi8(output_row1,21);
		filt[1][col+22] = (unsigned char) _mm256_extract_epi8(output_row1,22);
		filt[1][col+23] = (unsigned char) _mm256_extract_epi8(output_row1,23);
		filt[1][col+24] = (unsigned char) _mm256_extract_epi8(output_row1,24);
		filt[1][col+25] = (unsigned char) _mm256_extract_epi8(output_row1,25);
		filt[1][col+26] = (unsigned char) _mm256_extract_epi8(output_row1,26);
		filt[1][col+27] = (unsigned char) _mm256_extract_epi8(output_row1,27);
		filt[1][col+28] = (unsigned char) _mm256_extract_epi8(output_row1,28);

		filt[0][col+16] = (unsigned char) _mm256_extract_epi8(output_row0,16);
		filt[0][col+17] = (unsigned char) _mm256_extract_epi8(output_row0,17);
		filt[0][col+18] = (unsigned char) _mm256_extract_epi8(output_row0,18);
		filt[0][col+19] = (unsigned char) _mm256_extract_epi8(output_row0,19);
		filt[0][col+20] = (unsigned char) _mm256_extract_epi8(output_row0,20);
		filt[0][col+21] = (unsigned char) _mm256_extract_epi8(output_row0,21);
		filt[0][col+22] = (unsigned char) _mm256_extract_epi8(output_row0,22);
		filt[0][col+23] = (unsigned char) _mm256_extract_epi8(output_row0,23);
		filt[0][col+24] = (unsigned char) _mm256_extract_epi8(output_row0,24);
		filt[0][col+25] = (unsigned char) _mm256_extract_epi8(output_row0,25);
		filt[0][col+26] = (unsigned char) _mm256_extract_epi8(output_row0,26);
		filt[0][col+27] = (unsigned char) _mm256_extract_epi8(output_row0,27);
		filt[0][col+28] = (unsigned char) _mm256_extract_epi8(output_row0,28);

		  break;
	case 30:
	  	_mm_storeu_si128( (__m128i *) &filt[1][col],_mm256_extractf128_si256(output_row1, 0)); //store low 128bit - 16pixels
	  	_mm_storeu_si128( (__m128i *) &filt[0][col],_mm256_extractf128_si256(output_row0, 0)); //store low 128bit - 16pixels

		filt[1][col+16] = (unsigned char) _mm256_extract_epi8(output_row1,16);
		filt[1][col+17] = (unsigned char) _mm256_extract_epi8(output_row1,17);
		filt[1][col+18] = (unsigned char) _mm256_extract_epi8(output_row1,18);
		filt[1][col+19] = (unsigned char) _mm256_extract_epi8(output_row1,19);
		filt[1][col+20] = (unsigned char) _mm256_extract_epi8(output_row1,20);
		filt[1][col+21] = (unsigned char) _mm256_extract_epi8(output_row1,21);
		filt[1][col+22] = (unsigned char) _mm256_extract_epi8(output_row1,22);
		filt[1][col+23] = (unsigned char) _mm256_extract_epi8(output_row1,23);
		filt[1][col+24] = (unsigned char) _mm256_extract_epi8(output_row1,24);
		filt[1][col+25] = (unsigned char) _mm256_extract_epi8(output_row1,25);
		filt[1][col+26] = (unsigned char) _mm256_extract_epi8(output_row1,26);
		filt[1][col+27] = (unsigned char) _mm256_extract_epi8(output_row1,27);
		filt[1][col+28] = (unsigned char) _mm256_extract_epi8(output_row1,28);

		filt[0][col+16] = (unsigned char) _mm256_extract_epi8(output_row0,16);
		filt[0][col+17] = (unsigned char) _mm256_extract_epi8(output_row0,17);
		filt[0][col+18] = (unsigned char) _mm256_extract_epi8(output_row0,18);
		filt[0][col+19] = (unsigned char) _mm256_extract_epi8(output_row0,19);
		filt[0][col+20] = (unsigned char) _mm256_extract_epi8(output_row0,20);
		filt[0][col+21] = (unsigned char) _mm256_extract_epi8(output_row0,21);
		filt[0][col+22] = (unsigned char) _mm256_extract_epi8(output_row0,22);
		filt[0][col+23] = (unsigned char) _mm256_extract_epi8(output_row0,23);
		filt[0][col+24] = (unsigned char) _mm256_extract_epi8(output_row0,24);
		filt[0][col+25] = (unsigned char) _mm256_extract_epi8(output_row0,25);
		filt[0][col+26] = (unsigned char) _mm256_extract_epi8(output_row0,26);
		filt[0][col+27] = (unsigned char) _mm256_extract_epi8(output_row0,27);
		filt[0][col+28] = (unsigned char) _mm256_extract_epi8(output_row0,28);

	//the filt[row][col+29] is computed unvectorized
	newPixel = 0;
	newPixel += frame1[1-1][M-1-1] * filter[0][0];
	newPixel += frame1[1-1][M-1] * filter[0][1];

	newPixel += frame1[1][M-1-1] * filter[1][0];
	newPixel += frame1[1][M-1] * filter[1][1];

	newPixel += frame1[1+1][M-1-1] * filter[2][0];
	newPixel += frame1[1+1][M-1] * filter[2][1];

	filt[1][M-1] = (unsigned char) (newPixel / divisor);

	newPixel = 0;
	newPixel += frame1[1-1][M-1-1] * filter[1][0];
	newPixel += frame1[1-1][M-1] * filter[1][1];

	newPixel += frame1[1][M-1-1] * filter[2][0];
	newPixel += frame1[1][M-1] * filter[2][1];

	filt[0][M-1] = (unsigned char) (newPixel / divisor);

		  break;

	case 31:
	  	_mm_storeu_si128( (__m128i *) &filt[1][col],_mm256_extractf128_si256(output_row1, 0)); //store low 128bit - 16pixels
	  	_mm_storeu_si128( (__m128i *) &filt[0][col],_mm256_extractf128_si256(output_row0, 0)); //store low 128bit - 16pixels

		filt[1][col+16] = (unsigned char) _mm256_extract_epi8(output_row1,16);
		filt[1][col+17] = (unsigned char) _mm256_extract_epi8(output_row1,17);
		filt[1][col+18] = (unsigned char) _mm256_extract_epi8(output_row1,18);
		filt[1][col+19] = (unsigned char) _mm256_extract_epi8(output_row1,19);
		filt[1][col+20] = (unsigned char) _mm256_extract_epi8(output_row1,20);
		filt[1][col+21] = (unsigned char) _mm256_extract_epi8(output_row1,21);
		filt[1][col+22] = (unsigned char) _mm256_extract_epi8(output_row1,22);
		filt[1][col+23] = (unsigned char) _mm256_extract_epi8(output_row1,23);
		filt[1][col+24] = (unsigned char) _mm256_extract_epi8(output_row1,24);
		filt[1][col+25] = (unsigned char) _mm256_extract_epi8(output_row1,25);
		filt[1][col+26] = (unsigned char) _mm256_extract_epi8(output_row1,26);
		filt[1][col+27] = (unsigned char) _mm256_extract_epi8(output_row1,27);
		filt[1][col+28] = (unsigned char) _mm256_extract_epi8(output_row1,28);

		filt[0][col+16] = (unsigned char) _mm256_extract_epi8(output_row0,16);
		filt[0][col+17] = (unsigned char) _mm256_extract_epi8(output_row0,17);
		filt[0][col+18] = (unsigned char) _mm256_extract_epi8(output_row0,18);
		filt[0][col+19] = (unsigned char) _mm256_extract_epi8(output_row0,19);
		filt[0][col+20] = (unsigned char) _mm256_extract_epi8(output_row0,20);
		filt[0][col+21] = (unsigned char) _mm256_extract_epi8(output_row0,21);
		filt[0][col+22] = (unsigned char) _mm256_extract_epi8(output_row0,22);
		filt[0][col+23] = (unsigned char) _mm256_extract_epi8(output_row0,23);
		filt[0][col+24] = (unsigned char) _mm256_extract_epi8(output_row0,24);
		filt[0][col+25] = (unsigned char) _mm256_extract_epi8(output_row0,25);
		filt[0][col+26] = (unsigned char) _mm256_extract_epi8(output_row0,26);
		filt[0][col+27] = (unsigned char) _mm256_extract_epi8(output_row0,27);
		filt[0][col+28] = (unsigned char) _mm256_extract_epi8(output_row0,28);

	//the filt[1][col+29/30] are computed unvectorized
		newPixel = 0;
		newPixel += frame1[1-1][M-2-1] * filter[0][0];
		newPixel += frame1[1-1][M-2] * filter[0][1];
		newPixel += frame1[1-1][M-2+1] * filter[0][2];

		newPixel += frame1[1][M-2-1] * filter[1][0];
		newPixel += frame1[1][M-2] * filter[1][1];
		newPixel += frame1[1][M-2+1] * filter[1][2];

		newPixel += frame1[1+1][M-2-1] * filter[2][0];
		newPixel += frame1[1+1][M-2] * filter[2][1];
		newPixel += frame1[1+1][M-2+1] * filter[2][2];

		filt[1][M-2] = (unsigned char) (newPixel / divisor);

		newPixel = 0;
		newPixel += frame1[1-1][M-1-1] * filter[0][0];
		newPixel += frame1[1-1][M-1] * filter[0][1];

		newPixel += frame1[1][M-1-1] * filter[1][0];
		newPixel += frame1[1][M-1] * filter[1][1];

		newPixel += frame1[1+1][M-1-1] * filter[2][0];
		newPixel += frame1[1+1][M-1] * filter[2][1];

		filt[1][M-1] = (unsigned char) (newPixel / divisor);

		newPixel = 0;

		newPixel += frame1[0][M-2-1] * filter[1][0];
		newPixel += frame1[0][M-2] * filter[1][1];
		newPixel += frame1[0][M-2+1] * filter[1][2];

		newPixel += frame1[1][M-2-1] * filter[2][0];
		newPixel += frame1[1][M-2] * filter[2][1];
		newPixel += frame1[1][M-2+1] * filter[2][2];

		filt[0][M-2] = (unsigned char) (newPixel / divisor);

		newPixel = 0;

		newPixel += frame1[0][M-1-1] * filter[1][0];
		newPixel += frame1[0][M-1] * filter[1][1];

		newPixel += frame1[1][M-1-1] * filter[2][0];
		newPixel += frame1[1][M-1] * filter[2][1];

		filt[0][M-1] = (unsigned char) (newPixel / divisor);


		  break;

	default:
		printf("\nsomething went wrong");
		return -1;
 	}

return 0;

}



int conv_loop_reminder_3x3_last_values(unsigned char **frame1,unsigned char **filt,const unsigned int M, const unsigned int N, const unsigned int col,const unsigned int REMINDER_ITERATIONS,const unsigned int division_case,const unsigned short int divisor,signed char **filter,const __m256i c0,const __m256i c1,const __m256i c2,const __m256i c0_sh1,const __m256i c1_sh1,const __m256i c2_sh1,const __m256i c0_sh2,const __m256i c1_sh2,const __m256i c2_sh2, const __m256i c0_sh3,const __m256i c1_sh3,const __m256i c2_sh3,const __m256i f){

register	__m256i r0,r1,r2,m0,m1,m2,m4,output_even,output_odd,output_row0,output_row1;
int newPixel;
unsigned int i;
const __m256i mask3  = _mm256_set_epi16(0,0,0,0,0,0,0,65535,0,0,0,0,0,0,0,0);

const __m256i output_mask      = _mm256_set_epi16(0,65535,0,65535,0,65535,0,65535,0,65535,0,65535,0,65535,0,65535);
const __m256i output_mask_sh1  = _mm256_set_epi16(0,0,65535,0,65535,0,65535,0,65535,0,65535,0,65535,0,65535,0);

 __m256i reminder_mask1;

if (REMINDER_ITERATIONS == 0){
	return 0; //no loop reminder is needed
}

	reminder_mask1=_mm256_load_si256( (__m256i *) &reminder_msk1_3x3[REMINDER_ITERATIONS-1][0]);
	//reminder_mask2=_mm256_load_si256( (__m256i *) &reminder_msk2[REMINDER_ITERATIONS-1][0]);


	 //1st col iteration
	//load the 5 rows
	r0=_mm256_loadu_si256( (__m256i *) &frame1[N-3][col-1]);
	r1=_mm256_loadu_si256( (__m256i *) &frame1[N-2][col-1]);
	r2=_mm256_loadu_si256( (__m256i *) &frame1[N-1][col-1]);


	/*
	 * The above load operations load outside of the array bounds; these elements are filled with zeros just after
	 * Furthermore, I add one extra zero in the end to compute col=N-1. The last value of col is N-1, not N
	 */

	//AND r0-r4 with reminder_mask
	r0=_mm256_and_si256(r0,reminder_mask1);
	r1=_mm256_and_si256(r1,reminder_mask1);
	r2=_mm256_and_si256(r2,reminder_mask1);


	//col iteration computes output pixels of    1,5,9,13,17,21,25,29
	// col+1 iteration computes output pixels of 2,6,10,14,18,22,26,30
	// col+2 iteration computes output pixels of 3,7,11,15,19,23,27
	// col+3 iteration computes output pixels of 4,8,12,16,20,24,28
	//afterwards, col becomes 30 and repeat the above process

	//1st col iteration

	//multiply with the mask
	m0=_mm256_maddubs_epi16(r0,c0);
	m1=_mm256_maddubs_epi16(r1,c1);
	m2=_mm256_maddubs_epi16(r2,c2);

	//vertical add
	m0=_mm256_add_epi16(m0,m1);
	m0=_mm256_add_epi16(m0,m2);

	//hozizontal additions
	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);

	//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others. later on they will be stored into 0,4,8,12,16,20,24,28 positions (8bit register).
	output_even=_mm256_and_si256(m2,output_mask);


	//2nd col iteration
	//multiply with the mask
	m0=_mm256_maddubs_epi16(r0,c0_sh1);
	m1=_mm256_maddubs_epi16(r1,c1_sh1);
	m2=_mm256_maddubs_epi16(r2,c2_sh1);

	//vertical add
	m0=_mm256_add_epi16(m0,m1);
	m0=_mm256_add_epi16(m0,m2);

	//hozizontal additions
	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);


	//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others.
	output_odd = _mm256_and_si256(m2,output_mask);


     //3rd col iteration
	//multiply with the mask
	m0=_mm256_maddubs_epi16(r0,c0_sh2);
	m1=_mm256_maddubs_epi16(r1,c1_sh2);
	m2=_mm256_maddubs_epi16(r2,c2_sh2);


	//vertical add
	m0=_mm256_add_epi16(m0,m1);
	m0=_mm256_add_epi16(m0,m2);


	//hozizontal additions
	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);

	m4=_mm256_and_si256(m0,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
	m2=_mm256_add_epi16(m2,m4);

	//m2 has 16 16bit values now. the results I need are in positions 1,3,5,7,9,11,13. keep only those, discard others
	m2 = _mm256_and_si256(m2,output_mask_sh1);
	output_even = _mm256_add_epi16(output_even,m2);


	//4th col iteration

	//multiply with the mask
	m0=_mm256_maddubs_epi16(r0,c0_sh3);
	m1=_mm256_maddubs_epi16(r1,c1_sh3);
	m2=_mm256_maddubs_epi16(r2,c2_sh3);

	//vertical add
	m0=_mm256_add_epi16(m0,m1);
	m0=_mm256_add_epi16(m0,m2);


	//hozizontal additions
	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);

	m4=_mm256_and_si256(m0,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
	m2=_mm256_add_epi16(m2,m4);

	//m2 has 16 16bit values now. the results I need are in positions 1,3,5,7,9,11,13. keep only those, discard others
	m2 = _mm256_and_si256(m2,output_mask_sh1);
	output_odd = _mm256_add_epi16(output_odd,m2);


	//now division follows
	output_even=division(division_case,output_even,f);
	output_odd=division(division_case,output_odd,f);

	//shift odd 1 position and add to even
	output_odd = _mm256_slli_si256(output_odd,1);
	output_even = _mm256_add_epi8(output_even,output_odd);

	output_row1=output_even;

	 //row==N-1
	 //row==N-1
	//1st col iteration

	//multiply with the mask
	m1=_mm256_maddubs_epi16(r1,c0);
	m2=_mm256_maddubs_epi16(r2,c1);

	//vertical add
	m0=_mm256_add_epi16(m1,m2);

	//hozizontal additions
	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);

	//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others. later on they will be stored into 0,4,8,12,16,20,24,28 positions (8bit register).
	output_even=_mm256_and_si256(m2,output_mask);


	//2nd col iteration
	//multiply with the mask
	m1=_mm256_maddubs_epi16(r1,c0_sh1);
	m2=_mm256_maddubs_epi16(r2,c1_sh1);

	//vertical add
	m0=_mm256_add_epi16(m1,m2);

	//hozizontal additions
	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);


	//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others.
	output_odd = _mm256_and_si256(m2,output_mask);


  //3rd col iteration
	//multiply with the mask
	m1=_mm256_maddubs_epi16(r1,c0_sh2);
	m2=_mm256_maddubs_epi16(r2,c1_sh2);


	//vertical add
	m0=_mm256_add_epi16(m1,m2);


	//hozizontal additions
	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);

	m4=_mm256_and_si256(m0,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
	m2=_mm256_add_epi16(m2,m4);

	//m2 has 16 16bit values now. the results I need are in positions 1,3,5,7,9,11,13. keep only those, discard others
	m2 = _mm256_and_si256(m2,output_mask_sh1);
	output_even = _mm256_add_epi16(output_even,m2);


	//4th col iteration

	//multiply with the mask
	m1=_mm256_maddubs_epi16(r1,c0_sh3);
	m2=_mm256_maddubs_epi16(r2,c1_sh3);

	//vertical add
	m0=_mm256_add_epi16(m1,m2);

	//hozizontal additions
	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);

	m4=_mm256_and_si256(m0,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
	m2=_mm256_add_epi16(m2,m4);

	//m2 has 16 16bit values now. the results I need are in positions 1,3,5,7,9,11,13. keep only those, discard others
	m2 = _mm256_and_si256(m2,output_mask_sh1);
	output_odd = _mm256_add_epi16(output_odd,m2);


	//now division follows
	output_even=division(division_case,output_even,f);
	output_odd=division(division_case,output_odd,f);

	//shift odd 1 position and add to even
	output_odd = _mm256_slli_si256(output_odd,1);
	output_even = _mm256_add_epi8(output_even,output_odd);


	output_row0=output_even;

 	switch (REMINDER_ITERATIONS){
	case 1:
		filt[N-2][col] = (unsigned char) _mm256_extract_epi8(output_row1,0);
		filt[N-1][col] = (unsigned char) _mm256_extract_epi8(output_row0,0);

	  break;
	case 2:
		for (i=0;i<=1;i++){
			filt[N-2][col+i] = (unsigned char) _mm256_extract_epi8(output_row1,i);
			filt[N-1][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 3:
		for (i=0;i<=2;i++){
			filt[N-2][col+i] = (unsigned char) _mm256_extract_epi8(output_row1,i);
			filt[N-1][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 4:
		for (i=0;i<=3;i++){
			filt[N-2][col+i] = (unsigned char) _mm256_extract_epi8(output_row1,i);
			filt[N-1][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 5:
		for (i=0;i<=4;i++){
			filt[N-2][col+i] = (unsigned char) _mm256_extract_epi8(output_row1,i);
			filt[N-1][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 6:
		for (i=0;i<=5;i++){
			filt[N-2][col+i] = (unsigned char) _mm256_extract_epi8(output_row1,i);
			filt[N-1][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 7:
		for (i=0;i<=6;i++){
			filt[N-2][col+i] = (unsigned char) _mm256_extract_epi8(output_row1,i);
			filt[N-1][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 8:
		for (i=0;i<=7;i++){
			filt[N-2][col+i] = (unsigned char) _mm256_extract_epi8(output_row1,i);
			filt[N-1][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 9:
		for (i=0;i<=8;i++){
			filt[N-2][col+i] = (unsigned char) _mm256_extract_epi8(output_row1,i);
			filt[N-1][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 10:
		for (i=0;i<=9;i++){
			filt[N-2][col+i] = (unsigned char) _mm256_extract_epi8(output_row1,i);
			filt[N-1][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 11:
		for (i=0;i<=10;i++){
			filt[N-2][col+i] = (unsigned char) _mm256_extract_epi8(output_row1,i);
			filt[N-1][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 12:
		for (i=0;i<=11;i++){
			filt[N-2][col+i] = (unsigned char) _mm256_extract_epi8(output_row1,i);
			filt[N-1][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 13:
		for (i=0;i<=12;i++){
			filt[N-2][col+i] = (unsigned char) _mm256_extract_epi8(output_row1,i);
			filt[N-1][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 14:
		for (i=0;i<=13;i++){
			filt[N-2][col+i] = (unsigned char) _mm256_extract_epi8(output_row1,i);
			filt[N-1][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 15:
		for (i=0;i<=14;i++){
			filt[N-2][col+i] = (unsigned char) _mm256_extract_epi8(output_row1,i);
			filt[N-1][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 16:
  	_mm_storeu_si128( (__m128i *) &filt[N-2][col],_mm256_extractf128_si256(output_row1, 0)); //store low 128bit - 16pixels
  	_mm_storeu_si128( (__m128i *) &filt[N-1][col],_mm256_extractf128_si256(output_row0, 0)); //store low 128bit - 16pixels

		  break;
	case 17:
  	_mm_storeu_si128( (__m128i *) &filt[N-2][col],_mm256_extractf128_si256(output_row1, 0)); //store low 128bit - 16pixels
  	_mm_storeu_si128( (__m128i *) &filt[N-1][col],_mm256_extractf128_si256(output_row0, 0)); //store low 128bit - 16pixels
	filt[N-2][col+16] = (unsigned char) _mm256_extract_epi8(output_row1,16);
	filt[N-1][col+16] = (unsigned char) _mm256_extract_epi8(output_row0,16);
		  break;
	case 18:
  	_mm_storeu_si128( (__m128i *) &filt[N-2][col],_mm256_extractf128_si256(output_row1, 0)); //store low 128bit - 16pixels
  	_mm_storeu_si128( (__m128i *) &filt[N-1][col],_mm256_extractf128_si256(output_row0, 0)); //store low 128bit - 16pixels

	for (i=16;i<=17;i++){
	 filt[N-2][col+i] = (unsigned char) _mm256_extract_epi8(output_row1,i);
	 filt[N-1][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
	}
		  break;
	case 19:
	  	_mm_storeu_si128( (__m128i *) &filt[N-2][col],_mm256_extractf128_si256(output_row1, 0)); //store low 128bit - 16pixels
	  	_mm_storeu_si128( (__m128i *) &filt[N-1][col],_mm256_extractf128_si256(output_row0, 0)); //store low 128bit - 16pixels

		for (i=16;i<=18;i++){
		 filt[N-2][col+i] = (unsigned char) _mm256_extract_epi8(output_row1,i);
		 filt[N-1][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 20:
	  	_mm_storeu_si128( (__m128i *) &filt[N-2][col],_mm256_extractf128_si256(output_row1, 0)); //store low 128bit - 16pixels
	  	_mm_storeu_si128( (__m128i *) &filt[N-1][col],_mm256_extractf128_si256(output_row0, 0)); //store low 128bit - 16pixels

		for (i=16;i<=19;i++){
		 filt[N-2][col+i] = (unsigned char) _mm256_extract_epi8(output_row1,i);
		 filt[N-1][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 21:
	  	_mm_storeu_si128( (__m128i *) &filt[N-2][col],_mm256_extractf128_si256(output_row1, 0)); //store low 128bit - 16pixels
	  	_mm_storeu_si128( (__m128i *) &filt[N-1][col],_mm256_extractf128_si256(output_row0, 0)); //store low 128bit - 16pixels

		for (i=16;i<=20;i++){
		 filt[N-2][col+i] = (unsigned char) _mm256_extract_epi8(output_row1,i);
		 filt[N-1][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 22:
	  	_mm_storeu_si128( (__m128i *) &filt[N-2][col],_mm256_extractf128_si256(output_row1, 0)); //store low 128bit - 16pixels
	  	_mm_storeu_si128( (__m128i *) &filt[N-1][col],_mm256_extractf128_si256(output_row0, 0)); //store low 128bit - 16pixels

		for (i=16;i<=21;i++){
		 filt[N-2][col+i] = (unsigned char) _mm256_extract_epi8(output_row1,i);
		 filt[N-1][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		break;
	case 23:
	  	_mm_storeu_si128( (__m128i *) &filt[N-2][col],_mm256_extractf128_si256(output_row1, 0)); //store low 128bit - 16pixels
	  	_mm_storeu_si128( (__m128i *) &filt[N-1][col],_mm256_extractf128_si256(output_row0, 0)); //store low 128bit - 16pixels

		for (i=16;i<=22;i++){
		 filt[N-2][col+i] = (unsigned char) _mm256_extract_epi8(output_row1,i);
		 filt[N-1][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 24:
	  	_mm_storeu_si128( (__m128i *) &filt[N-2][col],_mm256_extractf128_si256(output_row1, 0)); //store low 128bit - 16pixels
	  	_mm_storeu_si128( (__m128i *) &filt[N-1][col],_mm256_extractf128_si256(output_row0, 0)); //store low 128bit - 16pixels

		for (i=16;i<=23;i++){
		 filt[N-2][col+i] = (unsigned char) _mm256_extract_epi8(output_row1,i);
		 filt[N-1][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 25:
	  	_mm_storeu_si128( (__m128i *) &filt[N-2][col],_mm256_extractf128_si256(output_row1, 0)); //store low 128bit - 16pixels
	  	_mm_storeu_si128( (__m128i *) &filt[N-1][col],_mm256_extractf128_si256(output_row0, 0)); //store low 128bit - 16pixels

		for (i=16;i<=24;i++){
		 filt[N-2][col+i] = (unsigned char) _mm256_extract_epi8(output_row1,i);
		 filt[N-1][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 26:
	  	_mm_storeu_si128( (__m128i *) &filt[N-2][col],_mm256_extractf128_si256(output_row1, 0)); //store low 128bit - 16pixels
	  	_mm_storeu_si128( (__m128i *) &filt[N-1][col],_mm256_extractf128_si256(output_row0, 0)); //store low 128bit - 16pixels

		for (i=16;i<=25;i++){
		 filt[N-2][col+i] = (unsigned char) _mm256_extract_epi8(output_row1,i);
		 filt[N-1][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}

		  break;
	case 27:
	  	_mm_storeu_si128( (__m128i *) &filt[N-2][col],_mm256_extractf128_si256(output_row1, 0)); //store low 128bit - 16pixels
	  	_mm_storeu_si128( (__m128i *) &filt[N-1][col],_mm256_extractf128_si256(output_row0, 0)); //store low 128bit - 16pixels

		for (i=16;i<=26;i++){
		 filt[N-2][col+i] = (unsigned char) _mm256_extract_epi8(output_row1,i);
		 filt[N-1][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}

		  break;
	case 28:
	  	_mm_storeu_si128( (__m128i *) &filt[N-2][col],_mm256_extractf128_si256(output_row1, 0)); //store low 128bit - 16pixels
	  	_mm_storeu_si128( (__m128i *) &filt[N-1][col],_mm256_extractf128_si256(output_row0, 0)); //store low 128bit - 16pixels

		for (i=16;i<=27;i++){
		 filt[N-2][col+i] = (unsigned char) _mm256_extract_epi8(output_row1,i);
		 filt[N-1][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 29:
	  	_mm_storeu_si128( (__m128i *) &filt[N-2][col],_mm256_extractf128_si256(output_row1, 0)); //store low 128bit - 16pixels
	  	_mm_storeu_si128( (__m128i *) &filt[N-1][col],_mm256_extractf128_si256(output_row0, 0)); //store low 128bit - 16pixels

		filt[N-2][col+16] = (unsigned char) _mm256_extract_epi8(output_row1,16);
		filt[N-2][col+17] = (unsigned char) _mm256_extract_epi8(output_row1,17);
		filt[N-2][col+18] = (unsigned char) _mm256_extract_epi8(output_row1,18);
		filt[N-2][col+19] = (unsigned char) _mm256_extract_epi8(output_row1,19);
		filt[N-2][col+20] = (unsigned char) _mm256_extract_epi8(output_row1,20);
		filt[N-2][col+21] = (unsigned char) _mm256_extract_epi8(output_row1,21);
		filt[N-2][col+22] = (unsigned char) _mm256_extract_epi8(output_row1,22);
		filt[N-2][col+23] = (unsigned char) _mm256_extract_epi8(output_row1,23);
		filt[N-2][col+24] = (unsigned char) _mm256_extract_epi8(output_row1,24);
		filt[N-2][col+25] = (unsigned char) _mm256_extract_epi8(output_row1,25);
		filt[N-2][col+26] = (unsigned char) _mm256_extract_epi8(output_row1,26);
		filt[N-2][col+27] = (unsigned char) _mm256_extract_epi8(output_row1,27);
		filt[N-2][col+28] = (unsigned char) _mm256_extract_epi8(output_row1,28);

		filt[N-1][col+16] = (unsigned char) _mm256_extract_epi8(output_row0,16);
		filt[N-1][col+17] = (unsigned char) _mm256_extract_epi8(output_row0,17);
		filt[N-1][col+18] = (unsigned char) _mm256_extract_epi8(output_row0,18);
		filt[N-1][col+19] = (unsigned char) _mm256_extract_epi8(output_row0,19);
		filt[N-1][col+20] = (unsigned char) _mm256_extract_epi8(output_row0,20);
		filt[N-1][col+21] = (unsigned char) _mm256_extract_epi8(output_row0,21);
		filt[N-1][col+22] = (unsigned char) _mm256_extract_epi8(output_row0,22);
		filt[N-1][col+23] = (unsigned char) _mm256_extract_epi8(output_row0,23);
		filt[N-1][col+24] = (unsigned char) _mm256_extract_epi8(output_row0,24);
		filt[N-1][col+25] = (unsigned char) _mm256_extract_epi8(output_row0,25);
		filt[N-1][col+26] = (unsigned char) _mm256_extract_epi8(output_row0,26);
		filt[N-1][col+27] = (unsigned char) _mm256_extract_epi8(output_row0,27);
		filt[N-1][col+28] = (unsigned char) _mm256_extract_epi8(output_row0,28);

		  break;
	case 30:
	  	_mm_storeu_si128( (__m128i *) &filt[N-2][col],_mm256_extractf128_si256(output_row1, 0)); //store low 128bit - 16pixels
	  	_mm_storeu_si128( (__m128i *) &filt[N-1][col],_mm256_extractf128_si256(output_row0, 0)); //store low 128bit - 16pixels

		filt[N-2][col+16] = (unsigned char) _mm256_extract_epi8(output_row1,16);
		filt[N-2][col+17] = (unsigned char) _mm256_extract_epi8(output_row1,17);
		filt[N-2][col+18] = (unsigned char) _mm256_extract_epi8(output_row1,18);
		filt[N-2][col+19] = (unsigned char) _mm256_extract_epi8(output_row1,19);
		filt[N-2][col+20] = (unsigned char) _mm256_extract_epi8(output_row1,20);
		filt[N-2][col+21] = (unsigned char) _mm256_extract_epi8(output_row1,21);
		filt[N-2][col+22] = (unsigned char) _mm256_extract_epi8(output_row1,22);
		filt[N-2][col+23] = (unsigned char) _mm256_extract_epi8(output_row1,23);
		filt[N-2][col+24] = (unsigned char) _mm256_extract_epi8(output_row1,24);
		filt[N-2][col+25] = (unsigned char) _mm256_extract_epi8(output_row1,25);
		filt[N-2][col+26] = (unsigned char) _mm256_extract_epi8(output_row1,26);
		filt[N-2][col+27] = (unsigned char) _mm256_extract_epi8(output_row1,27);
		filt[N-2][col+28] = (unsigned char) _mm256_extract_epi8(output_row1,28);

		filt[N-1][col+16] = (unsigned char) _mm256_extract_epi8(output_row0,16);
		filt[N-1][col+17] = (unsigned char) _mm256_extract_epi8(output_row0,17);
		filt[N-1][col+18] = (unsigned char) _mm256_extract_epi8(output_row0,18);
		filt[N-1][col+19] = (unsigned char) _mm256_extract_epi8(output_row0,19);
		filt[N-1][col+20] = (unsigned char) _mm256_extract_epi8(output_row0,20);
		filt[N-1][col+21] = (unsigned char) _mm256_extract_epi8(output_row0,21);
		filt[N-1][col+22] = (unsigned char) _mm256_extract_epi8(output_row0,22);
		filt[N-1][col+23] = (unsigned char) _mm256_extract_epi8(output_row0,23);
		filt[N-1][col+24] = (unsigned char) _mm256_extract_epi8(output_row0,24);
		filt[N-1][col+25] = (unsigned char) _mm256_extract_epi8(output_row0,25);
		filt[N-1][col+26] = (unsigned char) _mm256_extract_epi8(output_row0,26);
		filt[N-1][col+27] = (unsigned char) _mm256_extract_epi8(output_row0,27);
		filt[N-1][col+28] = (unsigned char) _mm256_extract_epi8(output_row0,28);

	//the filt[row][col+29] is computed unvectorized
	newPixel = 0;
	newPixel += frame1[N-2-1][M-1-1] * filter[0][0];
	newPixel += frame1[N-2-1][M-1] * filter[0][1];

	newPixel += frame1[N-2][M-1-1] * filter[1][0];
	newPixel += frame1[N-2][M-1] * filter[1][1];

	newPixel += frame1[N-2+1][M-1-1] * filter[2][0];
	newPixel += frame1[N-2+1][M-1] * filter[2][1];

	filt[N-2][M-1] = (unsigned char) (newPixel / divisor);

	newPixel = 0;
	newPixel += frame1[N-1-1][M-1-1] * filter[0][0];
	newPixel += frame1[N-1-1][M-1] * filter[0][1];

	newPixel += frame1[N-1][M-1-1] * filter[1][0];
	newPixel += frame1[N-1][M-1] * filter[1][1];

	filt[N-1][M-1] = (unsigned char) (newPixel / divisor);

		  break;

	case 31:
	  	_mm_storeu_si128( (__m128i *) &filt[N-2][col],_mm256_extractf128_si256(output_row1, 0)); //store low 128bit - 16pixels
	  	_mm_storeu_si128( (__m128i *) &filt[N-1][col],_mm256_extractf128_si256(output_row0, 0)); //store low 128bit - 16pixels

		filt[N-2][col+16] = (unsigned char) _mm256_extract_epi8(output_row1,16);
		filt[N-2][col+17] = (unsigned char) _mm256_extract_epi8(output_row1,17);
		filt[N-2][col+18] = (unsigned char) _mm256_extract_epi8(output_row1,18);
		filt[N-2][col+19] = (unsigned char) _mm256_extract_epi8(output_row1,19);
		filt[N-2][col+20] = (unsigned char) _mm256_extract_epi8(output_row1,20);
		filt[N-2][col+21] = (unsigned char) _mm256_extract_epi8(output_row1,21);
		filt[N-2][col+22] = (unsigned char) _mm256_extract_epi8(output_row1,22);
		filt[N-2][col+23] = (unsigned char) _mm256_extract_epi8(output_row1,23);
		filt[N-2][col+24] = (unsigned char) _mm256_extract_epi8(output_row1,24);
		filt[N-2][col+25] = (unsigned char) _mm256_extract_epi8(output_row1,25);
		filt[N-2][col+26] = (unsigned char) _mm256_extract_epi8(output_row1,26);
		filt[N-2][col+27] = (unsigned char) _mm256_extract_epi8(output_row1,27);
		filt[N-2][col+28] = (unsigned char) _mm256_extract_epi8(output_row1,28);

		filt[N-1][col+16] = (unsigned char) _mm256_extract_epi8(output_row0,16);
		filt[N-1][col+17] = (unsigned char) _mm256_extract_epi8(output_row0,17);
		filt[N-1][col+18] = (unsigned char) _mm256_extract_epi8(output_row0,18);
		filt[N-1][col+19] = (unsigned char) _mm256_extract_epi8(output_row0,19);
		filt[N-1][col+20] = (unsigned char) _mm256_extract_epi8(output_row0,20);
		filt[N-1][col+21] = (unsigned char) _mm256_extract_epi8(output_row0,21);
		filt[N-1][col+22] = (unsigned char) _mm256_extract_epi8(output_row0,22);
		filt[N-1][col+23] = (unsigned char) _mm256_extract_epi8(output_row0,23);
		filt[N-1][col+24] = (unsigned char) _mm256_extract_epi8(output_row0,24);
		filt[N-1][col+25] = (unsigned char) _mm256_extract_epi8(output_row0,25);
		filt[N-1][col+26] = (unsigned char) _mm256_extract_epi8(output_row0,26);
		filt[N-1][col+27] = (unsigned char) _mm256_extract_epi8(output_row0,27);
		filt[N-1][col+28] = (unsigned char) _mm256_extract_epi8(output_row0,28);

	//the filt[N-2][col+29/30] are computed unvectorized
		newPixel = 0;
		newPixel += frame1[N-2-1][M-2-1] * filter[0][0];
		newPixel += frame1[N-2-1][M-2] * filter[0][1];
		newPixel += frame1[N-2-1][M-2+1] * filter[0][2];

		newPixel += frame1[N-2][M-2-1] * filter[1][0];
		newPixel += frame1[N-2][M-2] * filter[1][1];
		newPixel += frame1[N-2][M-2+1] * filter[1][2];

		newPixel += frame1[N-2+1][M-2-1] * filter[2][0];
		newPixel += frame1[N-2+1][M-2] * filter[2][1];
		newPixel += frame1[N-2+1][M-2+1] * filter[2][2];

		filt[N-2][M-2] = (unsigned char) (newPixel / divisor);

		newPixel = 0;
		newPixel += frame1[N-2-1][M-1-1] * filter[0][0];
		newPixel += frame1[N-2-1][M-1] * filter[0][1];

		newPixel += frame1[N-2][M-1-1] * filter[1][0];
		newPixel += frame1[N-2][M-1] * filter[1][1];

		newPixel += frame1[N-2+1][M-1-1] * filter[2][0];
		newPixel += frame1[N-2+1][M-1] * filter[2][1];

		filt[N-2][M-1] = (unsigned char) (newPixel / divisor);

		newPixel = 0;

		newPixel += frame1[N-1-1][M-2-1] * filter[0][0];
		newPixel += frame1[N-1-1][M-2] * filter[0][1];
		newPixel += frame1[N-1-1][M-2+1] * filter[0][2];

		newPixel += frame1[N-1][M-2-1] * filter[1][0];
		newPixel += frame1[N-1][M-2] * filter[1][1];
		newPixel += frame1[N-1][M-2+1] * filter[1][2];

		filt[N-1][M-2] = (unsigned char) (newPixel / divisor);

		newPixel = 0;

		newPixel += frame1[N-1-1][M-1-1] * filter[0][0];
		newPixel += frame1[N-1-1][M-1] * filter[0][1];

		newPixel += frame1[N-1][M-1-1] * filter[1][0];
		newPixel += frame1[N-1][M-1] * filter[1][1];

		filt[N-1][M-1] = (unsigned char) (newPixel / divisor);


		  break;

	default:
		printf("\nsomething went wrong");
		return -1;
 	}

return 0;

}






void convolution_optimized_3x3_reg_blocking_16(unsigned char **frame1,unsigned char **filt,const unsigned int M, const unsigned int N, const unsigned short int divisor, signed char **filter){

const signed char f00=filter[0][0];
const signed char f01=filter[0][1];
const signed char f02=filter[0][2];

const signed char f10=filter[1][0];
const signed char f11=filter[1][1];
const signed char f12=filter[1][2];

const signed char f20=filter[2][0];
const signed char f21=filter[2][1];
const signed char f22=filter[2][2];

const __m256i c0=_mm256_set_epi8(0,f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00);
const __m256i c1=_mm256_set_epi8(0,f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10);
const __m256i c2=_mm256_set_epi8(0,f22,f21,f20,0,f22,f21,f20,0,f22,f21,f20,0,f22,f21,f20,0,f22,f21,f20,0,f22,f21,f20,0,f22,f21,f20,0,f22,f21,f20);


const __m256i c0_sh1=_mm256_set_epi8(f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00,0);
const __m256i c1_sh1=_mm256_set_epi8(f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10,0);
const __m256i c2_sh1=_mm256_set_epi8(f22,f21,f20,0,f22,f21,f20,0,f22,f21,f20,0,f22,f21,f20,0,f22,f21,f20,0,f22,f21,f20,0,f22,f21,f20,0,f22,f21,f20,0);


const __m256i c0_sh2=_mm256_set_epi8(0,0,0,f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00,0,0);
const __m256i c1_sh2=_mm256_set_epi8(0,0,0,f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10,0,0);
const __m256i c2_sh2=_mm256_set_epi8(0,0,0,f22,f21,f20,0,f22,f21,f20,0,f22,f21,f20,0,f22,f21,f20,0,f22,f21,f20,0,f22,f21,f20,0,f22,f21,f20,0,0);


const __m256i c0_sh3=_mm256_set_epi8(0,0,f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00,0,0,0);
const __m256i c1_sh3=_mm256_set_epi8(0,0,f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10,0,0,0);
const __m256i c2_sh3=_mm256_set_epi8(0,0,f22,f21,f20,0,f22,f21,f20,0,f22,f21,f20,0,f22,f21,f20,0,f22,f21,f20,0,f22,f21,f20,0,f22,f21,f20,0,0,0);


	const __m256i mask3  = _mm256_set_epi16(0,0,0,0,0,0,0,65535,0,0,0,0,0,0,0,0);
	const __m256i mask_prelude  = _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0);

	const __m256i output_mask      = _mm256_set_epi16(0,65535,0,65535,0,65535,0,65535,0,65535,0,65535,0,65535,0,65535);
	const __m256i output_mask_sh1  = _mm256_set_epi16(0,0,65535,0,65535,0,65535,0,65535,0,65535,0,65535,0,65535,0);




	const unsigned int REMINDER_ITERATIONS =  (M-((((M-32)/30)*30)+30)); //M-(last_col_value+30)
	//printf("\n%d",REMINDER_ITERATIONS);


	const unsigned int division_case=prepare_for_division(divisor); //determine which is the division case (A, B or C)
	const __m256i f = _mm256_load_si256( (__m256i *) &f_vector[0]);// initialize the division vector
print_vector_16(f);
//printf("\n%d %d",division_case,b);


#pragma omp parallel
{

int row,col;
register __m256i r0,r1,r2,r3,r4,m0,m1,m2,m3,m4;
//__m256i output_row0_even,output_row1_even,output_row0_odd,output_row1_odd; //these are for processing row #0 and #1 only.
__m256i output_even,output_odd;
//__m256i m0_prelude_row0,m0_prelude_row1;


/*---------------------- Gaussian Blur ---------------------------------*/

    #pragma omp for schedule(static)
	for (row = -1; row < N; row+=3) {

	if (row==-1){//special case compute filt[0:1][:]

		for (col = 0; col <= M-32; col+=30){

		  if (col==0){

				//load the 5 rows
				r0=_mm256_load_si256( (__m256i *) &frame1[0][0]);
				r1=_mm256_load_si256( (__m256i *) &frame1[1][0]);
				r2=_mm256_load_si256( (__m256i *) &frame1[2][0]);


		  			//START - extra code needed for prelude
		  					m0=_mm256_slli_si256(r0,1);//shift 1 elements left - equivalent to filling with one zero inthe beginning
		  					m1=_mm256_slli_si256(r1,1);//shift 1 elements left - equivalent to filling with one zero inthe beginning
		  					m2=_mm256_slli_si256(r2,1);//shift 1 elements left - equivalent to filling with one zero inthe beginning

		  					//preserve the element being lost because of the shift above
		  					r0=_mm256_and_si256(r0,mask_prelude);
		  					r0=_mm256_permute2f128_si256(r0,r0,1);
		  					r0=_mm256_srli_si256(r0,15);//shift 15 elements
		  					r0=_mm256_add_epi16(m0,r0);

		  					r1=_mm256_and_si256(r1,mask_prelude);
		  					r1=_mm256_permute2f128_si256(r1,r1,1);
		  					r1=_mm256_srli_si256(r1,15);//shift 15 elements
		  					r1=_mm256_add_epi16(m1,r1);

		  					r2=_mm256_and_si256(r2,mask_prelude);
		  					r2=_mm256_permute2f128_si256(r2,r2,1);
		  					r2=_mm256_srli_si256(r2,15);//shift 15 elements
		  					r2=_mm256_add_epi16(m2,r2);

		  			//END - extra code needed for prelude
		  		}
		  else {
				//load the 5 rows
				r0=_mm256_loadu_si256( (__m256i *) &frame1[0][col-1]);
				r1=_mm256_loadu_si256( (__m256i *) &frame1[1][col-1]);
				r2=_mm256_loadu_si256( (__m256i *) &frame1[2][col-1]);
		  }

		//col iteration computes output pixels of    1,5,9,13,17,21,25,29
		// col+1 iteration computes output pixels of 2,6,10,14,18,22,26,30
		// col+2 iteration computes output pixels of 3,7,11,15,19,23,27
		// col+3 iteration computes output pixels of 4,8,12,16,20,24,28
		//afterwards, col becomes 30 and repeat the above process

		 //row==1
		//1st col iteration

		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,c0);
		m1=_mm256_maddubs_epi16(r1,c1);
		m2=_mm256_maddubs_epi16(r2,c2);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m0=_mm256_add_epi16(m0,m2);

		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others. later on they will be stored into 0,4,8,12,16,20,24,28 positions (8bit register).
		output_even=_mm256_and_si256(m2,output_mask);


		//2nd col iteration
		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,c0_sh1);
		m1=_mm256_maddubs_epi16(r1,c1_sh1);
		m2=_mm256_maddubs_epi16(r2,c2_sh1);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m0=_mm256_add_epi16(m0,m2);

		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);


		//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others.
		output_odd = _mm256_and_si256(m2,output_mask);


       //3rd col iteration
		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,c0_sh2);
		m1=_mm256_maddubs_epi16(r1,c1_sh2);
		m2=_mm256_maddubs_epi16(r2,c2_sh2);


		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m0=_mm256_add_epi16(m0,m2);


		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		m4=_mm256_and_si256(m0,mask3);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
		m2=_mm256_add_epi16(m2,m4);

		//m2 has 16 16bit values now. the results I need are in positions 1,3,5,7,9,11,13. keep only those, discard others
		m2 = _mm256_and_si256(m2,output_mask_sh1);
		output_even = _mm256_add_epi16(output_even,m2);


		//4th col iteration

		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,c0_sh3);
		m1=_mm256_maddubs_epi16(r1,c1_sh3);
		m2=_mm256_maddubs_epi16(r2,c2_sh3);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m0=_mm256_add_epi16(m0,m2);


		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		m4=_mm256_and_si256(m0,mask3);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
		m2=_mm256_add_epi16(m2,m4);

		//m2 has 16 16bit values now. the results I need are in positions 1,3,5,7,9,11,13. keep only those, discard others
		m2 = _mm256_and_si256(m2,output_mask_sh1);
		output_odd = _mm256_add_epi16(output_odd,m2);


		//now division follows
		output_even=division(division_case,output_even,f);
		output_odd=division(division_case,output_odd,f);

		//shift odd 1 position and add to even
		output_odd = _mm256_slli_si256(output_odd,1);
		output_even = _mm256_add_epi8(output_even,output_odd);


		_mm256_storeu_si256( (__m256i *) &filt[1][col],output_even );

		 //row==0
		//1st col iteration

		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,c1);
		m1=_mm256_maddubs_epi16(r1,c2);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);

		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others. later on they will be stored into 0,4,8,12,16,20,24,28 positions (8bit register).
		output_even=_mm256_and_si256(m2,output_mask);


		//2nd col iteration
		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,c1_sh1);
		m1=_mm256_maddubs_epi16(r1,c2_sh1);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);

		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);


		//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others.
		output_odd = _mm256_and_si256(m2,output_mask);


      //3rd col iteration
		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,c1_sh2);
		m1=_mm256_maddubs_epi16(r1,c2_sh2);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);


		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		m4=_mm256_and_si256(m0,mask3);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
		m2=_mm256_add_epi16(m2,m4);

		//m2 has 16 16bit values now. the results I need are in positions 1,3,5,7,9,11,13. keep only those, discard others
		m2 = _mm256_and_si256(m2,output_mask_sh1);
		output_even = _mm256_add_epi16(output_even,m2);


		//4th col iteration

		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,c1_sh3);
		m1=_mm256_maddubs_epi16(r1,c2_sh3);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);

		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		m4=_mm256_and_si256(m0,mask3);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
		m2=_mm256_add_epi16(m2,m4);

		//m2 has 16 16bit values now. the results I need are in positions 1,3,5,7,9,11,13. keep only those, discard others
		m2 = _mm256_and_si256(m2,output_mask_sh1);
		output_odd = _mm256_add_epi16(output_odd,m2);


		//now division follows
		output_even=division(division_case,output_even,f);
		output_odd=division(division_case,output_odd,f);

		//shift odd 1 position and add to even
		output_odd = _mm256_slli_si256(output_odd,1);
		output_even = _mm256_add_epi8(output_even,output_odd);


		_mm256_storeu_si256( (__m256i *) &filt[0][col],output_even );
		}
	conv_loop_reminder_3x3_first_values(frame1,filt,M,N,col,REMINDER_ITERATIONS,division_case,divisor,filter,c0,c1,c2,c0_sh1,c1_sh1,c2_sh1,c0_sh2,c1_sh2,c2_sh2,c0_sh3,c1_sh3,c2_sh3,f);

	}

	else if (row==N-2){//special case compute filt[N-2:N-1][:]
		//printf("\nddd");
		for (col = 0; col <= M-32; col+=30){

		  if (col==0){

				//load the 5 rows
				r0=_mm256_load_si256( (__m256i *) &frame1[N-3][0]);
				r1=_mm256_load_si256( (__m256i *) &frame1[N-2][0]);
				r2=_mm256_load_si256( (__m256i *) &frame1[N-1][0]);


		  			//START - extra code needed for prelude
		  					m0=_mm256_slli_si256(r0,1);//shift 1 elements left - equivalent to filling with one zero inthe beginning
		  					m1=_mm256_slli_si256(r1,1);//shift 1 elements left - equivalent to filling with one zero inthe beginning
		  					m2=_mm256_slli_si256(r2,1);//shift 1 elements left - equivalent to filling with one zero inthe beginning

		  					//preserve the element being lost because of the shift above
		  					r0=_mm256_and_si256(r0,mask_prelude);
		  					r0=_mm256_permute2f128_si256(r0,r0,1);
		  					r0=_mm256_srli_si256(r0,15);//shift 15 elements
		  					r0=_mm256_add_epi16(m0,r0);

		  					r1=_mm256_and_si256(r1,mask_prelude);
		  					r1=_mm256_permute2f128_si256(r1,r1,1);
		  					r1=_mm256_srli_si256(r1,15);//shift 15 elements
		  					r1=_mm256_add_epi16(m1,r1);

		  					r2=_mm256_and_si256(r2,mask_prelude);
		  					r2=_mm256_permute2f128_si256(r2,r2,1);
		  					r2=_mm256_srli_si256(r2,15);//shift 15 elements
		  					r2=_mm256_add_epi16(m2,r2);

		  			//END - extra code needed for prelude
		  		}
		  else {
				//load the 5 rows
				r0=_mm256_loadu_si256( (__m256i *) &frame1[N-3][col-1]);
				r1=_mm256_loadu_si256( (__m256i *) &frame1[N-2][col-1]);
				r2=_mm256_loadu_si256( (__m256i *) &frame1[N-1][col-1]);
		  }

		//col iteration computes output pixels of    1,5,9,13,17,21,25,29
		// col+1 iteration computes output pixels of 2,6,10,14,18,22,26,30
		// col+2 iteration computes output pixels of 3,7,11,15,19,23,27
		// col+3 iteration computes output pixels of 4,8,12,16,20,24,28
		//afterwards, col becomes 30 and repeat the above process

		 //row==N-2
		//1st col iteration

		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,c0);
		m1=_mm256_maddubs_epi16(r1,c1);
		m2=_mm256_maddubs_epi16(r2,c2);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m0=_mm256_add_epi16(m0,m2);

		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others. later on they will be stored into 0,4,8,12,16,20,24,28 positions (8bit register).
		output_even=_mm256_and_si256(m2,output_mask);


		//2nd col iteration
		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,c0_sh1);
		m1=_mm256_maddubs_epi16(r1,c1_sh1);
		m2=_mm256_maddubs_epi16(r2,c2_sh1);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m0=_mm256_add_epi16(m0,m2);

		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);


		//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others.
		output_odd = _mm256_and_si256(m2,output_mask);


       //3rd col iteration
		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,c0_sh2);
		m1=_mm256_maddubs_epi16(r1,c1_sh2);
		m2=_mm256_maddubs_epi16(r2,c2_sh2);


		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m0=_mm256_add_epi16(m0,m2);


		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		m4=_mm256_and_si256(m0,mask3);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
		m2=_mm256_add_epi16(m2,m4);

		//m2 has 16 16bit values now. the results I need are in positions 1,3,5,7,9,11,13. keep only those, discard others
		m2 = _mm256_and_si256(m2,output_mask_sh1);
		output_even = _mm256_add_epi16(output_even,m2);


		//4th col iteration

		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,c0_sh3);
		m1=_mm256_maddubs_epi16(r1,c1_sh3);
		m2=_mm256_maddubs_epi16(r2,c2_sh3);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m0=_mm256_add_epi16(m0,m2);


		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		m4=_mm256_and_si256(m0,mask3);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
		m2=_mm256_add_epi16(m2,m4);

		//m2 has 16 16bit values now. the results I need are in positions 1,3,5,7,9,11,13. keep only those, discard others
		m2 = _mm256_and_si256(m2,output_mask_sh1);
		output_odd = _mm256_add_epi16(output_odd,m2);


		//now division follows
		output_even=division(division_case,output_even,f);
		output_odd=division(division_case,output_odd,f);

		//shift odd 1 position and add to even
		output_odd = _mm256_slli_si256(output_odd,1);
		output_even = _mm256_add_epi8(output_even,output_odd);


		_mm256_storeu_si256( (__m256i *) &filt[N-2][col],output_even );

		 //row==N-1
		//1st col iteration

		//multiply with the mask
		m1=_mm256_maddubs_epi16(r1,c0);
		m2=_mm256_maddubs_epi16(r2,c1);

		//vertical add
		m0=_mm256_add_epi16(m1,m2);

		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others. later on they will be stored into 0,4,8,12,16,20,24,28 positions (8bit register).
		output_even=_mm256_and_si256(m2,output_mask);


		//2nd col iteration
		//multiply with the mask
		m1=_mm256_maddubs_epi16(r1,c0_sh1);
		m2=_mm256_maddubs_epi16(r2,c1_sh1);

		//vertical add
		m0=_mm256_add_epi16(m1,m2);

		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);


		//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others.
		output_odd = _mm256_and_si256(m2,output_mask);


       //3rd col iteration
		//multiply with the mask
		m1=_mm256_maddubs_epi16(r1,c0_sh2);
		m2=_mm256_maddubs_epi16(r2,c1_sh2);


		//vertical add
		m0=_mm256_add_epi16(m1,m2);


		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		m4=_mm256_and_si256(m0,mask3);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
		m2=_mm256_add_epi16(m2,m4);

		//m2 has 16 16bit values now. the results I need are in positions 1,3,5,7,9,11,13. keep only those, discard others
		m2 = _mm256_and_si256(m2,output_mask_sh1);
		output_even = _mm256_add_epi16(output_even,m2);


		//4th col iteration

		//multiply with the mask
		m1=_mm256_maddubs_epi16(r1,c0_sh3);
		m2=_mm256_maddubs_epi16(r2,c1_sh3);

		//vertical add
		m0=_mm256_add_epi16(m1,m2);

		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		m4=_mm256_and_si256(m0,mask3);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
		m2=_mm256_add_epi16(m2,m4);

		//m2 has 16 16bit values now. the results I need are in positions 1,3,5,7,9,11,13. keep only those, discard others
		m2 = _mm256_and_si256(m2,output_mask_sh1);
		output_odd = _mm256_add_epi16(output_odd,m2);


		//now division follows
		output_even=division(division_case,output_even,f);
		output_odd=division(division_case,output_odd,f);

		//shift odd 1 position and add to even
		output_odd = _mm256_slli_si256(output_odd,1);
		output_even = _mm256_add_epi8(output_even,output_odd);


		_mm256_storeu_si256( (__m256i *) &filt[N-1][col],output_even );
		}
	conv_loop_reminder_3x3_last_values(frame1,filt,M,N,col,REMINDER_ITERATIONS,division_case,divisor,filter,c0,c1,c2,c0_sh1,c1_sh1,c2_sh1,c0_sh2,c1_sh2,c2_sh2,c0_sh3,c1_sh3,c2_sh3,f);

	}

	else if (row==N-1){//special case compute filt[N-1][:]

		for (col = 0; col <= M-32; col+=30){

		  if (col==0){

				r1=_mm256_load_si256( (__m256i *) &frame1[N-2][0]);
				r2=_mm256_load_si256( (__m256i *) &frame1[N-1][0]);


		  			//START - extra code needed for prelude
		  					m1=_mm256_slli_si256(r1,1);//shift 1 elements left - equivalent to filling with one zero inthe beginning
		  					m2=_mm256_slli_si256(r2,1);//shift 1 elements left - equivalent to filling with one zero inthe beginning

		  					r1=_mm256_and_si256(r1,mask_prelude);
		  					r1=_mm256_permute2f128_si256(r1,r1,1);
		  					r1=_mm256_srli_si256(r1,15);//shift 15 elements
		  					r1=_mm256_add_epi16(m1,r1);

		  					r2=_mm256_and_si256(r2,mask_prelude);
		  					r2=_mm256_permute2f128_si256(r2,r2,1);
		  					r2=_mm256_srli_si256(r2,15);//shift 15 elements
		  					r2=_mm256_add_epi16(m2,r2);

		  			//END - extra code needed for prelude
		  		}
		  else {
				r1=_mm256_loadu_si256( (__m256i *) &frame1[N-2][col-1]);
				r2=_mm256_loadu_si256( (__m256i *) &frame1[N-1][col-1]);
		  }


		 //row==N-1
		//1st col iteration

		//multiply with the mask
		m1=_mm256_maddubs_epi16(r1,c0);
		m2=_mm256_maddubs_epi16(r2,c1);

		//vertical add
		m0=_mm256_add_epi16(m1,m2);

		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others. later on they will be stored into 0,4,8,12,16,20,24,28 positions (8bit register).
		output_even=_mm256_and_si256(m2,output_mask);


		//2nd col iteration
		//multiply with the mask
		m1=_mm256_maddubs_epi16(r1,c0_sh1);
		m2=_mm256_maddubs_epi16(r2,c1_sh1);

		//vertical add
		m0=_mm256_add_epi16(m1,m2);

		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);


		//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others.
		output_odd = _mm256_and_si256(m2,output_mask);


       //3rd col iteration
		//multiply with the mask
		m1=_mm256_maddubs_epi16(r1,c0_sh2);
		m2=_mm256_maddubs_epi16(r2,c1_sh2);


		//vertical add
		m0=_mm256_add_epi16(m1,m2);


		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		m4=_mm256_and_si256(m0,mask3);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
		m2=_mm256_add_epi16(m2,m4);

		//m2 has 16 16bit values now. the results I need are in positions 1,3,5,7,9,11,13. keep only those, discard others
		m2 = _mm256_and_si256(m2,output_mask_sh1);
		output_even = _mm256_add_epi16(output_even,m2);


		//4th col iteration

		//multiply with the mask
		m1=_mm256_maddubs_epi16(r1,c0_sh3);
		m2=_mm256_maddubs_epi16(r2,c1_sh3);

		//vertical add
		m0=_mm256_add_epi16(m1,m2);

		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		m4=_mm256_and_si256(m0,mask3);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
		m2=_mm256_add_epi16(m2,m4);

		//m2 has 16 16bit values now. the results I need are in positions 1,3,5,7,9,11,13. keep only those, discard others
		m2 = _mm256_and_si256(m2,output_mask_sh1);
		output_odd = _mm256_add_epi16(output_odd,m2);


		//now division follows
		output_even=division(division_case,output_even,f);
		output_odd=division(division_case,output_odd,f);

		//shift odd 1 position and add to even
		output_odd = _mm256_slli_si256(output_odd,1);
		output_even = _mm256_add_epi8(output_even,output_odd);


		_mm256_storeu_si256( (__m256i *) &filt[N-1][col],output_even );
		}
	 loop_reminder_3x3_last_row_only(frame1,filt,M,N,col,REMINDER_ITERATIONS,division_case,divisor,filter,c0,c1,c0_sh1,c1_sh1,c0_sh2,c1_sh2,c0_sh3,c1_sh3,f);

	}

	else if (row==N-3){//special case compute filt[N-2:N-1][:] and filt[N-3][:]

		for (col = 0; col <= M-32; col+=30){

		  if (col==0){

				//load the 5 rows
				r0=_mm256_load_si256( (__m256i *) &frame1[N-4][0]);
				r1=_mm256_load_si256( (__m256i *) &frame1[N-3][0]);
				r2=_mm256_load_si256( (__m256i *) &frame1[N-2][0]);
				r3=_mm256_load_si256( (__m256i *) &frame1[N-1][0]);


		  			//START - extra code needed for prelude
		  					m0=_mm256_slli_si256(r0,1);//shift 1 elements left - equivalent to filling with one zero inthe beginning
		  					m1=_mm256_slli_si256(r1,1);//shift 1 elements left - equivalent to filling with one zero inthe beginning
		  					m2=_mm256_slli_si256(r2,1);//shift 1 elements left - equivalent to filling with one zero inthe beginning
		  					m3=_mm256_slli_si256(r3,1);//shift 1 elements left - equivalent to filling with one zero inthe beginning

		  					//preserve the element being lost because of the shift above
		  					r0=_mm256_and_si256(r0,mask_prelude);
		  					r0=_mm256_permute2f128_si256(r0,r0,1);
		  					r0=_mm256_srli_si256(r0,15);//shift 15 elements
		  					r0=_mm256_add_epi16(m0,r0);

		  					r1=_mm256_and_si256(r1,mask_prelude);
		  					r1=_mm256_permute2f128_si256(r1,r1,1);
		  					r1=_mm256_srli_si256(r1,15);//shift 15 elements
		  					r1=_mm256_add_epi16(m1,r1);

		  					r2=_mm256_and_si256(r2,mask_prelude);
		  					r2=_mm256_permute2f128_si256(r2,r2,1);
		  					r2=_mm256_srli_si256(r2,15);//shift 15 elements
		  					r2=_mm256_add_epi16(m2,r2);

		  					r3=_mm256_and_si256(r3,mask_prelude);
		  					r3=_mm256_permute2f128_si256(r3,r3,1);
		  					r3=_mm256_srli_si256(r3,15);//shift 15 elements
		  					r3=_mm256_add_epi16(m3,r3);

		  			//END - extra code needed for prelude
		  		}
		  else {
				r0=_mm256_loadu_si256( (__m256i *) &frame1[N-4][col-1]);
				r1=_mm256_loadu_si256( (__m256i *) &frame1[N-3][col-1]);
				r2=_mm256_loadu_si256( (__m256i *) &frame1[N-2][col-1]);
				r3=_mm256_loadu_si256( (__m256i *) &frame1[N-1][col-1]);
		  }

		//col iteration computes output pixels of    1,5,9,13,17,21,25,29
		// col+1 iteration computes output pixels of 2,6,10,14,18,22,26,30
		// col+2 iteration computes output pixels of 3,7,11,15,19,23,27
		// col+3 iteration computes output pixels of 4,8,12,16,20,24,28
		//afterwards, col becomes 30 and repeat the above process

		 //row==N-3
		//1st col iteration

		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,c0);
		m1=_mm256_maddubs_epi16(r1,c1);
		m2=_mm256_maddubs_epi16(r2,c2);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m0=_mm256_add_epi16(m0,m2);

		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others. later on they will be stored into 0,4,8,12,16,20,24,28 positions (8bit register).
		output_even=_mm256_and_si256(m2,output_mask);


		//2nd col iteration
		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,c0_sh1);
		m1=_mm256_maddubs_epi16(r1,c1_sh1);
		m2=_mm256_maddubs_epi16(r2,c2_sh1);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m0=_mm256_add_epi16(m0,m2);

		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);


		//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others.
		output_odd = _mm256_and_si256(m2,output_mask);


       //3rd col iteration
		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,c0_sh2);
		m1=_mm256_maddubs_epi16(r1,c1_sh2);
		m2=_mm256_maddubs_epi16(r2,c2_sh2);


		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m0=_mm256_add_epi16(m0,m2);


		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		m4=_mm256_and_si256(m0,mask3);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
		m2=_mm256_add_epi16(m2,m4);

		//m2 has 16 16bit values now. the results I need are in positions 1,3,5,7,9,11,13. keep only those, discard others
		m2 = _mm256_and_si256(m2,output_mask_sh1);
		output_even = _mm256_add_epi16(output_even,m2);


		//4th col iteration

		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,c0_sh3);
		m1=_mm256_maddubs_epi16(r1,c1_sh3);
		m2=_mm256_maddubs_epi16(r2,c2_sh3);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m0=_mm256_add_epi16(m0,m2);


		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		m4=_mm256_and_si256(m0,mask3);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
		m2=_mm256_add_epi16(m2,m4);

		//m2 has 16 16bit values now. the results I need are in positions 1,3,5,7,9,11,13. keep only those, discard others
		m2 = _mm256_and_si256(m2,output_mask_sh1);
		output_odd = _mm256_add_epi16(output_odd,m2);


		//now division follows
		output_even=division(division_case,output_even,f);
		output_odd=division(division_case,output_odd,f);

		//shift odd 1 position and add to even
		output_odd = _mm256_slli_si256(output_odd,1);
		output_even = _mm256_add_epi8(output_even,output_odd);


		_mm256_storeu_si256( (__m256i *) &filt[N-3][col],output_even );

		 //row==N-2
		//1st col iteration

		//multiply with the mask
		m0=_mm256_maddubs_epi16(r1,c0);
		m1=_mm256_maddubs_epi16(r2,c1);
		m2=_mm256_maddubs_epi16(r3,c2);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m0=_mm256_add_epi16(m0,m2);

		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others. later on they will be stored into 0,4,8,12,16,20,24,28 positions (8bit register).
		output_even=_mm256_and_si256(m2,output_mask);


		//2nd col iteration
		//multiply with the mask
		m0=_mm256_maddubs_epi16(r1,c0_sh1);
		m1=_mm256_maddubs_epi16(r2,c1_sh1);
		m2=_mm256_maddubs_epi16(r3,c2_sh1);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m0=_mm256_add_epi16(m0,m2);

		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);


		//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others.
		output_odd = _mm256_and_si256(m2,output_mask);


      //3rd col iteration
		//multiply with the mask
		m0=_mm256_maddubs_epi16(r1,c0_sh2);
		m1=_mm256_maddubs_epi16(r2,c1_sh2);
		m2=_mm256_maddubs_epi16(r3,c2_sh2);


		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m0=_mm256_add_epi16(m0,m2);


		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		m4=_mm256_and_si256(m0,mask3);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
		m2=_mm256_add_epi16(m2,m4);

		//m2 has 16 16bit values now. the results I need are in positions 1,3,5,7,9,11,13. keep only those, discard others
		m2 = _mm256_and_si256(m2,output_mask_sh1);
		output_even = _mm256_add_epi16(output_even,m2);


		//4th col iteration

		//multiply with the mask
		m0=_mm256_maddubs_epi16(r1,c0_sh3);
		m1=_mm256_maddubs_epi16(r2,c1_sh3);
		m2=_mm256_maddubs_epi16(r3,c2_sh3);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m0=_mm256_add_epi16(m0,m2);


		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		m4=_mm256_and_si256(m0,mask3);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
		m2=_mm256_add_epi16(m2,m4);

		//m2 has 16 16bit values now. the results I need are in positions 1,3,5,7,9,11,13. keep only those, discard others
		m2 = _mm256_and_si256(m2,output_mask_sh1);
		output_odd = _mm256_add_epi16(output_odd,m2);


		//now division follows
		output_even=division(division_case,output_even,f);
		output_odd=division(division_case,output_odd,f);

		//shift odd 1 position and add to even
		output_odd = _mm256_slli_si256(output_odd,1);
		output_even = _mm256_add_epi8(output_even,output_odd);


		_mm256_storeu_si256( (__m256i *) &filt[N-2][col],output_even );

		 //row==N-1
		//1st col iteration

		//multiply with the mask
		m1=_mm256_maddubs_epi16(r2,c0);
		m2=_mm256_maddubs_epi16(r3,c1);

		//vertical add
		m0=_mm256_add_epi16(m1,m2);

		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others. later on they will be stored into 0,4,8,12,16,20,24,28 positions (8bit register).
		output_even=_mm256_and_si256(m2,output_mask);


		//2nd col iteration
		//multiply with the mask
		m1=_mm256_maddubs_epi16(r2,c0_sh1);
		m2=_mm256_maddubs_epi16(r3,c1_sh1);

		//vertical add
		m0=_mm256_add_epi16(m1,m2);

		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);


		//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others.
		output_odd = _mm256_and_si256(m2,output_mask);


       //3rd col iteration
		//multiply with the mask
		m1=_mm256_maddubs_epi16(r2,c0_sh2);
		m2=_mm256_maddubs_epi16(r3,c1_sh2);


		//vertical add
		m0=_mm256_add_epi16(m1,m2);


		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		m4=_mm256_and_si256(m0,mask3);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
		m2=_mm256_add_epi16(m2,m4);

		//m2 has 16 16bit values now. the results I need are in positions 1,3,5,7,9,11,13. keep only those, discard others
		m2 = _mm256_and_si256(m2,output_mask_sh1);
		output_even = _mm256_add_epi16(output_even,m2);


		//4th col iteration

		//multiply with the mask
		m1=_mm256_maddubs_epi16(r2,c0_sh3);
		m2=_mm256_maddubs_epi16(r3,c1_sh3);

		//vertical add
		m0=_mm256_add_epi16(m1,m2);

		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		m4=_mm256_and_si256(m0,mask3);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
		m2=_mm256_add_epi16(m2,m4);

		//m2 has 16 16bit values now. the results I need are in positions 1,3,5,7,9,11,13. keep only those, discard others
		m2 = _mm256_and_si256(m2,output_mask_sh1);
		output_odd = _mm256_add_epi16(output_odd,m2);


		//now division follows
		output_even=division(division_case,output_even,f);
		output_odd=division(division_case,output_odd,f);

		//shift odd 1 position and add to even
		output_odd = _mm256_slli_si256(output_odd,1);
		output_even = _mm256_add_epi8(output_even,output_odd);


		_mm256_storeu_si256( (__m256i *) &filt[N-1][col],output_even );
		}
	conv_loop_reminder_3x3_last_values(frame1,filt,M,N,col,REMINDER_ITERATIONS,division_case,divisor,filter,c0,c1,c2,c0_sh1,c1_sh1,c2_sh1,c0_sh2,c1_sh2,c2_sh2,c0_sh3,c1_sh3,c2_sh3,f);
	conv_loop_reminder_3x3(frame1,filt,M,N,N-3,col,REMINDER_ITERATIONS,division_case,divisor,filter,c0,c1,c2,c0_sh1,c1_sh1,c2_sh1,c0_sh2,c1_sh2,c2_sh2,c0_sh3,c1_sh3,c2_sh3,f);

	}

	else {//main loop

	  for (col = 0; col <= M-32; col+=30){

		  if (col==0){

				//load the 5 rows
				r0=_mm256_load_si256( (__m256i *) &frame1[row-1][0]);
				r1=_mm256_load_si256( (__m256i *) &frame1[row][0]);
				r2=_mm256_load_si256( (__m256i *) &frame1[row+1][0]);
				r3=_mm256_load_si256( (__m256i *) &frame1[row+2][0]);
				r4=_mm256_load_si256( (__m256i *) &frame1[row+3][0]);



		  			//START - extra code needed for prelude
		  					m0=_mm256_slli_si256(r0,1);//shift 1 elements left - equivalent to filling with one zero inthe beginning
		  					m1=_mm256_slli_si256(r1,1);//shift 1 elements left - equivalent to filling with one zero inthe beginning
		  					m2=_mm256_slli_si256(r2,1);//shift 1 elements left - equivalent to filling with one zero inthe beginning
		  					m3=_mm256_slli_si256(r3,1);//shift 1 elements left - equivalent to filling with one zero inthe beginning
		  					m4=_mm256_slli_si256(r4,1);//shift 1 elements left - equivalent to filling with one zero inthe beginning


		  					//preserve the element being lost because of the shift above
		  					r0=_mm256_and_si256(r0,mask_prelude);
		  					r0=_mm256_permute2f128_si256(r0,r0,1);
		  					r0=_mm256_srli_si256(r0,15);//shift 15 elements
		  					r0=_mm256_add_epi16(m0,r0);

		  					r1=_mm256_and_si256(r1,mask_prelude);
		  					r1=_mm256_permute2f128_si256(r1,r1,1);
		  					r1=_mm256_srli_si256(r1,15);//shift 15 elements
		  					r1=_mm256_add_epi16(m1,r1);

		  					r2=_mm256_and_si256(r2,mask_prelude);
		  					r2=_mm256_permute2f128_si256(r2,r2,1);
		  					r2=_mm256_srli_si256(r2,15);//shift 15 elements
		  					r2=_mm256_add_epi16(m2,r2);

		  					r3=_mm256_and_si256(r3,mask_prelude);
		  					r3=_mm256_permute2f128_si256(r3,r3,1);
		  					r3=_mm256_srli_si256(r3,15);//shift 15 elements
		  					r3=_mm256_add_epi16(m3,r3);

		  					r4=_mm256_and_si256(r4,mask_prelude);
		  					r4=_mm256_permute2f128_si256(r4,r4,1);
		  					r4=_mm256_srli_si256(r4,15);//shift 15 elements
		  					r4=_mm256_add_epi16(m4,r4);

		  			//END - extra code needed for prelude
		  		}
		  else {
				//load the 5 rows
				r0=_mm256_loadu_si256( (__m256i *) &frame1[row-1][col-1]);
				r1=_mm256_loadu_si256( (__m256i *) &frame1[row][col-1]);
				r2=_mm256_loadu_si256( (__m256i *) &frame1[row+1][col-1]);
				r3=_mm256_loadu_si256( (__m256i *) &frame1[row+2][col-1]);
				r4=_mm256_loadu_si256( (__m256i *) &frame1[row+3][col-1]);

		  }

		//col iteration computes output pixels of    1,5,9,13,17,21,25,29
		// col+1 iteration computes output pixels of 2,6,10,14,18,22,26,30
		// col+2 iteration computes output pixels of 3,7,11,15,19,23,27
		// col+3 iteration computes output pixels of 4,8,12,16,20,24,28
		//afterwards, col becomes 30 and repeat the above process

		//first row
		//1st col iteration

		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,c0);
		m1=_mm256_maddubs_epi16(r1,c1);
		m2=_mm256_maddubs_epi16(r2,c2);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m0=_mm256_add_epi16(m0,m2);

		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others. later on they will be stored into 0,4,8,12,16,20,24,28 positions (8bit register).
		output_even=_mm256_and_si256(m2,output_mask);


		//2nd col iteration
		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,c0_sh1);
		m1=_mm256_maddubs_epi16(r1,c1_sh1);
		m2=_mm256_maddubs_epi16(r2,c2_sh1);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m0=_mm256_add_epi16(m0,m2);

		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);


		//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others.
		output_odd = _mm256_and_si256(m2,output_mask);


         //3rd col iteration
		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,c0_sh2);
		m1=_mm256_maddubs_epi16(r1,c1_sh2);
		m2=_mm256_maddubs_epi16(r2,c2_sh2);


		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m0=_mm256_add_epi16(m0,m2);


		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		m4=_mm256_and_si256(m0,mask3);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
		m2=_mm256_add_epi16(m2,m4);

		//m2 has 16 16bit values now. the results I need are in positions 1,3,5,7,9,11,13. keep only those, discard others
		m2 = _mm256_and_si256(m2,output_mask_sh1);
		output_even = _mm256_add_epi16(output_even,m2);


		//4th col iteration

		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,c0_sh3);
		m1=_mm256_maddubs_epi16(r1,c1_sh3);
		m2=_mm256_maddubs_epi16(r2,c2_sh3);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m0=_mm256_add_epi16(m0,m2);


		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		m4=_mm256_and_si256(m0,mask3);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
		m2=_mm256_add_epi16(m2,m4);

		//m2 has 16 16bit values now. the results I need are in positions 1,3,5,7,9,11,13. keep only those, discard others
		m2 = _mm256_and_si256(m2,output_mask_sh1);
		output_odd = _mm256_add_epi16(output_odd,m2);


		//now division follows
		output_even=division(division_case,output_even,f);
		output_odd=division(division_case,output_odd,f);

		//shift odd 1 position and add to even
		output_odd = _mm256_slli_si256(output_odd,1);
		output_even = _mm256_add_epi8(output_even,output_odd);


		_mm256_storeu_si256( (__m256i *) &filt[row][col],output_even );

		//2nd row
		//1st col iteration

		//multiply with the mask
		m0=_mm256_maddubs_epi16(r1,c0);
		m1=_mm256_maddubs_epi16(r2,c1);
		m2=_mm256_maddubs_epi16(r3,c2);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m0=_mm256_add_epi16(m0,m2);

		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others. later on they will be stored into 0,4,8,12,16,20,24,28 positions (8bit register).
		output_even=_mm256_and_si256(m2,output_mask);


		//2nd col iteration
		//multiply with the mask
		m0=_mm256_maddubs_epi16(r1,c0_sh1);
		m1=_mm256_maddubs_epi16(r2,c1_sh1);
		m2=_mm256_maddubs_epi16(r3,c2_sh1);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m0=_mm256_add_epi16(m0,m2);

		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);


		//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others.
		output_odd = _mm256_and_si256(m2,output_mask);


         //3rd col iteration
		//multiply with the mask
		m0=_mm256_maddubs_epi16(r1,c0_sh2);
		m1=_mm256_maddubs_epi16(r2,c1_sh2);
		m2=_mm256_maddubs_epi16(r3,c2_sh2);


		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m0=_mm256_add_epi16(m0,m2);


		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		m4=_mm256_and_si256(m0,mask3);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
		m2=_mm256_add_epi16(m2,m4);

		//m2 has 16 16bit values now. the results I need are in positions 1,3,5,7,9,11,13. keep only those, discard others
		m2 = _mm256_and_si256(m2,output_mask_sh1);
		output_even = _mm256_add_epi16(output_even,m2);


		//4th col iteration

		//multiply with the mask
		m0=_mm256_maddubs_epi16(r1,c0_sh3);
		m1=_mm256_maddubs_epi16(r2,c1_sh3);
		m2=_mm256_maddubs_epi16(r3,c2_sh3);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m0=_mm256_add_epi16(m0,m2);


		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		m4=_mm256_and_si256(m0,mask3);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
		m2=_mm256_add_epi16(m2,m4);

		//m2 has 16 16bit values now. the results I need are in positions 1,3,5,7,9,11,13. keep only those, discard others
		m2 = _mm256_and_si256(m2,output_mask_sh1);
		output_odd = _mm256_add_epi16(output_odd,m2);


		//now division follows
		output_even=division(division_case,output_even,f);
		output_odd=division(division_case,output_odd,f);

		//shift odd 1 position and add to even
		output_odd = _mm256_slli_si256(output_odd,1);
		output_even = _mm256_add_epi8(output_even,output_odd);


		_mm256_storeu_si256( (__m256i *) &filt[row+1][col],output_even );

		//3rd row
		//1st col iteration

		//multiply with the mask
		m0=_mm256_maddubs_epi16(r2,c0);
		m1=_mm256_maddubs_epi16(r3,c1);
		m2=_mm256_maddubs_epi16(r4,c2);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m0=_mm256_add_epi16(m0,m2);

		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others. later on they will be stored into 0,4,8,12,16,20,24,28 positions (8bit register).
		output_even=_mm256_and_si256(m2,output_mask);


		//2nd col iteration
		//multiply with the mask
		m0=_mm256_maddubs_epi16(r2,c0_sh1);
		m1=_mm256_maddubs_epi16(r3,c1_sh1);
		m2=_mm256_maddubs_epi16(r4,c2_sh1);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m0=_mm256_add_epi16(m0,m2);

		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);


		//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others.
		output_odd = _mm256_and_si256(m2,output_mask);


         //3rd col iteration
		//multiply with the mask
		m0=_mm256_maddubs_epi16(r2,c0_sh2);
		m1=_mm256_maddubs_epi16(r3,c1_sh2);
		m2=_mm256_maddubs_epi16(r4,c2_sh2);


		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m0=_mm256_add_epi16(m0,m2);


		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		m4=_mm256_and_si256(m0,mask3);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
		m2=_mm256_add_epi16(m2,m4);

		//m2 has 16 16bit values now. the results I need are in positions 1,3,5,7,9,11,13. keep only those, discard others
		m2 = _mm256_and_si256(m2,output_mask_sh1);
		output_even = _mm256_add_epi16(output_even,m2);


		//4th col iteration

		//multiply with the mask
		m0=_mm256_maddubs_epi16(r2,c0_sh3);
		m1=_mm256_maddubs_epi16(r3,c1_sh3);
		m2=_mm256_maddubs_epi16(r4,c2_sh3);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m0=_mm256_add_epi16(m0,m2);


		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		m4=_mm256_and_si256(m0,mask3);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
		m2=_mm256_add_epi16(m2,m4);

		//m2 has 16 16bit values now. the results I need are in positions 1,3,5,7,9,11,13. keep only those, discard others
		m2 = _mm256_and_si256(m2,output_mask_sh1);
		output_odd = _mm256_add_epi16(output_odd,m2);


		//now division follows
		output_even=division(division_case,output_even,f);
		output_odd=division(division_case,output_odd,f);

		//shift odd 1 position and add to even
		output_odd = _mm256_slli_si256(output_odd,1);
		output_even = _mm256_add_epi8(output_even,output_odd);


		_mm256_storeu_si256( (__m256i *) &filt[row+2][col],output_even );


		}
	  conv_loop_reminder_3x3(frame1,filt,M,N,row,col,REMINDER_ITERATIONS,division_case,divisor,filter,c0,c1,c2,c0_sh1,c1_sh1,c2_sh1,c0_sh2,c1_sh2,c2_sh2,c0_sh3,c1_sh3,c2_sh3,f);
	  conv_loop_reminder_3x3(frame1,filt,M,N,row+1,col,REMINDER_ITERATIONS,division_case,divisor,filter,c0,c1,c2,c0_sh1,c1_sh1,c2_sh1,c0_sh2,c1_sh2,c2_sh2,c0_sh3,c1_sh3,c2_sh3,f);
	  conv_loop_reminder_3x3(frame1,filt,M,N,row+2,col,REMINDER_ITERATIONS,division_case,divisor,filter,c0,c1,c2,c0_sh1,c1_sh1,c2_sh1,c0_sh2,c1_sh2,c2_sh2,c0_sh3,c1_sh3,c2_sh3,f);

	}

}

}//end of parallel


}





void convolution_optimized_5x5_16(unsigned char **frame1,unsigned char **filt,const unsigned int M, const unsigned int N, const unsigned short int divisor, signed char **filter5x5){

const signed char f00=filter5x5[0][0];
const signed char f01=filter5x5[0][1];
const signed char f02=filter5x5[0][2];
const signed char f03=filter5x5[0][3];
const signed char f04=filter5x5[0][4];

const signed char f10=filter5x5[1][0];
const signed char f11=filter5x5[1][1];
const signed char f12=filter5x5[1][2];
const signed char f13=filter5x5[1][3];
const signed char f14=filter5x5[1][4];

const signed char f20=filter5x5[2][0];
const signed char f21=filter5x5[2][1];
const signed char f22=filter5x5[2][2];
const signed char f23=filter5x5[2][3];
const signed char f24=filter5x5[2][4];

const signed char f30=filter5x5[3][0];
const signed char f31=filter5x5[3][1];
const signed char f32=filter5x5[3][2];
const signed char f33=filter5x5[3][3];
const signed char f34=filter5x5[3][4];

const signed char f40=filter5x5[4][0];
const signed char f41=filter5x5[4][1];
const signed char f42=filter5x5[4][2];
const signed char f43=filter5x5[4][3];
const signed char f44=filter5x5[4][4];

const signed char mask_vector[30][32] __attribute__((aligned(64))) ={
		{f00,f01,f02,f03,f04,0,f00,f01,f02,f03,f04,0,f00,f01,f02,f03,f04,0,f00,f01,f02,f03,f04,0,f00,f01,f02,f03,f04,0,0,0},
		{f10,f11,f12,f13,f14,0,f10,f11,f12,f13,f14,0,f10,f11,f12,f13,f14,0,f10,f11,f12,f13,f14,0,f10,f11,f12,f13,f14,0,0,0},
		{f20,f21,f22,f23,f24,0,f20,f21,f22,f23,f24,0,f20,f21,f22,f23,f24,0,f20,f21,f22,f23,f24,0,f20,f21,f22,f23,f24,0,0,0},
		{f30,f31,f32,f33,f34,0,f30,f31,f32,f33,f34,0,f30,f31,f32,f33,f34,0,f30,f31,f32,f33,f34,0,f30,f31,f32,f33,f34,0,0,0},
		{f40,f41,f42,f43,f44,0,f40,f41,f42,f43,f44,0,f40,f41,f42,f43,f44,0,f40,f41,f42,f43,f44,0,f40,f41,f42,f43,f44,0,0,0},

		{0,f00,f01,f02,f03,f04,0,f00,f01,f02,f03,f04,0,f00,f01,f02,f03,f04,0,f00,f01,f02,f03,f04,0,f00,f01,f02,f03,f04,0,0},
		{0,f10,f11,f12,f13,f14,0,f10,f11,f12,f13,f14,0,f10,f11,f12,f13,f14,0,f10,f11,f12,f13,f14,0,f10,f11,f12,f13,f14,0,0},
		{0,f20,f21,f22,f23,f24,0,f20,f21,f22,f23,f24,0,f20,f21,f22,f23,f24,0,f20,f21,f22,f23,f24,0,f20,f21,f22,f23,f24,0,0},
		{0,f30,f31,f32,f33,f34,0,f30,f31,f32,f33,f34,0,f30,f31,f32,f33,f34,0,f30,f31,f32,f33,f34,0,f30,f31,f32,f33,f34,0,0},
		{0,f40,f41,f42,f43,f44,0,f40,f41,f42,f43,f44,0,f40,f41,f42,f43,f44,0,f40,f41,f42,f43,f44,0,f40,f41,f42,f43,f44,0,0},

		{0,0,f00,f01,f02,f03,f04,0,f00,f01,f02,f03,f04,0,f00,f01,f02,f03,f04,0,f00,f01,f02,f03,f04,0,f00,f01,f02,f03,f04,0},
		{0,0,f10,f11,f12,f13,f14,0,f10,f11,f12,f13,f14,0,f10,f11,f12,f13,f14,0,f10,f11,f12,f13,f14,0,f10,f11,f12,f13,f14,0},
		{0,0,f20,f21,f22,f23,f24,0,f20,f21,f22,f23,f24,0,f20,f21,f22,f23,f24,0,f20,f21,f22,f23,f24,0,f20,f21,f22,f23,f24,0},
		{0,0,f30,f31,f32,f33,f34,0,f30,f31,f32,f33,f34,0,f30,f31,f32,f33,f34,0,f30,f31,f32,f33,f34,0,f30,f31,f32,f33,f34,0},
		{0,0,f40,f41,f42,f43,f44,0,f40,f41,f42,f43,f44,0,f40,f41,f42,f43,f44,0,f40,f41,f42,f43,f44,0,f40,f41,f42,f43,f44,0},

		{0,0,0,f00,f01,f02,f03,f04,0,f00,f01,f02,f03,f04,0,f00,f01,f02,f03,f04,0,f00,f01,f02,f03,f04,0,f00,f01,f02,f03,f04},
		{0,0,0,f10,f11,f12,f13,f14,0,f10,f11,f12,f13,f14,0,f10,f11,f12,f13,f14,0,f10,f11,f12,f13,f14,0,f10,f11,f12,f13,f14},
		{0,0,0,f20,f21,f22,f23,f24,0,f20,f21,f22,f23,f24,0,f20,f21,f22,f23,f24,0,f20,f21,f22,f23,f24,0,f20,f21,f22,f23,f24},
		{0,0,0,f30,f31,f32,f33,f34,0,f30,f31,f32,f33,f34,0,f30,f31,f32,f33,f34,0,f30,f31,f32,f33,f34,0,f30,f31,f32,f33,f34},
		{0,0,0,f40,f41,f42,f43,f44,0,f40,f41,f42,f43,f44,0,f40,f41,f42,f43,f44,0,f40,f41,f42,f43,f44,0,f40,f41,f42,f43,f44},

		{0,0,0,0,f00,f01,f02,f03,f04,0,f00,f01,f02,f03,f04,0,f00,f01,f02,f03,f04,0,f00,f01,f02,f03,f04,0,0,0,0,0},
		{0,0,0,0,f10,f11,f12,f13,f14,0,f10,f11,f12,f13,f14,0,f10,f11,f12,f13,f14,0,f10,f11,f12,f13,f14,0,0,0,0,0},
		{0,0,0,0,f20,f21,f22,f23,f24,0,f20,f21,f22,f23,f24,0,f20,f21,f22,f23,f24,0,f20,f21,f22,f23,f24,0,0,0,0,0},
		{0,0,0,0,f30,f31,f32,f33,f34,0,f30,f31,f32,f33,f34,0,f30,f31,f32,f33,f34,0,f30,f31,f32,f33,f34,0,0,0,0,0},
		{0,0,0,0,f40,f41,f42,f43,f44,0,f40,f41,f42,f43,f44,0,f40,f41,f42,f43,f44,0,f40,f41,f42,f43,f44,0,0,0,0,0},

		{0,0,0,0,0,f00,f01,f02,f03,f04,0,f00,f01,f02,f03,f04,0,f00,f01,f02,f03,f04,0,f00,f01,f02,f03,f04,0,0,0,0},
		{0,0,0,0,0,f10,f11,f12,f13,f14,0,f10,f11,f12,f13,f14,0,f10,f11,f12,f13,f14,0,f10,f11,f12,f13,f14,0,0,0,0},
		{0,0,0,0,0,f20,f21,f22,f23,f24,0,f20,f21,f22,f23,f24,0,f20,f21,f22,f23,f24,0,f20,f21,f22,f23,f24,0,0,0,0},
		{0,0,0,0,0,f30,f31,f32,f33,f34,0,f30,f31,f32,f33,f34,0,f30,f31,f32,f33,f34,0,f30,f31,f32,f33,f34,0,0,0,0},
		{0,0,0,0,0,f40,f41,f42,f43,f44,0,f40,f41,f42,f43,f44,0,f40,f41,f42,f43,f44,0,f40,f41,f42,f43,f44,0,0,0,0},
};


	const __m256i c0=_mm256_load_si256( (__m256i *) &mask_vector[0][0]);
	const __m256i c1=_mm256_load_si256( (__m256i *) &mask_vector[1][0]);
	const __m256i c2=_mm256_load_si256( (__m256i *) &mask_vector[2][0]);
	const __m256i c3=_mm256_load_si256( (__m256i *) &mask_vector[3][0]);
	const __m256i c4=_mm256_load_si256( (__m256i *) &mask_vector[4][0]);

	const __m256i c0_sh1=_mm256_load_si256( (__m256i *) &mask_vector[5][0]);
	const __m256i c1_sh1=_mm256_load_si256( (__m256i *) &mask_vector[6][0]);
	const __m256i c2_sh1=_mm256_load_si256( (__m256i *) &mask_vector[7][0]);
	const __m256i c3_sh1=_mm256_load_si256( (__m256i *) &mask_vector[8][0]);
	const __m256i c4_sh1=_mm256_load_si256( (__m256i *) &mask_vector[9][0]);

	const __m256i c0_sh2=_mm256_load_si256( (__m256i *) &mask_vector[10][0]);
	const __m256i c1_sh2=_mm256_load_si256( (__m256i *) &mask_vector[11][0]);
	const __m256i c2_sh2=_mm256_load_si256( (__m256i *) &mask_vector[12][0]);
	const __m256i c3_sh2=_mm256_load_si256( (__m256i *) &mask_vector[13][0]);
	const __m256i c4_sh2=_mm256_load_si256( (__m256i *) &mask_vector[14][0]);

	const __m256i c0_sh3=_mm256_load_si256( (__m256i *) &mask_vector[15][0]);
	const __m256i c1_sh3=_mm256_load_si256( (__m256i *) &mask_vector[16][0]);
	const __m256i c2_sh3=_mm256_load_si256( (__m256i *) &mask_vector[17][0]);
	const __m256i c3_sh3=_mm256_load_si256( (__m256i *) &mask_vector[18][0]);
	const __m256i c4_sh3=_mm256_load_si256( (__m256i *) &mask_vector[19][0]);

	const __m256i c0_sh4=_mm256_load_si256( (__m256i *) &mask_vector[20][0]);
	const __m256i c1_sh4=_mm256_load_si256( (__m256i *) &mask_vector[21][0]);
	const __m256i c2_sh4=_mm256_load_si256( (__m256i *) &mask_vector[22][0]);
	const __m256i c3_sh4=_mm256_load_si256( (__m256i *) &mask_vector[23][0]);
	const __m256i c4_sh4=_mm256_load_si256( (__m256i *) &mask_vector[24][0]);

	const __m256i c0_sh5=_mm256_load_si256( (__m256i *) &mask_vector[25][0]);
	const __m256i c1_sh5=_mm256_load_si256( (__m256i *) &mask_vector[26][0]);
	const __m256i c2_sh5=_mm256_load_si256( (__m256i *) &mask_vector[27][0]);
	const __m256i c3_sh5=_mm256_load_si256( (__m256i *) &mask_vector[28][0]);
	const __m256i c4_sh5=_mm256_load_si256( (__m256i *) &mask_vector[29][0]);


	//const __m256i f  = _mm256_set_epi16(52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759);
	//const __m256i mask  = _mm256_set_epi8(0,0,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255);
	//const __m256i mask2  = _mm256_set_epi8(0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0);
	//const __m256i mask6  = _mm256_set_epi8(0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0);

	const __m256i mask3  = _mm256_set_epi16(0,0,0,0,0,0,0,65535,0,0,0,0,0,0,0,0);
	const __m256i mask_prelude  = _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0);

	const __m256i output_mask      = _mm256_set_epi16(0,0,0,65535,0,0,65535,0,0,65535,0,0,65535,0,0,65535);
	const __m256i output_mask_sh1  = _mm256_set_epi16(0,0,65535,0,0,65535,0,0,65535,0,0,65535,0,0,65535,0);
	const __m256i output_mask_sh2  = _mm256_set_epi16(0,0,0,    0,65535,0,0,65535,0,0,65535,0,0,65535,0,0);



	const unsigned int REMINDER_ITERATIONS =  (M-((((M-32)/28)*28)+28)); //M-(last_col_value+28)
	//printf("\n%d",REMINDER_ITERATIONS);


	const unsigned int division_case=prepare_for_division(divisor); //determine which is the division case (A, B or C)
	const __m256i f = _mm256_load_si256( (__m256i *) &f_vector[0]);// initialize the division vector

//printf("\n%d %d",division_case,b);


#pragma omp parallel
{

unsigned int row,col;
register __m256i r0,r1,r2,r3,r4,m0,m1,m2,m3,m4;
__m256i output_row0_even,output_row1_even,output_row0_odd,output_row1_odd; //these are for processing row #0 and #1 only.
__m256i output_even,output_odd;
__m256i m0_prelude_row0,m0_prelude_row1;


/*---------------------- Gaussian Blur ---------------------------------*/

    #pragma omp for schedule(static)
	for (row = 2; row < N-2; row++) {

		if (row==2){//in this case I calculate filt[0][:] and filt[1][:] too. No extra loads or multiplications are required for this
			  for (col = 0; col <= M-32; col+=28){

				  if (col==0){//this is a special case as the mask gets outside of the array (it is like adding two zeros in the beginning of frame[][]

						//load the 5 rows
						r0=_mm256_loadu_si256( (__m256i *) &frame1[row-2][0]);
						r1=_mm256_loadu_si256( (__m256i *) &frame1[row-1][0]);
						r2=_mm256_loadu_si256( (__m256i *) &frame1[row][0]);
						r3=_mm256_loadu_si256( (__m256i *) &frame1[row+1][0]);
						r4=_mm256_loadu_si256( (__m256i *) &frame1[row+2][0]);

				  			//START - extra code needed for prelude
				  					m0=_mm256_slli_si256(r0,2);//shift 2 elements left - equivalent to filling with two zeros inthe beginning
				  					m1=_mm256_slli_si256(r1,2);//shift 2 elements left - equivalent to filling with two zeros inthe beginning
				  					m2=_mm256_slli_si256(r2,2);//shift 2 elements left - equivalent to filling with two zeros inthe beginning
				  					m3=_mm256_slli_si256(r3,2);//shift 2 elements left - equivalent to filling with two zeros inthe beginning
				  					m4=_mm256_slli_si256(r4,2);//shift 2 elements left - equivalent to filling with two zeros inthe beginning

				  					r0=_mm256_and_si256(r0,mask_prelude);
				  					r0=_mm256_permute2f128_si256(r0,r0,1);
				  					r0=_mm256_srli_si256(r0,14);//shift 14 elements
				  					r0=_mm256_add_epi16(m0,r0);

				  					r1=_mm256_and_si256(r1,mask_prelude);
				  					r1=_mm256_permute2f128_si256(r1,r1,1);
				  					r1=_mm256_srli_si256(r1,14);//shift 14 elements
				  					r1=_mm256_add_epi16(m1,r1);

				  					r2=_mm256_and_si256(r2,mask_prelude);
				  					r2=_mm256_permute2f128_si256(r2,r2,1);
				  					r2=_mm256_srli_si256(r2,14);//shift 14 elements
				  					r2=_mm256_add_epi16(m2,r2);

				  					r3=_mm256_and_si256(r3,mask_prelude);
				  					r3=_mm256_permute2f128_si256(r3,r3,1);
				  					r3=_mm256_srli_si256(r3,14);//shift 14 elements
				  					r3=_mm256_add_epi16(m3,r3);

				  					r4=_mm256_and_si256(r4,mask_prelude);
				  					r4=_mm256_permute2f128_si256(r4,r4,1);
				  					r4=_mm256_srli_si256(r4,14);//shift 14 elements
				  					r4=_mm256_add_epi16(m4,r4);

				  			//END - extra code needed for prelude
				  		}
				  else {
						//load the 5 rows
						r0=_mm256_loadu_si256( (__m256i *) &frame1[row-2][col-2]);
						r1=_mm256_loadu_si256( (__m256i *) &frame1[row-1][col-2]);
						r2=_mm256_loadu_si256( (__m256i *) &frame1[row][col-2]);
						r3=_mm256_loadu_si256( (__m256i *) &frame1[row+1][col-2]);
						r4=_mm256_loadu_si256( (__m256i *) &frame1[row+2][col-2]);
				  }

				//col iteration computes output pixels of 2,8,14,20,26
				// col+1 iteration computes output pixels of 3,9,15,21,27
				// col+2 iteration computes output pixels of 4,10,16,22,28
				// col+3 iteration computes output pixels of 5,11,17,23,29
				// col+4 iteration computes output pixels of 6,12,18,24,30
				// col+5 iteration computes output pixels of 7,13,19,25,31
				//afterwards, col2 becomes 32 and repeat the above process

				//1st col iteration

				  //--------------row=2
				//multiply with the mask
				m0=_mm256_maddubs_epi16(r0,c0);
				m1=_mm256_maddubs_epi16(r1,c1);
				m2=_mm256_maddubs_epi16(r2,c2);
				m3=_mm256_maddubs_epi16(r3,c3);
				m4=_mm256_maddubs_epi16(r4,c4);

				//vertical add
				m0=_mm256_add_epi16(m0,m1);
				m0=_mm256_add_epi16(m0,m2);
				m0=_mm256_add_epi16(m0,m3);
				m0=_mm256_add_epi16(m0,m4);


				m1=_mm256_srli_si256(m0,2);
				m2=_mm256_add_epi16(m1,m0);

				m1=_mm256_srli_si256(m0,4);
				m2=_mm256_add_epi16(m1,m2);

				//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
				//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
				m4=_mm256_and_si256(m0,mask3);
				m4=_mm256_permute2f128_si256(m4,m4,1);
				m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions
				m2=_mm256_add_epi16(m2,m4);

				//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
				output_even=_mm256_and_si256(m2,output_mask);

				//--------------row=0
				//multiply with the mask
				m0=_mm256_maddubs_epi16(r0,c2);
				m1=_mm256_maddubs_epi16(r1,c3);
				m2=_mm256_maddubs_epi16(r2,c4);

				//vertical add
				m0=_mm256_add_epi16(m0,m1);
				m0=_mm256_add_epi16(m0,m2);


				m1=_mm256_srli_si256(m0,2);
				m2=_mm256_add_epi16(m1,m0);

				m1=_mm256_srli_si256(m0,4);
				m2=_mm256_add_epi16(m1,m2);

				//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
				//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
				m4=_mm256_and_si256(m0,mask3);
				m4=_mm256_permute2f128_si256(m4,m4,1);
				m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions
				m2=_mm256_add_epi16(m2,m4);

				//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
				output_row0_even=_mm256_and_si256(m2,output_mask);

				//--------------row=1
				//multiply with the mask
				m0=_mm256_maddubs_epi16(r0,c1);
				m1=_mm256_maddubs_epi16(r1,c2);
				m2=_mm256_maddubs_epi16(r2,c3);
				m3=_mm256_maddubs_epi16(r3,c4);

				//vertical add
				m0=_mm256_add_epi16(m0,m1);
				m0=_mm256_add_epi16(m0,m2);
				m0=_mm256_add_epi16(m0,m3);

				m1=_mm256_srli_si256(m0,2);
				m2=_mm256_add_epi16(m1,m0);

				m1=_mm256_srli_si256(m0,4);
				m2=_mm256_add_epi16(m1,m2);

				//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
				//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
				m4=_mm256_and_si256(m0,mask3);
				m4=_mm256_permute2f128_si256(m4,m4,1);
				m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions
				m2=_mm256_add_epi16(m2,m4);

				//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
				output_row1_even=_mm256_and_si256(m2,output_mask);

				//2nd col iteration
				//multiply with the mask

				//----row=2
				m0=_mm256_maddubs_epi16(r0,c0_sh1);
				m1=_mm256_maddubs_epi16(r1,c1_sh1);
				m2=_mm256_maddubs_epi16(r2,c2_sh1);
				m3=_mm256_maddubs_epi16(r3,c3_sh1);
				m4=_mm256_maddubs_epi16(r4,c4_sh1);

				//vertical add
				m0=_mm256_add_epi16(m0,m1);
				m0=_mm256_add_epi16(m0,m2);
				m0=_mm256_add_epi16(m0,m3);
				m0=_mm256_add_epi16(m0,m4);


				m1=_mm256_srli_si256(m0,2);
				m2=_mm256_add_epi16(m1,m0);

				m1=_mm256_srli_si256(m0,4);
				m2=_mm256_add_epi16(m1,m2);

				//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
				//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
				m4=_mm256_and_si256(m0,mask3);
				m4=_mm256_permute2f128_si256(m4,m4,1);
				m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions
				m2=_mm256_add_epi16(m2,m4);

				//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
				output_odd = _mm256_and_si256(m2,output_mask);

				//----row=0
				m0=_mm256_maddubs_epi16(r0,c2_sh1);
				m1=_mm256_maddubs_epi16(r1,c3_sh1);
				m2=_mm256_maddubs_epi16(r2,c4_sh1);

				//vertical add
				m0=_mm256_add_epi16(m0,m1);
				m0=_mm256_add_epi16(m0,m2);


				m1=_mm256_srli_si256(m0,2);
				m2=_mm256_add_epi16(m1,m0);

				m1=_mm256_srli_si256(m0,4);
				m2=_mm256_add_epi16(m1,m2);

				//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
				//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
				m4=_mm256_and_si256(m0,mask3);
				m4=_mm256_permute2f128_si256(m4,m4,1);
				m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions
				m2=_mm256_add_epi16(m2,m4);

				//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
				output_row0_odd = _mm256_and_si256(m2,output_mask);

				//----row=1
				m0=_mm256_maddubs_epi16(r0,c1_sh1);
				m1=_mm256_maddubs_epi16(r1,c2_sh1);
				m2=_mm256_maddubs_epi16(r2,c3_sh1);
				m3=_mm256_maddubs_epi16(r3,c4_sh1);

				//vertical add
				m0=_mm256_add_epi16(m0,m1);
				m0=_mm256_add_epi16(m0,m2);
				m0=_mm256_add_epi16(m0,m3);


				m1=_mm256_srli_si256(m0,2);
				m2=_mm256_add_epi16(m1,m0);

				m1=_mm256_srli_si256(m0,4);
				m2=_mm256_add_epi16(m1,m2);

				//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
				//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
				m4=_mm256_and_si256(m0,mask3);
				m4=_mm256_permute2f128_si256(m4,m4,1);
				m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions
				m2=_mm256_add_epi16(m2,m4);

				//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
				output_row1_odd = _mm256_and_si256(m2,output_mask);

		                 //3rd col iteration
				//---row=2
				//multiply with the mask
				m0=_mm256_maddubs_epi16(r0,c0_sh2);
				m1=_mm256_maddubs_epi16(r1,c1_sh2);
				m2=_mm256_maddubs_epi16(r2,c2_sh2);
				m3=_mm256_maddubs_epi16(r3,c3_sh2);
				m4=_mm256_maddubs_epi16(r4,c4_sh2);

				//vertical add
				m0=_mm256_add_epi16(m0,m1);
				m0=_mm256_add_epi16(m0,m2);
				m0=_mm256_add_epi16(m0,m3);
				m0=_mm256_add_epi16(m0,m4);


				m1=_mm256_srli_si256(m0,2);
				m2=_mm256_add_epi16(m1,m0);

				//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,_mm256_extract_epi16(m2,8),0,0,0,0,0,0,0);
				m4=_mm256_and_si256(m2,mask3);
				m4=_mm256_permute2f128_si256(m4,m4,1);
				m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions

				m1=_mm256_srli_si256(m0,4);
				m2=_mm256_add_epi16(m1,m2);
				m2=_mm256_add_epi16(m2,m4);

				//m2 has 16 16bit values now. the results I need are in positions 1,4,7,10,13. keep only those, discard others
				m2 = _mm256_and_si256(m2,output_mask_sh1);
				output_even = _mm256_add_epi16(output_even,m2);

				//---row=0
				//multiply with the mask

				m0=_mm256_maddubs_epi16(r0,c2_sh2);
				m1=_mm256_maddubs_epi16(r1,c3_sh2);
				m2=_mm256_maddubs_epi16(r2,c4_sh2);

				//vertical add
				m0=_mm256_add_epi16(m0,m1);
				m0=_mm256_add_epi16(m0,m2);


				m1=_mm256_srli_si256(m0,2);
				m2=_mm256_add_epi16(m1,m0);

				//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,_mm256_extract_epi16(m2,8),0,0,0,0,0,0,0);
				m4=_mm256_and_si256(m2,mask3);
				m4=_mm256_permute2f128_si256(m4,m4,1);
				m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions

				m1=_mm256_srli_si256(m0,4);
				m2=_mm256_add_epi16(m1,m2);

				//prepei na prosthesw to pixel pu thelw sto r2 kai meta na kanw diairesi me ola mazi

				m2=_mm256_add_epi16(m2,m4);

				//m2 has 16 16bit values now. the results I need are in positions 1,4,7,10,13. keep only those, discard others
				m2 = _mm256_and_si256(m2,output_mask_sh1);
				output_row0_even = _mm256_add_epi16(output_row0_even,m2);


				//---row=1
				//multiply with the mask

				m0=_mm256_maddubs_epi16(r0,c1_sh2);
				m1=_mm256_maddubs_epi16(r1,c2_sh2);
				m2=_mm256_maddubs_epi16(r2,c3_sh2);
				m3=_mm256_maddubs_epi16(r3,c4_sh2);

				//vertical add
				m0=_mm256_add_epi16(m0,m1);
				m0=_mm256_add_epi16(m0,m2);
				m0=_mm256_add_epi16(m0,m3);

				m1=_mm256_srli_si256(m0,2);
				m2=_mm256_add_epi16(m1,m0);

				//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,_mm256_extract_epi16(m2,8),0,0,0,0,0,0,0);
				m4=_mm256_and_si256(m2,mask3);
				m4=_mm256_permute2f128_si256(m4,m4,1);
				m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions

				m1=_mm256_srli_si256(m0,4);
				m2=_mm256_add_epi16(m1,m2);

				//prepei na prosthesw to pixel pu thelw sto r2 kai meta na kanw diairesi me ola mazi

				m2=_mm256_add_epi16(m2,m4);

				//m2 has 16 16bit values now. the results I need are in positions 1,4,7,10,13. keep only those, discard others
				m2 = _mm256_and_si256(m2,output_mask_sh1);
				output_row1_even = _mm256_add_epi16(output_row1_even,m2);

				//4th col iteration

				//4th col iteration

				//multiply with the mask
				m0=_mm256_maddubs_epi16(r0,c0_sh3);
				m1=_mm256_maddubs_epi16(r1,c1_sh3);
				m2=_mm256_maddubs_epi16(r2,c2_sh3);
				m3=_mm256_maddubs_epi16(r3,c3_sh3);
				m4=_mm256_maddubs_epi16(r4,c4_sh3);

				//vertical add
				m0=_mm256_add_epi16(m0,m1);
				m2=_mm256_add_epi16(m2,m3);
				m0=_mm256_add_epi16(m0,m2);
				m0=_mm256_add_epi16(m0,m4);

				//hozizontal additions
				//hadd(3:8)
				//hadd(9:14)
				//hadd(15:20)
				//hadd(21:26)
				//hadd(27:31)
				//result after division will be in 2,8,14,20,26

				m1=_mm256_srli_si256(m0,2);
				m4=_mm256_add_epi16(m1,m0);

				m1=_mm256_srli_si256(m0,4);
				m2=_mm256_add_epi16(m1,m4);

				//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
				//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
				m4=_mm256_and_si256(m4,mask3);
				m4=_mm256_permute2f128_si256(m4,m4,1);
				m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions

				m2=_mm256_add_epi16(m2,m4);

				//m2 has 16 16bit values now. the results I need are in positions 1,4,7,10,13. keep only those, discard others
				m2 = _mm256_and_si256(m2,output_mask_sh1);
				output_odd = _mm256_add_epi16(output_odd,m2);

				//row0
				//multiply with the mask
								m0=_mm256_maddubs_epi16(r0,c2_sh3);
								m1=_mm256_maddubs_epi16(r1,c3_sh3);
								m2=_mm256_maddubs_epi16(r2,c4_sh3);


								//vertical add
								m0=_mm256_add_epi16(m0,m1);
								m0=_mm256_add_epi16(m0,m2);


								m1=_mm256_srli_si256(m0,2);
								m4=_mm256_add_epi16(m1,m0);

								m1=_mm256_srli_si256(m0,4);
								m2=_mm256_add_epi16(m1,m4);

								//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
								//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
								m4=_mm256_and_si256(m4,mask3);
								m4=_mm256_permute2f128_si256(m4,m4,1);
								m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions

								m2=_mm256_add_epi16(m2,m4);

								//m2 has 16 16bit values now. the results I need are in positions 1,4,7,10,13. keep only those, discard others
								m2 = _mm256_and_si256(m2,output_mask_sh1);
								output_row0_odd = _mm256_add_epi16(output_row0_odd,m2);


								//-----------row1
								//multiply with the mask
												m0=_mm256_maddubs_epi16(r0,c1_sh3);
												m1=_mm256_maddubs_epi16(r1,c2_sh3);
												m2=_mm256_maddubs_epi16(r2,c3_sh3);
												m3=_mm256_maddubs_epi16(r3,c4_sh3);

												//vertical add
												m0=_mm256_add_epi16(m0,m1);
												m0=_mm256_add_epi16(m0,m2);
												m0=_mm256_add_epi16(m0,m3);

												m1=_mm256_srli_si256(m0,2);
												m4=_mm256_add_epi16(m1,m0);

												m1=_mm256_srli_si256(m0,4);
												m2=_mm256_add_epi16(m1,m4);

												//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
												//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
												m4=_mm256_and_si256(m4,mask3);
												m4=_mm256_permute2f128_si256(m4,m4,1);
												m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions

												m2=_mm256_add_epi16(m2,m4);

												//m2 has 16 16bit values now. the results I need are in positions 1,4,7,10,13. keep only those, discard others
												m2 = _mm256_and_si256(m2,output_mask_sh1);
												output_row1_odd = _mm256_add_epi16(output_row1_odd,m2);



				//5th col iteration
				//multiply with the mask
				m0=_mm256_maddubs_epi16(r0,c0_sh4);
				m1=_mm256_maddubs_epi16(r1,c1_sh4);
				m2=_mm256_maddubs_epi16(r2,c2_sh4);
				m3=_mm256_maddubs_epi16(r3,c3_sh4);
				m4=_mm256_maddubs_epi16(r4,c4_sh4);

				//vertical add
				m0=_mm256_add_epi16(m0,m1);
				m2=_mm256_add_epi16(m2,m3);
				m0=_mm256_add_epi16(m0,m2);
				m0=_mm256_add_epi16(m0,m4);

				//hozizontal additions
				//hadd(4:9)
				//hadd(10:15)
				//hadd(16:21)
				//hadd(22:27)
				//result after division will be in 4,10,16,22

				m1=_mm256_srli_si256(m0,2);
				m2=_mm256_add_epi16(m1,m0);

				m1=_mm256_srli_si256(m0,4);
				m2=_mm256_add_epi16(m1,m2);


				//m2 has 16 16bit values now. the results I need are in positions 2,5,8,11. keep only those, discard others
				m2 = _mm256_and_si256(m2,output_mask_sh2);
				output_even = _mm256_add_epi16(output_even,m2);

				//row0
				//multiply with the mask
				m0=_mm256_maddubs_epi16(r0,c2_sh4);
				m1=_mm256_maddubs_epi16(r1,c3_sh4);
				m2=_mm256_maddubs_epi16(r2,c4_sh4);


				//vertical add
				m0=_mm256_add_epi16(m0,m1);
				m0=_mm256_add_epi16(m0,m2);


				//hozizontal additions
				//hadd(4:9)
				//hadd(10:15)
				//hadd(16:21)
				//hadd(22:27)
				//result after division will be in 4,10,16,22

				m1=_mm256_srli_si256(m0,2);
				m2=_mm256_add_epi16(m1,m0);

				m1=_mm256_srli_si256(m0,4);
				m2=_mm256_add_epi16(m1,m2);


				//m2 has 16 16bit values now. the results I need are in positions 2,5,8,11. keep only those, discard others
				m2 = _mm256_and_si256(m2,output_mask_sh2);
				output_row0_even = _mm256_add_epi16(output_row0_even,m2);

				//row1
				//multiply with the mask
				m0=_mm256_maddubs_epi16(r0,c1_sh4);
				m1=_mm256_maddubs_epi16(r1,c2_sh4);
				m2=_mm256_maddubs_epi16(r2,c3_sh4);
				m3=_mm256_maddubs_epi16(r3,c4_sh4);


				//vertical add
				m0=_mm256_add_epi16(m0,m1);
				m0=_mm256_add_epi16(m0,m2);
				m0=_mm256_add_epi16(m0,m3);


				m1=_mm256_srli_si256(m0,2);
				m2=_mm256_add_epi16(m1,m0);

				m1=_mm256_srli_si256(m0,4);
				m2=_mm256_add_epi16(m1,m2);


				//m2 has 16 16bit values now. the results I need are in positions 2,5,8,11. keep only those, discard others
				m2 = _mm256_and_si256(m2,output_mask_sh2);
				output_row1_even = _mm256_add_epi16(output_row1_even,m2);


		                 //6th col iteration
				//multiply with the mask
				m0=_mm256_maddubs_epi16(r0,c0_sh5);
				m1=_mm256_maddubs_epi16(r1,c1_sh5);
				m2=_mm256_maddubs_epi16(r2,c2_sh5);
				m3=_mm256_maddubs_epi16(r3,c3_sh5);
				m4=_mm256_maddubs_epi16(r4,c4_sh5);

				//vertical add
				m0=_mm256_add_epi16(m0,m1);
				m2=_mm256_add_epi16(m2,m3);
				m0=_mm256_add_epi16(m0,m2);
				m0=_mm256_add_epi16(m0,m4);

				//hozizontal additions
				//hadd(5:10)
				//hadd(11:16)
				//hadd(17:22)
				//hadd(23:28)
				//result after division will be in 4,10,16,22

				m1=_mm256_srli_si256(m0,2);
				m2=_mm256_add_epi16(m1,m0);

				m1=_mm256_srli_si256(m0,4);
				m2=_mm256_add_epi16(m1,m2);

				//m2 has 16 16bit values now. the results I need are in positions 2,5,8,11. keep only those, discard others
				m2 = _mm256_and_si256(m2,output_mask_sh2);
				output_odd = _mm256_add_epi16(output_odd,m2);

				//now division follows
				output_even=division(division_case,output_even,f);
				output_odd=division(division_case,output_odd,f);

				//shift odd 1 position and add to even
				output_odd = _mm256_slli_si256(output_odd,1);
				output_even = _mm256_add_epi8(output_even,output_odd);

				_mm256_storeu_si256( (__m256i *) &filt[row][col],output_even );

				//row0
				//multiply with the mask
				m0=_mm256_maddubs_epi16(r0,c2_sh5);
				m1=_mm256_maddubs_epi16(r1,c3_sh5);
				m2=_mm256_maddubs_epi16(r2,c4_sh5);

				//vertical add
				m0=_mm256_add_epi16(m0,m1);
				m0=_mm256_add_epi16(m0,m2);


				m1=_mm256_srli_si256(m0,2);
				m2=_mm256_add_epi16(m1,m0);

				m1=_mm256_srli_si256(m0,4);
				m2=_mm256_add_epi16(m1,m2);

				//m2 has 16 16bit values now. the results I need are in positions 2,5,8,11. keep only those, discard others
				m2 = _mm256_and_si256(m2,output_mask_sh2);
				output_row0_odd = _mm256_add_epi16(output_row0_odd,m2);

				//now division follows
				output_row0_even=division(division_case,output_row0_even,f);
				output_row0_odd=division(division_case,output_row0_odd,f);

				//shift odd 1 position and add to even
				output_row0_odd = _mm256_slli_si256(output_row0_odd,1);
				output_row0_even = _mm256_add_epi8(output_row0_even,output_row0_odd);


				_mm256_storeu_si256( (__m256i *) &filt[0][col],output_row0_even );

				//row1

				//multiply with the mask
				m0=_mm256_maddubs_epi16(r0,c1_sh5);
				m1=_mm256_maddubs_epi16(r1,c2_sh5);
				m2=_mm256_maddubs_epi16(r2,c3_sh5);
				m3=_mm256_maddubs_epi16(r3,c4_sh5);

				//vertical add
				m0=_mm256_add_epi16(m0,m1);
				m0=_mm256_add_epi16(m0,m2);
				m0=_mm256_add_epi16(m0,m3);


				m1=_mm256_srli_si256(m0,2);
				m2=_mm256_add_epi16(m1,m0);

				m1=_mm256_srli_si256(m0,4);
				m2=_mm256_add_epi16(m1,m2);

				//m2 has 16 16bit values now. the results I need are in positions 2,5,8,11. keep only those, discard others
				m2 = _mm256_and_si256(m2,output_mask_sh2);
				output_row1_odd = _mm256_add_epi16(output_row1_odd,m2);

				//now division follows
				output_row1_even=division(division_case,output_row1_even,f);
				output_row1_odd=division(division_case,output_row1_odd,f);

				//shift odd 1 position and add to even
				output_row1_odd = _mm256_slli_si256(output_row1_odd,1);
				output_row1_even = _mm256_add_epi8(output_row1_even,output_row1_odd);


				_mm256_storeu_si256( (__m256i *) &filt[1][col],output_row1_even );


				}


		    conv_loop_reminder_first_less_div(frame1,filt,M,N,col,REMINDER_ITERATIONS,division_case,mask_vector,f,divisor,filter5x5);
		}

		else if (row==N-3){//in this case I calculate filt[N-2][:] and filt[N-1][:] too. Below row0 refers to row=N-2 and row1 refers to row=N-1
			for (col = 0; col <= M-32; col+=28){
						 //last col value that does not read outside of the array bounds is (col<M-29)

							  if (col==0){

									//load the 5 rows
									r0=_mm256_loadu_si256( (__m256i *) &frame1[row-2][0]);
									r1=_mm256_loadu_si256( (__m256i *) &frame1[row-1][0]);
									r2=_mm256_loadu_si256( (__m256i *) &frame1[row][0]);
									r3=_mm256_loadu_si256( (__m256i *) &frame1[row+1][0]);
									r4=_mm256_loadu_si256( (__m256i *) &frame1[row+2][0]);

							  			//START - extra code needed for prelude
							  					m0=_mm256_slli_si256(r0,2);//shift 2 elements left - equivalent to filling with two zeros inthe beginning
							  					m1=_mm256_slli_si256(r1,2);//shift 2 elements left - equivalent to filling with two zeros inthe beginning
							  					m2=_mm256_slli_si256(r2,2);//shift 2 elements left - equivalent to filling with two zeros inthe beginning
							  					m3=_mm256_slli_si256(r3,2);//shift 2 elements left - equivalent to filling with two zeros inthe beginning
							  					m4=_mm256_slli_si256(r4,2);//shift 2 elements left - equivalent to filling with two zeros inthe beginning

							  					r0=_mm256_and_si256(r0,mask_prelude);
							  					r0=_mm256_permute2f128_si256(r0,r0,1);
							  					r0=_mm256_srli_si256(r0,14);//shift 14 elements
							  					r0=_mm256_add_epi16(m0,r0);

							  					r1=_mm256_and_si256(r1,mask_prelude);
							  					r1=_mm256_permute2f128_si256(r1,r1,1);
							  					r1=_mm256_srli_si256(r1,14);//shift 14 elements
							  					r1=_mm256_add_epi16(m1,r1);

							  					r2=_mm256_and_si256(r2,mask_prelude);
							  					r2=_mm256_permute2f128_si256(r2,r2,1);
							  					r2=_mm256_srli_si256(r2,14);//shift 14 elements
							  					r2=_mm256_add_epi16(m2,r2);

							  					r3=_mm256_and_si256(r3,mask_prelude);
							  					r3=_mm256_permute2f128_si256(r3,r3,1);
							  					r3=_mm256_srli_si256(r3,14);//shift 14 elements
							  					r3=_mm256_add_epi16(m3,r3);

							  					r4=_mm256_and_si256(r4,mask_prelude);
							  					r4=_mm256_permute2f128_si256(r4,r4,1);
							  					r4=_mm256_srli_si256(r4,14);//shift 14 elements
							  					r4=_mm256_add_epi16(m4,r4);

							  			//END - extra code needed for prelude
							  		}
							  else {
									//load the 5 rows
									r0=_mm256_loadu_si256( (__m256i *) &frame1[row-2][col-2]);
									r1=_mm256_loadu_si256( (__m256i *) &frame1[row-1][col-2]);
									r2=_mm256_loadu_si256( (__m256i *) &frame1[row][col-2]);
									r3=_mm256_loadu_si256( (__m256i *) &frame1[row+1][col-2]);
									r4=_mm256_loadu_si256( (__m256i *) &frame1[row+2][col-2]);
							  }

							//col iteration computes output pixels of 2,8,14,20,26
							// col+1 iteration computes output pixels of 3,9,15,21,27
							// col+2 iteration computes output pixels of 4,10,16,22,28
							// col+3 iteration computes output pixels of 5,11,17,23,29
							// col+4 iteration computes output pixels of 6,12,18,24,30
							// col+5 iteration computes output pixels of 7,13,19,25,31
							//afterwards, col2 becomes 32 and repeat the above process

							//1st col iteration
							 //--------------row=2

							//multiply with the mask
							m0=_mm256_maddubs_epi16(r0,c0);
							m1=_mm256_maddubs_epi16(r1,c1);
							m2=_mm256_maddubs_epi16(r2,c2);
							m3=_mm256_maddubs_epi16(r3,c3);
							m4=_mm256_maddubs_epi16(r4,c4);

							//vertical add
							m4=_mm256_add_epi16(m4,m3);
							m4=_mm256_add_epi16(m4,m2);
							m4=_mm256_add_epi16(m4,m1);
							m0=_mm256_add_epi16(m4,m0);

							m1=_mm256_srli_si256(m0,2);
							m2=_mm256_add_epi16(m1,m0);

							m1=_mm256_srli_si256(m0,4);
							m2=_mm256_add_epi16(m1,m2);

							//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
							//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
							m4=_mm256_and_si256(m0,mask3);
							m4=_mm256_permute2f128_si256(m4,m4,1);
							m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions

							m2=_mm256_add_epi16(m2,m4);

							//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
							output_even=_mm256_and_si256(m2,output_mask);

							//--------------row=0
							//multiply with the mask
							m1=_mm256_maddubs_epi16(r1,c0);
							m2=_mm256_maddubs_epi16(r2,c1);
							m3=_mm256_maddubs_epi16(r3,c2);
							m4=_mm256_maddubs_epi16(r4,c3);

							//vertical add
							m4=_mm256_add_epi16(m4,m3);
							m4=_mm256_add_epi16(m4,m2);
							m4=_mm256_add_epi16(m4,m1); m0_prelude_row0=m4;

							m1=_mm256_srli_si256(m0_prelude_row0,2);
							m2=_mm256_add_epi16(m1,m0_prelude_row0);

							m1=_mm256_srli_si256(m0_prelude_row0,4);
							m2=_mm256_add_epi16(m1,m2);

							//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
							//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
							m4=_mm256_and_si256(m0_prelude_row0,mask3);
							m4=_mm256_permute2f128_si256(m4,m4,1);
							m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions

							m2=_mm256_add_epi16(m2,m4);

							//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
							output_row0_even=_mm256_and_si256(m2,output_mask);

							//--------------row=1
							//multiply with the mask
							m2=_mm256_maddubs_epi16(r2,c0);
							m3=_mm256_maddubs_epi16(r3,c1);
							m4=_mm256_maddubs_epi16(r4,c2);


							//vertical add
							m4=_mm256_add_epi16(m4,m3);
							m4=_mm256_add_epi16(m4,m2);m0_prelude_row1=m4;



							m1=_mm256_srli_si256(m0_prelude_row1,2);
							m2=_mm256_add_epi16(m1,m0_prelude_row1);

							m1=_mm256_srli_si256(m0_prelude_row1,4);
							m2=_mm256_add_epi16(m1,m2);

							//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
							//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
							m4=_mm256_and_si256(m0_prelude_row1,mask3);
							m4=_mm256_permute2f128_si256(m4,m4,1);
							m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions

							m2=_mm256_add_epi16(m2,m4);

							//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
							output_row1_even=_mm256_and_si256(m2,output_mask);

							//2nd col iteration
							//----row=2

							//multiply with the mask
							m0=_mm256_maddubs_epi16(r0,c0_sh1);
							m1=_mm256_maddubs_epi16(r1,c1_sh1);
							m2=_mm256_maddubs_epi16(r2,c2_sh1);
							m3=_mm256_maddubs_epi16(r3,c3_sh1);
							m4=_mm256_maddubs_epi16(r4,c4_sh1);

							//vertical add
							m4=_mm256_add_epi16(m4,m3);
							m4=_mm256_add_epi16(m4,m2);
							m4=_mm256_add_epi16(m4,m1);
							m0=_mm256_add_epi16(m4,m0);


							m1=_mm256_srli_si256(m0,2);
							m2=_mm256_add_epi16(m1,m0);

							m1=_mm256_srli_si256(m0,4);
							m2=_mm256_add_epi16(m1,m2);

							//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
							//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
							m4=_mm256_and_si256(m0,mask3);
							m4=_mm256_permute2f128_si256(m4,m4,1);
							m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions
							m2=_mm256_add_epi16(m2,m4);

							//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
							output_odd = _mm256_and_si256(m2,output_mask);

							//----row=0
							//multiply with the mask
							m1=_mm256_maddubs_epi16(r1,c0_sh1);
							m2=_mm256_maddubs_epi16(r2,c1_sh1);
							m3=_mm256_maddubs_epi16(r3,c2_sh1);
							m4=_mm256_maddubs_epi16(r4,c3_sh1);


							//vertical add
							m4=_mm256_add_epi16(m4,m3);
							m4=_mm256_add_epi16(m4,m2);
							m4=_mm256_add_epi16(m4,m1);m0_prelude_row0=m4;


							m1=_mm256_srli_si256(m0_prelude_row0,2);
							m2=_mm256_add_epi16(m1,m0_prelude_row0);

							m1=_mm256_srli_si256(m0_prelude_row0,4);
							m2=_mm256_add_epi16(m1,m2);

							//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
							//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
							m4=_mm256_and_si256(m0_prelude_row0,mask3);
							m4=_mm256_permute2f128_si256(m4,m4,1);
							m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions
							m2=_mm256_add_epi16(m2,m4);

							//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
							output_row0_odd = _mm256_and_si256(m2,output_mask);

							//----row=1
							//multiply with the mask
							m2=_mm256_maddubs_epi16(r2,c0_sh1);
							m3=_mm256_maddubs_epi16(r3,c1_sh1);
							m4=_mm256_maddubs_epi16(r4,c2_sh1);


							//vertical add
							m4=_mm256_add_epi16(m4,m3);
							m4=_mm256_add_epi16(m4,m2);m0_prelude_row1=m4;


							m1=_mm256_srli_si256(m0_prelude_row1,2);
							m2=_mm256_add_epi16(m1,m0_prelude_row1);

							m1=_mm256_srli_si256(m0_prelude_row1,4);
							m2=_mm256_add_epi16(m1,m2);

							//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
							//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
							m4=_mm256_and_si256(m0_prelude_row1,mask3);
							m4=_mm256_permute2f128_si256(m4,m4,1);
							m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions
							m2=_mm256_add_epi16(m2,m4);

							//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
							output_row1_odd = _mm256_and_si256(m2,output_mask);

					                 //3rd col iteration
							//---row=2
							//multiply with the mask
							m0=_mm256_maddubs_epi16(r0,c0_sh2);
							m1=_mm256_maddubs_epi16(r1,c1_sh2);
							m2=_mm256_maddubs_epi16(r2,c2_sh2);
							m3=_mm256_maddubs_epi16(r3,c3_sh2);
							m4=_mm256_maddubs_epi16(r4,c4_sh2);

							//vertical add
							m4=_mm256_add_epi16(m4,m3);
							m4=_mm256_add_epi16(m4,m2);
							m4=_mm256_add_epi16(m4,m1);
							m0=_mm256_add_epi16(m4,m0);


							m1=_mm256_srli_si256(m0,2);
							m2=_mm256_add_epi16(m1,m0);

							//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,_mm256_extract_epi16(m2,8),0,0,0,0,0,0,0);
							m4=_mm256_and_si256(m2,mask3);
							m4=_mm256_permute2f128_si256(m4,m4,1);
							m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions

							m1=_mm256_srli_si256(m0,4);
							m2=_mm256_add_epi16(m1,m2);

							//prepei na prosthesw to pixel pu thelw sto r2 kai meta na kanw diairesi me ola mazi

							m2=_mm256_add_epi16(m2,m4);

							//m2 has 16 16bit values now. the results I need are in positions 1,4,7,10,13. keep only those, discard others
							m2 = _mm256_and_si256(m2,output_mask_sh1);
							output_even = _mm256_add_epi16(output_even,m2);

							//---row=0
							//multiply with the mask
							m1=_mm256_maddubs_epi16(r1,c0_sh2);
							m2=_mm256_maddubs_epi16(r2,c1_sh2);
							m3=_mm256_maddubs_epi16(r3,c2_sh2);
							m4=_mm256_maddubs_epi16(r4,c3_sh2);


							//vertical add
							m4=_mm256_add_epi16(m4,m3);
							m4=_mm256_add_epi16(m4,m2);
							m4=_mm256_add_epi16(m4,m1);m0_prelude_row0=m4;

							m1=_mm256_srli_si256(m0_prelude_row0,2);
							m2=_mm256_add_epi16(m1,m0_prelude_row0);

							//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,_mm256_extract_epi16(m2,8),0,0,0,0,0,0,0);
							m4=_mm256_and_si256(m2,mask3);
							m4=_mm256_permute2f128_si256(m4,m4,1);
							m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions

							m1=_mm256_srli_si256(m0_prelude_row0,4);
							m2=_mm256_add_epi16(m1,m2);

							//prepei na prosthesw to pixel pu thelw sto r2 kai meta na kanw diairesi me ola mazi

							m2=_mm256_add_epi16(m2,m4);

							//m2 has 16 16bit values now. the results I need are in positions 1,4,7,10,13. keep only those, discard others
							m2 = _mm256_and_si256(m2,output_mask_sh1);
							output_row0_even = _mm256_add_epi16(output_row0_even,m2);


							//---row=1
							//multiply with the mask
							m2=_mm256_maddubs_epi16(r2,c0_sh2);
							m3=_mm256_maddubs_epi16(r3,c1_sh2);
							m4=_mm256_maddubs_epi16(r4,c2_sh2);


							//vertical add
							m4=_mm256_add_epi16(m4,m3);
							m4=_mm256_add_epi16(m4,m2);m0_prelude_row1=m4;


							m1=_mm256_srli_si256(m0_prelude_row1,2);
							m2=_mm256_add_epi16(m1,m0_prelude_row1);

							//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,_mm256_extract_epi16(m2,8),0,0,0,0,0,0,0);
							m4=_mm256_and_si256(m2,mask3);
							m4=_mm256_permute2f128_si256(m4,m4,1);
							m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions

							m1=_mm256_srli_si256(m0_prelude_row1,4);
							m2=_mm256_add_epi16(m1,m2);

							//prepei na prosthesw to pixel pu thelw sto r2 kai meta na kanw diairesi me ola mazi

							m2=_mm256_add_epi16(m2,m4);

							//m2 has 16 16bit values now. the results I need are in positions 1,4,7,10,13. keep only those, discard others
							m2 = _mm256_and_si256(m2,output_mask_sh1);
							output_row1_even = _mm256_add_epi16(output_row1_even,m2);

							//4th col iteration
							//multiply with the mask
							m0=_mm256_maddubs_epi16(r0,c0_sh3);
							m1=_mm256_maddubs_epi16(r1,c1_sh3);
							m2=_mm256_maddubs_epi16(r2,c2_sh3);
							m3=_mm256_maddubs_epi16(r3,c3_sh3);
							m4=_mm256_maddubs_epi16(r4,c4_sh3);

							//vertical add
							m0=_mm256_add_epi16(m0,m1);
							m2=_mm256_add_epi16(m2,m3);
							m0=_mm256_add_epi16(m0,m2);
							m0=_mm256_add_epi16(m0,m4);

							//hozizontal additions
							//hadd(3:8)
							//hadd(9:14)
							//hadd(15:20)
							//hadd(21:26)
							//hadd(27:31)
							//result after division will be in 2,8,14,20,26

							m1=_mm256_srli_si256(m0,2);
							m4=_mm256_add_epi16(m1,m0);

							m1=_mm256_srli_si256(m0,4);
							m2=_mm256_add_epi16(m1,m4);

							//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
							//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
							m4=_mm256_and_si256(m4,mask3);
							m4=_mm256_permute2f128_si256(m4,m4,1);
							m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions

							m2=_mm256_add_epi16(m2,m4);

							//m2 has 16 16bit values now. the results I need are in positions 1,4,7,10,13. keep only those, discard others
							m2 = _mm256_and_si256(m2,output_mask_sh1);
							output_odd = _mm256_add_epi16(output_odd,m2);

							//row0
							//multiply with the mask
								m1=_mm256_maddubs_epi16(r1,c0_sh3);
								m2=_mm256_maddubs_epi16(r2,c1_sh3);
								m3=_mm256_maddubs_epi16(r3,c2_sh3);
								m4=_mm256_maddubs_epi16(r4,c3_sh3);

								//vertical add
								m0=_mm256_add_epi16(m1,m2);
								m0=_mm256_add_epi16(m0,m3);
								m0=_mm256_add_epi16(m0,m4);


								m1=_mm256_srli_si256(m0,2);
								m4=_mm256_add_epi16(m1,m0);

								m1=_mm256_srli_si256(m0,4);
								m2=_mm256_add_epi16(m1,m4);

								//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
								//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
								m4=_mm256_and_si256(m4,mask3);
								m4=_mm256_permute2f128_si256(m4,m4,1);
								m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions

								m2=_mm256_add_epi16(m2,m4);

								//m2 has 16 16bit values now. the results I need are in positions 1,4,7,10,13. keep only those, discard others
								m2 = _mm256_and_si256(m2,output_mask_sh1);
								output_row0_odd = _mm256_add_epi16(output_row0_odd,m2);

								//row1
								//multiply with the mask
									m2=_mm256_maddubs_epi16(r2,c0_sh3);
									m3=_mm256_maddubs_epi16(r3,c1_sh3);
									m4=_mm256_maddubs_epi16(r4,c2_sh3);

									//vertical add
									m0=_mm256_add_epi16(m2,m3);
									m0=_mm256_add_epi16(m0,m4);


									m1=_mm256_srli_si256(m0,2);
									m4=_mm256_add_epi16(m1,m0);

									m1=_mm256_srli_si256(m0,4);
									m2=_mm256_add_epi16(m1,m4);

									//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
									//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
									m4=_mm256_and_si256(m4,mask3);
									m4=_mm256_permute2f128_si256(m4,m4,1);
									m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions

									m2=_mm256_add_epi16(m2,m4);

									//m2 has 16 16bit values now. the results I need are in positions 1,4,7,10,13. keep only those, discard others
									m2 = _mm256_and_si256(m2,output_mask_sh1);
									output_row1_odd = _mm256_add_epi16(output_row1_odd,m2);


							//5th col iteration
							//multiply with the mask
							m0=_mm256_maddubs_epi16(r0,c0_sh4);
							m1=_mm256_maddubs_epi16(r1,c1_sh4);
							m2=_mm256_maddubs_epi16(r2,c2_sh4);
							m3=_mm256_maddubs_epi16(r3,c3_sh4);
							m4=_mm256_maddubs_epi16(r4,c4_sh4);

							//vertical add
							m0=_mm256_add_epi16(m0,m1);
							m2=_mm256_add_epi16(m2,m3);
							m0=_mm256_add_epi16(m0,m2);
							m0=_mm256_add_epi16(m0,m4);

							//hozizontal additions
							//hadd(4:9)
							//hadd(10:15)
							//hadd(16:21)
							//hadd(22:27)
							//result after division will be in 4,10,16,22

							m1=_mm256_srli_si256(m0,2);
							m2=_mm256_add_epi16(m1,m0);

							m1=_mm256_srli_si256(m0,4);
							m2=_mm256_add_epi16(m1,m2);


							//m2 has 16 16bit values now. the results I need are in positions 2,5,8,11. keep only those, discard others
							m2 = _mm256_and_si256(m2,output_mask_sh2);
							output_even = _mm256_add_epi16(output_even,m2);

							//row0
							//multiply with the mask
							m1=_mm256_maddubs_epi16(r1,c0_sh4);
							m2=_mm256_maddubs_epi16(r2,c1_sh4);
							m3=_mm256_maddubs_epi16(r3,c2_sh4);
							m4=_mm256_maddubs_epi16(r4,c3_sh4);

							//vertical add
							m0=_mm256_add_epi16(m1,m2);
							m0=_mm256_add_epi16(m0,m3);
							m0=_mm256_add_epi16(m0,m4);


							m1=_mm256_srli_si256(m0,2);
							m2=_mm256_add_epi16(m1,m0);

							m1=_mm256_srli_si256(m0,4);
							m2=_mm256_add_epi16(m1,m2);


							//m2 has 16 16bit values now. the results I need are in positions 2,5,8,11. keep only those, discard others
							m2 = _mm256_and_si256(m2,output_mask_sh2);
							output_row0_even = _mm256_add_epi16(output_row0_even,m2);

							//row1
							//multiply with the mask
							m2=_mm256_maddubs_epi16(r2,c0_sh4);
							m3=_mm256_maddubs_epi16(r3,c1_sh4);
							m4=_mm256_maddubs_epi16(r4,c2_sh4);

							//vertical add
							m0=_mm256_add_epi16(m2,m3);
							m0=_mm256_add_epi16(m0,m4);


							m1=_mm256_srli_si256(m0,2);
							m2=_mm256_add_epi16(m1,m0);

							m1=_mm256_srli_si256(m0,4);
							m2=_mm256_add_epi16(m1,m2);


							//m2 has 16 16bit values now. the results I need are in positions 2,5,8,11. keep only those, discard others
							m2 = _mm256_and_si256(m2,output_mask_sh2);
							output_row1_even = _mm256_add_epi16(output_row1_even,m2);

					                 //6th col iteration
							//multiply with the mask
							m0=_mm256_maddubs_epi16(r0,c0_sh5);
							m1=_mm256_maddubs_epi16(r1,c1_sh5);
							m2=_mm256_maddubs_epi16(r2,c2_sh5);
							m3=_mm256_maddubs_epi16(r3,c3_sh5);
							m4=_mm256_maddubs_epi16(r4,c4_sh5);

							//vertical add
							m0=_mm256_add_epi16(m0,m1);
							m2=_mm256_add_epi16(m2,m3);
							m0=_mm256_add_epi16(m0,m2);
							m0=_mm256_add_epi16(m0,m4);

							//hozizontal additions
							//hadd(5:10)
							//hadd(11:16)
							//hadd(17:22)
							//hadd(23:28)
							//result after division will be in 4,10,16,22

							m1=_mm256_srli_si256(m0,2);
							m2=_mm256_add_epi16(m1,m0);

							m1=_mm256_srli_si256(m0,4);
							m2=_mm256_add_epi16(m1,m2);

							//m2 has 16 16bit values now. the results I need are in positions 2,5,8,11. keep only those, discard others
							m2 = _mm256_and_si256(m2,output_mask_sh2);
							output_odd = _mm256_add_epi16(output_odd,m2);

							//now division follows
							output_even=division(division_case,output_even,f);
							output_odd=division(division_case,output_odd,f);

							//shift odd 1 position and add to even
							output_odd = _mm256_slli_si256(output_odd,1);
							output_even = _mm256_add_epi8(output_even,output_odd);


							_mm256_storeu_si256( (__m256i *) &filt[row][col],output_even );

							//row0
							//multiply with the mask
							m1=_mm256_maddubs_epi16(r1,c0_sh5);
							m2=_mm256_maddubs_epi16(r2,c1_sh5);
							m3=_mm256_maddubs_epi16(r3,c2_sh5);
							m4=_mm256_maddubs_epi16(r4,c3_sh5);

							//vertical add
							m0=_mm256_add_epi16(m1,m2);
							m0=_mm256_add_epi16(m0,m3);
							m0=_mm256_add_epi16(m0,m4);


							m1=_mm256_srli_si256(m0,2);
							m2=_mm256_add_epi16(m1,m0);

							m1=_mm256_srli_si256(m0,4);
							m2=_mm256_add_epi16(m1,m2);

							//m2 has 16 16bit values now. the results I need are in positions 2,5,8,11. keep only those, discard others
							m2 = _mm256_and_si256(m2,output_mask_sh2);
							output_row0_odd = _mm256_add_epi16(output_row0_odd,m2);

							//now division follows
							output_row0_even=division(division_case,output_row0_even,f);
							output_row0_odd=division(division_case,output_row0_odd,f);

							//shift odd 1 position and add to even
							output_row0_odd = _mm256_slli_si256(output_row0_odd,1);
							output_row0_even = _mm256_add_epi8(output_row0_even,output_row0_odd);


							_mm256_storeu_si256( (__m256i *) &filt[N-2][col],output_row0_even );

							//row1
							//multiply with the mask
							m2=_mm256_maddubs_epi16(r2,c0_sh5);
							m3=_mm256_maddubs_epi16(r3,c1_sh5);
							m4=_mm256_maddubs_epi16(r4,c2_sh5);

							//vertical add
							m0=_mm256_add_epi16(m2,m3);
							m0=_mm256_add_epi16(m0,m4);

								m1=_mm256_srli_si256(m0,2);
							m2=_mm256_add_epi16(m1,m0);

							m1=_mm256_srli_si256(m0,4);
							m2=_mm256_add_epi16(m1,m2);

							//m2 has 16 16bit values now. the results I need are in positions 2,5,8,11. keep only those, discard others
							m2 = _mm256_and_si256(m2,output_mask_sh2);
							output_row1_odd = _mm256_add_epi16(output_row1_odd,m2);

							//now division follows
							output_row1_even=division(division_case,output_row1_even,f);
							output_row1_odd=division(division_case,output_row1_odd,f);

							//shift odd 1 position and add to even
							output_row1_odd = _mm256_slli_si256(output_row1_odd,1);
							output_row1_even = _mm256_add_epi8(output_row1_even,output_row1_odd);


							_mm256_storeu_si256( (__m256i *) &filt[N-1][col],output_row1_even );


							}

		   conv_loop_reminder_last_less_div(frame1,filt,M,N,col,REMINDER_ITERATIONS,division_case,mask_vector,f,divisor,filter5x5);

					}
	else {


	  for (col = 0; col <= M-32; col+=28){
	 //last col value that does not read outside of the array bounds is (col<M-29)

		  if (col==0){

				//load the 5 rows
				r0=_mm256_loadu_si256( (__m256i *) &frame1[row-2][0]);
				r1=_mm256_loadu_si256( (__m256i *) &frame1[row-1][0]);
				r2=_mm256_loadu_si256( (__m256i *) &frame1[row][0]);
				r3=_mm256_loadu_si256( (__m256i *) &frame1[row+1][0]);
				r4=_mm256_loadu_si256( (__m256i *) &frame1[row+2][0]);

		  			//START - extra code needed for prelude
		  					m0=_mm256_slli_si256(r0,2);//shift 2 elements left - equivalent to filling with two zeros inthe beginning
		  					m1=_mm256_slli_si256(r1,2);//shift 2 elements left - equivalent to filling with two zeros inthe beginning
		  					m2=_mm256_slli_si256(r2,2);//shift 2 elements left - equivalent to filling with two zeros inthe beginning
		  					m3=_mm256_slli_si256(r3,2);//shift 2 elements left - equivalent to filling with two zeros inthe beginning
		  					m4=_mm256_slli_si256(r4,2);//shift 2 elements left - equivalent to filling with two zeros inthe beginning

		  					r0=_mm256_and_si256(r0,mask_prelude);
		  					r0=_mm256_permute2f128_si256(r0,r0,1);
		  					r0=_mm256_srli_si256(r0,14);//shift 14 elements
		  					r0=_mm256_add_epi16(m0,r0);

		  					r1=_mm256_and_si256(r1,mask_prelude);
		  					r1=_mm256_permute2f128_si256(r1,r1,1);
		  					r1=_mm256_srli_si256(r1,14);//shift 14 elements
		  					r1=_mm256_add_epi16(m1,r1);

		  					r2=_mm256_and_si256(r2,mask_prelude);
		  					r2=_mm256_permute2f128_si256(r2,r2,1);
		  					r2=_mm256_srli_si256(r2,14);//shift 14 elements
		  					r2=_mm256_add_epi16(m2,r2);

		  					r3=_mm256_and_si256(r3,mask_prelude);
		  					r3=_mm256_permute2f128_si256(r3,r3,1);
		  					r3=_mm256_srli_si256(r3,14);//shift 14 elements
		  					r3=_mm256_add_epi16(m3,r3);

		  					r4=_mm256_and_si256(r4,mask_prelude);
		  					r4=_mm256_permute2f128_si256(r4,r4,1);
		  					r4=_mm256_srli_si256(r4,14);//shift 14 elements
		  					r4=_mm256_add_epi16(m4,r4);

		  			//END - extra code needed for prelude
		  		}
		  else {
				//load the 5 rows
				r0=_mm256_loadu_si256( (__m256i *) &frame1[row-2][col-2]);
				r1=_mm256_loadu_si256( (__m256i *) &frame1[row-1][col-2]);
				r2=_mm256_loadu_si256( (__m256i *) &frame1[row][col-2]);
				r3=_mm256_loadu_si256( (__m256i *) &frame1[row+1][col-2]);
				r4=_mm256_loadu_si256( (__m256i *) &frame1[row+2][col-2]);
		  }

		//col iteration computes output pixels of 2,8,14,20,26
		// col+1 iteration computes output pixels of 3,9,15,21,27
		// col+2 iteration computes output pixels of 4,10,16,22,28
		// col+3 iteration computes output pixels of 5,11,17,23,29
		// col+4 iteration computes output pixels of 6,12,18,24
		// col+5 iteration computes output pixels of 7,13,19,25
		//afterwards, col becomes 30 and repeat the above process

		//1st col iteration


		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,c0);
		m1=_mm256_maddubs_epi16(r1,c1);
		m2=_mm256_maddubs_epi16(r2,c2);
		m3=_mm256_maddubs_epi16(r3,c3);
		m4=_mm256_maddubs_epi16(r4,c4);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m2=_mm256_add_epi16(m2,m3);
		m0=_mm256_add_epi16(m0,m2);
		m0=_mm256_add_epi16(m0,m4);

		//hozizontal additions
		//hadd(0:5)
		//hadd(6:11)
		//hadd(12:17)
		//hadd(18:23)
		//hadd(24:28)

		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		m1=_mm256_srli_si256(m0,4);
		m2=_mm256_add_epi16(m1,m2);

		//after shifts the 15th element is lost as it cannot be propagated to the 16th position (AVX registers are managed as two seperate SSE registers)
		m4=_mm256_and_si256(m0,mask3);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions
		m2=_mm256_add_epi16(m2,m4);

		//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
		output_even=_mm256_and_si256(m2,output_mask);


		//2nd col iteration
		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,c0_sh1);
		m1=_mm256_maddubs_epi16(r1,c1_sh1);
		m2=_mm256_maddubs_epi16(r2,c2_sh1);
		m3=_mm256_maddubs_epi16(r3,c3_sh1);
		m4=_mm256_maddubs_epi16(r4,c4_sh1);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m2=_mm256_add_epi16(m2,m3);
		m0=_mm256_add_epi16(m0,m2);
		m0=_mm256_add_epi16(m0,m4);

		//hozizontal additions
		//hadd(1:6)
		//hadd(7:12)
		//hadd(13:18)
		//hadd(19:24)
		//hadd(25:29)

		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		m1=_mm256_srli_si256(m0,4);
		m2=_mm256_add_epi16(m1,m2);

		//after shifts the 15th element is lost as it cannot be propagated to the 16th position (AVX registers are managed as two seperate SSE registers)
		m4=_mm256_and_si256(m0,mask3);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions
		m2=_mm256_add_epi16(m2,m4);

		//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
		output_odd = _mm256_and_si256(m2,output_mask);



         //3rd col iteration
		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,c0_sh2);
		m1=_mm256_maddubs_epi16(r1,c1_sh2);
		m2=_mm256_maddubs_epi16(r2,c2_sh2);
		m3=_mm256_maddubs_epi16(r3,c3_sh2);
		m4=_mm256_maddubs_epi16(r4,c4_sh2);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m2=_mm256_add_epi16(m2,m3);
		m0=_mm256_add_epi16(m0,m2);
		m0=_mm256_add_epi16(m0,m4);

		//hozizontal additions
		//hadd(2:7)
		//hadd(8:13)
		//hadd(14:19)
		//hadd(20:25)
		//hadd(26:30)

		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		m4=_mm256_and_si256(m2,mask3);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions

		m1=_mm256_srli_si256(m0,4);
		m2=_mm256_add_epi16(m1,m2);
		m2=_mm256_add_epi16(m2,m4);

		//m2 has 16 16bit values now. the results I need are in positions 1,4,7,10,13. keep only those, discard others
		m2 = _mm256_and_si256(m2,output_mask_sh1);
		output_even = _mm256_add_epi16(output_even,m2);


		//4th col iteration

		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,c0_sh3);
		m1=_mm256_maddubs_epi16(r1,c1_sh3);
		m2=_mm256_maddubs_epi16(r2,c2_sh3);
		m3=_mm256_maddubs_epi16(r3,c3_sh3);
		m4=_mm256_maddubs_epi16(r4,c4_sh3);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m2=_mm256_add_epi16(m2,m3);
		m0=_mm256_add_epi16(m0,m2);
		m0=_mm256_add_epi16(m0,m4);

		//hozizontal additions
		//hadd(3:8)
		//hadd(9:14)
		//hadd(15:20)
		//hadd(21:26)
		//hadd(27:31)
		//result after division will be in 2,8,14,20,26

		m1=_mm256_srli_si256(m0,2);
		m4=_mm256_add_epi16(m1,m0);

		m1=_mm256_srli_si256(m0,4);
		m2=_mm256_add_epi16(m1,m4);


		m4=_mm256_and_si256(m4,mask3);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions

		m2=_mm256_add_epi16(m2,m4);

		//m2 has 16 16bit values now. the results I need are in positions 1,4,7,10,13. keep only those, discard others
		m2 = _mm256_and_si256(m2,output_mask_sh1);
		output_odd = _mm256_add_epi16(output_odd,m2);




		//5th col iteration
		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,c0_sh4);
		m1=_mm256_maddubs_epi16(r1,c1_sh4);
		m2=_mm256_maddubs_epi16(r2,c2_sh4);
		m3=_mm256_maddubs_epi16(r3,c3_sh4);
		m4=_mm256_maddubs_epi16(r4,c4_sh4);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m2=_mm256_add_epi16(m2,m3);
		m0=_mm256_add_epi16(m0,m2);
		m0=_mm256_add_epi16(m0,m4);

		//hozizontal additions
		//hadd(4:9)
		//hadd(10:15)
		//hadd(16:21)
		//hadd(22:27)
		//result after division will be in 4,10,16,22

		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		m1=_mm256_srli_si256(m0,4);
		m2=_mm256_add_epi16(m1,m2);


		//m2 has 16 16bit values now. the results I need are in positions 2,5,8,11. keep only those, discard others
		m2 = _mm256_and_si256(m2,output_mask_sh2);
		output_even = _mm256_add_epi16(output_even,m2);



                 //6th col iteration
		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,c0_sh5);
		m1=_mm256_maddubs_epi16(r1,c1_sh5);
		m2=_mm256_maddubs_epi16(r2,c2_sh5);
		m3=_mm256_maddubs_epi16(r3,c3_sh5);
		m4=_mm256_maddubs_epi16(r4,c4_sh5);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m2=_mm256_add_epi16(m2,m3);
		m0=_mm256_add_epi16(m0,m2);
		m0=_mm256_add_epi16(m0,m4);

		//hozizontal additions
		//hadd(5:10)
		//hadd(11:16)
		//hadd(17:22)
		//hadd(23:28)
		//result after division will be in 4,10,16,22

		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		m1=_mm256_srli_si256(m0,4);
		m2=_mm256_add_epi16(m1,m2);

		//m2 has 16 16bit values now. the results I need are in positions 2,5,8,11. keep only those, discard others
		m2 = _mm256_and_si256(m2,output_mask_sh2);
		output_odd = _mm256_add_epi16(output_odd,m2);

		//now division follows
		output_even=division(division_case,output_even,f);
		output_odd=division(division_case,output_odd,f);

		//shift odd 1 position and add to even
		output_odd = _mm256_slli_si256(output_odd,1);
		output_even = _mm256_add_epi8(output_even,output_odd);


		_mm256_storeu_si256( (__m256i *) &filt[row][col],output_even );


		}

        if (REMINDER_ITERATIONS>=29)
        	conv_loop_reminder_high_reminder_values_less_div(frame1,filt,M,N,row,col,REMINDER_ITERATIONS,division_case,mask_vector,f,divisor,filter5x5);
        else
        	conv_loop_reminder_low_reminder_values_less_div(frame1,filt,M,N,row,col,REMINDER_ITERATIONS,division_case,mask_vector,f);

		}
}

}//end of parallel


}









void convolution_optimized_5x5_reg_blocking_16(unsigned char **frame1,unsigned char **filt,const unsigned int M, const unsigned int N, const unsigned short int divisor, signed char **filter5x5){

	const signed char f00=filter5x5[0][0];
	const signed char f01=filter5x5[0][1];
	const signed char f02=filter5x5[0][2];
	const signed char f03=filter5x5[0][3];
	const signed char f04=filter5x5[0][4];

	const signed char f10=filter5x5[1][0];
	const signed char f11=filter5x5[1][1];
	const signed char f12=filter5x5[1][2];
	const signed char f13=filter5x5[1][3];
	const signed char f14=filter5x5[1][4];

	const signed char f20=filter5x5[2][0];
	const signed char f21=filter5x5[2][1];
	const signed char f22=filter5x5[2][2];
	const signed char f23=filter5x5[2][3];
	const signed char f24=filter5x5[2][4];

	const signed char f30=filter5x5[3][0];
	const signed char f31=filter5x5[3][1];
	const signed char f32=filter5x5[3][2];
	const signed char f33=filter5x5[3][3];
	const signed char f34=filter5x5[3][4];

	const signed char f40=filter5x5[4][0];
	const signed char f41=filter5x5[4][1];
	const signed char f42=filter5x5[4][2];
	const signed char f43=filter5x5[4][3];
	const signed char f44=filter5x5[4][4];

	const signed char mask_vector[30][32] __attribute__((aligned(64))) ={
			{f00,f01,f02,f03,f04,0,f00,f01,f02,f03,f04,0,f00,f01,f02,f03,f04,0,f00,f01,f02,f03,f04,0,f00,f01,f02,f03,f04,0,0,0},
			{f10,f11,f12,f13,f14,0,f10,f11,f12,f13,f14,0,f10,f11,f12,f13,f14,0,f10,f11,f12,f13,f14,0,f10,f11,f12,f13,f14,0,0,0},
			{f20,f21,f22,f23,f24,0,f20,f21,f22,f23,f24,0,f20,f21,f22,f23,f24,0,f20,f21,f22,f23,f24,0,f20,f21,f22,f23,f24,0,0,0},
			{f30,f31,f32,f33,f34,0,f30,f31,f32,f33,f34,0,f30,f31,f32,f33,f34,0,f30,f31,f32,f33,f34,0,f30,f31,f32,f33,f34,0,0,0},
			{f40,f41,f42,f43,f44,0,f40,f41,f42,f43,f44,0,f40,f41,f42,f43,f44,0,f40,f41,f42,f43,f44,0,f40,f41,f42,f43,f44,0,0,0},

			{0,f00,f01,f02,f03,f04,0,f00,f01,f02,f03,f04,0,f00,f01,f02,f03,f04,0,f00,f01,f02,f03,f04,0,f00,f01,f02,f03,f04,0,0},
			{0,f10,f11,f12,f13,f14,0,f10,f11,f12,f13,f14,0,f10,f11,f12,f13,f14,0,f10,f11,f12,f13,f14,0,f10,f11,f12,f13,f14,0,0},
			{0,f20,f21,f22,f23,f24,0,f20,f21,f22,f23,f24,0,f20,f21,f22,f23,f24,0,f20,f21,f22,f23,f24,0,f20,f21,f22,f23,f24,0,0},
			{0,f30,f31,f32,f33,f34,0,f30,f31,f32,f33,f34,0,f30,f31,f32,f33,f34,0,f30,f31,f32,f33,f34,0,f30,f31,f32,f33,f34,0,0},
			{0,f40,f41,f42,f43,f44,0,f40,f41,f42,f43,f44,0,f40,f41,f42,f43,f44,0,f40,f41,f42,f43,f44,0,f40,f41,f42,f43,f44,0,0},

			{0,0,f00,f01,f02,f03,f04,0,f00,f01,f02,f03,f04,0,f00,f01,f02,f03,f04,0,f00,f01,f02,f03,f04,0,f00,f01,f02,f03,f04,0},
			{0,0,f10,f11,f12,f13,f14,0,f10,f11,f12,f13,f14,0,f10,f11,f12,f13,f14,0,f10,f11,f12,f13,f14,0,f10,f11,f12,f13,f14,0},
			{0,0,f20,f21,f22,f23,f24,0,f20,f21,f22,f23,f24,0,f20,f21,f22,f23,f24,0,f20,f21,f22,f23,f24,0,f20,f21,f22,f23,f24,0},
			{0,0,f30,f31,f32,f33,f34,0,f30,f31,f32,f33,f34,0,f30,f31,f32,f33,f34,0,f30,f31,f32,f33,f34,0,f30,f31,f32,f33,f34,0},
			{0,0,f40,f41,f42,f43,f44,0,f40,f41,f42,f43,f44,0,f40,f41,f42,f43,f44,0,f40,f41,f42,f43,f44,0,f40,f41,f42,f43,f44,0},

			{0,0,0,f00,f01,f02,f03,f04,0,f00,f01,f02,f03,f04,0,f00,f01,f02,f03,f04,0,f00,f01,f02,f03,f04,0,f00,f01,f02,f03,f04},
			{0,0,0,f10,f11,f12,f13,f14,0,f10,f11,f12,f13,f14,0,f10,f11,f12,f13,f14,0,f10,f11,f12,f13,f14,0,f10,f11,f12,f13,f14},
			{0,0,0,f20,f21,f22,f23,f24,0,f20,f21,f22,f23,f24,0,f20,f21,f22,f23,f24,0,f20,f21,f22,f23,f24,0,f20,f21,f22,f23,f24},
			{0,0,0,f30,f31,f32,f33,f34,0,f30,f31,f32,f33,f34,0,f30,f31,f32,f33,f34,0,f30,f31,f32,f33,f34,0,f30,f31,f32,f33,f34},
			{0,0,0,f40,f41,f42,f43,f44,0,f40,f41,f42,f43,f44,0,f40,f41,f42,f43,f44,0,f40,f41,f42,f43,f44,0,f40,f41,f42,f43,f44},

			{0,0,0,0,f00,f01,f02,f03,f04,0,f00,f01,f02,f03,f04,0,f00,f01,f02,f03,f04,0,f00,f01,f02,f03,f04,0,0,0,0,0},
			{0,0,0,0,f10,f11,f12,f13,f14,0,f10,f11,f12,f13,f14,0,f10,f11,f12,f13,f14,0,f10,f11,f12,f13,f14,0,0,0,0,0},
			{0,0,0,0,f20,f21,f22,f23,f24,0,f20,f21,f22,f23,f24,0,f20,f21,f22,f23,f24,0,f20,f21,f22,f23,f24,0,0,0,0,0},
			{0,0,0,0,f30,f31,f32,f33,f34,0,f30,f31,f32,f33,f34,0,f30,f31,f32,f33,f34,0,f30,f31,f32,f33,f34,0,0,0,0,0},
			{0,0,0,0,f40,f41,f42,f43,f44,0,f40,f41,f42,f43,f44,0,f40,f41,f42,f43,f44,0,f40,f41,f42,f43,f44,0,0,0,0,0},

			{0,0,0,0,0,f00,f01,f02,f03,f04,0,f00,f01,f02,f03,f04,0,f00,f01,f02,f03,f04,0,f00,f01,f02,f03,f04,0,0,0,0},
			{0,0,0,0,0,f10,f11,f12,f13,f14,0,f10,f11,f12,f13,f14,0,f10,f11,f12,f13,f14,0,f10,f11,f12,f13,f14,0,0,0,0},
			{0,0,0,0,0,f20,f21,f22,f23,f24,0,f20,f21,f22,f23,f24,0,f20,f21,f22,f23,f24,0,f20,f21,f22,f23,f24,0,0,0,0},
			{0,0,0,0,0,f30,f31,f32,f33,f34,0,f30,f31,f32,f33,f34,0,f30,f31,f32,f33,f34,0,f30,f31,f32,f33,f34,0,0,0,0},
			{0,0,0,0,0,f40,f41,f42,f43,f44,0,f40,f41,f42,f43,f44,0,f40,f41,f42,f43,f44,0,f40,f41,f42,f43,f44,0,0,0,0},
	};


		const __m256i c0=_mm256_load_si256( (__m256i *) &mask_vector[0][0]);
		const __m256i c1=_mm256_load_si256( (__m256i *) &mask_vector[1][0]);
		const __m256i c2=_mm256_load_si256( (__m256i *) &mask_vector[2][0]);
		const __m256i c3=_mm256_load_si256( (__m256i *) &mask_vector[3][0]);
		const __m256i c4=_mm256_load_si256( (__m256i *) &mask_vector[4][0]);

		const __m256i c0_sh1=_mm256_load_si256( (__m256i *) &mask_vector[5][0]);
		const __m256i c1_sh1=_mm256_load_si256( (__m256i *) &mask_vector[6][0]);
		const __m256i c2_sh1=_mm256_load_si256( (__m256i *) &mask_vector[7][0]);
		const __m256i c3_sh1=_mm256_load_si256( (__m256i *) &mask_vector[8][0]);
		const __m256i c4_sh1=_mm256_load_si256( (__m256i *) &mask_vector[9][0]);

		const __m256i c0_sh2=_mm256_load_si256( (__m256i *) &mask_vector[10][0]);
		const __m256i c1_sh2=_mm256_load_si256( (__m256i *) &mask_vector[11][0]);
		const __m256i c2_sh2=_mm256_load_si256( (__m256i *) &mask_vector[12][0]);
		const __m256i c3_sh2=_mm256_load_si256( (__m256i *) &mask_vector[13][0]);
		const __m256i c4_sh2=_mm256_load_si256( (__m256i *) &mask_vector[14][0]);

		const __m256i c0_sh3=_mm256_load_si256( (__m256i *) &mask_vector[15][0]);
		const __m256i c1_sh3=_mm256_load_si256( (__m256i *) &mask_vector[16][0]);
		const __m256i c2_sh3=_mm256_load_si256( (__m256i *) &mask_vector[17][0]);
		const __m256i c3_sh3=_mm256_load_si256( (__m256i *) &mask_vector[18][0]);
		const __m256i c4_sh3=_mm256_load_si256( (__m256i *) &mask_vector[19][0]);

		const __m256i c0_sh4=_mm256_load_si256( (__m256i *) &mask_vector[20][0]);
		const __m256i c1_sh4=_mm256_load_si256( (__m256i *) &mask_vector[21][0]);
		const __m256i c2_sh4=_mm256_load_si256( (__m256i *) &mask_vector[22][0]);
		const __m256i c3_sh4=_mm256_load_si256( (__m256i *) &mask_vector[23][0]);
		const __m256i c4_sh4=_mm256_load_si256( (__m256i *) &mask_vector[24][0]);

		const __m256i c0_sh5=_mm256_load_si256( (__m256i *) &mask_vector[25][0]);
		const __m256i c1_sh5=_mm256_load_si256( (__m256i *) &mask_vector[26][0]);
		const __m256i c2_sh5=_mm256_load_si256( (__m256i *) &mask_vector[27][0]);
		const __m256i c3_sh5=_mm256_load_si256( (__m256i *) &mask_vector[28][0]);
		const __m256i c4_sh5=_mm256_load_si256( (__m256i *) &mask_vector[29][0]);



	//const __m256i f  = _mm256_set_epi16(52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759);
	//const __m256i mask  = _mm256_set_epi8(0,0,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255);
	//const __m256i mask2  = _mm256_set_epi8(0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0);
	//const __m256i mask6  = _mm256_set_epi8(0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0);

	const __m256i mask3  = _mm256_set_epi16(0,0,0,0,0,0,0,65535,0,0,0,0,0,0,0,0);
	const __m256i mask_prelude  = _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0);

	const __m256i output_mask      = _mm256_set_epi16(0,0,0,65535,0,0,65535,0,0,65535,0,0,65535,0,0,65535);
	const __m256i output_mask_sh1  = _mm256_set_epi16(0,0,65535,0,0,65535,0,0,65535,0,0,65535,0,0,65535,0);
	const __m256i output_mask_sh2  = _mm256_set_epi16(0,0,0,    0,65535,0,0,65535,0,0,65535,0,0,65535,0,0);



	const unsigned int REMINDER_ITERATIONS =  (M-((((M-32)/28)*28)+28)); //M-(last_col_value+28)
	//printf("\n%d",REMINDER_ITERATIONS);


	const unsigned int division_case=prepare_for_division(divisor); //determine which is the division case (A, B or C)
	const __m256i f = _mm256_load_si256( (__m256i *) &f_vector[0]);// initialize the division vector

//printf("\n%d %d",division_case,b);


#pragma omp parallel
{

unsigned int row,col;
register __m256i r0,r1,r2,r3,r4,r5,m0,m1,m2,m3,m4,m5;
__m256i output_row0_even,output_row1_even,output_row0_odd,output_row1_odd; //these are for processing row #0 and #1 only.
__m256i output_even,output_odd;
__m256i m0_prelude_row0,m0_prelude_row1;


/*---------------------- Gaussian Blur ---------------------------------*/

    #pragma omp for schedule(static)
	for (row = 1; row <= N-2; row+=2) {

		if (row==1){//in this case I calculate filt[0][:] and filt[1][:] too. No extra loads or multiplications are required for this
			  for (col = 0; col <= M-32; col+=28){

				  if (col==0){//this is a special case as the mask gets outside of the array (it is like adding two zeros in the beginning of frame[][]

						//load the 5 rows
						r0=_mm256_load_si256( (__m256i *) &frame1[0][0]);
						r1=_mm256_load_si256( (__m256i *) &frame1[1][0]);
						r2=_mm256_load_si256( (__m256i *) &frame1[2][0]);
						r3=_mm256_load_si256( (__m256i *) &frame1[3][0]);
						r4=_mm256_load_si256( (__m256i *) &frame1[4][0]);

				  			//START - extra code needed for prelude
				  					m0=_mm256_slli_si256(r0,2);//shift 2 elements left - equivalent to filling with two zeros inthe beginning
				  					m1=_mm256_slli_si256(r1,2);//shift 2 elements left - equivalent to filling with two zeros inthe beginning
				  					m2=_mm256_slli_si256(r2,2);//shift 2 elements left - equivalent to filling with two zeros inthe beginning
				  					m3=_mm256_slli_si256(r3,2);//shift 2 elements left - equivalent to filling with two zeros inthe beginning
				  					m4=_mm256_slli_si256(r4,2);//shift 2 elements left - equivalent to filling with two zeros inthe beginning

				  					r0=_mm256_and_si256(r0,mask_prelude);
				  					r0=_mm256_permute2f128_si256(r0,r0,1);
				  					r0=_mm256_srli_si256(r0,14);//shift 14 elements
				  					r0=_mm256_add_epi16(m0,r0);

				  					r1=_mm256_and_si256(r1,mask_prelude);
				  					r1=_mm256_permute2f128_si256(r1,r1,1);
				  					r1=_mm256_srli_si256(r1,14);//shift 14 elements
				  					r1=_mm256_add_epi16(m1,r1);

				  					r2=_mm256_and_si256(r2,mask_prelude);
				  					r2=_mm256_permute2f128_si256(r2,r2,1);
				  					r2=_mm256_srli_si256(r2,14);//shift 14 elements
				  					r2=_mm256_add_epi16(m2,r2);

				  					r3=_mm256_and_si256(r3,mask_prelude);
				  					r3=_mm256_permute2f128_si256(r3,r3,1);
				  					r3=_mm256_srli_si256(r3,14);//shift 14 elements
				  					r3=_mm256_add_epi16(m3,r3);

				  					r4=_mm256_and_si256(r4,mask_prelude);
				  					r4=_mm256_permute2f128_si256(r4,r4,1);
				  					r4=_mm256_srli_si256(r4,14);//shift 14 elements
				  					r4=_mm256_add_epi16(m4,r4);

				  			//END - extra code needed for prelude
				  		}
				  else {
						//load the 5 rows
						r0=_mm256_loadu_si256( (__m256i *) &frame1[0][col-2]);
						r1=_mm256_loadu_si256( (__m256i *) &frame1[1][col-2]);
						r2=_mm256_loadu_si256( (__m256i *) &frame1[2][col-2]);
						r3=_mm256_loadu_si256( (__m256i *) &frame1[3][col-2]);
						r4=_mm256_loadu_si256( (__m256i *) &frame1[4][col-2]);
				  }

				//col iteration computes output pixels of 2,8,14,20,26
				// col+1 iteration computes output pixels of 3,9,15,21,27
				// col+2 iteration computes output pixels of 4,10,16,22,28
				// col+3 iteration computes output pixels of 5,11,17,23,29
				// col+4 iteration computes output pixels of 6,12,18,24,30
				// col+5 iteration computes output pixels of 7,13,19,25,31
				//afterwards, col2 becomes 32 and repeat the above process

				//1st col iteration

				  //--------------row=2
				//multiply with the mask
				m0=_mm256_maddubs_epi16(r0,c0);
				m1=_mm256_maddubs_epi16(r1,c1);
				m2=_mm256_maddubs_epi16(r2,c2);
				m3=_mm256_maddubs_epi16(r3,c3);
				m4=_mm256_maddubs_epi16(r4,c4);

				//vertical add
				m0=_mm256_add_epi16(m0,m1);
				m0=_mm256_add_epi16(m0,m2);
				m0=_mm256_add_epi16(m0,m3);
				m0=_mm256_add_epi16(m0,m4);


				m1=_mm256_srli_si256(m0,2);
				m2=_mm256_add_epi16(m1,m0);

				m1=_mm256_srli_si256(m0,4);
				m2=_mm256_add_epi16(m1,m2);

				//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
				//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
				m4=_mm256_and_si256(m0,mask3);
				m4=_mm256_permute2f128_si256(m4,m4,1);
				m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions
				m2=_mm256_add_epi16(m2,m4);

				//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
				output_even=_mm256_and_si256(m2,output_mask);

				//--------------row=0
				//multiply with the mask
				m0=_mm256_maddubs_epi16(r0,c2);
				m1=_mm256_maddubs_epi16(r1,c3);
				m2=_mm256_maddubs_epi16(r2,c4);

				//vertical add
				m0=_mm256_add_epi16(m0,m1);
				m0=_mm256_add_epi16(m0,m2);


				m1=_mm256_srli_si256(m0,2);
				m2=_mm256_add_epi16(m1,m0);

				m1=_mm256_srli_si256(m0,4);
				m2=_mm256_add_epi16(m1,m2);

				//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
				//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
				m4=_mm256_and_si256(m0,mask3);
				m4=_mm256_permute2f128_si256(m4,m4,1);
				m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions
				m2=_mm256_add_epi16(m2,m4);

				//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
				output_row0_even=_mm256_and_si256(m2,output_mask);

				//--------------row=1
				//multiply with the mask
				m0=_mm256_maddubs_epi16(r0,c1);
				m1=_mm256_maddubs_epi16(r1,c2);
				m2=_mm256_maddubs_epi16(r2,c3);
				m3=_mm256_maddubs_epi16(r3,c4);

				//vertical add
				m0=_mm256_add_epi16(m0,m1);
				m0=_mm256_add_epi16(m0,m2);
				m0=_mm256_add_epi16(m0,m3);

				m1=_mm256_srli_si256(m0,2);
				m2=_mm256_add_epi16(m1,m0);

				m1=_mm256_srli_si256(m0,4);
				m2=_mm256_add_epi16(m1,m2);

				//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
				//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
				m4=_mm256_and_si256(m0,mask3);
				m4=_mm256_permute2f128_si256(m4,m4,1);
				m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions
				m2=_mm256_add_epi16(m2,m4);

				//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
				output_row1_even=_mm256_and_si256(m2,output_mask);

				//2nd col iteration
				//multiply with the mask

				//----row=2
				m0=_mm256_maddubs_epi16(r0,c0_sh1);
				m1=_mm256_maddubs_epi16(r1,c1_sh1);
				m2=_mm256_maddubs_epi16(r2,c2_sh1);
				m3=_mm256_maddubs_epi16(r3,c3_sh1);
				m4=_mm256_maddubs_epi16(r4,c4_sh1);

				//vertical add
				m0=_mm256_add_epi16(m0,m1);
				m0=_mm256_add_epi16(m0,m2);
				m0=_mm256_add_epi16(m0,m3);
				m0=_mm256_add_epi16(m0,m4);


				m1=_mm256_srli_si256(m0,2);
				m2=_mm256_add_epi16(m1,m0);

				m1=_mm256_srli_si256(m0,4);
				m2=_mm256_add_epi16(m1,m2);

				//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
				//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
				m4=_mm256_and_si256(m0,mask3);
				m4=_mm256_permute2f128_si256(m4,m4,1);
				m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions
				m2=_mm256_add_epi16(m2,m4);

				//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
				output_odd = _mm256_and_si256(m2,output_mask);

				//----row=0
				m0=_mm256_maddubs_epi16(r0,c2_sh1);
				m1=_mm256_maddubs_epi16(r1,c3_sh1);
				m2=_mm256_maddubs_epi16(r2,c4_sh1);

				//vertical add
				m0=_mm256_add_epi16(m0,m1);
				m0=_mm256_add_epi16(m0,m2);


				m1=_mm256_srli_si256(m0,2);
				m2=_mm256_add_epi16(m1,m0);

				m1=_mm256_srli_si256(m0,4);
				m2=_mm256_add_epi16(m1,m2);

				//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
				//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
				m4=_mm256_and_si256(m0,mask3);
				m4=_mm256_permute2f128_si256(m4,m4,1);
				m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions
				m2=_mm256_add_epi16(m2,m4);

				//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
				output_row0_odd = _mm256_and_si256(m2,output_mask);

				//----row=1
				m0=_mm256_maddubs_epi16(r0,c1_sh1);
				m1=_mm256_maddubs_epi16(r1,c2_sh1);
				m2=_mm256_maddubs_epi16(r2,c3_sh1);
				m3=_mm256_maddubs_epi16(r3,c4_sh1);

				//vertical add
				m0=_mm256_add_epi16(m0,m1);
				m0=_mm256_add_epi16(m0,m2);
				m0=_mm256_add_epi16(m0,m3);


				m1=_mm256_srli_si256(m0,2);
				m2=_mm256_add_epi16(m1,m0);

				m1=_mm256_srli_si256(m0,4);
				m2=_mm256_add_epi16(m1,m2);

				//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
				//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
				m4=_mm256_and_si256(m0,mask3);
				m4=_mm256_permute2f128_si256(m4,m4,1);
				m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions
				m2=_mm256_add_epi16(m2,m4);

				//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
				output_row1_odd = _mm256_and_si256(m2,output_mask);

		                 //3rd col iteration
				//---row=2
				//multiply with the mask
				m0=_mm256_maddubs_epi16(r0,c0_sh2);
				m1=_mm256_maddubs_epi16(r1,c1_sh2);
				m2=_mm256_maddubs_epi16(r2,c2_sh2);
				m3=_mm256_maddubs_epi16(r3,c3_sh2);
				m4=_mm256_maddubs_epi16(r4,c4_sh2);

				//vertical add
				m0=_mm256_add_epi16(m0,m1);
				m0=_mm256_add_epi16(m0,m2);
				m0=_mm256_add_epi16(m0,m3);
				m0=_mm256_add_epi16(m0,m4);


				m1=_mm256_srli_si256(m0,2);
				m2=_mm256_add_epi16(m1,m0);

				//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,_mm256_extract_epi16(m2,8),0,0,0,0,0,0,0);
				m4=_mm256_and_si256(m2,mask3);
				m4=_mm256_permute2f128_si256(m4,m4,1);
				m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions

				m1=_mm256_srli_si256(m0,4);
				m2=_mm256_add_epi16(m1,m2);
				m2=_mm256_add_epi16(m2,m4);

				//m2 has 16 16bit values now. the results I need are in positions 1,4,7,10,13. keep only those, discard others
				m2 = _mm256_and_si256(m2,output_mask_sh1);
				output_even = _mm256_add_epi16(output_even,m2);

				//---row=0
				//multiply with the mask

				m0=_mm256_maddubs_epi16(r0,c2_sh2);
				m1=_mm256_maddubs_epi16(r1,c3_sh2);
				m2=_mm256_maddubs_epi16(r2,c4_sh2);

				//vertical add
				m0=_mm256_add_epi16(m0,m1);
				m0=_mm256_add_epi16(m0,m2);


				m1=_mm256_srli_si256(m0,2);
				m2=_mm256_add_epi16(m1,m0);

				//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,_mm256_extract_epi16(m2,8),0,0,0,0,0,0,0);
				m4=_mm256_and_si256(m2,mask3);
				m4=_mm256_permute2f128_si256(m4,m4,1);
				m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions

				m1=_mm256_srli_si256(m0,4);
				m2=_mm256_add_epi16(m1,m2);

				//prepei na prosthesw to pixel pu thelw sto r2 kai meta na kanw diairesi me ola mazi

				m2=_mm256_add_epi16(m2,m4);

				//m2 has 16 16bit values now. the results I need are in positions 1,4,7,10,13. keep only those, discard others
				m2 = _mm256_and_si256(m2,output_mask_sh1);
				output_row0_even = _mm256_add_epi16(output_row0_even,m2);


				//---row=1
				//multiply with the mask

				m0=_mm256_maddubs_epi16(r0,c1_sh2);
				m1=_mm256_maddubs_epi16(r1,c2_sh2);
				m2=_mm256_maddubs_epi16(r2,c3_sh2);
				m3=_mm256_maddubs_epi16(r3,c4_sh2);

				//vertical add
				m0=_mm256_add_epi16(m0,m1);
				m0=_mm256_add_epi16(m0,m2);
				m0=_mm256_add_epi16(m0,m3);

				m1=_mm256_srli_si256(m0,2);
				m2=_mm256_add_epi16(m1,m0);

				//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,_mm256_extract_epi16(m2,8),0,0,0,0,0,0,0);
				m4=_mm256_and_si256(m2,mask3);
				m4=_mm256_permute2f128_si256(m4,m4,1);
				m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions

				m1=_mm256_srli_si256(m0,4);
				m2=_mm256_add_epi16(m1,m2);

				//prepei na prosthesw to pixel pu thelw sto r2 kai meta na kanw diairesi me ola mazi

				m2=_mm256_add_epi16(m2,m4);

				//m2 has 16 16bit values now. the results I need are in positions 1,4,7,10,13. keep only those, discard others
				m2 = _mm256_and_si256(m2,output_mask_sh1);
				output_row1_even = _mm256_add_epi16(output_row1_even,m2);

				//4th col iteration

				//4th col iteration

				//multiply with the mask
				m0=_mm256_maddubs_epi16(r0,c0_sh3);
				m1=_mm256_maddubs_epi16(r1,c1_sh3);
				m2=_mm256_maddubs_epi16(r2,c2_sh3);
				m3=_mm256_maddubs_epi16(r3,c3_sh3);
				m4=_mm256_maddubs_epi16(r4,c4_sh3);

				//vertical add
				m0=_mm256_add_epi16(m0,m1);
				m2=_mm256_add_epi16(m2,m3);
				m0=_mm256_add_epi16(m0,m2);
				m0=_mm256_add_epi16(m0,m4);

				//hozizontal additions
				//hadd(3:8)
				//hadd(9:14)
				//hadd(15:20)
				//hadd(21:26)
				//hadd(27:31)
				//result after division will be in 2,8,14,20,26

				m1=_mm256_srli_si256(m0,2);
				m4=_mm256_add_epi16(m1,m0);

				m1=_mm256_srli_si256(m0,4);
				m2=_mm256_add_epi16(m1,m4);

				//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
				//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
				m4=_mm256_and_si256(m4,mask3);
				m4=_mm256_permute2f128_si256(m4,m4,1);
				m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions

				m2=_mm256_add_epi16(m2,m4);

				//m2 has 16 16bit values now. the results I need are in positions 1,4,7,10,13. keep only those, discard others
				m2 = _mm256_and_si256(m2,output_mask_sh1);
				output_odd = _mm256_add_epi16(output_odd,m2);

				//row0
				//multiply with the mask
								m0=_mm256_maddubs_epi16(r0,c2_sh3);
								m1=_mm256_maddubs_epi16(r1,c3_sh3);
								m2=_mm256_maddubs_epi16(r2,c4_sh3);


								//vertical add
								m0=_mm256_add_epi16(m0,m1);
								m0=_mm256_add_epi16(m0,m2);


								m1=_mm256_srli_si256(m0,2);
								m4=_mm256_add_epi16(m1,m0);

								m1=_mm256_srli_si256(m0,4);
								m2=_mm256_add_epi16(m1,m4);

								//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
								//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
								m4=_mm256_and_si256(m4,mask3);
								m4=_mm256_permute2f128_si256(m4,m4,1);
								m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions

								m2=_mm256_add_epi16(m2,m4);

								//m2 has 16 16bit values now. the results I need are in positions 1,4,7,10,13. keep only those, discard others
								m2 = _mm256_and_si256(m2,output_mask_sh1);
								output_row0_odd = _mm256_add_epi16(output_row0_odd,m2);


								//-----------row1
								//multiply with the mask
												m0=_mm256_maddubs_epi16(r0,c1_sh3);
												m1=_mm256_maddubs_epi16(r1,c2_sh3);
												m2=_mm256_maddubs_epi16(r2,c3_sh3);
												m3=_mm256_maddubs_epi16(r3,c4_sh3);

												//vertical add
												m0=_mm256_add_epi16(m0,m1);
												m0=_mm256_add_epi16(m0,m2);
												m0=_mm256_add_epi16(m0,m3);

												m1=_mm256_srli_si256(m0,2);
												m4=_mm256_add_epi16(m1,m0);

												m1=_mm256_srli_si256(m0,4);
												m2=_mm256_add_epi16(m1,m4);

												//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
												//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
												m4=_mm256_and_si256(m4,mask3);
												m4=_mm256_permute2f128_si256(m4,m4,1);
												m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions

												m2=_mm256_add_epi16(m2,m4);

												//m2 has 16 16bit values now. the results I need are in positions 1,4,7,10,13. keep only those, discard others
												m2 = _mm256_and_si256(m2,output_mask_sh1);
												output_row1_odd = _mm256_add_epi16(output_row1_odd,m2);



				//5th col iteration
				//multiply with the mask
				m0=_mm256_maddubs_epi16(r0,c0_sh4);
				m1=_mm256_maddubs_epi16(r1,c1_sh4);
				m2=_mm256_maddubs_epi16(r2,c2_sh4);
				m3=_mm256_maddubs_epi16(r3,c3_sh4);
				m4=_mm256_maddubs_epi16(r4,c4_sh4);

				//vertical add
				m0=_mm256_add_epi16(m0,m1);
				m2=_mm256_add_epi16(m2,m3);
				m0=_mm256_add_epi16(m0,m2);
				m0=_mm256_add_epi16(m0,m4);

				//hozizontal additions
				//hadd(4:9)
				//hadd(10:15)
				//hadd(16:21)
				//hadd(22:27)
				//result after division will be in 4,10,16,22

				m1=_mm256_srli_si256(m0,2);
				m2=_mm256_add_epi16(m1,m0);

				m1=_mm256_srli_si256(m0,4);
				m2=_mm256_add_epi16(m1,m2);


				//m2 has 16 16bit values now. the results I need are in positions 2,5,8,11. keep only those, discard others
				m2 = _mm256_and_si256(m2,output_mask_sh2);
				output_even = _mm256_add_epi16(output_even,m2);

				//row0
				//multiply with the mask
				m0=_mm256_maddubs_epi16(r0,c2_sh4);
				m1=_mm256_maddubs_epi16(r1,c3_sh4);
				m2=_mm256_maddubs_epi16(r2,c4_sh4);


				//vertical add
				m0=_mm256_add_epi16(m0,m1);
				m0=_mm256_add_epi16(m0,m2);


				//hozizontal additions
				//hadd(4:9)
				//hadd(10:15)
				//hadd(16:21)
				//hadd(22:27)
				//result after division will be in 4,10,16,22

				m1=_mm256_srli_si256(m0,2);
				m2=_mm256_add_epi16(m1,m0);

				m1=_mm256_srli_si256(m0,4);
				m2=_mm256_add_epi16(m1,m2);


				//m2 has 16 16bit values now. the results I need are in positions 2,5,8,11. keep only those, discard others
				m2 = _mm256_and_si256(m2,output_mask_sh2);
				output_row0_even = _mm256_add_epi16(output_row0_even,m2);

				//row1
				//multiply with the mask
				m0=_mm256_maddubs_epi16(r0,c1_sh4);
				m1=_mm256_maddubs_epi16(r1,c2_sh4);
				m2=_mm256_maddubs_epi16(r2,c3_sh4);
				m3=_mm256_maddubs_epi16(r3,c4_sh4);


				//vertical add
				m0=_mm256_add_epi16(m0,m1);
				m0=_mm256_add_epi16(m0,m2);
				m0=_mm256_add_epi16(m0,m3);


				m1=_mm256_srli_si256(m0,2);
				m2=_mm256_add_epi16(m1,m0);

				m1=_mm256_srli_si256(m0,4);
				m2=_mm256_add_epi16(m1,m2);


				//m2 has 16 16bit values now. the results I need are in positions 2,5,8,11. keep only those, discard others
				m2 = _mm256_and_si256(m2,output_mask_sh2);
				output_row1_even = _mm256_add_epi16(output_row1_even,m2);


		                 //6th col iteration
				//multiply with the mask
				m0=_mm256_maddubs_epi16(r0,c0_sh5);
				m1=_mm256_maddubs_epi16(r1,c1_sh5);
				m2=_mm256_maddubs_epi16(r2,c2_sh5);
				m3=_mm256_maddubs_epi16(r3,c3_sh5);
				m4=_mm256_maddubs_epi16(r4,c4_sh5);

				//vertical add
				m0=_mm256_add_epi16(m0,m1);
				m2=_mm256_add_epi16(m2,m3);
				m0=_mm256_add_epi16(m0,m2);
				m0=_mm256_add_epi16(m0,m4);

				//hozizontal additions
				//hadd(5:10)
				//hadd(11:16)
				//hadd(17:22)
				//hadd(23:28)
				//result after division will be in 4,10,16,22

				m1=_mm256_srli_si256(m0,2);
				m2=_mm256_add_epi16(m1,m0);

				m1=_mm256_srli_si256(m0,4);
				m2=_mm256_add_epi16(m1,m2);

				//m2 has 16 16bit values now. the results I need are in positions 2,5,8,11. keep only those, discard others
				m2 = _mm256_and_si256(m2,output_mask_sh2);
				output_odd = _mm256_add_epi16(output_odd,m2);

				//now division follows
				output_even=division(division_case,output_even,f);
				output_odd=division(division_case,output_odd,f);

				//shift odd 1 position and add to even
				output_odd = _mm256_slli_si256(output_odd,1);
				output_even = _mm256_add_epi8(output_even,output_odd);

				_mm256_storeu_si256( (__m256i *) &filt[2][col],output_even );

				//row0
				//multiply with the mask
				m0=_mm256_maddubs_epi16(r0,c2_sh5);
				m1=_mm256_maddubs_epi16(r1,c3_sh5);
				m2=_mm256_maddubs_epi16(r2,c4_sh5);

				//vertical add
				m0=_mm256_add_epi16(m0,m1);
				m0=_mm256_add_epi16(m0,m2);


				m1=_mm256_srli_si256(m0,2);
				m2=_mm256_add_epi16(m1,m0);

				m1=_mm256_srli_si256(m0,4);
				m2=_mm256_add_epi16(m1,m2);

				//m2 has 16 16bit values now. the results I need are in positions 2,5,8,11. keep only those, discard others
				m2 = _mm256_and_si256(m2,output_mask_sh2);
				output_row0_odd = _mm256_add_epi16(output_row0_odd,m2);

				//now division follows
				output_row0_even=division(division_case,output_row0_even,f);
				output_row0_odd=division(division_case,output_row0_odd,f);

				//shift odd 1 position and add to even
				output_row0_odd = _mm256_slli_si256(output_row0_odd,1);
				output_row0_even = _mm256_add_epi8(output_row0_even,output_row0_odd);


				_mm256_storeu_si256( (__m256i *) &filt[0][col],output_row0_even );

				//row1

				//multiply with the mask
				m0=_mm256_maddubs_epi16(r0,c1_sh5);
				m1=_mm256_maddubs_epi16(r1,c2_sh5);
				m2=_mm256_maddubs_epi16(r2,c3_sh5);
				m3=_mm256_maddubs_epi16(r3,c4_sh5);

				//vertical add
				m0=_mm256_add_epi16(m0,m1);
				m0=_mm256_add_epi16(m0,m2);
				m0=_mm256_add_epi16(m0,m3);


				m1=_mm256_srli_si256(m0,2);
				m2=_mm256_add_epi16(m1,m0);

				m1=_mm256_srli_si256(m0,4);
				m2=_mm256_add_epi16(m1,m2);

				//m2 has 16 16bit values now. the results I need are in positions 2,5,8,11. keep only those, discard others
				m2 = _mm256_and_si256(m2,output_mask_sh2);
				output_row1_odd = _mm256_add_epi16(output_row1_odd,m2);

				//now division follows
				output_row1_even=division(division_case,output_row1_even,f);
				output_row1_odd=division(division_case,output_row1_odd,f);

				//shift odd 1 position and add to even
				output_row1_odd = _mm256_slli_si256(output_row1_odd,1);
				output_row1_even = _mm256_add_epi8(output_row1_even,output_row1_odd);


				_mm256_storeu_si256( (__m256i *) &filt[1][col],output_row1_even );


				}


		    conv_loop_reminder_first_less_div(frame1,filt,M,N,col,REMINDER_ITERATIONS,division_case,mask_vector,f,divisor,filter5x5);
		}

		else if ((row==N-3) ){//in this case I calculate filt[N-3][:], filt[N-2][:] and filt[N-1][:]. Below row0 refers to row=N-2 and row1 refers to row=N-1
			//printf("\nN=%d row=%d N-2=%d",N,row,N-2);
			for (col = 0; col <= M-32; col+=28){
						 //last col value that does not read outside of the array bounds is (col<M-29)

							  if (col==0){

									//load the 5 rows
									r0=_mm256_load_si256( (__m256i *) &frame1[N-5][0]);
									r1=_mm256_load_si256( (__m256i *) &frame1[N-4][0]);
									r2=_mm256_load_si256( (__m256i *) &frame1[N-3][0]);
									r3=_mm256_load_si256( (__m256i *) &frame1[N-2][0]);
									r4=_mm256_load_si256( (__m256i *) &frame1[N-1][0]);

							  			//START - extra code needed for prelude
							  					m0=_mm256_slli_si256(r0,2);//shift 2 elements left - equivalent to filling with two zeros inthe beginning
							  					m1=_mm256_slli_si256(r1,2);//shift 2 elements left - equivalent to filling with two zeros inthe beginning
							  					m2=_mm256_slli_si256(r2,2);//shift 2 elements left - equivalent to filling with two zeros inthe beginning
							  					m3=_mm256_slli_si256(r3,2);//shift 2 elements left - equivalent to filling with two zeros inthe beginning
							  					m4=_mm256_slli_si256(r4,2);//shift 2 elements left - equivalent to filling with two zeros inthe beginning

							  					r0=_mm256_and_si256(r0,mask_prelude);
							  					r0=_mm256_permute2f128_si256(r0,r0,1);
							  					r0=_mm256_srli_si256(r0,14);//shift 14 elements
							  					r0=_mm256_add_epi16(m0,r0);

							  					r1=_mm256_and_si256(r1,mask_prelude);
							  					r1=_mm256_permute2f128_si256(r1,r1,1);
							  					r1=_mm256_srli_si256(r1,14);//shift 14 elements
							  					r1=_mm256_add_epi16(m1,r1);

							  					r2=_mm256_and_si256(r2,mask_prelude);
							  					r2=_mm256_permute2f128_si256(r2,r2,1);
							  					r2=_mm256_srli_si256(r2,14);//shift 14 elements
							  					r2=_mm256_add_epi16(m2,r2);

							  					r3=_mm256_and_si256(r3,mask_prelude);
							  					r3=_mm256_permute2f128_si256(r3,r3,1);
							  					r3=_mm256_srli_si256(r3,14);//shift 14 elements
							  					r3=_mm256_add_epi16(m3,r3);

							  					r4=_mm256_and_si256(r4,mask_prelude);
							  					r4=_mm256_permute2f128_si256(r4,r4,1);
							  					r4=_mm256_srli_si256(r4,14);//shift 14 elements
							  					r4=_mm256_add_epi16(m4,r4);

							  			//END - extra code needed for prelude
							  		}
							  else {
									//load the 5 rows
									r0=_mm256_loadu_si256( (__m256i *) &frame1[N-5][col-2]);
									r1=_mm256_loadu_si256( (__m256i *) &frame1[N-4][col-2]);
									r2=_mm256_loadu_si256( (__m256i *) &frame1[N-3][col-2]);
									r3=_mm256_loadu_si256( (__m256i *) &frame1[N-2][col-2]);
									r4=_mm256_loadu_si256( (__m256i *) &frame1[N-1][col-2]);
							  }

							//col iteration computes output pixels of 2,8,14,20,26
							// col+1 iteration computes output pixels of 3,9,15,21,27
							// col+2 iteration computes output pixels of 4,10,16,22,28
							// col+3 iteration computes output pixels of 5,11,17,23,29
							// col+4 iteration computes output pixels of 6,12,18,24,30
							// col+5 iteration computes output pixels of 7,13,19,25,31
							//afterwards, col2 becomes 32 and repeat the above process

							//1st col iteration
							 //--------------row=2

							//multiply with the mask
							m0=_mm256_maddubs_epi16(r0,c0);
							m1=_mm256_maddubs_epi16(r1,c1);
							m2=_mm256_maddubs_epi16(r2,c2);
							m3=_mm256_maddubs_epi16(r3,c3);
							m4=_mm256_maddubs_epi16(r4,c4);

							//vertical add
							m4=_mm256_add_epi16(m4,m3);
							m4=_mm256_add_epi16(m4,m2);
							m4=_mm256_add_epi16(m4,m1);
							m0=_mm256_add_epi16(m4,m0);

							m1=_mm256_srli_si256(m0,2);
							m2=_mm256_add_epi16(m1,m0);

							m1=_mm256_srli_si256(m0,4);
							m2=_mm256_add_epi16(m1,m2);

							//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
							//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
							m4=_mm256_and_si256(m0,mask3);
							m4=_mm256_permute2f128_si256(m4,m4,1);
							m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions

							m2=_mm256_add_epi16(m2,m4);

							//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
							output_even=_mm256_and_si256(m2,output_mask);

							//--------------row=0
							//multiply with the mask
							m1=_mm256_maddubs_epi16(r1,c0);
							m2=_mm256_maddubs_epi16(r2,c1);
							m3=_mm256_maddubs_epi16(r3,c2);
							m4=_mm256_maddubs_epi16(r4,c3);

							//vertical add
							m4=_mm256_add_epi16(m4,m3);
							m4=_mm256_add_epi16(m4,m2);
							m4=_mm256_add_epi16(m4,m1); m0_prelude_row0=m4;

							m1=_mm256_srli_si256(m0_prelude_row0,2);
							m2=_mm256_add_epi16(m1,m0_prelude_row0);

							m1=_mm256_srli_si256(m0_prelude_row0,4);
							m2=_mm256_add_epi16(m1,m2);

							//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
							//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
							m4=_mm256_and_si256(m0_prelude_row0,mask3);
							m4=_mm256_permute2f128_si256(m4,m4,1);
							m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions

							m2=_mm256_add_epi16(m2,m4);

							//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
							output_row0_even=_mm256_and_si256(m2,output_mask);

							//--------------row=1
							//multiply with the mask
							m2=_mm256_maddubs_epi16(r2,c0);
							m3=_mm256_maddubs_epi16(r3,c1);
							m4=_mm256_maddubs_epi16(r4,c2);


							//vertical add
							m4=_mm256_add_epi16(m4,m3);
							m4=_mm256_add_epi16(m4,m2);m0_prelude_row1=m4;



							m1=_mm256_srli_si256(m0_prelude_row1,2);
							m2=_mm256_add_epi16(m1,m0_prelude_row1);

							m1=_mm256_srli_si256(m0_prelude_row1,4);
							m2=_mm256_add_epi16(m1,m2);

							//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
							//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
							m4=_mm256_and_si256(m0_prelude_row1,mask3);
							m4=_mm256_permute2f128_si256(m4,m4,1);
							m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions

							m2=_mm256_add_epi16(m2,m4);

							//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
							output_row1_even=_mm256_and_si256(m2,output_mask);

							//2nd col iteration
							//----row=2

							//multiply with the mask
							m0=_mm256_maddubs_epi16(r0,c0_sh1);
							m1=_mm256_maddubs_epi16(r1,c1_sh1);
							m2=_mm256_maddubs_epi16(r2,c2_sh1);
							m3=_mm256_maddubs_epi16(r3,c3_sh1);
							m4=_mm256_maddubs_epi16(r4,c4_sh1);

							//vertical add
							m4=_mm256_add_epi16(m4,m3);
							m4=_mm256_add_epi16(m4,m2);
							m4=_mm256_add_epi16(m4,m1);
							m0=_mm256_add_epi16(m4,m0);


							m1=_mm256_srli_si256(m0,2);
							m2=_mm256_add_epi16(m1,m0);

							m1=_mm256_srli_si256(m0,4);
							m2=_mm256_add_epi16(m1,m2);

							//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
							//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
							m4=_mm256_and_si256(m0,mask3);
							m4=_mm256_permute2f128_si256(m4,m4,1);
							m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions
							m2=_mm256_add_epi16(m2,m4);

							//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
							output_odd = _mm256_and_si256(m2,output_mask);

							//----row=0
							//multiply with the mask
							m1=_mm256_maddubs_epi16(r1,c0_sh1);
							m2=_mm256_maddubs_epi16(r2,c1_sh1);
							m3=_mm256_maddubs_epi16(r3,c2_sh1);
							m4=_mm256_maddubs_epi16(r4,c3_sh1);


							//vertical add
							m4=_mm256_add_epi16(m4,m3);
							m4=_mm256_add_epi16(m4,m2);
							m4=_mm256_add_epi16(m4,m1);m0_prelude_row0=m4;


							m1=_mm256_srli_si256(m0_prelude_row0,2);
							m2=_mm256_add_epi16(m1,m0_prelude_row0);

							m1=_mm256_srli_si256(m0_prelude_row0,4);
							m2=_mm256_add_epi16(m1,m2);

							//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
							//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
							m4=_mm256_and_si256(m0_prelude_row0,mask3);
							m4=_mm256_permute2f128_si256(m4,m4,1);
							m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions
							m2=_mm256_add_epi16(m2,m4);

							//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
							output_row0_odd = _mm256_and_si256(m2,output_mask);

							//----row=1
							//multiply with the mask
							m2=_mm256_maddubs_epi16(r2,c0_sh1);
							m3=_mm256_maddubs_epi16(r3,c1_sh1);
							m4=_mm256_maddubs_epi16(r4,c2_sh1);


							//vertical add
							m4=_mm256_add_epi16(m4,m3);
							m4=_mm256_add_epi16(m4,m2);m0_prelude_row1=m4;


							m1=_mm256_srli_si256(m0_prelude_row1,2);
							m2=_mm256_add_epi16(m1,m0_prelude_row1);

							m1=_mm256_srli_si256(m0_prelude_row1,4);
							m2=_mm256_add_epi16(m1,m2);

							//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
							//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
							m4=_mm256_and_si256(m0_prelude_row1,mask3);
							m4=_mm256_permute2f128_si256(m4,m4,1);
							m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions
							m2=_mm256_add_epi16(m2,m4);

							//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
							output_row1_odd = _mm256_and_si256(m2,output_mask);

					                 //3rd col iteration
							//---row=2
							//multiply with the mask
							m0=_mm256_maddubs_epi16(r0,c0_sh2);
							m1=_mm256_maddubs_epi16(r1,c1_sh2);
							m2=_mm256_maddubs_epi16(r2,c2_sh2);
							m3=_mm256_maddubs_epi16(r3,c3_sh2);
							m4=_mm256_maddubs_epi16(r4,c4_sh2);

							//vertical add
							m4=_mm256_add_epi16(m4,m3);
							m4=_mm256_add_epi16(m4,m2);
							m4=_mm256_add_epi16(m4,m1);
							m0=_mm256_add_epi16(m4,m0);


							m1=_mm256_srli_si256(m0,2);
							m2=_mm256_add_epi16(m1,m0);

							//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,_mm256_extract_epi16(m2,8),0,0,0,0,0,0,0);
							m4=_mm256_and_si256(m2,mask3);
							m4=_mm256_permute2f128_si256(m4,m4,1);
							m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions

							m1=_mm256_srli_si256(m0,4);
							m2=_mm256_add_epi16(m1,m2);

							//prepei na prosthesw to pixel pu thelw sto r2 kai meta na kanw diairesi me ola mazi

							m2=_mm256_add_epi16(m2,m4);

							//m2 has 16 16bit values now. the results I need are in positions 1,4,7,10,13. keep only those, discard others
							m2 = _mm256_and_si256(m2,output_mask_sh1);
							output_even = _mm256_add_epi16(output_even,m2);

							//---row=0
							//multiply with the mask
							m1=_mm256_maddubs_epi16(r1,c0_sh2);
							m2=_mm256_maddubs_epi16(r2,c1_sh2);
							m3=_mm256_maddubs_epi16(r3,c2_sh2);
							m4=_mm256_maddubs_epi16(r4,c3_sh2);


							//vertical add
							m4=_mm256_add_epi16(m4,m3);
							m4=_mm256_add_epi16(m4,m2);
							m4=_mm256_add_epi16(m4,m1);m0_prelude_row0=m4;

							m1=_mm256_srli_si256(m0_prelude_row0,2);
							m2=_mm256_add_epi16(m1,m0_prelude_row0);

							//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,_mm256_extract_epi16(m2,8),0,0,0,0,0,0,0);
							m4=_mm256_and_si256(m2,mask3);
							m4=_mm256_permute2f128_si256(m4,m4,1);
							m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions

							m1=_mm256_srli_si256(m0_prelude_row0,4);
							m2=_mm256_add_epi16(m1,m2);

							//prepei na prosthesw to pixel pu thelw sto r2 kai meta na kanw diairesi me ola mazi

							m2=_mm256_add_epi16(m2,m4);

							//m2 has 16 16bit values now. the results I need are in positions 1,4,7,10,13. keep only those, discard others
							m2 = _mm256_and_si256(m2,output_mask_sh1);
							output_row0_even = _mm256_add_epi16(output_row0_even,m2);


							//---row=1
							//multiply with the mask
							m2=_mm256_maddubs_epi16(r2,c0_sh2);
							m3=_mm256_maddubs_epi16(r3,c1_sh2);
							m4=_mm256_maddubs_epi16(r4,c2_sh2);


							//vertical add
							m4=_mm256_add_epi16(m4,m3);
							m4=_mm256_add_epi16(m4,m2);m0_prelude_row1=m4;


							m1=_mm256_srli_si256(m0_prelude_row1,2);
							m2=_mm256_add_epi16(m1,m0_prelude_row1);

							//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,_mm256_extract_epi16(m2,8),0,0,0,0,0,0,0);
							m4=_mm256_and_si256(m2,mask3);
							m4=_mm256_permute2f128_si256(m4,m4,1);
							m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions

							m1=_mm256_srli_si256(m0_prelude_row1,4);
							m2=_mm256_add_epi16(m1,m2);

							//prepei na prosthesw to pixel pu thelw sto r2 kai meta na kanw diairesi me ola mazi

							m2=_mm256_add_epi16(m2,m4);

							//m2 has 16 16bit values now. the results I need are in positions 1,4,7,10,13. keep only those, discard others
							m2 = _mm256_and_si256(m2,output_mask_sh1);
							output_row1_even = _mm256_add_epi16(output_row1_even,m2);

							//4th col iteration
							//multiply with the mask
							m0=_mm256_maddubs_epi16(r0,c0_sh3);
							m1=_mm256_maddubs_epi16(r1,c1_sh3);
							m2=_mm256_maddubs_epi16(r2,c2_sh3);
							m3=_mm256_maddubs_epi16(r3,c3_sh3);
							m4=_mm256_maddubs_epi16(r4,c4_sh3);

							//vertical add
							m0=_mm256_add_epi16(m0,m1);
							m2=_mm256_add_epi16(m2,m3);
							m0=_mm256_add_epi16(m0,m2);
							m0=_mm256_add_epi16(m0,m4);

							//hozizontal additions
							//hadd(3:8)
							//hadd(9:14)
							//hadd(15:20)
							//hadd(21:26)
							//hadd(27:31)
							//result after division will be in 2,8,14,20,26

							m1=_mm256_srli_si256(m0,2);
							m4=_mm256_add_epi16(m1,m0);

							m1=_mm256_srli_si256(m0,4);
							m2=_mm256_add_epi16(m1,m4);

							//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
							//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
							m4=_mm256_and_si256(m4,mask3);
							m4=_mm256_permute2f128_si256(m4,m4,1);
							m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions

							m2=_mm256_add_epi16(m2,m4);

							//m2 has 16 16bit values now. the results I need are in positions 1,4,7,10,13. keep only those, discard others
							m2 = _mm256_and_si256(m2,output_mask_sh1);
							output_odd = _mm256_add_epi16(output_odd,m2);

							//row0
							//multiply with the mask
								m1=_mm256_maddubs_epi16(r1,c0_sh3);
								m2=_mm256_maddubs_epi16(r2,c1_sh3);
								m3=_mm256_maddubs_epi16(r3,c2_sh3);
								m4=_mm256_maddubs_epi16(r4,c3_sh3);

								//vertical add
								m0=_mm256_add_epi16(m1,m2);
								m0=_mm256_add_epi16(m0,m3);
								m0=_mm256_add_epi16(m0,m4);


								m1=_mm256_srli_si256(m0,2);
								m4=_mm256_add_epi16(m1,m0);

								m1=_mm256_srli_si256(m0,4);
								m2=_mm256_add_epi16(m1,m4);

								//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
								//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
								m4=_mm256_and_si256(m4,mask3);
								m4=_mm256_permute2f128_si256(m4,m4,1);
								m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions

								m2=_mm256_add_epi16(m2,m4);

								//m2 has 16 16bit values now. the results I need are in positions 1,4,7,10,13. keep only those, discard others
								m2 = _mm256_and_si256(m2,output_mask_sh1);
								output_row0_odd = _mm256_add_epi16(output_row0_odd,m2);

								//row1
								//multiply with the mask
									m2=_mm256_maddubs_epi16(r2,c0_sh3);
									m3=_mm256_maddubs_epi16(r3,c1_sh3);
									m4=_mm256_maddubs_epi16(r4,c2_sh3);

									//vertical add
									m0=_mm256_add_epi16(m2,m3);
									m0=_mm256_add_epi16(m0,m4);


									m1=_mm256_srli_si256(m0,2);
									m4=_mm256_add_epi16(m1,m0);

									m1=_mm256_srli_si256(m0,4);
									m2=_mm256_add_epi16(m1,m4);

									//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
									//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
									m4=_mm256_and_si256(m4,mask3);
									m4=_mm256_permute2f128_si256(m4,m4,1);
									m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions

									m2=_mm256_add_epi16(m2,m4);

									//m2 has 16 16bit values now. the results I need are in positions 1,4,7,10,13. keep only those, discard others
									m2 = _mm256_and_si256(m2,output_mask_sh1);
									output_row1_odd = _mm256_add_epi16(output_row1_odd,m2);


							//5th col iteration
							//multiply with the mask
							m0=_mm256_maddubs_epi16(r0,c0_sh4);
							m1=_mm256_maddubs_epi16(r1,c1_sh4);
							m2=_mm256_maddubs_epi16(r2,c2_sh4);
							m3=_mm256_maddubs_epi16(r3,c3_sh4);
							m4=_mm256_maddubs_epi16(r4,c4_sh4);

							//vertical add
							m0=_mm256_add_epi16(m0,m1);
							m2=_mm256_add_epi16(m2,m3);
							m0=_mm256_add_epi16(m0,m2);
							m0=_mm256_add_epi16(m0,m4);

							//hozizontal additions
							//hadd(4:9)
							//hadd(10:15)
							//hadd(16:21)
							//hadd(22:27)
							//result after division will be in 4,10,16,22

							m1=_mm256_srli_si256(m0,2);
							m2=_mm256_add_epi16(m1,m0);

							m1=_mm256_srli_si256(m0,4);
							m2=_mm256_add_epi16(m1,m2);


							//m2 has 16 16bit values now. the results I need are in positions 2,5,8,11. keep only those, discard others
							m2 = _mm256_and_si256(m2,output_mask_sh2);
							output_even = _mm256_add_epi16(output_even,m2);

							//row0
							//multiply with the mask
							m1=_mm256_maddubs_epi16(r1,c0_sh4);
							m2=_mm256_maddubs_epi16(r2,c1_sh4);
							m3=_mm256_maddubs_epi16(r3,c2_sh4);
							m4=_mm256_maddubs_epi16(r4,c3_sh4);

							//vertical add
							m0=_mm256_add_epi16(m1,m2);
							m0=_mm256_add_epi16(m0,m3);
							m0=_mm256_add_epi16(m0,m4);


							m1=_mm256_srli_si256(m0,2);
							m2=_mm256_add_epi16(m1,m0);

							m1=_mm256_srli_si256(m0,4);
							m2=_mm256_add_epi16(m1,m2);


							//m2 has 16 16bit values now. the results I need are in positions 2,5,8,11. keep only those, discard others
							m2 = _mm256_and_si256(m2,output_mask_sh2);
							output_row0_even = _mm256_add_epi16(output_row0_even,m2);

							//row1
							//multiply with the mask
							m2=_mm256_maddubs_epi16(r2,c0_sh4);
							m3=_mm256_maddubs_epi16(r3,c1_sh4);
							m4=_mm256_maddubs_epi16(r4,c2_sh4);

							//vertical add
							m0=_mm256_add_epi16(m2,m3);
							m0=_mm256_add_epi16(m0,m4);


							m1=_mm256_srli_si256(m0,2);
							m2=_mm256_add_epi16(m1,m0);

							m1=_mm256_srli_si256(m0,4);
							m2=_mm256_add_epi16(m1,m2);


							//m2 has 16 16bit values now. the results I need are in positions 2,5,8,11. keep only those, discard others
							m2 = _mm256_and_si256(m2,output_mask_sh2);
							output_row1_even = _mm256_add_epi16(output_row1_even,m2);

					                 //6th col iteration
							//multiply with the mask
							m0=_mm256_maddubs_epi16(r0,c0_sh5);
							m1=_mm256_maddubs_epi16(r1,c1_sh5);
							m2=_mm256_maddubs_epi16(r2,c2_sh5);
							m3=_mm256_maddubs_epi16(r3,c3_sh5);
							m4=_mm256_maddubs_epi16(r4,c4_sh5);

							//vertical add
							m0=_mm256_add_epi16(m0,m1);
							m2=_mm256_add_epi16(m2,m3);
							m0=_mm256_add_epi16(m0,m2);
							m0=_mm256_add_epi16(m0,m4);

							//hozizontal additions
							//hadd(5:10)
							//hadd(11:16)
							//hadd(17:22)
							//hadd(23:28)
							//result after division will be in 4,10,16,22

							m1=_mm256_srli_si256(m0,2);
							m2=_mm256_add_epi16(m1,m0);

							m1=_mm256_srli_si256(m0,4);
							m2=_mm256_add_epi16(m1,m2);

							//m2 has 16 16bit values now. the results I need are in positions 2,5,8,11. keep only those, discard others
							m2 = _mm256_and_si256(m2,output_mask_sh2);
							output_odd = _mm256_add_epi16(output_odd,m2);

							//now division follows
							output_even=division(division_case,output_even,f);
							output_odd=division(division_case,output_odd,f);

							//shift odd 1 position and add to even
							output_odd = _mm256_slli_si256(output_odd,1);
							output_even = _mm256_add_epi8(output_even,output_odd);


							_mm256_storeu_si256( (__m256i *) &filt[N-3][col],output_even );

							//row0
							//multiply with the mask
							m1=_mm256_maddubs_epi16(r1,c0_sh5);
							m2=_mm256_maddubs_epi16(r2,c1_sh5);
							m3=_mm256_maddubs_epi16(r3,c2_sh5);
							m4=_mm256_maddubs_epi16(r4,c3_sh5);

							//vertical add
							m0=_mm256_add_epi16(m1,m2);
							m0=_mm256_add_epi16(m0,m3);
							m0=_mm256_add_epi16(m0,m4);


							m1=_mm256_srli_si256(m0,2);
							m2=_mm256_add_epi16(m1,m0);

							m1=_mm256_srli_si256(m0,4);
							m2=_mm256_add_epi16(m1,m2);

							//m2 has 16 16bit values now. the results I need are in positions 2,5,8,11. keep only those, discard others
							m2 = _mm256_and_si256(m2,output_mask_sh2);
							output_row0_odd = _mm256_add_epi16(output_row0_odd,m2);

							//now division follows
							output_row0_even=division(division_case,output_row0_even,f);
							output_row0_odd=division(division_case,output_row0_odd,f);

							//shift odd 1 position and add to even
							output_row0_odd = _mm256_slli_si256(output_row0_odd,1);
							output_row0_even = _mm256_add_epi8(output_row0_even,output_row0_odd);


							_mm256_storeu_si256( (__m256i *) &filt[N-2][col],output_row0_even );

							//row1
							//multiply with the mask
							m2=_mm256_maddubs_epi16(r2,c0_sh5);
							m3=_mm256_maddubs_epi16(r3,c1_sh5);
							m4=_mm256_maddubs_epi16(r4,c2_sh5);

							//vertical add
							m0=_mm256_add_epi16(m2,m3);
							m0=_mm256_add_epi16(m0,m4);

								m1=_mm256_srli_si256(m0,2);
							m2=_mm256_add_epi16(m1,m0);

							m1=_mm256_srli_si256(m0,4);
							m2=_mm256_add_epi16(m1,m2);

							//m2 has 16 16bit values now. the results I need are in positions 2,5,8,11. keep only those, discard others
							m2 = _mm256_and_si256(m2,output_mask_sh2);
							output_row1_odd = _mm256_add_epi16(output_row1_odd,m2);

							//now division follows
							output_row1_even=division(division_case,output_row1_even,f);
							output_row1_odd=division(division_case,output_row1_odd,f);

							//shift odd 1 position and add to even
							output_row1_odd = _mm256_slli_si256(output_row1_odd,1);
							output_row1_even = _mm256_add_epi8(output_row1_even,output_row1_odd);


							_mm256_storeu_si256( (__m256i *) &filt[N-1][col],output_row1_even );


							}

		    conv_loop_reminder_last_less_div(frame1,filt,M,N,col,REMINDER_ITERATIONS,division_case,mask_vector,f,divisor,filter5x5);

					}
		//an extra condition is needed because of register blocking
		else if ((row==N-2) ){//in this case I calculate just filt[N-2][:] and filt[N-1][:]. Below row0 refers to row=N-2 and row1 refers to row=N-1
					//printf("\nN=%d row=%d N-2=%d",N,row,N-2);
					for (col = 0; col <= M-32; col+=28){
								 //last col value that does not read outside of the array bounds is (col<M-29)

									  if (col==0){

											r1=_mm256_load_si256( (__m256i *) &frame1[N-4][0]);
											r2=_mm256_load_si256( (__m256i *) &frame1[N-3][0]);
											r3=_mm256_load_si256( (__m256i *) &frame1[N-2][0]);
											r4=_mm256_load_si256( (__m256i *) &frame1[N-1][0]);

									  			//START - extra code needed for prelude
									  					m1=_mm256_slli_si256(r1,2);//shift 2 elements left - equivalent to filling with two zeros inthe beginning
									  					m2=_mm256_slli_si256(r2,2);//shift 2 elements left - equivalent to filling with two zeros inthe beginning
									  					m3=_mm256_slli_si256(r3,2);//shift 2 elements left - equivalent to filling with two zeros inthe beginning
									  					m4=_mm256_slli_si256(r4,2);//shift 2 elements left - equivalent to filling with two zeros inthe beginning

									  					r1=_mm256_and_si256(r1,mask_prelude);
									  					r1=_mm256_permute2f128_si256(r1,r1,1);
									  					r1=_mm256_srli_si256(r1,14);//shift 14 elements
									  					r1=_mm256_add_epi16(m1,r1);

									  					r2=_mm256_and_si256(r2,mask_prelude);
									  					r2=_mm256_permute2f128_si256(r2,r2,1);
									  					r2=_mm256_srli_si256(r2,14);//shift 14 elements
									  					r2=_mm256_add_epi16(m2,r2);

									  					r3=_mm256_and_si256(r3,mask_prelude);
									  					r3=_mm256_permute2f128_si256(r3,r3,1);
									  					r3=_mm256_srli_si256(r3,14);//shift 14 elements
									  					r3=_mm256_add_epi16(m3,r3);

									  					r4=_mm256_and_si256(r4,mask_prelude);
									  					r4=_mm256_permute2f128_si256(r4,r4,1);
									  					r4=_mm256_srli_si256(r4,14);//shift 14 elements
									  					r4=_mm256_add_epi16(m4,r4);

									  			//END - extra code needed for prelude
									  		}
									  else {

											r1=_mm256_loadu_si256( (__m256i *) &frame1[N-4][col-2]);
											r2=_mm256_loadu_si256( (__m256i *) &frame1[N-3][col-2]);
											r3=_mm256_loadu_si256( (__m256i *) &frame1[N-2][col-2]);
											r4=_mm256_loadu_si256( (__m256i *) &frame1[N-1][col-2]);
									  }

									//col iteration computes output pixels of 2,8,14,20,26
									// col+1 iteration computes output pixels of 3,9,15,21,27
									// col+2 iteration computes output pixels of 4,10,16,22,28
									// col+3 iteration computes output pixels of 5,11,17,23,29
									// col+4 iteration computes output pixels of 6,12,18,24,30
									// col+5 iteration computes output pixels of 7,13,19,25,31
									//afterwards, col2 becomes 32 and repeat the above process

									//1st col iteration

									//--------------row=0
									//multiply with the mask
									m1=_mm256_maddubs_epi16(r1,c0);
									m2=_mm256_maddubs_epi16(r2,c1);
									m3=_mm256_maddubs_epi16(r3,c2);
									m4=_mm256_maddubs_epi16(r4,c3);

									//vertical add
									m4=_mm256_add_epi16(m4,m3);
									m4=_mm256_add_epi16(m4,m2);
									m4=_mm256_add_epi16(m4,m1); m0_prelude_row0=m4;

									m1=_mm256_srli_si256(m0_prelude_row0,2);
									m2=_mm256_add_epi16(m1,m0_prelude_row0);

									m1=_mm256_srli_si256(m0_prelude_row0,4);
									m2=_mm256_add_epi16(m1,m2);

									//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
									//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
									m4=_mm256_and_si256(m0_prelude_row0,mask3);
									m4=_mm256_permute2f128_si256(m4,m4,1);
									m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions

									m2=_mm256_add_epi16(m2,m4);

									//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
									output_row0_even=_mm256_and_si256(m2,output_mask);

									//--------------row=1
									//multiply with the mask
									m2=_mm256_maddubs_epi16(r2,c0);
									m3=_mm256_maddubs_epi16(r3,c1);
									m4=_mm256_maddubs_epi16(r4,c2);


									//vertical add
									m4=_mm256_add_epi16(m4,m3);
									m4=_mm256_add_epi16(m4,m2);m0_prelude_row1=m4;



									m1=_mm256_srli_si256(m0_prelude_row1,2);
									m2=_mm256_add_epi16(m1,m0_prelude_row1);

									m1=_mm256_srli_si256(m0_prelude_row1,4);
									m2=_mm256_add_epi16(m1,m2);

									//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
									//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
									m4=_mm256_and_si256(m0_prelude_row1,mask3);
									m4=_mm256_permute2f128_si256(m4,m4,1);
									m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions

									m2=_mm256_add_epi16(m2,m4);

									//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
									output_row1_even=_mm256_and_si256(m2,output_mask);

									//2nd col iteration

									//----row=0
									//multiply with the mask
									m1=_mm256_maddubs_epi16(r1,c0_sh1);
									m2=_mm256_maddubs_epi16(r2,c1_sh1);
									m3=_mm256_maddubs_epi16(r3,c2_sh1);
									m4=_mm256_maddubs_epi16(r4,c3_sh1);


									//vertical add
									m4=_mm256_add_epi16(m4,m3);
									m4=_mm256_add_epi16(m4,m2);
									m4=_mm256_add_epi16(m4,m1);m0_prelude_row0=m4;


									m1=_mm256_srli_si256(m0_prelude_row0,2);
									m2=_mm256_add_epi16(m1,m0_prelude_row0);

									m1=_mm256_srli_si256(m0_prelude_row0,4);
									m2=_mm256_add_epi16(m1,m2);

									//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
									//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
									m4=_mm256_and_si256(m0_prelude_row0,mask3);
									m4=_mm256_permute2f128_si256(m4,m4,1);
									m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions
									m2=_mm256_add_epi16(m2,m4);

									//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
									output_row0_odd = _mm256_and_si256(m2,output_mask);

									//----row=1
									//multiply with the mask
									m2=_mm256_maddubs_epi16(r2,c0_sh1);
									m3=_mm256_maddubs_epi16(r3,c1_sh1);
									m4=_mm256_maddubs_epi16(r4,c2_sh1);


									//vertical add
									m4=_mm256_add_epi16(m4,m3);
									m4=_mm256_add_epi16(m4,m2);m0_prelude_row1=m4;


									m1=_mm256_srli_si256(m0_prelude_row1,2);
									m2=_mm256_add_epi16(m1,m0_prelude_row1);

									m1=_mm256_srli_si256(m0_prelude_row1,4);
									m2=_mm256_add_epi16(m1,m2);

									//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
									//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
									m4=_mm256_and_si256(m0_prelude_row1,mask3);
									m4=_mm256_permute2f128_si256(m4,m4,1);
									m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions
									m2=_mm256_add_epi16(m2,m4);

									//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
									output_row1_odd = _mm256_and_si256(m2,output_mask);

							                 //3rd col iteration


									//---row=0
									//multiply with the mask
									m1=_mm256_maddubs_epi16(r1,c0_sh2);
									m2=_mm256_maddubs_epi16(r2,c1_sh2);
									m3=_mm256_maddubs_epi16(r3,c2_sh2);
									m4=_mm256_maddubs_epi16(r4,c3_sh2);


									//vertical add
									m4=_mm256_add_epi16(m4,m3);
									m4=_mm256_add_epi16(m4,m2);
									m4=_mm256_add_epi16(m4,m1);m0_prelude_row0=m4;

									m1=_mm256_srli_si256(m0_prelude_row0,2);
									m2=_mm256_add_epi16(m1,m0_prelude_row0);

									//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,_mm256_extract_epi16(m2,8),0,0,0,0,0,0,0);
									m4=_mm256_and_si256(m2,mask3);
									m4=_mm256_permute2f128_si256(m4,m4,1);
									m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions

									m1=_mm256_srli_si256(m0_prelude_row0,4);
									m2=_mm256_add_epi16(m1,m2);

									//prepei na prosthesw to pixel pu thelw sto r2 kai meta na kanw diairesi me ola mazi

									m2=_mm256_add_epi16(m2,m4);

									//m2 has 16 16bit values now. the results I need are in positions 1,4,7,10,13. keep only those, discard others
									m2 = _mm256_and_si256(m2,output_mask_sh1);
									output_row0_even = _mm256_add_epi16(output_row0_even,m2);


									//---row=1
									//multiply with the mask
									m2=_mm256_maddubs_epi16(r2,c0_sh2);
									m3=_mm256_maddubs_epi16(r3,c1_sh2);
									m4=_mm256_maddubs_epi16(r4,c2_sh2);


									//vertical add
									m4=_mm256_add_epi16(m4,m3);
									m4=_mm256_add_epi16(m4,m2);m0_prelude_row1=m4;


									m1=_mm256_srli_si256(m0_prelude_row1,2);
									m2=_mm256_add_epi16(m1,m0_prelude_row1);

									//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,_mm256_extract_epi16(m2,8),0,0,0,0,0,0,0);
									m4=_mm256_and_si256(m2,mask3);
									m4=_mm256_permute2f128_si256(m4,m4,1);
									m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions

									m1=_mm256_srli_si256(m0_prelude_row1,4);
									m2=_mm256_add_epi16(m1,m2);

									//prepei na prosthesw to pixel pu thelw sto r2 kai meta na kanw diairesi me ola mazi

									m2=_mm256_add_epi16(m2,m4);

									//m2 has 16 16bit values now. the results I need are in positions 1,4,7,10,13. keep only those, discard others
									m2 = _mm256_and_si256(m2,output_mask_sh1);
									output_row1_even = _mm256_add_epi16(output_row1_even,m2);

									//4th col iteration

									//row0
									//multiply with the mask
										m1=_mm256_maddubs_epi16(r1,c0_sh3);
										m2=_mm256_maddubs_epi16(r2,c1_sh3);
										m3=_mm256_maddubs_epi16(r3,c2_sh3);
										m4=_mm256_maddubs_epi16(r4,c3_sh3);

										//vertical add
										m0=_mm256_add_epi16(m1,m2);
										m0=_mm256_add_epi16(m0,m3);
										m0=_mm256_add_epi16(m0,m4);


										m1=_mm256_srli_si256(m0,2);
										m4=_mm256_add_epi16(m1,m0);

										m1=_mm256_srli_si256(m0,4);
										m2=_mm256_add_epi16(m1,m4);

										//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
										//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
										m4=_mm256_and_si256(m4,mask3);
										m4=_mm256_permute2f128_si256(m4,m4,1);
										m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions

										m2=_mm256_add_epi16(m2,m4);

										//m2 has 16 16bit values now. the results I need are in positions 1,4,7,10,13. keep only those, discard others
										m2 = _mm256_and_si256(m2,output_mask_sh1);
										output_row0_odd = _mm256_add_epi16(output_row0_odd,m2);

										//row1
										//multiply with the mask
											m2=_mm256_maddubs_epi16(r2,c0_sh3);
											m3=_mm256_maddubs_epi16(r3,c1_sh3);
											m4=_mm256_maddubs_epi16(r4,c2_sh3);

											//vertical add
											m0=_mm256_add_epi16(m2,m3);
											m0=_mm256_add_epi16(m0,m4);


											m1=_mm256_srli_si256(m0,2);
											m4=_mm256_add_epi16(m1,m0);

											m1=_mm256_srli_si256(m0,4);
											m2=_mm256_add_epi16(m1,m4);

											//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
											//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
											m4=_mm256_and_si256(m4,mask3);
											m4=_mm256_permute2f128_si256(m4,m4,1);
											m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions

											m2=_mm256_add_epi16(m2,m4);

											//m2 has 16 16bit values now. the results I need are in positions 1,4,7,10,13. keep only those, discard others
											m2 = _mm256_and_si256(m2,output_mask_sh1);
											output_row1_odd = _mm256_add_epi16(output_row1_odd,m2);


									//5th col iteration


									//row0
									//multiply with the mask
									m1=_mm256_maddubs_epi16(r1,c0_sh4);
									m2=_mm256_maddubs_epi16(r2,c1_sh4);
									m3=_mm256_maddubs_epi16(r3,c2_sh4);
									m4=_mm256_maddubs_epi16(r4,c3_sh4);

									//vertical add
									m0=_mm256_add_epi16(m1,m2);
									m0=_mm256_add_epi16(m0,m3);
									m0=_mm256_add_epi16(m0,m4);


									m1=_mm256_srli_si256(m0,2);
									m2=_mm256_add_epi16(m1,m0);

									m1=_mm256_srli_si256(m0,4);
									m2=_mm256_add_epi16(m1,m2);


									//m2 has 16 16bit values now. the results I need are in positions 2,5,8,11. keep only those, discard others
									m2 = _mm256_and_si256(m2,output_mask_sh2);
									output_row0_even = _mm256_add_epi16(output_row0_even,m2);

									//row1
									//multiply with the mask
									m2=_mm256_maddubs_epi16(r2,c0_sh4);
									m3=_mm256_maddubs_epi16(r3,c1_sh4);
									m4=_mm256_maddubs_epi16(r4,c2_sh4);

									//vertical add
									m0=_mm256_add_epi16(m2,m3);
									m0=_mm256_add_epi16(m0,m4);


									m1=_mm256_srli_si256(m0,2);
									m2=_mm256_add_epi16(m1,m0);

									m1=_mm256_srli_si256(m0,4);
									m2=_mm256_add_epi16(m1,m2);


									//m2 has 16 16bit values now. the results I need are in positions 2,5,8,11. keep only those, discard others
									m2 = _mm256_and_si256(m2,output_mask_sh2);
									output_row1_even = _mm256_add_epi16(output_row1_even,m2);

							                 //6th col iteration

									//row0
									//multiply with the mask
									m1=_mm256_maddubs_epi16(r1,c0_sh5);
									m2=_mm256_maddubs_epi16(r2,c1_sh5);
									m3=_mm256_maddubs_epi16(r3,c2_sh5);
									m4=_mm256_maddubs_epi16(r4,c3_sh5);

									//vertical add
									m0=_mm256_add_epi16(m1,m2);
									m0=_mm256_add_epi16(m0,m3);
									m0=_mm256_add_epi16(m0,m4);


									m1=_mm256_srli_si256(m0,2);
									m2=_mm256_add_epi16(m1,m0);

									m1=_mm256_srli_si256(m0,4);
									m2=_mm256_add_epi16(m1,m2);

									//m2 has 16 16bit values now. the results I need are in positions 2,5,8,11. keep only those, discard others
									m2 = _mm256_and_si256(m2,output_mask_sh2);
									output_row0_odd = _mm256_add_epi16(output_row0_odd,m2);

									//now division follows
									output_row0_even=division(division_case,output_row0_even,f);
									output_row0_odd=division(division_case,output_row0_odd,f);

									//shift odd 1 position and add to even
									output_row0_odd = _mm256_slli_si256(output_row0_odd,1);
									output_row0_even = _mm256_add_epi8(output_row0_even,output_row0_odd);


									_mm256_storeu_si256( (__m256i *) &filt[N-2][col],output_row0_even );

									//row1
									//multiply with the mask
									m2=_mm256_maddubs_epi16(r2,c0_sh5);
									m3=_mm256_maddubs_epi16(r3,c1_sh5);
									m4=_mm256_maddubs_epi16(r4,c2_sh5);

									//vertical add
									m0=_mm256_add_epi16(m2,m3);
									m0=_mm256_add_epi16(m0,m4);

										m1=_mm256_srli_si256(m0,2);
									m2=_mm256_add_epi16(m1,m0);

									m1=_mm256_srli_si256(m0,4);
									m2=_mm256_add_epi16(m1,m2);

									//m2 has 16 16bit values now. the results I need are in positions 2,5,8,11. keep only those, discard others
									m2 = _mm256_and_si256(m2,output_mask_sh2);
									output_row1_odd = _mm256_add_epi16(output_row1_odd,m2);

									//now division follows
									output_row1_even=division(division_case,output_row1_even,f);
									output_row1_odd=division(division_case,output_row1_odd,f);

									//shift odd 1 position and add to even
									output_row1_odd = _mm256_slli_si256(output_row1_odd,1);
									output_row1_even = _mm256_add_epi8(output_row1_even,output_row1_odd);


									_mm256_storeu_si256( (__m256i *) &filt[N-1][col],output_row1_even );


									}

				    conv_loop_reminder_last_less_div_special_case(frame1,filt,M,N,col,REMINDER_ITERATIONS,division_case,mask_vector,f,divisor,filter5x5);

							}

	else {


	  for (col = 0; col <= M-32; col+=28){
	 //last col value that does not read outside of the array bounds is (col<M-29)

		  if (col==0){

				//load the 5 rows
				r0=_mm256_load_si256( (__m256i *) &frame1[row-2][0]);
				r1=_mm256_load_si256( (__m256i *) &frame1[row-1][0]);
				r2=_mm256_load_si256( (__m256i *) &frame1[row][0]);
				r3=_mm256_load_si256( (__m256i *) &frame1[row+1][0]);
				r4=_mm256_load_si256( (__m256i *) &frame1[row+2][0]);
				r5=_mm256_load_si256( (__m256i *) &frame1[row+3][0]);


		  			//START - extra code needed for prelude
		  					m0=_mm256_slli_si256(r0,2);//shift 2 elements left - equivalent to filling with two zeros inthe beginning
		  					m1=_mm256_slli_si256(r1,2);//shift 2 elements left - equivalent to filling with two zeros inthe beginning
		  					m2=_mm256_slli_si256(r2,2);//shift 2 elements left - equivalent to filling with two zeros inthe beginning
		  					m3=_mm256_slli_si256(r3,2);//shift 2 elements left - equivalent to filling with two zeros inthe beginning
		  					m4=_mm256_slli_si256(r4,2);//shift 2 elements left - equivalent to filling with two zeros inthe beginning
		  					m5=_mm256_slli_si256(r5,2);//shift 2 elements left - equivalent to filling with two zeros inthe beginning


		  					r0=_mm256_and_si256(r0,mask_prelude);
		  					r0=_mm256_permute2f128_si256(r0,r0,1);
		  					r0=_mm256_srli_si256(r0,14);//shift 14 elements
		  					r0=_mm256_add_epi16(m0,r0);

		  					r1=_mm256_and_si256(r1,mask_prelude);
		  					r1=_mm256_permute2f128_si256(r1,r1,1);
		  					r1=_mm256_srli_si256(r1,14);//shift 14 elements
		  					r1=_mm256_add_epi16(m1,r1);

		  					r2=_mm256_and_si256(r2,mask_prelude);
		  					r2=_mm256_permute2f128_si256(r2,r2,1);
		  					r2=_mm256_srli_si256(r2,14);//shift 14 elements
		  					r2=_mm256_add_epi16(m2,r2);

		  					r3=_mm256_and_si256(r3,mask_prelude);
		  					r3=_mm256_permute2f128_si256(r3,r3,1);
		  					r3=_mm256_srli_si256(r3,14);//shift 14 elements
		  					r3=_mm256_add_epi16(m3,r3);

		  					r4=_mm256_and_si256(r4,mask_prelude);
		  					r4=_mm256_permute2f128_si256(r4,r4,1);
		  					r4=_mm256_srli_si256(r4,14);//shift 14 elements
		  					r4=_mm256_add_epi16(m4,r4);

		  					r5=_mm256_and_si256(r5,mask_prelude);
		  					r5=_mm256_permute2f128_si256(r5,r5,1);
		  					r5=_mm256_srli_si256(r5,14);//shift 14 elements
		  					r5=_mm256_add_epi16(m5,r5);

		  			//END - extra code needed for prelude
		  		}
		  else {
				//load the 5 rows
				r0=_mm256_loadu_si256( (__m256i *) &frame1[row-2][col-2]);
				r1=_mm256_loadu_si256( (__m256i *) &frame1[row-1][col-2]);
				r2=_mm256_loadu_si256( (__m256i *) &frame1[row][col-2]);
				r3=_mm256_loadu_si256( (__m256i *) &frame1[row+1][col-2]);
				r4=_mm256_loadu_si256( (__m256i *) &frame1[row+2][col-2]);
				r5=_mm256_loadu_si256( (__m256i *) &frame1[row+3][col-2]);

		  }

		//col iteration computes output pixels of 2,8,14,20,26
		// col+1 iteration computes output pixels of 3,9,15,21,27
		// col+2 iteration computes output pixels of 4,10,16,22,28
		// col+3 iteration computes output pixels of 5,11,17,23,29
		// col+4 iteration computes output pixels of 6,12,18,24
		// col+5 iteration computes output pixels of 7,13,19,25
		//afterwards, col becomes 30 and repeat the above process

		//1st col iteration


		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,c0);
		m1=_mm256_maddubs_epi16(r1,c1);
		m2=_mm256_maddubs_epi16(r2,c2);
		m3=_mm256_maddubs_epi16(r3,c3);
		m4=_mm256_maddubs_epi16(r4,c4);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m2=_mm256_add_epi16(m2,m3);
		m0=_mm256_add_epi16(m0,m2);
		m0=_mm256_add_epi16(m0,m4);

		//hozizontal additions
		//hadd(0:5)
		//hadd(6:11)
		//hadd(12:17)
		//hadd(18:23)
		//hadd(24:28)

		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		m1=_mm256_srli_si256(m0,4);
		m2=_mm256_add_epi16(m1,m2);

		//after shifts the 15th element is lost as it cannot be propagated to the 16th position (AVX registers are managed as two seperate SSE registers)
		m4=_mm256_and_si256(m0,mask3);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions
		m2=_mm256_add_epi16(m2,m4);

		//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
		output_even=_mm256_and_si256(m2,output_mask);


		//2nd col iteration
		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,c0_sh1);
		m1=_mm256_maddubs_epi16(r1,c1_sh1);
		m2=_mm256_maddubs_epi16(r2,c2_sh1);
		m3=_mm256_maddubs_epi16(r3,c3_sh1);
		m4=_mm256_maddubs_epi16(r4,c4_sh1);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m2=_mm256_add_epi16(m2,m3);
		m0=_mm256_add_epi16(m0,m2);
		m0=_mm256_add_epi16(m0,m4);

		//hozizontal additions
		//hadd(1:6)
		//hadd(7:12)
		//hadd(13:18)
		//hadd(19:24)
		//hadd(25:29)

		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		m1=_mm256_srli_si256(m0,4);
		m2=_mm256_add_epi16(m1,m2);

		//after shifts the 15th element is lost as it cannot be propagated to the 16th position (AVX registers are managed as two seperate SSE registers)
		m4=_mm256_and_si256(m0,mask3);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions
		m2=_mm256_add_epi16(m2,m4);

		//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
		output_odd = _mm256_and_si256(m2,output_mask);



         //3rd col iteration
		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,c0_sh2);
		m1=_mm256_maddubs_epi16(r1,c1_sh2);
		m2=_mm256_maddubs_epi16(r2,c2_sh2);
		m3=_mm256_maddubs_epi16(r3,c3_sh2);
		m4=_mm256_maddubs_epi16(r4,c4_sh2);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m2=_mm256_add_epi16(m2,m3);
		m0=_mm256_add_epi16(m0,m2);
		m0=_mm256_add_epi16(m0,m4);

		//hozizontal additions
		//hadd(2:7)
		//hadd(8:13)
		//hadd(14:19)
		//hadd(20:25)
		//hadd(26:30)

		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		m4=_mm256_and_si256(m2,mask3);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions

		m1=_mm256_srli_si256(m0,4);
		m2=_mm256_add_epi16(m1,m2);
		m2=_mm256_add_epi16(m2,m4);

		//m2 has 16 16bit values now. the results I need are in positions 1,4,7,10,13. keep only those, discard others
		m2 = _mm256_and_si256(m2,output_mask_sh1);
		output_even = _mm256_add_epi16(output_even,m2);


		//4th col iteration

		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,c0_sh3);
		m1=_mm256_maddubs_epi16(r1,c1_sh3);
		m2=_mm256_maddubs_epi16(r2,c2_sh3);
		m3=_mm256_maddubs_epi16(r3,c3_sh3);
		m4=_mm256_maddubs_epi16(r4,c4_sh3);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m2=_mm256_add_epi16(m2,m3);
		m0=_mm256_add_epi16(m0,m2);
		m0=_mm256_add_epi16(m0,m4);

		//hozizontal additions
		//hadd(3:8)
		//hadd(9:14)
		//hadd(15:20)
		//hadd(21:26)
		//hadd(27:31)
		//result after division will be in 2,8,14,20,26

		m1=_mm256_srli_si256(m0,2);
		m4=_mm256_add_epi16(m1,m0);

		m1=_mm256_srli_si256(m0,4);
		m2=_mm256_add_epi16(m1,m4);


		m4=_mm256_and_si256(m4,mask3);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions

		m2=_mm256_add_epi16(m2,m4);

		//m2 has 16 16bit values now. the results I need are in positions 1,4,7,10,13. keep only those, discard others
		m2 = _mm256_and_si256(m2,output_mask_sh1);
		output_odd = _mm256_add_epi16(output_odd,m2);




		//5th col iteration
		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,c0_sh4);
		m1=_mm256_maddubs_epi16(r1,c1_sh4);
		m2=_mm256_maddubs_epi16(r2,c2_sh4);
		m3=_mm256_maddubs_epi16(r3,c3_sh4);
		m4=_mm256_maddubs_epi16(r4,c4_sh4);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m2=_mm256_add_epi16(m2,m3);
		m0=_mm256_add_epi16(m0,m2);
		m0=_mm256_add_epi16(m0,m4);

		//hozizontal additions
		//hadd(4:9)
		//hadd(10:15)
		//hadd(16:21)
		//hadd(22:27)
		//result after division will be in 4,10,16,22

		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		m1=_mm256_srli_si256(m0,4);
		m2=_mm256_add_epi16(m1,m2);


		//m2 has 16 16bit values now. the results I need are in positions 2,5,8,11. keep only those, discard others
		m2 = _mm256_and_si256(m2,output_mask_sh2);
		output_even = _mm256_add_epi16(output_even,m2);



                 //6th col iteration
		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,c0_sh5);
		m1=_mm256_maddubs_epi16(r1,c1_sh5);
		m2=_mm256_maddubs_epi16(r2,c2_sh5);
		m3=_mm256_maddubs_epi16(r3,c3_sh5);
		m4=_mm256_maddubs_epi16(r4,c4_sh5);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m2=_mm256_add_epi16(m2,m3);
		m0=_mm256_add_epi16(m0,m2);
		m0=_mm256_add_epi16(m0,m4);

		//hozizontal additions
		//hadd(5:10)
		//hadd(11:16)
		//hadd(17:22)
		//hadd(23:28)
		//result after division will be in 4,10,16,22

		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		m1=_mm256_srli_si256(m0,4);
		m2=_mm256_add_epi16(m1,m2);

		//m2 has 16 16bit values now. the results I need are in positions 2,5,8,11. keep only those, discard others
		m2 = _mm256_and_si256(m2,output_mask_sh2);
		output_odd = _mm256_add_epi16(output_odd,m2);

		//now division follows
		output_even=division(division_case,output_even,f);
		output_odd=division(division_case,output_odd,f);

		//shift odd 1 position and add to even
		output_odd = _mm256_slli_si256(output_odd,1);
		output_even = _mm256_add_epi8(output_even,output_odd);


		_mm256_storeu_si256( (__m256i *) &filt[row][col],output_even );


		//row+1


		//multiply with the mask
		m0=_mm256_maddubs_epi16(r1,c0);
		m1=_mm256_maddubs_epi16(r2,c1);
		m2=_mm256_maddubs_epi16(r3,c2);
		m3=_mm256_maddubs_epi16(r4,c3);
		m4=_mm256_maddubs_epi16(r5,c4);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m2=_mm256_add_epi16(m2,m3);
		m0=_mm256_add_epi16(m0,m2);
		m0=_mm256_add_epi16(m0,m4);

		//hozizontal additions
		//hadd(0:5)
		//hadd(6:11)
		//hadd(12:17)
		//hadd(18:23)
		//hadd(24:28)

		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		m1=_mm256_srli_si256(m0,4);
		m2=_mm256_add_epi16(m1,m2);

		//after shifts the 15th element is lost as it cannot be propagated to the 16th position (AVX registers are managed as two seperate SSE registers)
		m4=_mm256_and_si256(m0,mask3);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions
		m2=_mm256_add_epi16(m2,m4);

		//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
		output_even=_mm256_and_si256(m2,output_mask);


		//2nd col iteration
		//multiply with the mask
		m0=_mm256_maddubs_epi16(r1,c0_sh1);
		m1=_mm256_maddubs_epi16(r2,c1_sh1);
		m2=_mm256_maddubs_epi16(r3,c2_sh1);
		m3=_mm256_maddubs_epi16(r4,c3_sh1);
		m4=_mm256_maddubs_epi16(r5,c4_sh1);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m2=_mm256_add_epi16(m2,m3);
		m0=_mm256_add_epi16(m0,m2);
		m0=_mm256_add_epi16(m0,m4);

		//hozizontal additions
		//hadd(1:6)
		//hadd(7:12)
		//hadd(13:18)
		//hadd(19:24)
		//hadd(25:29)

		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		m1=_mm256_srli_si256(m0,4);
		m2=_mm256_add_epi16(m1,m2);

		//after shifts the 15th element is lost as it cannot be propagated to the 16th position (AVX registers are managed as two seperate SSE registers)
		m4=_mm256_and_si256(m0,mask3);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions
		m2=_mm256_add_epi16(m2,m4);

		//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
		output_odd = _mm256_and_si256(m2,output_mask);



         //3rd col iteration
		//multiply with the mask
		m0=_mm256_maddubs_epi16(r1,c0_sh2);
		m1=_mm256_maddubs_epi16(r2,c1_sh2);
		m2=_mm256_maddubs_epi16(r3,c2_sh2);
		m3=_mm256_maddubs_epi16(r4,c3_sh2);
		m4=_mm256_maddubs_epi16(r5,c4_sh2);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m2=_mm256_add_epi16(m2,m3);
		m0=_mm256_add_epi16(m0,m2);
		m0=_mm256_add_epi16(m0,m4);

		//hozizontal additions
		//hadd(2:7)
		//hadd(8:13)
		//hadd(14:19)
		//hadd(20:25)
		//hadd(26:30)

		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		m4=_mm256_and_si256(m2,mask3);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions

		m1=_mm256_srli_si256(m0,4);
		m2=_mm256_add_epi16(m1,m2);
		m2=_mm256_add_epi16(m2,m4);

		//m2 has 16 16bit values now. the results I need are in positions 1,4,7,10,13. keep only those, discard others
		m2 = _mm256_and_si256(m2,output_mask_sh1);
		output_even = _mm256_add_epi16(output_even,m2);


		//4th col iteration

		//multiply with the mask
		m0=_mm256_maddubs_epi16(r1,c0_sh3);
		m1=_mm256_maddubs_epi16(r2,c1_sh3);
		m2=_mm256_maddubs_epi16(r3,c2_sh3);
		m3=_mm256_maddubs_epi16(r4,c3_sh3);
		m4=_mm256_maddubs_epi16(r5,c4_sh3);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m2=_mm256_add_epi16(m2,m3);
		m0=_mm256_add_epi16(m0,m2);
		m0=_mm256_add_epi16(m0,m4);

		//hozizontal additions
		//hadd(3:8)
		//hadd(9:14)
		//hadd(15:20)
		//hadd(21:26)
		//hadd(27:31)
		//result after division will be in 2,8,14,20,26

		m1=_mm256_srli_si256(m0,2);
		m4=_mm256_add_epi16(m1,m0);

		m1=_mm256_srli_si256(m0,4);
		m2=_mm256_add_epi16(m1,m4);


		m4=_mm256_and_si256(m4,mask3);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions

		m2=_mm256_add_epi16(m2,m4);

		//m2 has 16 16bit values now. the results I need are in positions 1,4,7,10,13. keep only those, discard others
		m2 = _mm256_and_si256(m2,output_mask_sh1);
		output_odd = _mm256_add_epi16(output_odd,m2);




		//5th col iteration
		//multiply with the mask
		m0=_mm256_maddubs_epi16(r1,c0_sh4);
		m1=_mm256_maddubs_epi16(r2,c1_sh4);
		m2=_mm256_maddubs_epi16(r3,c2_sh4);
		m3=_mm256_maddubs_epi16(r4,c3_sh4);
		m4=_mm256_maddubs_epi16(r5,c4_sh4);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m2=_mm256_add_epi16(m2,m3);
		m0=_mm256_add_epi16(m0,m2);
		m0=_mm256_add_epi16(m0,m4);

		//hozizontal additions
		//hadd(4:9)
		//hadd(10:15)
		//hadd(16:21)
		//hadd(22:27)
		//result after division will be in 4,10,16,22

		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		m1=_mm256_srli_si256(m0,4);
		m2=_mm256_add_epi16(m1,m2);


		//m2 has 16 16bit values now. the results I need are in positions 2,5,8,11. keep only those, discard others
		m2 = _mm256_and_si256(m2,output_mask_sh2);
		output_even = _mm256_add_epi16(output_even,m2);



                 //6th col iteration
		//multiply with the mask
		m0=_mm256_maddubs_epi16(r1,c0_sh5);
		m1=_mm256_maddubs_epi16(r2,c1_sh5);
		m2=_mm256_maddubs_epi16(r3,c2_sh5);
		m3=_mm256_maddubs_epi16(r4,c3_sh5);
		m4=_mm256_maddubs_epi16(r5,c4_sh5);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m2=_mm256_add_epi16(m2,m3);
		m0=_mm256_add_epi16(m0,m2);
		m0=_mm256_add_epi16(m0,m4);

		//hozizontal additions
		//hadd(5:10)
		//hadd(11:16)
		//hadd(17:22)
		//hadd(23:28)
		//result after division will be in 4,10,16,22

		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		m1=_mm256_srli_si256(m0,4);
		m2=_mm256_add_epi16(m1,m2);

		//m2 has 16 16bit values now. the results I need are in positions 2,5,8,11. keep only those, discard others
		m2 = _mm256_and_si256(m2,output_mask_sh2);
		output_odd = _mm256_add_epi16(output_odd,m2);

		//now division follows
		output_even=division(division_case,output_even,f);
		output_odd=division(division_case,output_odd,f);

		//shift odd 1 position and add to even
		output_odd = _mm256_slli_si256(output_odd,1);
		output_even = _mm256_add_epi8(output_even,output_odd);


		_mm256_storeu_si256( (__m256i *) &filt[row+1][col],output_even );


		}

        if (REMINDER_ITERATIONS>=29){
        	conv_loop_reminder_high_reminder_values_less_div(frame1,filt,M,N,row,col,REMINDER_ITERATIONS,division_case,mask_vector,f,divisor,filter5x5);
        	conv_loop_reminder_high_reminder_values_less_div(frame1,filt,M,N,row+1,col,REMINDER_ITERATIONS,division_case,mask_vector,f,divisor,filter5x5);

        }
        else{
        	conv_loop_reminder_low_reminder_values_less_div(frame1,filt,M,N,row,col,REMINDER_ITERATIONS,division_case,mask_vector,f);
        	conv_loop_reminder_low_reminder_values_less_div(frame1,filt,M,N,row+1,col,REMINDER_ITERATIONS,division_case,mask_vector,f);

        }

	}

}

}//end of parallel


}




//for REMINDER_ITERATIONS >=29 only
int conv_loop_reminder_high_reminder_values_less_div(unsigned char **frame1,unsigned char **filt,const unsigned int M, const unsigned int N,const unsigned int row, const unsigned int col, const unsigned int REMINDER_ITERATIONS,const unsigned int division_case,const signed char mask_vector[][32], const __m256i f, const unsigned short int divisor, signed char **filter ){

__m256i r0,r1,r2,r3,r4,m0,m1,m2,m3,m4;
__m256i output_even,output_odd;


		//const __m256i f  = _mm256_set_epi16(52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759);
		//const __m256i mask  = _mm256_set_epi8(0,0,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255);
		//const __m256i mask2  = _mm256_set_epi8(0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0);
		const __m256i mask3  = _mm256_set_epi16(0,0,0,0,0,0,0,65535,0,0,0,0,0,0,0,0);
		//const __m256i mask4  = _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255,0,0,0,0,0,0,0,0,0,0,0,0);
		//const __m256i mask5  = _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0);

		const __m256i output_mask      = _mm256_set_epi16(0,0,0,65535,0,0,65535,0,0,65535,0,0,65535,0,0,65535);
		const __m256i output_mask_sh1  = _mm256_set_epi16(0,0,65535,0,0,65535,0,0,65535,0,0,65535,0,0,65535,0);
		//const __m256i output_mask_sh2  = _mm256_set_epi16(0,0,0,    0,65535,0,0,65535,0,0,65535,0,0,65535,0,0);
		const __m256i mask_new      = _mm256_set_epi16(0,0,0,0,0,0,0,0,0,65535,0,0,0,0,0,0);
		const __m256i mask_new2      = _mm256_set_epi16(0,0,0,0,0,0,0,0,65535,0,0,0,0,0,0,0);

		const __m256i c0=_mm256_load_si256( (__m256i *) &mask_vector[0][0]);
		const __m256i c1=_mm256_load_si256( (__m256i *) &mask_vector[1][0]);
		const __m256i c2=_mm256_load_si256( (__m256i *) &mask_vector[2][0]);
		const __m256i c3=_mm256_load_si256( (__m256i *) &mask_vector[3][0]);
		const __m256i c4=_mm256_load_si256( (__m256i *) &mask_vector[4][0]);

		const __m256i c0_sh1=_mm256_load_si256( (__m256i *) &mask_vector[5][0]);
		const __m256i c1_sh1=_mm256_load_si256( (__m256i *) &mask_vector[6][0]);
		const __m256i c2_sh1=_mm256_load_si256( (__m256i *) &mask_vector[7][0]);
		const __m256i c3_sh1=_mm256_load_si256( (__m256i *) &mask_vector[8][0]);
		const __m256i c4_sh1=_mm256_load_si256( (__m256i *) &mask_vector[9][0]);

		const __m256i c0_sh2=_mm256_load_si256( (__m256i *) &mask_vector[10][0]);
		const __m256i c1_sh2=_mm256_load_si256( (__m256i *) &mask_vector[11][0]);
		const __m256i c2_sh2=_mm256_load_si256( (__m256i *) &mask_vector[12][0]);
		const __m256i c3_sh2=_mm256_load_si256( (__m256i *) &mask_vector[13][0]);
		const __m256i c4_sh2=_mm256_load_si256( (__m256i *) &mask_vector[14][0]);




 __m256i reminder_mask1,reminder_mask2;

	reminder_mask1=_mm256_load_si256( (__m256i *) &reminder_msk1[REMINDER_ITERATIONS-1][0]);
	reminder_mask2=_mm256_load_si256( (__m256i *) &reminder_msk2[REMINDER_ITERATIONS-1][0]);




	 //1st col iteration
	//load the 5 rows
	r0=_mm256_loadu_si256( (__m256i *) &frame1[row-2][col-2]);
	r1=_mm256_loadu_si256( (__m256i *) &frame1[row-1][col-2]);
	r2=_mm256_loadu_si256( (__m256i *) &frame1[row][col-2]);
	r3=_mm256_loadu_si256( (__m256i *) &frame1[row+1][col-2]);
	r4=_mm256_loadu_si256( (__m256i *) &frame1[row+2][col-2]);

	/*
	 * The above load operations load outside of the array bounds; these elements are filled with zeros just after
	 * Furthermore, I add two extra zeros in the end to compute col=N-2 and col=N-1. The last value of col is N-1, not N
	 */

	//AND r0-r4 with reminder_mask
	r0=_mm256_and_si256(r0,reminder_mask1);
	r1=_mm256_and_si256(r1,reminder_mask1);
	r2=_mm256_and_si256(r2,reminder_mask1);
	r3=_mm256_and_si256(r3,reminder_mask1);
	r4=_mm256_and_si256(r4,reminder_mask1);

	//multiply with the mask
	m0=_mm256_maddubs_epi16(r0,c0);
	m1=_mm256_maddubs_epi16(r1,c1);
	m2=_mm256_maddubs_epi16(r2,c2);
	m3=_mm256_maddubs_epi16(r3,c3);
	m4=_mm256_maddubs_epi16(r4,c4);

	//vertical add
	m0=_mm256_add_epi16(m0,m1);
	m2=_mm256_add_epi16(m2,m3);
	m0=_mm256_add_epi16(m0,m2);
	m0=_mm256_add_epi16(m0,m4);


	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);

	m1=_mm256_srli_si256(m0,4);
	m2=_mm256_add_epi16(m1,m2);

	//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
	//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
	m4=_mm256_and_si256(m0,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions

	m2=_mm256_add_epi16(m2,m4);

	//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
	output_even=_mm256_and_si256(m2,output_mask);

	//2nd col iteration
	//multiply with the mask
	m0=_mm256_maddubs_epi16(r0,c0_sh1);
	m1=_mm256_maddubs_epi16(r1,c1_sh1);
	m2=_mm256_maddubs_epi16(r2,c2_sh1);
	m3=_mm256_maddubs_epi16(r3,c3_sh1);
	m4=_mm256_maddubs_epi16(r4,c4_sh1);

	//vertical add
	m0=_mm256_add_epi16(m0,m1);
	m2=_mm256_add_epi16(m2,m3);
	m0=_mm256_add_epi16(m0,m2);
	m0=_mm256_add_epi16(m0,m4);

	//hozizontal additions
	//hadd(0:5)   and store filt[row[col]
	//hadd(6:11)   and store filt[row[col+8]
	//hadd(12:17) and store filt[row[col+14]
	//hadd(18:23) and store filt[row[col+20]
	//hadd(24:30) and store filt[row[col+26]

	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);

	m1=_mm256_srli_si256(m0,4);
	m2=_mm256_add_epi16(m1,m2);

	//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
	//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
	m4=_mm256_and_si256(m0,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions
	m2=_mm256_add_epi16(m2,m4);

	//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
	output_odd = _mm256_and_si256(m2,output_mask);

             //3rd col iteration
	//multiply with the mask
	m0=_mm256_maddubs_epi16(r0,c0_sh2);
	m1=_mm256_maddubs_epi16(r1,c1_sh2);
	m2=_mm256_maddubs_epi16(r2,c2_sh2);
	m3=_mm256_maddubs_epi16(r3,c3_sh2);
	m4=_mm256_maddubs_epi16(r4,c4_sh2);

	//vertical add
	m0=_mm256_add_epi16(m0,m1);
	m2=_mm256_add_epi16(m2,m3);
	m0=_mm256_add_epi16(m0,m2);
	m0=_mm256_add_epi16(m0,m4);

	//hozizontal additions
	//hadd(0:5)   and store filt[row[col]
	//hadd(6:11)   and store filt[row[col+8]
	//hadd(12:17) and store filt[row[col+14]
	//hadd(18:23) and store filt[row[col+20]
	//hadd(24:30) and store filt[row[col+26]

	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);

	//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,_mm256_extract_epi16(m2,8),0,0,0,0,0,0,0);
	m4=_mm256_and_si256(m2,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions

	m1=_mm256_srli_si256(m0,4);
	m2=_mm256_add_epi16(m1,m2);

	//prepei na prosthesw to pixel pu thelw sto r2 kai meta na kanw diairesi me ola mazi

	m2=_mm256_add_epi16(m2,m4);

	//m2 has 16 16bit values now. the results I need are in positions 1,4,7,10,13. keep only those, discard others
	m2 = _mm256_and_si256(m2,output_mask_sh1);
	output_even = _mm256_add_epi16(output_even,m2);


	//4th col iteration
	//load the 5 rows
	r0=_mm256_loadu_si256( (__m256i *) &frame1[row-2][col+3-2]);
	r1=_mm256_loadu_si256( (__m256i *) &frame1[row-1][col+3-2]);
	r2=_mm256_loadu_si256( (__m256i *) &frame1[row][col+3-2]);
	r3=_mm256_loadu_si256( (__m256i *) &frame1[row+1][col+3-2]);
	r4=_mm256_loadu_si256( (__m256i *) &frame1[row+2][col+3-2]);

	//AND r0-r4 with reminder_mask
	r0=_mm256_and_si256(r0,reminder_mask2);
	r1=_mm256_and_si256(r1,reminder_mask2);
	r2=_mm256_and_si256(r2,reminder_mask2);
	r3=_mm256_and_si256(r3,reminder_mask2);
	r4=_mm256_and_si256(r4,reminder_mask2);

	//multiply with the mask
	m0=_mm256_maddubs_epi16(r0,c0);
	m1=_mm256_maddubs_epi16(r1,c1);
	m2=_mm256_maddubs_epi16(r2,c2);
	m3=_mm256_maddubs_epi16(r3,c3);
	m4=_mm256_maddubs_epi16(r4,c4);

	//vertical add
	m0=_mm256_add_epi16(m0,m1);
	m2=_mm256_add_epi16(m2,m3);
	m0=_mm256_add_epi16(m0,m2);
	m0=_mm256_add_epi16(m0,m4);


	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);

	m1=_mm256_srli_si256(m0,4);
	m2=_mm256_add_epi16(m1,m2);

	//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
	//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
	m4=_mm256_and_si256(m0,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions

	m2=_mm256_add_epi16(m2,m4);

	//m2 has 16 16bit values now. the results I need are in positions 1,4,7,10,13. keep only those, discard others
	m2 = _mm256_and_si256(m2,output_mask);
	m2 = _mm256_slli_si256(m2,2);
	output_odd = _mm256_add_epi16(output_odd,m2);



	//5th col iteration
	//multiply with the mask
	m0=_mm256_maddubs_epi16(r0,c0_sh1);
	m1=_mm256_maddubs_epi16(r1,c1_sh1);
	m2=_mm256_maddubs_epi16(r2,c2_sh1);
	m3=_mm256_maddubs_epi16(r3,c3_sh1);
	m4=_mm256_maddubs_epi16(r4,c4_sh1);

	//vertical add
	m0=_mm256_add_epi16(m0,m1);
	m2=_mm256_add_epi16(m2,m3);
	m0=_mm256_add_epi16(m0,m2);
	m0=_mm256_add_epi16(m0,m4);


	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);

	m1=_mm256_srli_si256(m0,4);
	m2=_mm256_add_epi16(m1,m2);

	//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
	//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
	m4=_mm256_and_si256(m0,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions

	m2=_mm256_add_epi16(m2,m4);

	//m2 has 16 16bit values now.
	m2 = _mm256_and_si256(m2,output_mask); //holds results in positions 0,3,6,9,12
	m1 = _mm256_slli_si256(m2,4); //put results in positions 2,5,8,11,14 , but 8 is not written
	m2=_mm256_and_si256(m2,mask_new);//extract only the 6th
	m2=_mm256_permute2f128_si256(m2,m2,1);
	m2=_mm256_srli_si256(m2,12);
	m2 = _mm256_add_epi16(m1,m2);
	output_even = _mm256_add_epi16(output_even,m2);



             //6th col iteration
	//multiply with the mask
	m0=_mm256_maddubs_epi16(r0,c0_sh2);
	m1=_mm256_maddubs_epi16(r1,c1_sh2);
	m2=_mm256_maddubs_epi16(r2,c2_sh2);
	m3=_mm256_maddubs_epi16(r3,c3_sh2);
	m4=_mm256_maddubs_epi16(r4,c4_sh2);

	//vertical add
	m0=_mm256_add_epi16(m0,m1);
	m2=_mm256_add_epi16(m2,m3);
	m0=_mm256_add_epi16(m0,m2);
	m0=_mm256_add_epi16(m0,m4);


	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);
	//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,_mm256_extract_epi16(m2,8),0,0,0,0,0,0,0);
	m4=_mm256_and_si256(m2,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions

	m1=_mm256_srli_si256(m0,4);
	m2=_mm256_add_epi16(m1,m2);
	m2=_mm256_add_epi16(m2,m4);

	//m2 has 16 16bit values now.
	m2 = _mm256_and_si256(m2,output_mask_sh1);//holds results in positions 1,4,7,10,13
	m1 = _mm256_slli_si256(m2,2); //put results in positions 2,5,8,11,14 , but 8 is not written
	m2=_mm256_and_si256(m2,mask_new2);//extract only the 7th
	m2=_mm256_permute2f128_si256(m2,m2,1);
	m2=_mm256_srli_si256(m2,14);
	m2 = _mm256_add_epi16(m1,m2);
	output_odd = _mm256_add_epi16(output_odd,m2);


	//now division follows
	output_even=division(division_case,output_even,f);
	output_odd=division(division_case,output_odd,f);

	//shift odd 1 position and add to even
	output_odd = _mm256_slli_si256(output_odd,1);
	output_even = _mm256_add_epi8(output_even,output_odd);


	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels

	switch (REMINDER_ITERATIONS){
	case 29:
		filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
		filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
		filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
		filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output_even,19);
		filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output_even,20);
		filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output_even,21);
		filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output_even,22);
		filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output_even,23);
		filt[row][col+24] = (unsigned char) _mm256_extract_epi8(output_even,24);
		filt[row][col+25] = (unsigned char) _mm256_extract_epi8(output_even,25);
		filt[row][col+26] = (unsigned char) _mm256_extract_epi8(output_even,26);
		filt[row][col+27] = (unsigned char) _mm256_extract_epi8(output_even,27);
		filt[row][col+28] = (unsigned char) _mm256_extract_epi8(output_even,28);
		  break;
	case 30:
		filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
		filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
		filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
		filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output_even,19);
		filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output_even,20);
		filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output_even,21);
		filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output_even,22);
		filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output_even,23);
		filt[row][col+24] = (unsigned char) _mm256_extract_epi8(output_even,24);
		filt[row][col+25] = (unsigned char) _mm256_extract_epi8(output_even,25);
		filt[row][col+26] = (unsigned char) _mm256_extract_epi8(output_even,26);
		filt[row][col+27] = (unsigned char) _mm256_extract_epi8(output_even,27);
		filt[row][col+28] = (unsigned char) _mm256_extract_epi8(output_even,28);
		filt[row][col+29] = (unsigned char) _mm256_extract_epi8(output_even,29);
		  break;
	case 31:
		filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
		filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
		filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
		filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output_even,19);
		filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output_even,20);
		filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output_even,21);
		filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output_even,22);
		filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output_even,23);
		filt[row][col+24] = (unsigned char) _mm256_extract_epi8(output_even,24);
		filt[row][col+25] = (unsigned char) _mm256_extract_epi8(output_even,25);
		filt[row][col+26] = (unsigned char) _mm256_extract_epi8(output_even,26);
		filt[row][col+27] = (unsigned char) _mm256_extract_epi8(output_even,27);
		filt[row][col+28] = (unsigned char) _mm256_extract_epi8(output_even,28);
		filt[row][col+29] = (unsigned char) _mm256_extract_epi8(output_even,29);

		//the filt[row][M-1] pixel will be computed without vectorization
		int newPixel = 0;
		newPixel += frame1[row-2][M-1-2] * filter[0][0];
		newPixel += frame1[row-2][M-1-1] * filter[0][1];
		newPixel += frame1[row-2][M-1-0] * filter[0][2];

		newPixel += frame1[row-1][M-1-2] * filter[1][0];
		newPixel += frame1[row-1][M-1-1] * filter[1][1];
		newPixel += frame1[row-1][M-1-0] * filter[1][2];

		newPixel += frame1[row-0][M-1-2] * filter[2][0];
		newPixel += frame1[row-0][M-1-1] * filter[2][1];
		newPixel += frame1[row-0][M-1-0] * filter[2][2];

		newPixel += frame1[row+1][M-1-2] * filter[3][0];
		newPixel += frame1[row+1][M-1-1] * filter[3][1];
		newPixel += frame1[row+1][M-1-0] * filter[3][2];

		newPixel += frame1[row+2][M-1-2] * filter[4][0];
		newPixel += frame1[row+2][M-1-1] * filter[4][1];
		newPixel += frame1[row+2][M-1-0] * filter[4][2];

    	filt[row][M-1] = (unsigned char) (newPixel / divisor);

		  break;
	default:
		printf("\n something went wrong");
	}

return 0;

}





//for loop reminder<29 only
int conv_loop_reminder_low_reminder_values_less_div(unsigned char **frame1,unsigned char **filt,const unsigned int M, const unsigned int N,const unsigned int row, const unsigned int col,const unsigned int REMINDER_ITERATIONS,const unsigned int division_case,const signed char mask_vector[][32] ,const __m256i f){

register	__m256i r0,r1,r2,r3,r4,m0,m1,m2,m3,m4,output_even,output_odd;


	//const __m256i f  = _mm256_set_epi16(52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759);
const __m256i output_mask      = _mm256_set_epi16(0,0,0,65535,0,0,65535,0,0,65535,0,0,65535,0,0,65535);
const __m256i output_mask_sh1  = _mm256_set_epi16(0,0,65535,0,0,65535,0,0,65535,0,0,65535,0,0,65535,0);
const __m256i output_mask_sh2  = _mm256_set_epi16(0,0,0,    0,65535,0,0,65535,0,0,65535,0,0,65535,0,0);
const __m256i mask3  = _mm256_set_epi16(0,0,0,0,0,0,0,65535,0,0,0,0,0,0,0,0);

const __m256i c0=_mm256_load_si256( (__m256i *) &mask_vector[0][0]);
	const __m256i c1=_mm256_load_si256( (__m256i *) &mask_vector[1][0]);
	const __m256i c2=_mm256_load_si256( (__m256i *) &mask_vector[2][0]);
	const __m256i c3=_mm256_load_si256( (__m256i *) &mask_vector[3][0]);
	const __m256i c4=_mm256_load_si256( (__m256i *) &mask_vector[4][0]);

	const __m256i c0_sh1=_mm256_load_si256( (__m256i *) &mask_vector[5][0]);
	const __m256i c1_sh1=_mm256_load_si256( (__m256i *) &mask_vector[6][0]);
	const __m256i c2_sh1=_mm256_load_si256( (__m256i *) &mask_vector[7][0]);
	const __m256i c3_sh1=_mm256_load_si256( (__m256i *) &mask_vector[8][0]);
	const __m256i c4_sh1=_mm256_load_si256( (__m256i *) &mask_vector[9][0]);

	const __m256i c0_sh2=_mm256_load_si256( (__m256i *) &mask_vector[10][0]);
	const __m256i c1_sh2=_mm256_load_si256( (__m256i *) &mask_vector[11][0]);
	const __m256i c2_sh2=_mm256_load_si256( (__m256i *) &mask_vector[12][0]);
	const __m256i c3_sh2=_mm256_load_si256( (__m256i *) &mask_vector[13][0]);
	const __m256i c4_sh2=_mm256_load_si256( (__m256i *) &mask_vector[14][0]);

	const __m256i c0_sh3=_mm256_load_si256( (__m256i *) &mask_vector[15][0]);
	const __m256i c1_sh3=_mm256_load_si256( (__m256i *) &mask_vector[16][0]);
	const __m256i c2_sh3=_mm256_load_si256( (__m256i *) &mask_vector[17][0]);
	const __m256i c3_sh3=_mm256_load_si256( (__m256i *) &mask_vector[18][0]);
	const __m256i c4_sh3=_mm256_load_si256( (__m256i *) &mask_vector[19][0]);

	const __m256i c0_sh4=_mm256_load_si256( (__m256i *) &mask_vector[20][0]);
	const __m256i c1_sh4=_mm256_load_si256( (__m256i *) &mask_vector[21][0]);
	const __m256i c2_sh4=_mm256_load_si256( (__m256i *) &mask_vector[22][0]);
	const __m256i c3_sh4=_mm256_load_si256( (__m256i *) &mask_vector[23][0]);
	const __m256i c4_sh4=_mm256_load_si256( (__m256i *) &mask_vector[24][0]);

	const __m256i c0_sh5=_mm256_load_si256( (__m256i *) &mask_vector[25][0]);
	const __m256i c1_sh5=_mm256_load_si256( (__m256i *) &mask_vector[26][0]);
	const __m256i c2_sh5=_mm256_load_si256( (__m256i *) &mask_vector[27][0]);
	const __m256i c3_sh5=_mm256_load_si256( (__m256i *) &mask_vector[28][0]);
	const __m256i c4_sh5=_mm256_load_si256( (__m256i *) &mask_vector[29][0]);

 __m256i reminder_mask1;

if (REMINDER_ITERATIONS == 0){
	return 0; //no loop reminder is needed
}

	reminder_mask1=_mm256_load_si256( (__m256i *) &reminder_msk1[REMINDER_ITERATIONS-1][0]);
	//reminder_mask2=_mm256_load_si256( (__m256i *) &reminder_msk2[REMINDER_ITERATIONS-1][0]);



	 //1st col iteration
	//load the 5 rows
	r0=_mm256_loadu_si256( (__m256i *) &frame1[row-2][col-2]);
	r1=_mm256_loadu_si256( (__m256i *) &frame1[row-1][col-2]);
	r2=_mm256_loadu_si256( (__m256i *) &frame1[row][col-2]);
	r3=_mm256_loadu_si256( (__m256i *) &frame1[row+1][col-2]);
	r4=_mm256_loadu_si256( (__m256i *) &frame1[row+2][col-2]);

	/*
	 * The above load operations load outside of the array bounds; these elements are filled with zeros just after
	 * Furthermore, I add two extra zeros in the end to compute col=N-2 and col=N-1. The last value of col is N-1, not N
	 */

	//AND r0-r4 with reminder_mask
	r0=_mm256_and_si256(r0,reminder_mask1);
	r1=_mm256_and_si256(r1,reminder_mask1);
	r2=_mm256_and_si256(r2,reminder_mask1);
	r3=_mm256_and_si256(r3,reminder_mask1);
	r4=_mm256_and_si256(r4,reminder_mask1);

	//multiply with the mask
	m0=_mm256_maddubs_epi16(r0,c0);
	m1=_mm256_maddubs_epi16(r1,c1);
	m2=_mm256_maddubs_epi16(r2,c2);
	m3=_mm256_maddubs_epi16(r3,c3);
	m4=_mm256_maddubs_epi16(r4,c4);

	//vertical add
	m0=_mm256_add_epi16(m0,m1);
	m2=_mm256_add_epi16(m2,m3);
	m0=_mm256_add_epi16(m0,m2);
	m0=_mm256_add_epi16(m0,m4);

	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);

	m1=_mm256_srli_si256(m0,4);
	m2=_mm256_add_epi16(m1,m2);

	//after shifts the 15th element is lost as it cannot be propagated to the 16th position (AVX registers are managed as two seperate SSE registers)
	m4=_mm256_and_si256(m0,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions
	m2=_mm256_add_epi16(m2,m4);

	//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
	output_even=_mm256_and_si256(m2,output_mask);



    //2nd col iteration

	//multiply with the mask
	m0=_mm256_maddubs_epi16(r0,c0_sh1);
	m1=_mm256_maddubs_epi16(r1,c1_sh1);
	m2=_mm256_maddubs_epi16(r2,c2_sh1);
	m3=_mm256_maddubs_epi16(r3,c3_sh1);
	m4=_mm256_maddubs_epi16(r4,c4_sh1);

	//vertical add
	m0=_mm256_add_epi16(m0,m1);
	m2=_mm256_add_epi16(m2,m3);
	m0=_mm256_add_epi16(m0,m2);
	m0=_mm256_add_epi16(m0,m4);


	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);

	m1=_mm256_srli_si256(m0,4);
	m2=_mm256_add_epi16(m1,m2);

	//after shifts the 15th element is lost as it cannot be propagated to the 16th position (AVX registers are managed as two seperate SSE registers)
	m4=_mm256_and_si256(m0,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions
	m2=_mm256_add_epi16(m2,m4);

	//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
	output_odd = _mm256_and_si256(m2,output_mask);




     //3rd col iteration

	//multiply with the mask
	m0=_mm256_maddubs_epi16(r0,c0_sh2);
	m1=_mm256_maddubs_epi16(r1,c1_sh2);
	m2=_mm256_maddubs_epi16(r2,c2_sh2);
	m3=_mm256_maddubs_epi16(r3,c3_sh2);
	m4=_mm256_maddubs_epi16(r4,c4_sh2);

	//vertical add
	m0=_mm256_add_epi16(m0,m1);
	m2=_mm256_add_epi16(m2,m3);
	m0=_mm256_add_epi16(m0,m2);
	m0=_mm256_add_epi16(m0,m4);

	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);

	m4=_mm256_and_si256(m2,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions

	m1=_mm256_srli_si256(m0,4);
	m2=_mm256_add_epi16(m1,m2);
	m2=_mm256_add_epi16(m2,m4);

	//m2 has 16 16bit values now. the results I need are in positions 1,4,7,10,13. keep only those, discard others
	m2 = _mm256_and_si256(m2,output_mask_sh1);
	output_even = _mm256_add_epi16(output_even,m2);



	 //4th col iteration

	//multiply with the mask
	m0=_mm256_maddubs_epi16(r0,c0_sh3);
	m1=_mm256_maddubs_epi16(r1,c1_sh3);
	m2=_mm256_maddubs_epi16(r2,c2_sh3);
	m3=_mm256_maddubs_epi16(r3,c3_sh3);
	m4=_mm256_maddubs_epi16(r4,c4_sh3);

	//vertical add
	m0=_mm256_add_epi16(m0,m1);
	m2=_mm256_add_epi16(m2,m3);
	m0=_mm256_add_epi16(m0,m2);
	m0=_mm256_add_epi16(m0,m4);


	m1=_mm256_srli_si256(m0,2);
	m4=_mm256_add_epi16(m1,m0);

	m1=_mm256_srli_si256(m0,4);
	m2=_mm256_add_epi16(m1,m4);


	m4=_mm256_and_si256(m4,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions

	m2=_mm256_add_epi16(m2,m4);

	//m2 has 16 16bit values now. the results I need are in positions 1,4,7,10,13. keep only those, discard others
	m2 = _mm256_and_si256(m2,output_mask_sh1);
	output_odd = _mm256_add_epi16(output_odd,m2);


   //5th col iteration

			m0=_mm256_maddubs_epi16(r0,c0_sh4);
			m1=_mm256_maddubs_epi16(r1,c1_sh4);
			m2=_mm256_maddubs_epi16(r2,c2_sh4);
			m3=_mm256_maddubs_epi16(r3,c3_sh4);
			m4=_mm256_maddubs_epi16(r4,c4_sh4);

			//vertical add
			m0=_mm256_add_epi16(m0,m1);
			m2=_mm256_add_epi16(m2,m3);
			m0=_mm256_add_epi16(m0,m2);
			m0=_mm256_add_epi16(m0,m4);

			//hozizontal additions
			//hadd(4:9)
			//hadd(10:15)
			//hadd(16:21)
			//hadd(22:27)
			//result after division will be in 4,10,16,22

			m1=_mm256_srli_si256(m0,2);
			m2=_mm256_add_epi16(m1,m0);

			m1=_mm256_srli_si256(m0,4);
			m2=_mm256_add_epi16(m1,m2);


			//m2 has 16 16bit values now. the results I need are in positions 2,5,8,11. keep only those, discard others
			m2 = _mm256_and_si256(m2,output_mask_sh2);
			output_even = _mm256_add_epi16(output_even,m2);

    //6th col iteration

			//multiply with the mask
			m0=_mm256_maddubs_epi16(r0,c0_sh5);
			m1=_mm256_maddubs_epi16(r1,c1_sh5);
			m2=_mm256_maddubs_epi16(r2,c2_sh5);
			m3=_mm256_maddubs_epi16(r3,c3_sh5);
			m4=_mm256_maddubs_epi16(r4,c4_sh5);


	//vertical add
	m0=_mm256_add_epi16(m0,m1);
	m2=_mm256_add_epi16(m2,m3);
	m0=_mm256_add_epi16(m0,m2);
	m0=_mm256_add_epi16(m0,m4);

	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);

	m1=_mm256_srli_si256(m0,4);
	m2=_mm256_add_epi16(m1,m2);

	//m2 has 16 16bit values now. the results I need are in positions 2,5,8,11. keep only those, discard others
	m2 = _mm256_and_si256(m2,output_mask_sh2);
	output_odd = _mm256_add_epi16(output_odd,m2);

	//now division follows
	output_even=division(division_case,output_even,f);
	output_odd=division(division_case,output_odd,f);

	//shift odd 1 position and add to even
	output_odd = _mm256_slli_si256(output_odd,1);
	output_even = _mm256_add_epi8(output_even,output_odd);

 	switch (REMINDER_ITERATIONS){
	case 1:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
	  break;
	case 2:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		  break;
	case 3:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		  break;
	case 4:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output_even,3);
		  break;
	case 5:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output_even,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output_even,4);
		  break;
	case 6:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output_even,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output_even,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output_even,5);
		  break;
	case 7:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output_even,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output_even,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output_even,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output_even,6);
		  break;
	case 8:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output_even,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output_even,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output_even,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output_even,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output_even,7);
		  break;
	case 9:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output_even,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output_even,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output_even,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output_even,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output_even,7);
		filt[row][col+8] = (unsigned char) _mm256_extract_epi8(output_even,8);
		  break;
	case 10:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output_even,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output_even,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output_even,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output_even,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output_even,7);
		filt[row][col+8] = (unsigned char) _mm256_extract_epi8(output_even,8);
		filt[row][col+9] = (unsigned char) _mm256_extract_epi8(output_even,9);
		  break;
	case 11:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output_even,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output_even,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output_even,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output_even,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output_even,7);
		filt[row][col+8] = (unsigned char) _mm256_extract_epi8(output_even,8);
		filt[row][col+9] = (unsigned char) _mm256_extract_epi8(output_even,9);
		filt[row][col+10] = (unsigned char) _mm256_extract_epi8(output_even,10);
		  break;
	case 12:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output_even,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output_even,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output_even,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output_even,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output_even,7);
		filt[row][col+8] = (unsigned char) _mm256_extract_epi8(output_even,8);
		filt[row][col+9] = (unsigned char) _mm256_extract_epi8(output_even,9);
		filt[row][col+10] = (unsigned char) _mm256_extract_epi8(output_even,10);
		filt[row][col+11] = (unsigned char) _mm256_extract_epi8(output_even,11);
		  break;
	case 13:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output_even,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output_even,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output_even,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output_even,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output_even,7);
		filt[row][col+8] = (unsigned char) _mm256_extract_epi8(output_even,8);
		filt[row][col+9] = (unsigned char) _mm256_extract_epi8(output_even,9);
		filt[row][col+10] = (unsigned char) _mm256_extract_epi8(output_even,10);
		filt[row][col+11] = (unsigned char) _mm256_extract_epi8(output_even,11);
		filt[row][col+12] = (unsigned char) _mm256_extract_epi8(output_even,12);
		  break;
	case 14:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output_even,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output_even,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output_even,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output_even,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output_even,7);
		filt[row][col+8] = (unsigned char) _mm256_extract_epi8(output_even,8);
		filt[row][col+9] = (unsigned char) _mm256_extract_epi8(output_even,9);
		filt[row][col+10] = (unsigned char) _mm256_extract_epi8(output_even,10);
		filt[row][col+11] = (unsigned char) _mm256_extract_epi8(output_even,11);
		filt[row][col+12] = (unsigned char) _mm256_extract_epi8(output_even,12);
		filt[row][col+13] = (unsigned char) _mm256_extract_epi8(output_even,13);
		  break;
	case 15:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output_even,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output_even,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output_even,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output_even,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output_even,7);
		filt[row][col+8] = (unsigned char) _mm256_extract_epi8(output_even,8);
		filt[row][col+9] = (unsigned char) _mm256_extract_epi8(output_even,9);
		filt[row][col+10] = (unsigned char) _mm256_extract_epi8(output_even,10);
		filt[row][col+11] = (unsigned char) _mm256_extract_epi8(output_even,11);
		filt[row][col+12] = (unsigned char) _mm256_extract_epi8(output_even,12);
		filt[row][col+13] = (unsigned char) _mm256_extract_epi8(output_even,13);
		filt[row][col+14] = (unsigned char) _mm256_extract_epi8(output_even,14);
		  break;
	case 16:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
		  break;
	case 17:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
		  break;
	case 18:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
		  break;
	case 19:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
		  break;
	case 20:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output_even,19);
		  break;
	case 21:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output_even,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output_even,20);
		  break;
	case 22:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output_even,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output_even,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output_even,21);
		break;
	case 23:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output_even,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output_even,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output_even,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output_even,22);
		  break;
	case 24:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output_even,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output_even,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output_even,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output_even,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output_even,23);
		  break;
	case 25:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output_even,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output_even,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output_even,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output_even,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output_even,23);
	filt[row][col+24] = (unsigned char) _mm256_extract_epi8(output_even,24);
		  break;
	case 26:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output_even,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output_even,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output_even,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output_even,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output_even,23);
	filt[row][col+24] = (unsigned char) _mm256_extract_epi8(output_even,24);
	filt[row][col+25] = (unsigned char) _mm256_extract_epi8(output_even,25);
		  break;
	case 27:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output_even,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output_even,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output_even,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output_even,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output_even,23);
	filt[row][col+24] = (unsigned char) _mm256_extract_epi8(output_even,24);
	filt[row][col+25] = (unsigned char) _mm256_extract_epi8(output_even,25);
	filt[row][col+26] = (unsigned char) _mm256_extract_epi8(output_even,26);
		  break;
	case 28:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output_even,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output_even,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output_even,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output_even,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output_even,23);
	filt[row][col+24] = (unsigned char) _mm256_extract_epi8(output_even,24);
	filt[row][col+25] = (unsigned char) _mm256_extract_epi8(output_even,25);
	filt[row][col+26] = (unsigned char) _mm256_extract_epi8(output_even,26);
	filt[row][col+27] = (unsigned char) _mm256_extract_epi8(output_even,27);
		  break;
	default:
		printf("\nsomething went wrong");
		return -1;
 	}

return 0;

}





//for row=0,1,2 only
int conv_loop_reminder_first_less_div(unsigned char **frame1,unsigned char **filt,const unsigned int M, const unsigned int N, const unsigned int col, const unsigned int REMINDER_ITERATIONS,const unsigned int division_case,const signed char mask_vector[][32], const __m256i f, const unsigned short int divisor, signed char **filter ){

__m256i r0,r1,r2,r3,r4,m0,m1,m2,m3,m4;
__m256i output_even,output_odd;
__m256i output_row0_even,output_row1_even,output_row0_odd,output_row1_odd;

		//const __m256i f  = _mm256_set_epi16(52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759);
		//const __m256i mask  = _mm256_set_epi8(0,0,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255);
		//const __m256i mask2  = _mm256_set_epi8(0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0);
		const __m256i mask3  = _mm256_set_epi16(0,0,0,0,0,0,0,65535,0,0,0,0,0,0,0,0);
		//const __m256i mask4  = _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255,0,0,0,0,0,0,0,0,0,0,0,0);
		//const __m256i mask5  = _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0);

		const __m256i output_mask      = _mm256_set_epi16(0,0,0,65535,0,0,65535,0,0,65535,0,0,65535,0,0,65535);
		const __m256i output_mask_sh1  = _mm256_set_epi16(0,0,65535,0,0,65535,0,0,65535,0,0,65535,0,0,65535,0);
		//const __m256i output_mask_sh2  = _mm256_set_epi16(0,0,0,    0,65535,0,0,65535,0,0,65535,0,0,65535,0,0);
		const __m256i mask_new      = _mm256_set_epi16(0,0,0,0,0,0,0,0,0,65535,0,0,0,0,0,0);
		const __m256i mask_new2      = _mm256_set_epi16(0,0,0,0,0,0,0,0,65535,0,0,0,0,0,0,0);

		const __m256i c0=_mm256_load_si256( (__m256i *) &mask_vector[0][0]);
			const __m256i c1=_mm256_load_si256( (__m256i *) &mask_vector[1][0]);
			const __m256i c2=_mm256_load_si256( (__m256i *) &mask_vector[2][0]);
			const __m256i c3=_mm256_load_si256( (__m256i *) &mask_vector[3][0]);
			const __m256i c4=_mm256_load_si256( (__m256i *) &mask_vector[4][0]);

			const __m256i c0_sh1=_mm256_load_si256( (__m256i *) &mask_vector[5][0]);
			const __m256i c1_sh1=_mm256_load_si256( (__m256i *) &mask_vector[6][0]);
			const __m256i c2_sh1=_mm256_load_si256( (__m256i *) &mask_vector[7][0]);
			const __m256i c3_sh1=_mm256_load_si256( (__m256i *) &mask_vector[8][0]);
			const __m256i c4_sh1=_mm256_load_si256( (__m256i *) &mask_vector[9][0]);

			const __m256i c0_sh2=_mm256_load_si256( (__m256i *) &mask_vector[10][0]);
			const __m256i c1_sh2=_mm256_load_si256( (__m256i *) &mask_vector[11][0]);
			const __m256i c2_sh2=_mm256_load_si256( (__m256i *) &mask_vector[12][0]);
			const __m256i c3_sh2=_mm256_load_si256( (__m256i *) &mask_vector[13][0]);
			const __m256i c4_sh2=_mm256_load_si256( (__m256i *) &mask_vector[14][0]);



 __m256i reminder_mask1,reminder_mask2;

	reminder_mask1=_mm256_load_si256( (__m256i *) &reminder_msk1[REMINDER_ITERATIONS-1][0]);
	reminder_mask2=_mm256_load_si256( (__m256i *) &reminder_msk2[REMINDER_ITERATIONS-1][0]);


	 //1st col iteration
	//load the 5 rows
	r0=_mm256_loadu_si256( (__m256i *) &frame1[0][col-2]);
	r1=_mm256_loadu_si256( (__m256i *) &frame1[1][col-2]);
	r2=_mm256_loadu_si256( (__m256i *) &frame1[2][col-2]);
	r3=_mm256_loadu_si256( (__m256i *) &frame1[3][col-2]);
	r4=_mm256_loadu_si256( (__m256i *) &frame1[4][col-2]);

	/*
	 * The above load operations load outside of the array bounds; these elements are filled with zeros just after
	 * Furthermore, I add two extra zeros in the end to compute col=N-2 and col=N-1. The last value of col is N-1, not N
	 */

	//AND r0-r4 with reminder_mask
	r0=_mm256_and_si256(r0,reminder_mask1);
	r1=_mm256_and_si256(r1,reminder_mask1);
	r2=_mm256_and_si256(r2,reminder_mask1);
	r3=_mm256_and_si256(r3,reminder_mask1);
	r4=_mm256_and_si256(r4,reminder_mask1);

	//row=2
	//multiply with the mask
	m0=_mm256_maddubs_epi16(r0,c0);
	m1=_mm256_maddubs_epi16(r1,c1);
	m2=_mm256_maddubs_epi16(r2,c2);
	m3=_mm256_maddubs_epi16(r3,c3);
	m4=_mm256_maddubs_epi16(r4,c4);

	//vertical add
	m0=_mm256_add_epi16(m0,m1);
	m2=_mm256_add_epi16(m2,m3);
	m0=_mm256_add_epi16(m0,m2);
	m0=_mm256_add_epi16(m0,m4);


	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);

	m1=_mm256_srli_si256(m0,4);
	m2=_mm256_add_epi16(m1,m2);

	//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
	//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
	m4=_mm256_and_si256(m0,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions

	m2=_mm256_add_epi16(m2,m4);

	//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
	output_even=_mm256_and_si256(m2,output_mask);

	//row=0
	//multiply with the mask
	m0=_mm256_maddubs_epi16(r0,c2);
	m1=_mm256_maddubs_epi16(r1,c3);
	m2=_mm256_maddubs_epi16(r2,c4);


	//vertical add
	m0=_mm256_add_epi16(m0,m1);
	m0=_mm256_add_epi16(m0,m2);


	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);

	m1=_mm256_srli_si256(m0,4);
	m2=_mm256_add_epi16(m1,m2);

	//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
	//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
	m4=_mm256_and_si256(m0,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions

	m2=_mm256_add_epi16(m2,m4);

	//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
	output_row0_even=_mm256_and_si256(m2,output_mask);


	//row=1
	//multiply with the mask
	m0=_mm256_maddubs_epi16(r0,c1);
	m1=_mm256_maddubs_epi16(r1,c2);
	m2=_mm256_maddubs_epi16(r2,c3);
	m3=_mm256_maddubs_epi16(r3,c4);


	//vertical add
	m0=_mm256_add_epi16(m0,m1);
	m0=_mm256_add_epi16(m0,m2);
	m0=_mm256_add_epi16(m0,m3);

	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);

	m1=_mm256_srli_si256(m0,4);
	m2=_mm256_add_epi16(m1,m2);

	//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
	//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
	m4=_mm256_and_si256(m0,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions

	m2=_mm256_add_epi16(m2,m4);

	//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
	output_row1_even=_mm256_and_si256(m2,output_mask);

	//2nd col iteration

	//row=2
	//multiply with the mask
	m0=_mm256_maddubs_epi16(r0,c0_sh1);
	m1=_mm256_maddubs_epi16(r1,c1_sh1);
	m2=_mm256_maddubs_epi16(r2,c2_sh1);
	m3=_mm256_maddubs_epi16(r3,c3_sh1);
	m4=_mm256_maddubs_epi16(r4,c4_sh1);

	//vertical add
	m0=_mm256_add_epi16(m0,m1);
	m2=_mm256_add_epi16(m2,m3);
	m0=_mm256_add_epi16(m0,m2);
	m0=_mm256_add_epi16(m0,m4);

	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);

	m1=_mm256_srli_si256(m0,4);
	m2=_mm256_add_epi16(m1,m2);

	//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
	//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
	m4=_mm256_and_si256(m0,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions
	m2=_mm256_add_epi16(m2,m4);

	//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
	output_odd = _mm256_and_si256(m2,output_mask);

	//row=0
	//multiply with the mask
	m0=_mm256_maddubs_epi16(r0,c2_sh1);
	m1=_mm256_maddubs_epi16(r1,c3_sh1);
	m2=_mm256_maddubs_epi16(r2,c4_sh1);


	//vertical add
	m0=_mm256_add_epi16(m0,m1);
	m0=_mm256_add_epi16(m0,m2);


	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);

	m1=_mm256_srli_si256(m0,4);
	m2=_mm256_add_epi16(m1,m2);

	//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
	//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
	m4=_mm256_and_si256(m0,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions
	m2=_mm256_add_epi16(m2,m4);

	//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
	output_row0_odd = _mm256_and_si256(m2,output_mask);

	//row=1
	//multiply with the mask
	m0=_mm256_maddubs_epi16(r0,c1_sh1);
	m1=_mm256_maddubs_epi16(r1,c2_sh1);
	m2=_mm256_maddubs_epi16(r2,c3_sh1);
	m3=_mm256_maddubs_epi16(r3,c4_sh1);

	//vertical add
	m0=_mm256_add_epi16(m0,m1);
	m0=_mm256_add_epi16(m0,m2);
	m0=_mm256_add_epi16(m0,m3);

	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);

	m1=_mm256_srli_si256(m0,4);
	m2=_mm256_add_epi16(m1,m2);

	//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
	//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
	m4=_mm256_and_si256(m0,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions
	m2=_mm256_add_epi16(m2,m4);

	//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
	output_row1_odd = _mm256_and_si256(m2,output_mask);

             //3rd col iteration

	//row=2
	//multiply with the mask
	m0=_mm256_maddubs_epi16(r0,c0_sh2);
	m1=_mm256_maddubs_epi16(r1,c1_sh2);
	m2=_mm256_maddubs_epi16(r2,c2_sh2);
	m3=_mm256_maddubs_epi16(r3,c3_sh2);
	m4=_mm256_maddubs_epi16(r4,c4_sh2);

	//vertical add
	m0=_mm256_add_epi16(m0,m1);
	m2=_mm256_add_epi16(m2,m3);
	m0=_mm256_add_epi16(m0,m2);
	m0=_mm256_add_epi16(m0,m4);


	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);

	//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,_mm256_extract_epi16(m2,8),0,0,0,0,0,0,0);
	m4=_mm256_and_si256(m2,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions

	m1=_mm256_srli_si256(m0,4);
	m2=_mm256_add_epi16(m1,m2);

	//prepei na prosthesw to pixel pu thelw sto r2 kai meta na kanw diairesi me ola mazi

	m2=_mm256_add_epi16(m2,m4);

	//m2 has 16 16bit values now. the results I need are in positions 1,4,7,10,13. keep only those, discard others
	m2 = _mm256_and_si256(m2,output_mask_sh1);
	output_even = _mm256_add_epi16(output_even,m2);

	//row=0
	//multiply with the mask
	m0=_mm256_maddubs_epi16(r0,c2_sh2);
	m1=_mm256_maddubs_epi16(r1,c3_sh2);
	m2=_mm256_maddubs_epi16(r2,c4_sh2);

	//vertical add
	m0=_mm256_add_epi16(m0,m1);
	m0=_mm256_add_epi16(m0,m2);


	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);

	//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,_mm256_extract_epi16(m2,8),0,0,0,0,0,0,0);
	m4=_mm256_and_si256(m2,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions

	m1=_mm256_srli_si256(m0,4);
	m2=_mm256_add_epi16(m1,m2);

	//prepei na prosthesw to pixel pu thelw sto r2 kai meta na kanw diairesi me ola mazi

	m2=_mm256_add_epi16(m2,m4);

	//m2 has 16 16bit values now. the results I need are in positions 1,4,7,10,13. keep only those, discard others
	m2 = _mm256_and_si256(m2,output_mask_sh1);
	output_row0_even = _mm256_add_epi16(output_row0_even,m2);

	//row=1
	//multiply with the mask
	m0=_mm256_maddubs_epi16(r0,c1_sh2);
	m1=_mm256_maddubs_epi16(r1,c2_sh2);
	m2=_mm256_maddubs_epi16(r2,c3_sh2);
	m3=_mm256_maddubs_epi16(r3,c4_sh2);

	//vertical add
	m0=_mm256_add_epi16(m0,m1);
	m0=_mm256_add_epi16(m0,m2);
	m0=_mm256_add_epi16(m0,m3);


	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);

	//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,_mm256_extract_epi16(m2,8),0,0,0,0,0,0,0);
	m4=_mm256_and_si256(m2,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions

	m1=_mm256_srli_si256(m0,4);
	m2=_mm256_add_epi16(m1,m2);

	//prepei na prosthesw to pixel pu thelw sto r2 kai meta na kanw diairesi me ola mazi

	m2=_mm256_add_epi16(m2,m4);

	//m2 has 16 16bit values now. the results I need are in positions 1,4,7,10,13. keep only those, discard others
	m2 = _mm256_and_si256(m2,output_mask_sh1);
	output_row1_even = _mm256_add_epi16(output_row1_even,m2);

	//4th col iteration
	//load the 5 rows
	r0=_mm256_loadu_si256( (__m256i *) &frame1[0][col+3-2]);
	r1=_mm256_loadu_si256( (__m256i *) &frame1[1][col+3-2]);
	r2=_mm256_loadu_si256( (__m256i *) &frame1[2][col+3-2]);
	r3=_mm256_loadu_si256( (__m256i *) &frame1[3][col+3-2]);
	r4=_mm256_loadu_si256( (__m256i *) &frame1[4][col+3-2]);

	//AND r0-r4 with reminder_mask
	r0=_mm256_and_si256(r0,reminder_mask2);
	r1=_mm256_and_si256(r1,reminder_mask2);
	r2=_mm256_and_si256(r2,reminder_mask2);
	r3=_mm256_and_si256(r3,reminder_mask2);
	r4=_mm256_and_si256(r4,reminder_mask2);

	//row=2
	//multiply with the mask
	m0=_mm256_maddubs_epi16(r0,c0);
	m1=_mm256_maddubs_epi16(r1,c1);
	m2=_mm256_maddubs_epi16(r2,c2);
	m3=_mm256_maddubs_epi16(r3,c3);
	m4=_mm256_maddubs_epi16(r4,c4);

	//vertical add
	m0=_mm256_add_epi16(m0,m1);
	m2=_mm256_add_epi16(m2,m3);
	m0=_mm256_add_epi16(m0,m2);
	m0=_mm256_add_epi16(m0,m4);


	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);

	m1=_mm256_srli_si256(m0,4);
	m2=_mm256_add_epi16(m1,m2);

	//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
	//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
	m4=_mm256_and_si256(m0,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions

	m2=_mm256_add_epi16(m2,m4);

	//m2 has 16 16bit values now. the results I need are in positions 1,4,7,10,13. keep only those, discard others
	m2 = _mm256_and_si256(m2,output_mask);
	m2 = _mm256_slli_si256(m2,2);
	output_odd = _mm256_add_epi16(output_odd,m2);

	//row=0
	//multiply with the mask
	m0=_mm256_maddubs_epi16(r0,c2);
	m1=_mm256_maddubs_epi16(r1,c3);
	m2=_mm256_maddubs_epi16(r2,c4);


	//vertical add
	m0=_mm256_add_epi16(m0,m1);
	m0=_mm256_add_epi16(m2,m0);


	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);

	m1=_mm256_srli_si256(m0,4);
	m2=_mm256_add_epi16(m1,m2);

	//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
	//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
	m4=_mm256_and_si256(m0,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions

	m2=_mm256_add_epi16(m2,m4);

	//m2 has 16 16bit values now. the results I need are in positions 1,4,7,10,13. keep only those, discard others
	m2 = _mm256_and_si256(m2,output_mask);
	m2 = _mm256_slli_si256(m2,2);
	output_row0_odd = _mm256_add_epi16(output_row0_odd,m2);

	//row=1
	//multiply with the mask
	m0=_mm256_maddubs_epi16(r0,c1);
	m1=_mm256_maddubs_epi16(r1,c2);
	m2=_mm256_maddubs_epi16(r2,c3);
	m3=_mm256_maddubs_epi16(r3,c4);

	//vertical add
	m0=_mm256_add_epi16(m0,m1);
	m0=_mm256_add_epi16(m2,m0);
	m0=_mm256_add_epi16(m0,m3);


	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);

	m1=_mm256_srli_si256(m0,4);
	m2=_mm256_add_epi16(m1,m2);

	//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
	//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
	m4=_mm256_and_si256(m0,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions

	m2=_mm256_add_epi16(m2,m4);

	//m2 has 16 16bit values now. the results I need are in positions 1,4,7,10,13. keep only those, discard others
	m2 = _mm256_and_si256(m2,output_mask);
	m2 = _mm256_slli_si256(m2,2);
	output_row1_odd = _mm256_add_epi16(output_row1_odd,m2);

	//5th col iteration

	//row=2
	//multiply with the mask
	m0=_mm256_maddubs_epi16(r0,c0_sh1);
	m1=_mm256_maddubs_epi16(r1,c1_sh1);
	m2=_mm256_maddubs_epi16(r2,c2_sh1);
	m3=_mm256_maddubs_epi16(r3,c3_sh1);
	m4=_mm256_maddubs_epi16(r4,c4_sh1);

	//vertical add
	m0=_mm256_add_epi16(m0,m1);
	m2=_mm256_add_epi16(m2,m3);
	m0=_mm256_add_epi16(m0,m2);
	m0=_mm256_add_epi16(m0,m4);


	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);

	m1=_mm256_srli_si256(m0,4);
	m2=_mm256_add_epi16(m1,m2);

	//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
	//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
	m4=_mm256_and_si256(m0,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions

	m2=_mm256_add_epi16(m2,m4);

	//m2 has 16 16bit values now.
	m2 = _mm256_and_si256(m2,output_mask); //holds results in positions 0,3,6,9,12
	m1 = _mm256_slli_si256(m2,4); //put results in positions 2,5,8,11,14 , but 8 is not written
	m2=_mm256_and_si256(m2,mask_new);//extract only the 6th
	m2=_mm256_permute2f128_si256(m2,m2,1);
	m2=_mm256_srli_si256(m2,12);
	m2 = _mm256_add_epi16(m1,m2);
	output_even = _mm256_add_epi16(output_even,m2);

	//row=0
	//multiply with the mask
	m0=_mm256_maddubs_epi16(r0,c2_sh1);
	m1=_mm256_maddubs_epi16(r1,c3_sh1);
	m2=_mm256_maddubs_epi16(r2,c4_sh1);


	//vertical add
	m0=_mm256_add_epi16(m0,m1);
	m0=_mm256_add_epi16(m2,m0);



	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);

	m1=_mm256_srli_si256(m0,4);
	m2=_mm256_add_epi16(m1,m2);

	//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
	//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
	m4=_mm256_and_si256(m0,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions

	m2=_mm256_add_epi16(m2,m4);

	//m2 has 16 16bit values now.
	m2 = _mm256_and_si256(m2,output_mask); //holds results in positions 0,3,6,9,12
	m1 = _mm256_slli_si256(m2,4); //put results in positions 2,5,8,11,14 , but 8 is not written
	m2=_mm256_and_si256(m2,mask_new);//extract only the 6th
	m2=_mm256_permute2f128_si256(m2,m2,1);
	m2=_mm256_srli_si256(m2,12);
	m2 = _mm256_add_epi16(m1,m2);
	output_row0_even = _mm256_add_epi16(output_row0_even,m2);

	//row=1
	//multiply with the mask
	m0=_mm256_maddubs_epi16(r0,c1_sh1);
	m1=_mm256_maddubs_epi16(r1,c2_sh1);
	m2=_mm256_maddubs_epi16(r2,c3_sh1);
	m3=_mm256_maddubs_epi16(r3,c4_sh1);

	//vertical add
	m0=_mm256_add_epi16(m0,m1);
	m0=_mm256_add_epi16(m2,m0);
	m0=_mm256_add_epi16(m0,m3);


	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);

	m1=_mm256_srli_si256(m0,4);
	m2=_mm256_add_epi16(m1,m2);

	//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
	//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
	m4=_mm256_and_si256(m0,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions

	m2=_mm256_add_epi16(m2,m4);

	//m2 has 16 16bit values now.
	m2 = _mm256_and_si256(m2,output_mask); //holds results in positions 0,3,6,9,12
	m1 = _mm256_slli_si256(m2,4); //put results in positions 2,5,8,11,14 , but 8 is not written
	m2=_mm256_and_si256(m2,mask_new);//extract only the 6th
	m2=_mm256_permute2f128_si256(m2,m2,1);
	m2=_mm256_srli_si256(m2,12);
	m2 = _mm256_add_epi16(m1,m2);
	output_row1_even = _mm256_add_epi16(output_row1_even,m2);

             //6th col iteration
	//row=2
	//multiply with the mask
	m0=_mm256_maddubs_epi16(r0,c0_sh2);
	m1=_mm256_maddubs_epi16(r1,c1_sh2);
	m2=_mm256_maddubs_epi16(r2,c2_sh2);
	m3=_mm256_maddubs_epi16(r3,c3_sh2);
	m4=_mm256_maddubs_epi16(r4,c4_sh2);

	//vertical add
	m0=_mm256_add_epi16(m0,m1);
	m2=_mm256_add_epi16(m2,m3);
	m0=_mm256_add_epi16(m0,m2);
	m0=_mm256_add_epi16(m0,m4);


	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);
	//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,_mm256_extract_epi16(m2,8),0,0,0,0,0,0,0);
	m4=_mm256_and_si256(m2,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions

	m1=_mm256_srli_si256(m0,4);
	m2=_mm256_add_epi16(m1,m2);
	m2=_mm256_add_epi16(m2,m4);

	//m2 has 16 16bit values now.
	m2 = _mm256_and_si256(m2,output_mask_sh1);//holds results in positions 1,4,7,10,13
	m1 = _mm256_slli_si256(m2,2); //put results in positions 2,5,8,11,14 , but 8 is not written
	m2=_mm256_and_si256(m2,mask_new2);//extract only the 7th
	m2=_mm256_permute2f128_si256(m2,m2,1);
	m2=_mm256_srli_si256(m2,14);
	m2 = _mm256_add_epi16(m1,m2);
	output_odd = _mm256_add_epi16(output_odd,m2);


	//now division follows
	output_even=division(division_case,output_even,f);
	output_odd=division(division_case,output_odd,f);

	//shift odd 1 position and add to even
	output_odd = _mm256_slli_si256(output_odd,1);
	output_even = _mm256_add_epi8(output_even,output_odd);

	//row=0
	//multiply with the mask
	m0=_mm256_maddubs_epi16(r0,c2_sh2);
	m1=_mm256_maddubs_epi16(r1,c3_sh2);
	m2=_mm256_maddubs_epi16(r2,c4_sh2);

	//vertical add
	m0=_mm256_add_epi16(m0,m1);
	m0=_mm256_add_epi16(m2,m0);



	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);
	//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,_mm256_extract_epi16(m2,8),0,0,0,0,0,0,0);
	m4=_mm256_and_si256(m2,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions

	m1=_mm256_srli_si256(m0,4);
	m2=_mm256_add_epi16(m1,m2);
	m2=_mm256_add_epi16(m2,m4);

	//m2 has 16 16bit values now.
	m2 = _mm256_and_si256(m2,output_mask_sh1);//holds results in positions 1,4,7,10,13
	m1 = _mm256_slli_si256(m2,2); //put results in positions 2,5,8,11,14 , but 8 is not written
	m2=_mm256_and_si256(m2,mask_new2);//extract only the 7th
	m2=_mm256_permute2f128_si256(m2,m2,1);
	m2=_mm256_srli_si256(m2,14);
	m2 = _mm256_add_epi16(m1,m2);
	output_row0_odd = _mm256_add_epi16(output_row0_odd,m2);


	//now division follows
	output_row0_even=division(division_case,output_row0_even,f);
	output_row0_odd=division(division_case,output_row0_odd,f);

	//shift odd 1 position and add to even
	output_row0_odd = _mm256_slli_si256(output_row0_odd,1);
	output_row0_even = _mm256_add_epi8(output_row0_even,output_row0_odd);

	//row=1
	//multiply with the mask
	m0=_mm256_maddubs_epi16(r0,c1_sh2);
	m1=_mm256_maddubs_epi16(r1,c2_sh2);
	m2=_mm256_maddubs_epi16(r2,c3_sh2);
	m3=_mm256_maddubs_epi16(r3,c4_sh2);

	//vertical add
	m0=_mm256_add_epi16(m0,m1);
	m0=_mm256_add_epi16(m2,m0);
	m0=_mm256_add_epi16(m0,m3);


	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);
	//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,_mm256_extract_epi16(m2,8),0,0,0,0,0,0,0);
	m4=_mm256_and_si256(m2,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions

	m1=_mm256_srli_si256(m0,4);
	m2=_mm256_add_epi16(m1,m2);
	m2=_mm256_add_epi16(m2,m4);

	//m2 has 16 16bit values now.
	m2 = _mm256_and_si256(m2,output_mask_sh1);//holds results in positions 1,4,7,10,13
	m1 = _mm256_slli_si256(m2,2); //put results in positions 2,5,8,11,14 , but 8 is not written
	m2=_mm256_and_si256(m2,mask_new2);//extract only the 7th
	m2=_mm256_permute2f128_si256(m2,m2,1);
	m2=_mm256_srli_si256(m2,14);
	m2 = _mm256_add_epi16(m1,m2);
	output_row1_odd = _mm256_add_epi16(output_row1_odd,m2);


	//now division follows
	output_row1_even=division(division_case,output_row1_even,f);
	output_row1_odd=division(division_case,output_row1_odd,f);

	//shift odd 1 position and add to even
	output_row1_odd = _mm256_slli_si256(output_row1_odd,1);
	output_row1_even = _mm256_add_epi8(output_row1_even,output_row1_odd);




	switch (REMINDER_ITERATIONS){
	case 1:
		filt[2][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[0][col] = (unsigned char) _mm256_extract_epi8(output_row0_even,0);
		filt[1][col] = (unsigned char) _mm256_extract_epi8(output_row1_even,0);
		 break;
	case 2:
		filt[2][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[2][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);

		filt[0][col] = (unsigned char) _mm256_extract_epi8(output_row0_even,0);
		filt[0][col+1] = (unsigned char) _mm256_extract_epi8(output_row0_even,1);

		filt[1][col] = (unsigned char) _mm256_extract_epi8(output_row1_even,0);
		filt[1][col+1] = (unsigned char) _mm256_extract_epi8(output_row1_even,1);
		 break;
	case 3:
		filt[2][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[2][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[2][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);

		filt[0][col] = (unsigned char) _mm256_extract_epi8(output_row0_even,0);
		filt[0][col+1] = (unsigned char) _mm256_extract_epi8(output_row0_even,1);
		filt[0][col+2] = (unsigned char) _mm256_extract_epi8(output_row0_even,2);

		filt[1][col] = (unsigned char) _mm256_extract_epi8(output_row1_even,0);
		filt[1][col+1] = (unsigned char) _mm256_extract_epi8(output_row1_even,1);
		filt[1][col+2] = (unsigned char) _mm256_extract_epi8(output_row1_even,2);
		  break;
	case 4:
		filt[2][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[2][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[2][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		filt[2][col+3] = (unsigned char) _mm256_extract_epi8(output_even,3);

		filt[0][col] = (unsigned char) _mm256_extract_epi8(output_row0_even,0);
		filt[0][col+1] = (unsigned char) _mm256_extract_epi8(output_row0_even,1);
		filt[0][col+2] = (unsigned char) _mm256_extract_epi8(output_row0_even,2);
		filt[0][col+3] = (unsigned char) _mm256_extract_epi8(output_row0_even,3);

		filt[1][col] = (unsigned char) _mm256_extract_epi8(output_row1_even,0);
		filt[1][col+1] = (unsigned char) _mm256_extract_epi8(output_row1_even,1);
		filt[1][col+2] = (unsigned char) _mm256_extract_epi8(output_row1_even,2);
		filt[1][col+3] = (unsigned char) _mm256_extract_epi8(output_row1_even,3);
		  break;
	case 5:
		filt[2][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[2][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[2][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		filt[2][col+3] = (unsigned char) _mm256_extract_epi8(output_even,3);
		filt[2][col+4] = (unsigned char) _mm256_extract_epi8(output_even,4);

		filt[0][col] = (unsigned char) _mm256_extract_epi8(output_row0_even,0);
		filt[0][col+1] = (unsigned char) _mm256_extract_epi8(output_row0_even,1);
		filt[0][col+2] = (unsigned char) _mm256_extract_epi8(output_row0_even,2);
		filt[0][col+3] = (unsigned char) _mm256_extract_epi8(output_row0_even,3);
		filt[0][col+4] = (unsigned char) _mm256_extract_epi8(output_row0_even,4);

		filt[1][col] = (unsigned char) _mm256_extract_epi8(output_row1_even,0);
		filt[1][col+1] = (unsigned char) _mm256_extract_epi8(output_row1_even,1);
		filt[1][col+2] = (unsigned char) _mm256_extract_epi8(output_row1_even,2);
		filt[1][col+3] = (unsigned char) _mm256_extract_epi8(output_row1_even,3);
		filt[1][col+4] = (unsigned char) _mm256_extract_epi8(output_row1_even,4);
		  break;
	case 6:
		filt[2][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[2][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[2][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		filt[2][col+3] = (unsigned char) _mm256_extract_epi8(output_even,3);
		filt[2][col+4] = (unsigned char) _mm256_extract_epi8(output_even,4);
		filt[2][col+5] = (unsigned char) _mm256_extract_epi8(output_even,5);

		filt[0][col] = (unsigned char) _mm256_extract_epi8(output_row0_even,0);
		filt[0][col+1] = (unsigned char) _mm256_extract_epi8(output_row0_even,1);
		filt[0][col+2] = (unsigned char) _mm256_extract_epi8(output_row0_even,2);
		filt[0][col+3] = (unsigned char) _mm256_extract_epi8(output_row0_even,3);
		filt[0][col+4] = (unsigned char) _mm256_extract_epi8(output_row0_even,4);
		filt[0][col+5] = (unsigned char) _mm256_extract_epi8(output_row0_even,5);

		filt[1][col] = (unsigned char) _mm256_extract_epi8(output_row1_even,0);
		filt[1][col+1] = (unsigned char) _mm256_extract_epi8(output_row1_even,1);
		filt[1][col+2] = (unsigned char) _mm256_extract_epi8(output_row1_even,2);
		filt[1][col+3] = (unsigned char) _mm256_extract_epi8(output_row1_even,3);
		filt[1][col+4] = (unsigned char) _mm256_extract_epi8(output_row1_even,4);
		filt[1][col+5] = (unsigned char) _mm256_extract_epi8(output_row1_even,5);
		  break;
	case 7:
		filt[2][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[2][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[2][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		filt[2][col+3] = (unsigned char) _mm256_extract_epi8(output_even,3);
		filt[2][col+4] = (unsigned char) _mm256_extract_epi8(output_even,4);
		filt[2][col+5] = (unsigned char) _mm256_extract_epi8(output_even,5);
		filt[2][col+6] = (unsigned char) _mm256_extract_epi8(output_even,6);

		filt[0][col] = (unsigned char) _mm256_extract_epi8(output_row0_even,0);
		filt[0][col+1] = (unsigned char) _mm256_extract_epi8(output_row0_even,1);
		filt[0][col+2] = (unsigned char) _mm256_extract_epi8(output_row0_even,2);
		filt[0][col+3] = (unsigned char) _mm256_extract_epi8(output_row0_even,3);
		filt[0][col+4] = (unsigned char) _mm256_extract_epi8(output_row0_even,4);
		filt[0][col+5] = (unsigned char) _mm256_extract_epi8(output_row0_even,5);
		filt[0][col+6] = (unsigned char) _mm256_extract_epi8(output_row0_even,6);

		filt[1][col] = (unsigned char) _mm256_extract_epi8(output_row1_even,0);
		filt[1][col+1] = (unsigned char) _mm256_extract_epi8(output_row1_even,1);
		filt[1][col+2] = (unsigned char) _mm256_extract_epi8(output_row1_even,2);
		filt[1][col+3] = (unsigned char) _mm256_extract_epi8(output_row1_even,3);
		filt[1][col+4] = (unsigned char) _mm256_extract_epi8(output_row1_even,4);
		filt[1][col+5] = (unsigned char) _mm256_extract_epi8(output_row1_even,5);
		filt[1][col+6] = (unsigned char) _mm256_extract_epi8(output_row1_even,6);
		  break;
	case 8:
		filt[2][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[2][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[2][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		filt[2][col+3] = (unsigned char) _mm256_extract_epi8(output_even,3);
		filt[2][col+4] = (unsigned char) _mm256_extract_epi8(output_even,4);
		filt[2][col+5] = (unsigned char) _mm256_extract_epi8(output_even,5);
		filt[2][col+6] = (unsigned char) _mm256_extract_epi8(output_even,6);
		filt[2][col+7] = (unsigned char) _mm256_extract_epi8(output_even,7);

		filt[0][col] = (unsigned char) _mm256_extract_epi8(output_row0_even,0);
		filt[0][col+1] = (unsigned char) _mm256_extract_epi8(output_row0_even,1);
		filt[0][col+2] = (unsigned char) _mm256_extract_epi8(output_row0_even,2);
		filt[0][col+3] = (unsigned char) _mm256_extract_epi8(output_row0_even,3);
		filt[0][col+4] = (unsigned char) _mm256_extract_epi8(output_row0_even,4);
		filt[0][col+5] = (unsigned char) _mm256_extract_epi8(output_row0_even,5);
		filt[0][col+6] = (unsigned char) _mm256_extract_epi8(output_row0_even,6);
		filt[0][col+7] = (unsigned char) _mm256_extract_epi8(output_row0_even,7);

		filt[1][col] = (unsigned char) _mm256_extract_epi8(output_row1_even,0);
		filt[1][col+1] = (unsigned char) _mm256_extract_epi8(output_row1_even,1);
		filt[1][col+2] = (unsigned char) _mm256_extract_epi8(output_row1_even,2);
		filt[1][col+3] = (unsigned char) _mm256_extract_epi8(output_row1_even,3);
		filt[1][col+4] = (unsigned char) _mm256_extract_epi8(output_row1_even,4);
		filt[1][col+5] = (unsigned char) _mm256_extract_epi8(output_row1_even,5);
		filt[1][col+6] = (unsigned char) _mm256_extract_epi8(output_row1_even,6);
		filt[1][col+7] = (unsigned char) _mm256_extract_epi8(output_row1_even,7);
		  break;
	case 9:
		filt[2][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[2][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[2][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		filt[2][col+3] = (unsigned char) _mm256_extract_epi8(output_even,3);
		filt[2][col+4] = (unsigned char) _mm256_extract_epi8(output_even,4);
		filt[2][col+5] = (unsigned char) _mm256_extract_epi8(output_even,5);
		filt[2][col+6] = (unsigned char) _mm256_extract_epi8(output_even,6);
		filt[2][col+7] = (unsigned char) _mm256_extract_epi8(output_even,7);
		filt[2][col+8] = (unsigned char) _mm256_extract_epi8(output_even,8);

		filt[0][col] = (unsigned char) _mm256_extract_epi8(output_row0_even,0);
		filt[0][col+1] = (unsigned char) _mm256_extract_epi8(output_row0_even,1);
		filt[0][col+2] = (unsigned char) _mm256_extract_epi8(output_row0_even,2);
		filt[0][col+3] = (unsigned char) _mm256_extract_epi8(output_row0_even,3);
		filt[0][col+4] = (unsigned char) _mm256_extract_epi8(output_row0_even,4);
		filt[0][col+5] = (unsigned char) _mm256_extract_epi8(output_row0_even,5);
		filt[0][col+6] = (unsigned char) _mm256_extract_epi8(output_row0_even,6);
		filt[0][col+7] = (unsigned char) _mm256_extract_epi8(output_row0_even,7);
		filt[0][col+8] = (unsigned char) _mm256_extract_epi8(output_row0_even,8);

		filt[1][col] = (unsigned char) _mm256_extract_epi8(output_row1_even,0);
		filt[1][col+1] = (unsigned char) _mm256_extract_epi8(output_row1_even,1);
		filt[1][col+2] = (unsigned char) _mm256_extract_epi8(output_row1_even,2);
		filt[1][col+3] = (unsigned char) _mm256_extract_epi8(output_row1_even,3);
		filt[1][col+4] = (unsigned char) _mm256_extract_epi8(output_row1_even,4);
		filt[1][col+5] = (unsigned char) _mm256_extract_epi8(output_row1_even,5);
		filt[1][col+6] = (unsigned char) _mm256_extract_epi8(output_row1_even,6);
		filt[1][col+7] = (unsigned char) _mm256_extract_epi8(output_row1_even,7);
		filt[1][col+8] = (unsigned char) _mm256_extract_epi8(output_row1_even,8);
		  break;
	case 10:
		filt[2][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[2][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[2][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		filt[2][col+3] = (unsigned char) _mm256_extract_epi8(output_even,3);
		filt[2][col+4] = (unsigned char) _mm256_extract_epi8(output_even,4);
		filt[2][col+5] = (unsigned char) _mm256_extract_epi8(output_even,5);
		filt[2][col+6] = (unsigned char) _mm256_extract_epi8(output_even,6);
		filt[2][col+7] = (unsigned char) _mm256_extract_epi8(output_even,7);
		filt[2][col+8] = (unsigned char) _mm256_extract_epi8(output_even,8);
		filt[2][col+9] = (unsigned char) _mm256_extract_epi8(output_even,9);

		filt[0][col] = (unsigned char) _mm256_extract_epi8(output_row0_even,0);
		filt[0][col+1] = (unsigned char) _mm256_extract_epi8(output_row0_even,1);
		filt[0][col+2] = (unsigned char) _mm256_extract_epi8(output_row0_even,2);
		filt[0][col+3] = (unsigned char) _mm256_extract_epi8(output_row0_even,3);
		filt[0][col+4] = (unsigned char) _mm256_extract_epi8(output_row0_even,4);
		filt[0][col+5] = (unsigned char) _mm256_extract_epi8(output_row0_even,5);
		filt[0][col+6] = (unsigned char) _mm256_extract_epi8(output_row0_even,6);
		filt[0][col+7] = (unsigned char) _mm256_extract_epi8(output_row0_even,7);
		filt[0][col+8] = (unsigned char) _mm256_extract_epi8(output_row0_even,8);
		filt[0][col+9] = (unsigned char) _mm256_extract_epi8(output_row0_even,9);

		filt[1][col] = (unsigned char) _mm256_extract_epi8(output_row1_even,0);
		filt[1][col+1] = (unsigned char) _mm256_extract_epi8(output_row1_even,1);
		filt[1][col+2] = (unsigned char) _mm256_extract_epi8(output_row1_even,2);
		filt[1][col+3] = (unsigned char) _mm256_extract_epi8(output_row1_even,3);
		filt[1][col+4] = (unsigned char) _mm256_extract_epi8(output_row1_even,4);
		filt[1][col+5] = (unsigned char) _mm256_extract_epi8(output_row1_even,5);
		filt[1][col+6] = (unsigned char) _mm256_extract_epi8(output_row1_even,6);
		filt[1][col+7] = (unsigned char) _mm256_extract_epi8(output_row1_even,7);
		filt[1][col+8] = (unsigned char) _mm256_extract_epi8(output_row1_even,8);
		filt[1][col+9] = (unsigned char) _mm256_extract_epi8(output_row1_even,9);
		  break;
	case 11:
		filt[2][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[2][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[2][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		filt[2][col+3] = (unsigned char) _mm256_extract_epi8(output_even,3);
		filt[2][col+4] = (unsigned char) _mm256_extract_epi8(output_even,4);
		filt[2][col+5] = (unsigned char) _mm256_extract_epi8(output_even,5);
		filt[2][col+6] = (unsigned char) _mm256_extract_epi8(output_even,6);
		filt[2][col+7] = (unsigned char) _mm256_extract_epi8(output_even,7);
		filt[2][col+8] = (unsigned char) _mm256_extract_epi8(output_even,8);
		filt[2][col+9] = (unsigned char) _mm256_extract_epi8(output_even,9);
		filt[2][col+10] = (unsigned char) _mm256_extract_epi8(output_even,10);

		filt[0][col] = (unsigned char) _mm256_extract_epi8(output_row0_even,0);
		filt[0][col+1] = (unsigned char) _mm256_extract_epi8(output_row0_even,1);
		filt[0][col+2] = (unsigned char) _mm256_extract_epi8(output_row0_even,2);
		filt[0][col+3] = (unsigned char) _mm256_extract_epi8(output_row0_even,3);
		filt[0][col+4] = (unsigned char) _mm256_extract_epi8(output_row0_even,4);
		filt[0][col+5] = (unsigned char) _mm256_extract_epi8(output_row0_even,5);
		filt[0][col+6] = (unsigned char) _mm256_extract_epi8(output_row0_even,6);
		filt[0][col+7] = (unsigned char) _mm256_extract_epi8(output_row0_even,7);
		filt[0][col+8] = (unsigned char) _mm256_extract_epi8(output_row0_even,8);
		filt[0][col+9] = (unsigned char) _mm256_extract_epi8(output_row0_even,9);
		filt[0][col+10] = (unsigned char) _mm256_extract_epi8(output_row0_even,10);

		filt[1][col] = (unsigned char) _mm256_extract_epi8(output_row1_even,0);
		filt[1][col+1] = (unsigned char) _mm256_extract_epi8(output_row1_even,1);
		filt[1][col+2] = (unsigned char) _mm256_extract_epi8(output_row1_even,2);
		filt[1][col+3] = (unsigned char) _mm256_extract_epi8(output_row1_even,3);
		filt[1][col+4] = (unsigned char) _mm256_extract_epi8(output_row1_even,4);
		filt[1][col+5] = (unsigned char) _mm256_extract_epi8(output_row1_even,5);
		filt[1][col+6] = (unsigned char) _mm256_extract_epi8(output_row1_even,6);
		filt[1][col+7] = (unsigned char) _mm256_extract_epi8(output_row1_even,7);
		filt[1][col+8] = (unsigned char) _mm256_extract_epi8(output_row1_even,8);
		filt[1][col+9] = (unsigned char) _mm256_extract_epi8(output_row1_even,9);
		filt[1][col+10] = (unsigned char) _mm256_extract_epi8(output_row1_even,10);
		  break;
	case 12:
		filt[2][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[2][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[2][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		filt[2][col+3] = (unsigned char) _mm256_extract_epi8(output_even,3);
		filt[2][col+4] = (unsigned char) _mm256_extract_epi8(output_even,4);
		filt[2][col+5] = (unsigned char) _mm256_extract_epi8(output_even,5);
		filt[2][col+6] = (unsigned char) _mm256_extract_epi8(output_even,6);
		filt[2][col+7] = (unsigned char) _mm256_extract_epi8(output_even,7);
		filt[2][col+8] = (unsigned char) _mm256_extract_epi8(output_even,8);
		filt[2][col+9] = (unsigned char) _mm256_extract_epi8(output_even,9);
		filt[2][col+10] = (unsigned char) _mm256_extract_epi8(output_even,10);
		filt[2][col+11] = (unsigned char) _mm256_extract_epi8(output_even,11);

		filt[0][col] = (unsigned char) _mm256_extract_epi8(output_row0_even,0);
		filt[0][col+1] = (unsigned char) _mm256_extract_epi8(output_row0_even,1);
		filt[0][col+2] = (unsigned char) _mm256_extract_epi8(output_row0_even,2);
		filt[0][col+3] = (unsigned char) _mm256_extract_epi8(output_row0_even,3);
		filt[0][col+4] = (unsigned char) _mm256_extract_epi8(output_row0_even,4);
		filt[0][col+5] = (unsigned char) _mm256_extract_epi8(output_row0_even,5);
		filt[0][col+6] = (unsigned char) _mm256_extract_epi8(output_row0_even,6);
		filt[0][col+7] = (unsigned char) _mm256_extract_epi8(output_row0_even,7);
		filt[0][col+8] = (unsigned char) _mm256_extract_epi8(output_row0_even,8);
		filt[0][col+9] = (unsigned char) _mm256_extract_epi8(output_row0_even,9);
		filt[0][col+10] = (unsigned char) _mm256_extract_epi8(output_row0_even,10);
		filt[0][col+11] = (unsigned char) _mm256_extract_epi8(output_row0_even,11);

		filt[1][col] = (unsigned char) _mm256_extract_epi8(output_row1_even,0);
		filt[1][col+1] = (unsigned char) _mm256_extract_epi8(output_row1_even,1);
		filt[1][col+2] = (unsigned char) _mm256_extract_epi8(output_row1_even,2);
		filt[1][col+3] = (unsigned char) _mm256_extract_epi8(output_row1_even,3);
		filt[1][col+4] = (unsigned char) _mm256_extract_epi8(output_row1_even,4);
		filt[1][col+5] = (unsigned char) _mm256_extract_epi8(output_row1_even,5);
		filt[1][col+6] = (unsigned char) _mm256_extract_epi8(output_row1_even,6);
		filt[1][col+7] = (unsigned char) _mm256_extract_epi8(output_row1_even,7);
		filt[1][col+8] = (unsigned char) _mm256_extract_epi8(output_row1_even,8);
		filt[1][col+9] = (unsigned char) _mm256_extract_epi8(output_row1_even,9);
		filt[1][col+10] = (unsigned char) _mm256_extract_epi8(output_row1_even,10);
		filt[1][col+11] = (unsigned char) _mm256_extract_epi8(output_row1_even,11);
		  break;
	case 13:
		filt[2][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[2][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[2][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		filt[2][col+3] = (unsigned char) _mm256_extract_epi8(output_even,3);
		filt[2][col+4] = (unsigned char) _mm256_extract_epi8(output_even,4);
		filt[2][col+5] = (unsigned char) _mm256_extract_epi8(output_even,5);
		filt[2][col+6] = (unsigned char) _mm256_extract_epi8(output_even,6);
		filt[2][col+7] = (unsigned char) _mm256_extract_epi8(output_even,7);
		filt[2][col+8] = (unsigned char) _mm256_extract_epi8(output_even,8);
		filt[2][col+9] = (unsigned char) _mm256_extract_epi8(output_even,9);
		filt[2][col+10] = (unsigned char) _mm256_extract_epi8(output_even,10);
		filt[2][col+11] = (unsigned char) _mm256_extract_epi8(output_even,11);
		filt[2][col+12] = (unsigned char) _mm256_extract_epi8(output_even,12);

		filt[0][col] = (unsigned char) _mm256_extract_epi8(output_row0_even,0);
		filt[0][col+1] = (unsigned char) _mm256_extract_epi8(output_row0_even,1);
		filt[0][col+2] = (unsigned char) _mm256_extract_epi8(output_row0_even,2);
		filt[0][col+3] = (unsigned char) _mm256_extract_epi8(output_row0_even,3);
		filt[0][col+4] = (unsigned char) _mm256_extract_epi8(output_row0_even,4);
		filt[0][col+5] = (unsigned char) _mm256_extract_epi8(output_row0_even,5);
		filt[0][col+6] = (unsigned char) _mm256_extract_epi8(output_row0_even,6);
		filt[0][col+7] = (unsigned char) _mm256_extract_epi8(output_row0_even,7);
		filt[0][col+8] = (unsigned char) _mm256_extract_epi8(output_row0_even,8);
		filt[0][col+9] = (unsigned char) _mm256_extract_epi8(output_row0_even,9);
		filt[0][col+10] = (unsigned char) _mm256_extract_epi8(output_row0_even,10);
		filt[0][col+11] = (unsigned char) _mm256_extract_epi8(output_row0_even,11);
		filt[0][col+12] = (unsigned char) _mm256_extract_epi8(output_row0_even,12);

		filt[1][col] = (unsigned char) _mm256_extract_epi8(output_row1_even,0);
		filt[1][col+1] = (unsigned char) _mm256_extract_epi8(output_row1_even,1);
		filt[1][col+2] = (unsigned char) _mm256_extract_epi8(output_row1_even,2);
		filt[1][col+3] = (unsigned char) _mm256_extract_epi8(output_row1_even,3);
		filt[1][col+4] = (unsigned char) _mm256_extract_epi8(output_row1_even,4);
		filt[1][col+5] = (unsigned char) _mm256_extract_epi8(output_row1_even,5);
		filt[1][col+6] = (unsigned char) _mm256_extract_epi8(output_row1_even,6);
		filt[1][col+7] = (unsigned char) _mm256_extract_epi8(output_row1_even,7);
		filt[1][col+8] = (unsigned char) _mm256_extract_epi8(output_row1_even,8);
		filt[1][col+9] = (unsigned char) _mm256_extract_epi8(output_row1_even,9);
		filt[1][col+10] = (unsigned char) _mm256_extract_epi8(output_row1_even,10);
		filt[1][col+11] = (unsigned char) _mm256_extract_epi8(output_row1_even,11);
		filt[1][col+12] = (unsigned char) _mm256_extract_epi8(output_row1_even,12);
		  break;
	case 14:
		filt[2][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[2][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[2][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		filt[2][col+3] = (unsigned char) _mm256_extract_epi8(output_even,3);
		filt[2][col+4] = (unsigned char) _mm256_extract_epi8(output_even,4);
		filt[2][col+5] = (unsigned char) _mm256_extract_epi8(output_even,5);
		filt[2][col+6] = (unsigned char) _mm256_extract_epi8(output_even,6);
		filt[2][col+7] = (unsigned char) _mm256_extract_epi8(output_even,7);
		filt[2][col+8] = (unsigned char) _mm256_extract_epi8(output_even,8);
		filt[2][col+9] = (unsigned char) _mm256_extract_epi8(output_even,9);
		filt[2][col+10] = (unsigned char) _mm256_extract_epi8(output_even,10);
		filt[2][col+11] = (unsigned char) _mm256_extract_epi8(output_even,11);
		filt[2][col+12] = (unsigned char) _mm256_extract_epi8(output_even,12);
		filt[2][col+13] = (unsigned char) _mm256_extract_epi8(output_even,13);

		filt[0][col] = (unsigned char) _mm256_extract_epi8(output_row0_even,0);
		filt[0][col+1] = (unsigned char) _mm256_extract_epi8(output_row0_even,1);
		filt[0][col+2] = (unsigned char) _mm256_extract_epi8(output_row0_even,2);
		filt[0][col+3] = (unsigned char) _mm256_extract_epi8(output_row0_even,3);
		filt[0][col+4] = (unsigned char) _mm256_extract_epi8(output_row0_even,4);
		filt[0][col+5] = (unsigned char) _mm256_extract_epi8(output_row0_even,5);
		filt[0][col+6] = (unsigned char) _mm256_extract_epi8(output_row0_even,6);
		filt[0][col+7] = (unsigned char) _mm256_extract_epi8(output_row0_even,7);
		filt[0][col+8] = (unsigned char) _mm256_extract_epi8(output_row0_even,8);
		filt[0][col+9] = (unsigned char) _mm256_extract_epi8(output_row0_even,9);
		filt[0][col+10] = (unsigned char) _mm256_extract_epi8(output_row0_even,10);
		filt[0][col+11] = (unsigned char) _mm256_extract_epi8(output_row0_even,11);
		filt[0][col+12] = (unsigned char) _mm256_extract_epi8(output_row0_even,12);
		filt[0][col+13] = (unsigned char) _mm256_extract_epi8(output_row0_even,13);

		filt[1][col] = (unsigned char) _mm256_extract_epi8(output_row1_even,0);
		filt[1][col+1] = (unsigned char) _mm256_extract_epi8(output_row1_even,1);
		filt[1][col+2] = (unsigned char) _mm256_extract_epi8(output_row1_even,2);
		filt[1][col+3] = (unsigned char) _mm256_extract_epi8(output_row1_even,3);
		filt[1][col+4] = (unsigned char) _mm256_extract_epi8(output_row1_even,4);
		filt[1][col+5] = (unsigned char) _mm256_extract_epi8(output_row1_even,5);
		filt[1][col+6] = (unsigned char) _mm256_extract_epi8(output_row1_even,6);
		filt[1][col+7] = (unsigned char) _mm256_extract_epi8(output_row1_even,7);
		filt[1][col+8] = (unsigned char) _mm256_extract_epi8(output_row1_even,8);
		filt[1][col+9] = (unsigned char) _mm256_extract_epi8(output_row1_even,9);
		filt[1][col+10] = (unsigned char) _mm256_extract_epi8(output_row1_even,10);
		filt[1][col+11] = (unsigned char) _mm256_extract_epi8(output_row1_even,11);
		filt[1][col+12] = (unsigned char) _mm256_extract_epi8(output_row1_even,12);
		filt[1][col+13] = (unsigned char) _mm256_extract_epi8(output_row1_even,13);
		  break;
	case 15:
		filt[2][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[2][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[2][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		filt[2][col+3] = (unsigned char) _mm256_extract_epi8(output_even,3);
		filt[2][col+4] = (unsigned char) _mm256_extract_epi8(output_even,4);
		filt[2][col+5] = (unsigned char) _mm256_extract_epi8(output_even,5);
		filt[2][col+6] = (unsigned char) _mm256_extract_epi8(output_even,6);
		filt[2][col+7] = (unsigned char) _mm256_extract_epi8(output_even,7);
		filt[2][col+8] = (unsigned char) _mm256_extract_epi8(output_even,8);
		filt[2][col+9] = (unsigned char) _mm256_extract_epi8(output_even,9);
		filt[2][col+10] = (unsigned char) _mm256_extract_epi8(output_even,10);
		filt[2][col+11] = (unsigned char) _mm256_extract_epi8(output_even,11);
		filt[2][col+12] = (unsigned char) _mm256_extract_epi8(output_even,12);
		filt[2][col+13] = (unsigned char) _mm256_extract_epi8(output_even,13);
		filt[2][col+14] = (unsigned char) _mm256_extract_epi8(output_even,14);

		filt[0][col] = (unsigned char) _mm256_extract_epi8(output_row0_even,0);
		filt[0][col+1] = (unsigned char) _mm256_extract_epi8(output_row0_even,1);
		filt[0][col+2] = (unsigned char) _mm256_extract_epi8(output_row0_even,2);
		filt[0][col+3] = (unsigned char) _mm256_extract_epi8(output_row0_even,3);
		filt[0][col+4] = (unsigned char) _mm256_extract_epi8(output_row0_even,4);
		filt[0][col+5] = (unsigned char) _mm256_extract_epi8(output_row0_even,5);
		filt[0][col+6] = (unsigned char) _mm256_extract_epi8(output_row0_even,6);
		filt[0][col+7] = (unsigned char) _mm256_extract_epi8(output_row0_even,7);
		filt[0][col+8] = (unsigned char) _mm256_extract_epi8(output_row0_even,8);
		filt[0][col+9] = (unsigned char) _mm256_extract_epi8(output_row0_even,9);
		filt[0][col+10] = (unsigned char) _mm256_extract_epi8(output_row0_even,10);
		filt[0][col+11] = (unsigned char) _mm256_extract_epi8(output_row0_even,11);
		filt[0][col+12] = (unsigned char) _mm256_extract_epi8(output_row0_even,12);
		filt[0][col+13] = (unsigned char) _mm256_extract_epi8(output_row0_even,13);
		filt[0][col+14] = (unsigned char) _mm256_extract_epi8(output_row0_even,14);

		filt[1][col] = (unsigned char) _mm256_extract_epi8(output_row1_even,0);
		filt[1][col+1] = (unsigned char) _mm256_extract_epi8(output_row1_even,1);
		filt[1][col+2] = (unsigned char) _mm256_extract_epi8(output_row1_even,2);
		filt[1][col+3] = (unsigned char) _mm256_extract_epi8(output_row1_even,3);
		filt[1][col+4] = (unsigned char) _mm256_extract_epi8(output_row1_even,4);
		filt[1][col+5] = (unsigned char) _mm256_extract_epi8(output_row1_even,5);
		filt[1][col+6] = (unsigned char) _mm256_extract_epi8(output_row1_even,6);
		filt[1][col+7] = (unsigned char) _mm256_extract_epi8(output_row1_even,7);
		filt[1][col+8] = (unsigned char) _mm256_extract_epi8(output_row1_even,8);
		filt[1][col+9] = (unsigned char) _mm256_extract_epi8(output_row1_even,9);
		filt[1][col+10] = (unsigned char) _mm256_extract_epi8(output_row1_even,10);
		filt[1][col+11] = (unsigned char) _mm256_extract_epi8(output_row1_even,11);
		filt[1][col+12] = (unsigned char) _mm256_extract_epi8(output_row1_even,12);
		filt[1][col+13] = (unsigned char) _mm256_extract_epi8(output_row1_even,13);
		filt[1][col+14] = (unsigned char) _mm256_extract_epi8(output_row1_even,14);
		  break;
	case 16:
		_mm_storeu_si128( (__m128i *) &filt[2][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
		_mm_storeu_si128( (__m128i *) &filt[0][col],_mm256_extractf128_si256(output_row0_even, 0)); //store low 128bit - 16pixels
		_mm_storeu_si128( (__m128i *) &filt[1][col],_mm256_extractf128_si256(output_row1_even, 0)); //store low 128bit - 16pixels
		  break;
	case 17:
		_mm_storeu_si128( (__m128i *) &filt[2][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
		filt[2][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);

		_mm_storeu_si128( (__m128i *) &filt[0][col],_mm256_extractf128_si256(output_row0_even, 0)); //store low 128bit - 16pixels
		filt[0][col+16] = (unsigned char) _mm256_extract_epi8(output_row0_even,16);

		_mm_storeu_si128( (__m128i *) &filt[1][col],_mm256_extractf128_si256(output_row1_even, 0)); //store low 128bit - 16pixels
		filt[1][col+16] = (unsigned char) _mm256_extract_epi8(output_row1_even,16);
		  break;
	case 18:
		_mm_storeu_si128( (__m128i *) &filt[2][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
		filt[2][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
		filt[2][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);

		_mm_storeu_si128( (__m128i *) &filt[0][col],_mm256_extractf128_si256(output_row0_even, 0)); //store low 128bit - 16pixels
		filt[0][col+16] = (unsigned char) _mm256_extract_epi8(output_row0_even,16);
		filt[0][col+17] = (unsigned char) _mm256_extract_epi8(output_row0_even,17);

		_mm_storeu_si128( (__m128i *) &filt[1][col],_mm256_extractf128_si256(output_row1_even, 0)); //store low 128bit - 16pixels
		filt[1][col+16] = (unsigned char) _mm256_extract_epi8(output_row1_even,16);
		filt[1][col+17] = (unsigned char) _mm256_extract_epi8(output_row1_even,17);
		  break;
	case 19:
		_mm_storeu_si128( (__m128i *) &filt[2][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
		filt[2][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
		filt[2][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
		filt[2][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);

		_mm_storeu_si128( (__m128i *) &filt[0][col],_mm256_extractf128_si256(output_row0_even, 0)); //store low 128bit - 16pixels
		filt[0][col+16] = (unsigned char) _mm256_extract_epi8(output_row0_even,16);
		filt[0][col+17] = (unsigned char) _mm256_extract_epi8(output_row0_even,17);
		filt[0][col+18] = (unsigned char) _mm256_extract_epi8(output_row0_even,18);

		_mm_storeu_si128( (__m128i *) &filt[1][col],_mm256_extractf128_si256(output_row1_even, 0)); //store low 128bit - 16pixels
		filt[1][col+16] = (unsigned char) _mm256_extract_epi8(output_row1_even,16);
		filt[1][col+17] = (unsigned char) _mm256_extract_epi8(output_row1_even,17);
		filt[1][col+18] = (unsigned char) _mm256_extract_epi8(output_row1_even,18);
		  break;
	case 20:
		_mm_storeu_si128( (__m128i *) &filt[2][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
		filt[2][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
		filt[2][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
		filt[2][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
		filt[2][col+19] = (unsigned char) _mm256_extract_epi8(output_even,19);

		_mm_storeu_si128( (__m128i *) &filt[0][col],_mm256_extractf128_si256(output_row0_even, 0)); //store low 128bit - 16pixels
		filt[0][col+16] = (unsigned char) _mm256_extract_epi8(output_row0_even,16);
		filt[0][col+17] = (unsigned char) _mm256_extract_epi8(output_row0_even,17);
		filt[0][col+18] = (unsigned char) _mm256_extract_epi8(output_row0_even,18);
		filt[0][col+19] = (unsigned char) _mm256_extract_epi8(output_row0_even,19);

		_mm_storeu_si128( (__m128i *) &filt[1][col],_mm256_extractf128_si256(output_row1_even, 0)); //store low 128bit - 16pixels
		filt[1][col+16] = (unsigned char) _mm256_extract_epi8(output_row1_even,16);
		filt[1][col+17] = (unsigned char) _mm256_extract_epi8(output_row1_even,17);
		filt[1][col+18] = (unsigned char) _mm256_extract_epi8(output_row1_even,18);
		filt[1][col+19] = (unsigned char) _mm256_extract_epi8(output_row1_even,19);
		  break;
	case 21:
		_mm_storeu_si128( (__m128i *) &filt[2][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
		filt[2][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
		filt[2][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
		filt[2][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
		filt[2][col+19] = (unsigned char) _mm256_extract_epi8(output_even,19);
		filt[2][col+20] = (unsigned char) _mm256_extract_epi8(output_even,20);

		_mm_storeu_si128( (__m128i *) &filt[0][col],_mm256_extractf128_si256(output_row0_even, 0)); //store low 128bit - 16pixels
		filt[0][col+16] = (unsigned char) _mm256_extract_epi8(output_row0_even,16);
		filt[0][col+17] = (unsigned char) _mm256_extract_epi8(output_row0_even,17);
		filt[0][col+18] = (unsigned char) _mm256_extract_epi8(output_row0_even,18);
		filt[0][col+19] = (unsigned char) _mm256_extract_epi8(output_row0_even,19);
		filt[0][col+20] = (unsigned char) _mm256_extract_epi8(output_row0_even,20);

		_mm_storeu_si128( (__m128i *) &filt[1][col],_mm256_extractf128_si256(output_row1_even, 0)); //store low 128bit - 16pixels
		filt[1][col+16] = (unsigned char) _mm256_extract_epi8(output_row1_even,16);
		filt[1][col+17] = (unsigned char) _mm256_extract_epi8(output_row1_even,17);
		filt[1][col+18] = (unsigned char) _mm256_extract_epi8(output_row1_even,18);
		filt[1][col+19] = (unsigned char) _mm256_extract_epi8(output_row1_even,19);
		filt[1][col+20] = (unsigned char) _mm256_extract_epi8(output_row1_even,20);
		  break;
	case 22:
		_mm_storeu_si128( (__m128i *) &filt[2][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
		filt[2][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
		filt[2][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
		filt[2][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
		filt[2][col+19] = (unsigned char) _mm256_extract_epi8(output_even,19);
		filt[2][col+20] = (unsigned char) _mm256_extract_epi8(output_even,20);
		filt[2][col+21] = (unsigned char) _mm256_extract_epi8(output_even,21);

		_mm_storeu_si128( (__m128i *) &filt[0][col],_mm256_extractf128_si256(output_row0_even, 0)); //store low 128bit - 16pixels
		filt[0][col+16] = (unsigned char) _mm256_extract_epi8(output_row0_even,16);
		filt[0][col+17] = (unsigned char) _mm256_extract_epi8(output_row0_even,17);
		filt[0][col+18] = (unsigned char) _mm256_extract_epi8(output_row0_even,18);
		filt[0][col+19] = (unsigned char) _mm256_extract_epi8(output_row0_even,19);
		filt[0][col+20] = (unsigned char) _mm256_extract_epi8(output_row0_even,20);
		filt[0][col+21] = (unsigned char) _mm256_extract_epi8(output_row0_even,21);

		_mm_storeu_si128( (__m128i *) &filt[1][col],_mm256_extractf128_si256(output_row1_even, 0)); //store low 128bit - 16pixels
		filt[1][col+16] = (unsigned char) _mm256_extract_epi8(output_row1_even,16);
		filt[1][col+17] = (unsigned char) _mm256_extract_epi8(output_row1_even,17);
		filt[1][col+18] = (unsigned char) _mm256_extract_epi8(output_row1_even,18);
		filt[1][col+19] = (unsigned char) _mm256_extract_epi8(output_row1_even,19);
		filt[1][col+20] = (unsigned char) _mm256_extract_epi8(output_row1_even,20);
		filt[1][col+21] = (unsigned char) _mm256_extract_epi8(output_row1_even,21);
		  break;
	case 23:
		_mm_storeu_si128( (__m128i *) &filt[2][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
		filt[2][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
		filt[2][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
		filt[2][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
		filt[2][col+19] = (unsigned char) _mm256_extract_epi8(output_even,19);
		filt[2][col+20] = (unsigned char) _mm256_extract_epi8(output_even,20);
		filt[2][col+21] = (unsigned char) _mm256_extract_epi8(output_even,21);
		filt[2][col+22] = (unsigned char) _mm256_extract_epi8(output_even,22);


		_mm_storeu_si128( (__m128i *) &filt[0][col],_mm256_extractf128_si256(output_row0_even, 0)); //store low 128bit - 16pixels
		filt[0][col+16] = (unsigned char) _mm256_extract_epi8(output_row0_even,16);
		filt[0][col+17] = (unsigned char) _mm256_extract_epi8(output_row0_even,17);
		filt[0][col+18] = (unsigned char) _mm256_extract_epi8(output_row0_even,18);
		filt[0][col+19] = (unsigned char) _mm256_extract_epi8(output_row0_even,19);
		filt[0][col+20] = (unsigned char) _mm256_extract_epi8(output_row0_even,20);
		filt[0][col+21] = (unsigned char) _mm256_extract_epi8(output_row0_even,21);
		filt[0][col+22] = (unsigned char) _mm256_extract_epi8(output_row0_even,22);

		_mm_storeu_si128( (__m128i *) &filt[1][col],_mm256_extractf128_si256(output_row1_even, 0)); //store low 128bit - 16pixels
		filt[1][col+16] = (unsigned char) _mm256_extract_epi8(output_row1_even,16);
		filt[1][col+17] = (unsigned char) _mm256_extract_epi8(output_row1_even,17);
		filt[1][col+18] = (unsigned char) _mm256_extract_epi8(output_row1_even,18);
		filt[1][col+19] = (unsigned char) _mm256_extract_epi8(output_row1_even,19);
		filt[1][col+20] = (unsigned char) _mm256_extract_epi8(output_row1_even,20);
		filt[1][col+21] = (unsigned char) _mm256_extract_epi8(output_row1_even,21);
		filt[1][col+22] = (unsigned char) _mm256_extract_epi8(output_row1_even,22);
		  break;
	case 24:
		_mm_storeu_si128( (__m128i *) &filt[2][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
		filt[2][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
		filt[2][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
		filt[2][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
		filt[2][col+19] = (unsigned char) _mm256_extract_epi8(output_even,19);
		filt[2][col+20] = (unsigned char) _mm256_extract_epi8(output_even,20);
		filt[2][col+21] = (unsigned char) _mm256_extract_epi8(output_even,21);
		filt[2][col+22] = (unsigned char) _mm256_extract_epi8(output_even,22);
		filt[2][col+23] = (unsigned char) _mm256_extract_epi8(output_even,23);

		_mm_storeu_si128( (__m128i *) &filt[0][col],_mm256_extractf128_si256(output_row0_even, 0)); //store low 128bit - 16pixels
		filt[0][col+16] = (unsigned char) _mm256_extract_epi8(output_row0_even,16);
		filt[0][col+17] = (unsigned char) _mm256_extract_epi8(output_row0_even,17);
		filt[0][col+18] = (unsigned char) _mm256_extract_epi8(output_row0_even,18);
		filt[0][col+19] = (unsigned char) _mm256_extract_epi8(output_row0_even,19);
		filt[0][col+20] = (unsigned char) _mm256_extract_epi8(output_row0_even,20);
		filt[0][col+21] = (unsigned char) _mm256_extract_epi8(output_row0_even,21);
		filt[0][col+22] = (unsigned char) _mm256_extract_epi8(output_row0_even,22);
		filt[0][col+23] = (unsigned char) _mm256_extract_epi8(output_row0_even,23);

		_mm_storeu_si128( (__m128i *) &filt[1][col],_mm256_extractf128_si256(output_row1_even, 0)); //store low 128bit - 16pixels
		filt[1][col+16] = (unsigned char) _mm256_extract_epi8(output_row1_even,16);
		filt[1][col+17] = (unsigned char) _mm256_extract_epi8(output_row1_even,17);
		filt[1][col+18] = (unsigned char) _mm256_extract_epi8(output_row1_even,18);
		filt[1][col+19] = (unsigned char) _mm256_extract_epi8(output_row1_even,19);
		filt[1][col+20] = (unsigned char) _mm256_extract_epi8(output_row1_even,20);
		filt[1][col+21] = (unsigned char) _mm256_extract_epi8(output_row1_even,21);
		filt[1][col+22] = (unsigned char) _mm256_extract_epi8(output_row1_even,22);
		filt[1][col+23] = (unsigned char) _mm256_extract_epi8(output_row1_even,23);
		  break;
	case 25:
		_mm_storeu_si128( (__m128i *) &filt[2][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
		filt[2][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
		filt[2][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
		filt[2][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
		filt[2][col+19] = (unsigned char) _mm256_extract_epi8(output_even,19);
		filt[2][col+20] = (unsigned char) _mm256_extract_epi8(output_even,20);
		filt[2][col+21] = (unsigned char) _mm256_extract_epi8(output_even,21);
		filt[2][col+22] = (unsigned char) _mm256_extract_epi8(output_even,22);
		filt[2][col+23] = (unsigned char) _mm256_extract_epi8(output_even,23);
		filt[2][col+24] = (unsigned char) _mm256_extract_epi8(output_even,24);

		_mm_storeu_si128( (__m128i *) &filt[0][col],_mm256_extractf128_si256(output_row0_even, 0)); //store low 128bit - 16pixels
		filt[0][col+16] = (unsigned char) _mm256_extract_epi8(output_row0_even,16);
		filt[0][col+17] = (unsigned char) _mm256_extract_epi8(output_row0_even,17);
		filt[0][col+18] = (unsigned char) _mm256_extract_epi8(output_row0_even,18);
		filt[0][col+19] = (unsigned char) _mm256_extract_epi8(output_row0_even,19);
		filt[0][col+20] = (unsigned char) _mm256_extract_epi8(output_row0_even,20);
		filt[0][col+21] = (unsigned char) _mm256_extract_epi8(output_row0_even,21);
		filt[0][col+22] = (unsigned char) _mm256_extract_epi8(output_row0_even,22);
		filt[0][col+23] = (unsigned char) _mm256_extract_epi8(output_row0_even,23);
		filt[0][col+24] = (unsigned char) _mm256_extract_epi8(output_row0_even,24);

		_mm_storeu_si128( (__m128i *) &filt[1][col],_mm256_extractf128_si256(output_row1_even, 0)); //store low 128bit - 16pixels
		filt[1][col+16] = (unsigned char) _mm256_extract_epi8(output_row1_even,16);
		filt[1][col+17] = (unsigned char) _mm256_extract_epi8(output_row1_even,17);
		filt[1][col+18] = (unsigned char) _mm256_extract_epi8(output_row1_even,18);
		filt[1][col+19] = (unsigned char) _mm256_extract_epi8(output_row1_even,19);
		filt[1][col+20] = (unsigned char) _mm256_extract_epi8(output_row1_even,20);
		filt[1][col+21] = (unsigned char) _mm256_extract_epi8(output_row1_even,21);
		filt[1][col+22] = (unsigned char) _mm256_extract_epi8(output_row1_even,22);
		filt[1][col+23] = (unsigned char) _mm256_extract_epi8(output_row1_even,23);
		filt[1][col+24] = (unsigned char) _mm256_extract_epi8(output_row1_even,24);
		  break;
	case 26:
		_mm_storeu_si128( (__m128i *) &filt[2][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
		filt[2][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
		filt[2][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
		filt[2][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
		filt[2][col+19] = (unsigned char) _mm256_extract_epi8(output_even,19);
		filt[2][col+20] = (unsigned char) _mm256_extract_epi8(output_even,20);
		filt[2][col+21] = (unsigned char) _mm256_extract_epi8(output_even,21);
		filt[2][col+22] = (unsigned char) _mm256_extract_epi8(output_even,22);
		filt[2][col+23] = (unsigned char) _mm256_extract_epi8(output_even,23);
		filt[2][col+24] = (unsigned char) _mm256_extract_epi8(output_even,24);
		filt[2][col+25] = (unsigned char) _mm256_extract_epi8(output_even,25);

		_mm_storeu_si128( (__m128i *) &filt[0][col],_mm256_extractf128_si256(output_row0_even, 0)); //store low 128bit - 16pixels
		filt[0][col+16] = (unsigned char) _mm256_extract_epi8(output_row0_even,16);
		filt[0][col+17] = (unsigned char) _mm256_extract_epi8(output_row0_even,17);
		filt[0][col+18] = (unsigned char) _mm256_extract_epi8(output_row0_even,18);
		filt[0][col+19] = (unsigned char) _mm256_extract_epi8(output_row0_even,19);
		filt[0][col+20] = (unsigned char) _mm256_extract_epi8(output_row0_even,20);
		filt[0][col+21] = (unsigned char) _mm256_extract_epi8(output_row0_even,21);
		filt[0][col+22] = (unsigned char) _mm256_extract_epi8(output_row0_even,22);
		filt[0][col+23] = (unsigned char) _mm256_extract_epi8(output_row0_even,23);
		filt[0][col+24] = (unsigned char) _mm256_extract_epi8(output_row0_even,24);
		filt[0][col+25] = (unsigned char) _mm256_extract_epi8(output_row0_even,25);

		_mm_storeu_si128( (__m128i *) &filt[1][col],_mm256_extractf128_si256(output_row1_even, 0)); //store low 128bit - 16pixels
		filt[1][col+16] = (unsigned char) _mm256_extract_epi8(output_row1_even,16);
		filt[1][col+17] = (unsigned char) _mm256_extract_epi8(output_row1_even,17);
		filt[1][col+18] = (unsigned char) _mm256_extract_epi8(output_row1_even,18);
		filt[1][col+19] = (unsigned char) _mm256_extract_epi8(output_row1_even,19);
		filt[1][col+20] = (unsigned char) _mm256_extract_epi8(output_row1_even,20);
		filt[1][col+21] = (unsigned char) _mm256_extract_epi8(output_row1_even,21);
		filt[1][col+22] = (unsigned char) _mm256_extract_epi8(output_row1_even,22);
		filt[1][col+23] = (unsigned char) _mm256_extract_epi8(output_row1_even,23);
		filt[1][col+24] = (unsigned char) _mm256_extract_epi8(output_row1_even,24);
		filt[1][col+25] = (unsigned char) _mm256_extract_epi8(output_row1_even,25);
		  break;
	case 27:
		_mm_storeu_si128( (__m128i *) &filt[2][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
		filt[2][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
		filt[2][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
		filt[2][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
		filt[2][col+19] = (unsigned char) _mm256_extract_epi8(output_even,19);
		filt[2][col+20] = (unsigned char) _mm256_extract_epi8(output_even,20);
		filt[2][col+21] = (unsigned char) _mm256_extract_epi8(output_even,21);
		filt[2][col+22] = (unsigned char) _mm256_extract_epi8(output_even,22);
		filt[2][col+23] = (unsigned char) _mm256_extract_epi8(output_even,23);
		filt[2][col+24] = (unsigned char) _mm256_extract_epi8(output_even,24);
		filt[2][col+25] = (unsigned char) _mm256_extract_epi8(output_even,25);
		filt[2][col+26] = (unsigned char) _mm256_extract_epi8(output_even,26);

		_mm_storeu_si128( (__m128i *) &filt[0][col],_mm256_extractf128_si256(output_row0_even, 0)); //store low 128bit - 16pixels
		filt[0][col+16] = (unsigned char) _mm256_extract_epi8(output_row0_even,16);
		filt[0][col+17] = (unsigned char) _mm256_extract_epi8(output_row0_even,17);
		filt[0][col+18] = (unsigned char) _mm256_extract_epi8(output_row0_even,18);
		filt[0][col+19] = (unsigned char) _mm256_extract_epi8(output_row0_even,19);
		filt[0][col+20] = (unsigned char) _mm256_extract_epi8(output_row0_even,20);
		filt[0][col+21] = (unsigned char) _mm256_extract_epi8(output_row0_even,21);
		filt[0][col+22] = (unsigned char) _mm256_extract_epi8(output_row0_even,22);
		filt[0][col+23] = (unsigned char) _mm256_extract_epi8(output_row0_even,23);
		filt[0][col+24] = (unsigned char) _mm256_extract_epi8(output_row0_even,24);
		filt[0][col+25] = (unsigned char) _mm256_extract_epi8(output_row0_even,25);
		filt[0][col+26] = (unsigned char) _mm256_extract_epi8(output_row0_even,26);

		_mm_storeu_si128( (__m128i *) &filt[1][col],_mm256_extractf128_si256(output_row1_even, 0)); //store low 128bit - 16pixels
		filt[1][col+16] = (unsigned char) _mm256_extract_epi8(output_row1_even,16);
		filt[1][col+17] = (unsigned char) _mm256_extract_epi8(output_row1_even,17);
		filt[1][col+18] = (unsigned char) _mm256_extract_epi8(output_row1_even,18);
		filt[1][col+19] = (unsigned char) _mm256_extract_epi8(output_row1_even,19);
		filt[1][col+20] = (unsigned char) _mm256_extract_epi8(output_row1_even,20);
		filt[1][col+21] = (unsigned char) _mm256_extract_epi8(output_row1_even,21);
		filt[1][col+22] = (unsigned char) _mm256_extract_epi8(output_row1_even,22);
		filt[1][col+23] = (unsigned char) _mm256_extract_epi8(output_row1_even,23);
		filt[1][col+24] = (unsigned char) _mm256_extract_epi8(output_row1_even,24);
		filt[1][col+25] = (unsigned char) _mm256_extract_epi8(output_row1_even,25);
		filt[1][col+26] = (unsigned char) _mm256_extract_epi8(output_row1_even,26);
		  break;
	case 28:
		_mm_storeu_si128( (__m128i *) &filt[2][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
		filt[2][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
		filt[2][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
		filt[2][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
		filt[2][col+19] = (unsigned char) _mm256_extract_epi8(output_even,19);
		filt[2][col+20] = (unsigned char) _mm256_extract_epi8(output_even,20);
		filt[2][col+21] = (unsigned char) _mm256_extract_epi8(output_even,21);
		filt[2][col+22] = (unsigned char) _mm256_extract_epi8(output_even,22);
		filt[2][col+23] = (unsigned char) _mm256_extract_epi8(output_even,23);
		filt[2][col+24] = (unsigned char) _mm256_extract_epi8(output_even,24);
		filt[2][col+25] = (unsigned char) _mm256_extract_epi8(output_even,25);
		filt[2][col+26] = (unsigned char) _mm256_extract_epi8(output_even,26);
		filt[2][col+27] = (unsigned char) _mm256_extract_epi8(output_even,27);

		_mm_storeu_si128( (__m128i *) &filt[0][col],_mm256_extractf128_si256(output_row0_even, 0)); //store low 128bit - 16pixels
		filt[0][col+16] = (unsigned char) _mm256_extract_epi8(output_row0_even,16);
		filt[0][col+17] = (unsigned char) _mm256_extract_epi8(output_row0_even,17);
		filt[0][col+18] = (unsigned char) _mm256_extract_epi8(output_row0_even,18);
		filt[0][col+19] = (unsigned char) _mm256_extract_epi8(output_row0_even,19);
		filt[0][col+20] = (unsigned char) _mm256_extract_epi8(output_row0_even,20);
		filt[0][col+21] = (unsigned char) _mm256_extract_epi8(output_row0_even,21);
		filt[0][col+22] = (unsigned char) _mm256_extract_epi8(output_row0_even,22);
		filt[0][col+23] = (unsigned char) _mm256_extract_epi8(output_row0_even,23);
		filt[0][col+24] = (unsigned char) _mm256_extract_epi8(output_row0_even,24);
		filt[0][col+25] = (unsigned char) _mm256_extract_epi8(output_row0_even,25);
		filt[0][col+26] = (unsigned char) _mm256_extract_epi8(output_row0_even,26);
		filt[0][col+27] = (unsigned char) _mm256_extract_epi8(output_row0_even,27);

		_mm_storeu_si128( (__m128i *) &filt[1][col],_mm256_extractf128_si256(output_row1_even, 0)); //store low 128bit - 16pixels
		filt[1][col+16] = (unsigned char) _mm256_extract_epi8(output_row1_even,16);
		filt[1][col+17] = (unsigned char) _mm256_extract_epi8(output_row1_even,17);
		filt[1][col+18] = (unsigned char) _mm256_extract_epi8(output_row1_even,18);
		filt[1][col+19] = (unsigned char) _mm256_extract_epi8(output_row1_even,19);
		filt[1][col+20] = (unsigned char) _mm256_extract_epi8(output_row1_even,20);
		filt[1][col+21] = (unsigned char) _mm256_extract_epi8(output_row1_even,21);
		filt[1][col+22] = (unsigned char) _mm256_extract_epi8(output_row1_even,22);
		filt[1][col+23] = (unsigned char) _mm256_extract_epi8(output_row1_even,23);
		filt[1][col+24] = (unsigned char) _mm256_extract_epi8(output_row1_even,24);
		filt[1][col+25] = (unsigned char) _mm256_extract_epi8(output_row1_even,25);
		filt[1][col+26] = (unsigned char) _mm256_extract_epi8(output_row1_even,26);
		filt[1][col+27] = (unsigned char) _mm256_extract_epi8(output_row1_even,27);
		  break;
	case 29:
		_mm_storeu_si128( (__m128i *) &filt[2][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
		filt[2][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
		filt[2][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
		filt[2][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
		filt[2][col+19] = (unsigned char) _mm256_extract_epi8(output_even,19);
		filt[2][col+20] = (unsigned char) _mm256_extract_epi8(output_even,20);
		filt[2][col+21] = (unsigned char) _mm256_extract_epi8(output_even,21);
		filt[2][col+22] = (unsigned char) _mm256_extract_epi8(output_even,22);
		filt[2][col+23] = (unsigned char) _mm256_extract_epi8(output_even,23);
		filt[2][col+24] = (unsigned char) _mm256_extract_epi8(output_even,24);
		filt[2][col+25] = (unsigned char) _mm256_extract_epi8(output_even,25);
		filt[2][col+26] = (unsigned char) _mm256_extract_epi8(output_even,26);
		filt[2][col+27] = (unsigned char) _mm256_extract_epi8(output_even,27);
		filt[2][col+28] = (unsigned char) _mm256_extract_epi8(output_even,28);

		_mm_storeu_si128( (__m128i *) &filt[0][col],_mm256_extractf128_si256(output_row0_even, 0)); //store low 128bit - 16pixels
		filt[0][col+16] = (unsigned char) _mm256_extract_epi8(output_row0_even,16);
		filt[0][col+17] = (unsigned char) _mm256_extract_epi8(output_row0_even,17);
		filt[0][col+18] = (unsigned char) _mm256_extract_epi8(output_row0_even,18);
		filt[0][col+19] = (unsigned char) _mm256_extract_epi8(output_row0_even,19);
		filt[0][col+20] = (unsigned char) _mm256_extract_epi8(output_row0_even,20);
		filt[0][col+21] = (unsigned char) _mm256_extract_epi8(output_row0_even,21);
		filt[0][col+22] = (unsigned char) _mm256_extract_epi8(output_row0_even,22);
		filt[0][col+23] = (unsigned char) _mm256_extract_epi8(output_row0_even,23);
		filt[0][col+24] = (unsigned char) _mm256_extract_epi8(output_row0_even,24);
		filt[0][col+25] = (unsigned char) _mm256_extract_epi8(output_row0_even,25);
		filt[0][col+26] = (unsigned char) _mm256_extract_epi8(output_row0_even,26);
		filt[0][col+27] = (unsigned char) _mm256_extract_epi8(output_row0_even,27);
		filt[0][col+28] = (unsigned char) _mm256_extract_epi8(output_row0_even,28);

		_mm_storeu_si128( (__m128i *) &filt[1][col],_mm256_extractf128_si256(output_row1_even, 0)); //store low 128bit - 16pixels
		filt[1][col+16] = (unsigned char) _mm256_extract_epi8(output_row1_even,16);
		filt[1][col+17] = (unsigned char) _mm256_extract_epi8(output_row1_even,17);
		filt[1][col+18] = (unsigned char) _mm256_extract_epi8(output_row1_even,18);
		filt[1][col+19] = (unsigned char) _mm256_extract_epi8(output_row1_even,19);
		filt[1][col+20] = (unsigned char) _mm256_extract_epi8(output_row1_even,20);
		filt[1][col+21] = (unsigned char) _mm256_extract_epi8(output_row1_even,21);
		filt[1][col+22] = (unsigned char) _mm256_extract_epi8(output_row1_even,22);
		filt[1][col+23] = (unsigned char) _mm256_extract_epi8(output_row1_even,23);
		filt[1][col+24] = (unsigned char) _mm256_extract_epi8(output_row1_even,24);
		filt[1][col+25] = (unsigned char) _mm256_extract_epi8(output_row1_even,25);
		filt[1][col+26] = (unsigned char) _mm256_extract_epi8(output_row1_even,26);
		filt[1][col+27] = (unsigned char) _mm256_extract_epi8(output_row1_even,27);
		filt[1][col+28] = (unsigned char) _mm256_extract_epi8(output_row1_even,28);
		  break;
	case 30:
		_mm_storeu_si128( (__m128i *) &filt[2][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
		filt[2][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
		filt[2][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
		filt[2][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
		filt[2][col+19] = (unsigned char) _mm256_extract_epi8(output_even,19);
		filt[2][col+20] = (unsigned char) _mm256_extract_epi8(output_even,20);
		filt[2][col+21] = (unsigned char) _mm256_extract_epi8(output_even,21);
		filt[2][col+22] = (unsigned char) _mm256_extract_epi8(output_even,22);
		filt[2][col+23] = (unsigned char) _mm256_extract_epi8(output_even,23);
		filt[2][col+24] = (unsigned char) _mm256_extract_epi8(output_even,24);
		filt[2][col+25] = (unsigned char) _mm256_extract_epi8(output_even,25);
		filt[2][col+26] = (unsigned char) _mm256_extract_epi8(output_even,26);
		filt[2][col+27] = (unsigned char) _mm256_extract_epi8(output_even,27);
		filt[2][col+28] = (unsigned char) _mm256_extract_epi8(output_even,28);
		filt[2][col+29] = (unsigned char) _mm256_extract_epi8(output_even,29);

		_mm_storeu_si128( (__m128i *) &filt[0][col],_mm256_extractf128_si256(output_row0_even, 0)); //store low 128bit - 16pixels
		filt[0][col+16] = (unsigned char) _mm256_extract_epi8(output_row0_even,16);
		filt[0][col+17] = (unsigned char) _mm256_extract_epi8(output_row0_even,17);
		filt[0][col+18] = (unsigned char) _mm256_extract_epi8(output_row0_even,18);
		filt[0][col+19] = (unsigned char) _mm256_extract_epi8(output_row0_even,19);
		filt[0][col+20] = (unsigned char) _mm256_extract_epi8(output_row0_even,20);
		filt[0][col+21] = (unsigned char) _mm256_extract_epi8(output_row0_even,21);
		filt[0][col+22] = (unsigned char) _mm256_extract_epi8(output_row0_even,22);
		filt[0][col+23] = (unsigned char) _mm256_extract_epi8(output_row0_even,23);
		filt[0][col+24] = (unsigned char) _mm256_extract_epi8(output_row0_even,24);
		filt[0][col+25] = (unsigned char) _mm256_extract_epi8(output_row0_even,25);
		filt[0][col+26] = (unsigned char) _mm256_extract_epi8(output_row0_even,26);
		filt[0][col+27] = (unsigned char) _mm256_extract_epi8(output_row0_even,27);
		filt[0][col+28] = (unsigned char) _mm256_extract_epi8(output_row0_even,28);
		filt[0][col+29] = (unsigned char) _mm256_extract_epi8(output_row0_even,29);


		_mm_storeu_si128( (__m128i *) &filt[1][col],_mm256_extractf128_si256(output_row1_even, 0)); //store low 128bit - 16pixels
		filt[1][col+16] = (unsigned char) _mm256_extract_epi8(output_row1_even,16);
		filt[1][col+17] = (unsigned char) _mm256_extract_epi8(output_row1_even,17);
		filt[1][col+18] = (unsigned char) _mm256_extract_epi8(output_row1_even,18);
		filt[1][col+19] = (unsigned char) _mm256_extract_epi8(output_row1_even,19);
		filt[1][col+20] = (unsigned char) _mm256_extract_epi8(output_row1_even,20);
		filt[1][col+21] = (unsigned char) _mm256_extract_epi8(output_row1_even,21);
		filt[1][col+22] = (unsigned char) _mm256_extract_epi8(output_row1_even,22);
		filt[1][col+23] = (unsigned char) _mm256_extract_epi8(output_row1_even,23);
		filt[1][col+24] = (unsigned char) _mm256_extract_epi8(output_row1_even,24);
		filt[1][col+25] = (unsigned char) _mm256_extract_epi8(output_row1_even,25);
		filt[1][col+26] = (unsigned char) _mm256_extract_epi8(output_row1_even,26);
		filt[1][col+27] = (unsigned char) _mm256_extract_epi8(output_row1_even,27);
		filt[1][col+28] = (unsigned char) _mm256_extract_epi8(output_row1_even,28);
		filt[1][col+29] = (unsigned char) _mm256_extract_epi8(output_row1_even,29);
		  break;
	case 31:
		_mm_storeu_si128( (__m128i *) &filt[2][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
		filt[2][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
		filt[2][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
		filt[2][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
		filt[2][col+19] = (unsigned char) _mm256_extract_epi8(output_even,19);
		filt[2][col+20] = (unsigned char) _mm256_extract_epi8(output_even,20);
		filt[2][col+21] = (unsigned char) _mm256_extract_epi8(output_even,21);
		filt[2][col+22] = (unsigned char) _mm256_extract_epi8(output_even,22);
		filt[2][col+23] = (unsigned char) _mm256_extract_epi8(output_even,23);
		filt[2][col+24] = (unsigned char) _mm256_extract_epi8(output_even,24);
		filt[2][col+25] = (unsigned char) _mm256_extract_epi8(output_even,25);
		filt[2][col+26] = (unsigned char) _mm256_extract_epi8(output_even,26);
		filt[2][col+27] = (unsigned char) _mm256_extract_epi8(output_even,27);
		filt[2][col+28] = (unsigned char) _mm256_extract_epi8(output_even,28);
		filt[2][col+29] = (unsigned char) _mm256_extract_epi8(output_even,29);

		_mm_storeu_si128( (__m128i *) &filt[0][col],_mm256_extractf128_si256(output_row0_even, 0)); //store low 128bit - 16pixels
		filt[0][col+16] = (unsigned char) _mm256_extract_epi8(output_row0_even,16);
		filt[0][col+17] = (unsigned char) _mm256_extract_epi8(output_row0_even,17);
		filt[0][col+18] = (unsigned char) _mm256_extract_epi8(output_row0_even,18);
		filt[0][col+19] = (unsigned char) _mm256_extract_epi8(output_row0_even,19);
		filt[0][col+20] = (unsigned char) _mm256_extract_epi8(output_row0_even,20);
		filt[0][col+21] = (unsigned char) _mm256_extract_epi8(output_row0_even,21);
		filt[0][col+22] = (unsigned char) _mm256_extract_epi8(output_row0_even,22);
		filt[0][col+23] = (unsigned char) _mm256_extract_epi8(output_row0_even,23);
		filt[0][col+24] = (unsigned char) _mm256_extract_epi8(output_row0_even,24);
		filt[0][col+25] = (unsigned char) _mm256_extract_epi8(output_row0_even,25);
		filt[0][col+26] = (unsigned char) _mm256_extract_epi8(output_row0_even,26);
		filt[0][col+27] = (unsigned char) _mm256_extract_epi8(output_row0_even,27);
		filt[0][col+28] = (unsigned char) _mm256_extract_epi8(output_row0_even,28);
		filt[0][col+29] = (unsigned char) _mm256_extract_epi8(output_row0_even,29);


		_mm_storeu_si128( (__m128i *) &filt[1][col],_mm256_extractf128_si256(output_row1_even, 0)); //store low 128bit - 16pixels
		filt[1][col+16] = (unsigned char) _mm256_extract_epi8(output_row1_even,16);
		filt[1][col+17] = (unsigned char) _mm256_extract_epi8(output_row1_even,17);
		filt[1][col+18] = (unsigned char) _mm256_extract_epi8(output_row1_even,18);
		filt[1][col+19] = (unsigned char) _mm256_extract_epi8(output_row1_even,19);
		filt[1][col+20] = (unsigned char) _mm256_extract_epi8(output_row1_even,20);
		filt[1][col+21] = (unsigned char) _mm256_extract_epi8(output_row1_even,21);
		filt[1][col+22] = (unsigned char) _mm256_extract_epi8(output_row1_even,22);
		filt[1][col+23] = (unsigned char) _mm256_extract_epi8(output_row1_even,23);
		filt[1][col+24] = (unsigned char) _mm256_extract_epi8(output_row1_even,24);
		filt[1][col+25] = (unsigned char) _mm256_extract_epi8(output_row1_even,25);
		filt[1][col+26] = (unsigned char) _mm256_extract_epi8(output_row1_even,26);
		filt[1][col+27] = (unsigned char) _mm256_extract_epi8(output_row1_even,27);
		filt[1][col+28] = (unsigned char) _mm256_extract_epi8(output_row1_even,28);
		filt[1][col+29] = (unsigned char) _mm256_extract_epi8(output_row1_even,29);

		//the filt[0:2][M-1] pixel will be computed without vectorization
		int newPixel;
		for (int row=0;row<=2;row++){
		 newPixel = 0;
		for (int rowOffset=-2; rowOffset<=2; rowOffset++) {
			for (int colOffset=-2; colOffset<=2; colOffset++) {

			   if ( ((row+rowOffset)<N) && ((M-1+colOffset)<M) && ((row+rowOffset)>=0) && ((M-1+colOffset)>=0) )
	              newPixel += frame1[row+rowOffset][M-1+colOffset] * filter[2 + rowOffset][2 + colOffset];

			      }
		        }

	   filt[row][M-1] = (unsigned char) (newPixel / divisor);
		}

		  break;
	default:
		printf("\n something went wrong");
	}

return 0;

}





//for row=N-3,N-2,N-1 only
//row0 refers to N-2, while row1 refers to N-1
int conv_loop_reminder_last_less_div(unsigned char **frame1,unsigned char **filt,const unsigned int M, const unsigned int N, const unsigned int col, const unsigned int REMINDER_ITERATIONS,const unsigned int division_case,const signed char mask_vector[][32], const __m256i f, const unsigned short int divisor,signed char **filter ){

__m256i r0,r1,r2,r3,r4,m0,m1,m2,m3,m4;
__m256i output_even,output_odd;
__m256i output_row0_even,output_row1_even,output_row0_odd,output_row1_odd;

		//const __m256i f  = _mm256_set_epi16(52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759);
		//const __m256i mask  = _mm256_set_epi8(0,0,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255);
		//const __m256i mask2  = _mm256_set_epi8(0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0);
		const __m256i mask3  = _mm256_set_epi16(0,0,0,0,0,0,0,65535,0,0,0,0,0,0,0,0);
		//const __m256i mask4  = _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255,0,0,0,0,0,0,0,0,0,0,0,0);
		//const __m256i mask5  = _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0);

		const __m256i output_mask      = _mm256_set_epi16(0,0,0,65535,0,0,65535,0,0,65535,0,0,65535,0,0,65535);
		const __m256i output_mask_sh1  = _mm256_set_epi16(0,0,65535,0,0,65535,0,0,65535,0,0,65535,0,0,65535,0);
		//const __m256i output_mask_sh2  = _mm256_set_epi16(0,0,0,    0,65535,0,0,65535,0,0,65535,0,0,65535,0,0);
		const __m256i mask_new      = _mm256_set_epi16(0,0,0,0,0,0,0,0,0,65535,0,0,0,0,0,0);
		const __m256i mask_new2      = _mm256_set_epi16(0,0,0,0,0,0,0,0,65535,0,0,0,0,0,0,0);


		const __m256i c0=_mm256_load_si256( (__m256i *) &mask_vector[0][0]);
			const __m256i c1=_mm256_load_si256( (__m256i *) &mask_vector[1][0]);
			const __m256i c2=_mm256_load_si256( (__m256i *) &mask_vector[2][0]);
			const __m256i c3=_mm256_load_si256( (__m256i *) &mask_vector[3][0]);
			const __m256i c4=_mm256_load_si256( (__m256i *) &mask_vector[4][0]);

			const __m256i c0_sh1=_mm256_load_si256( (__m256i *) &mask_vector[5][0]);
			const __m256i c1_sh1=_mm256_load_si256( (__m256i *) &mask_vector[6][0]);
			const __m256i c2_sh1=_mm256_load_si256( (__m256i *) &mask_vector[7][0]);
			const __m256i c3_sh1=_mm256_load_si256( (__m256i *) &mask_vector[8][0]);
			const __m256i c4_sh1=_mm256_load_si256( (__m256i *) &mask_vector[9][0]);

			const __m256i c0_sh2=_mm256_load_si256( (__m256i *) &mask_vector[10][0]);
			const __m256i c1_sh2=_mm256_load_si256( (__m256i *) &mask_vector[11][0]);
			const __m256i c2_sh2=_mm256_load_si256( (__m256i *) &mask_vector[12][0]);
			const __m256i c3_sh2=_mm256_load_si256( (__m256i *) &mask_vector[13][0]);
			const __m256i c4_sh2=_mm256_load_si256( (__m256i *) &mask_vector[14][0]);



 __m256i reminder_mask1,reminder_mask2;

	reminder_mask1=_mm256_load_si256( (__m256i *) &reminder_msk1[REMINDER_ITERATIONS-1][0]);
	reminder_mask2=_mm256_load_si256( (__m256i *) &reminder_msk2[REMINDER_ITERATIONS-1][0]);


	 //1st col iteration
	//load the 5 rows
	r0=_mm256_loadu_si256( (__m256i *) &frame1[N-5][col-2]);
	r1=_mm256_loadu_si256( (__m256i *) &frame1[N-4][col-2]);
	r2=_mm256_loadu_si256( (__m256i *) &frame1[N-3][col-2]);
	r3=_mm256_loadu_si256( (__m256i *) &frame1[N-2][col-2]);
	r4=_mm256_loadu_si256( (__m256i *) &frame1[N-1][col-2]);

	/*
	 * The above load operations load outside of the array bounds; these elements are filled with zeros just after
	 * Furthermore, I add two extra zeros in the end to compute col=N-2 and col=N-1. The last value of col is N-1, not N
	 */

	//AND r0-r4 with reminder_mask
	r0=_mm256_and_si256(r0,reminder_mask1);
	r1=_mm256_and_si256(r1,reminder_mask1);
	r2=_mm256_and_si256(r2,reminder_mask1);
	r3=_mm256_and_si256(r3,reminder_mask1);
	r4=_mm256_and_si256(r4,reminder_mask1);

	//row=2
	//multiply with the mask
	m0=_mm256_maddubs_epi16(r0,c0);
	m1=_mm256_maddubs_epi16(r1,c1);
	m2=_mm256_maddubs_epi16(r2,c2);
	m3=_mm256_maddubs_epi16(r3,c3);
	m4=_mm256_maddubs_epi16(r4,c4);

	//vertical add
	m0=_mm256_add_epi16(m0,m1);
	m2=_mm256_add_epi16(m2,m3);
	m0=_mm256_add_epi16(m0,m2);
	m0=_mm256_add_epi16(m0,m4);


	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);

	m1=_mm256_srli_si256(m0,4);
	m2=_mm256_add_epi16(m1,m2);

	//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
	//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
	m4=_mm256_and_si256(m0,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions

	m2=_mm256_add_epi16(m2,m4);

	//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
	output_even=_mm256_and_si256(m2,output_mask);

	//row=0
	//multiply with the mask
	m1=_mm256_maddubs_epi16(r1,c0);
	m2=_mm256_maddubs_epi16(r2,c1);
	m3=_mm256_maddubs_epi16(r3,c2);
	m4=_mm256_maddubs_epi16(r4,c3);

	//vertical add
	m0=_mm256_add_epi16(m1,m2);
	m0=_mm256_add_epi16(m0,m3);
	m0=_mm256_add_epi16(m0,m4);


	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);

	m1=_mm256_srli_si256(m0,4);
	m2=_mm256_add_epi16(m1,m2);

	//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
	//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
	m4=_mm256_and_si256(m0,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions

	m2=_mm256_add_epi16(m2,m4);

	//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
	output_row0_even=_mm256_and_si256(m2,output_mask);


	//row=1
	//multiply with the mask
	m2=_mm256_maddubs_epi16(r2,c0);
	m3=_mm256_maddubs_epi16(r3,c1);
	m4=_mm256_maddubs_epi16(r4,c2);

	//vertical add
	m0=_mm256_add_epi16(m2,m3);
	m0=_mm256_add_epi16(m0,m4);


	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);

	m1=_mm256_srli_si256(m0,4);
	m2=_mm256_add_epi16(m1,m2);

	//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
	//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
	m4=_mm256_and_si256(m0,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions

	m2=_mm256_add_epi16(m2,m4);

	//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
	output_row1_even=_mm256_and_si256(m2,output_mask);

	//2nd col iteration

	//row=2
	//multiply with the mask
	m0=_mm256_maddubs_epi16(r0,c0_sh1);
	m1=_mm256_maddubs_epi16(r1,c1_sh1);
	m2=_mm256_maddubs_epi16(r2,c2_sh1);
	m3=_mm256_maddubs_epi16(r3,c3_sh1);
	m4=_mm256_maddubs_epi16(r4,c4_sh1);

	//vertical add
	m0=_mm256_add_epi16(m0,m1);
	m2=_mm256_add_epi16(m2,m3);
	m0=_mm256_add_epi16(m0,m2);
	m0=_mm256_add_epi16(m0,m4);

	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);

	m1=_mm256_srli_si256(m0,4);
	m2=_mm256_add_epi16(m1,m2);

	//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
	//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
	m4=_mm256_and_si256(m0,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions
	m2=_mm256_add_epi16(m2,m4);

	//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
	output_odd = _mm256_and_si256(m2,output_mask);

	//row=0
	//multiply with the mask
	m1=_mm256_maddubs_epi16(r1,c0_sh1);
	m2=_mm256_maddubs_epi16(r2,c1_sh1);
	m3=_mm256_maddubs_epi16(r3,c2_sh1);
	m4=_mm256_maddubs_epi16(r4,c3_sh1);

	//vertical add
	m0=_mm256_add_epi16(m1,m2);
	m0=_mm256_add_epi16(m0,m3);
	m0=_mm256_add_epi16(m0,m4);


	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);

	m1=_mm256_srli_si256(m0,4);
	m2=_mm256_add_epi16(m1,m2);

	//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
	//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
	m4=_mm256_and_si256(m0,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions
	m2=_mm256_add_epi16(m2,m4);

	//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
	output_row0_odd = _mm256_and_si256(m2,output_mask);

	//row=1
	//multiply with the mask
	m2=_mm256_maddubs_epi16(r2,c0_sh1);
	m3=_mm256_maddubs_epi16(r3,c1_sh1);
	m4=_mm256_maddubs_epi16(r4,c2_sh1);

	//vertical add
	m0=_mm256_add_epi16(m2,m3);
	m0=_mm256_add_epi16(m0,m4);

	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);

	m1=_mm256_srli_si256(m0,4);
	m2=_mm256_add_epi16(m1,m2);

	//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
	//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
	m4=_mm256_and_si256(m0,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions
	m2=_mm256_add_epi16(m2,m4);

	//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
	output_row1_odd = _mm256_and_si256(m2,output_mask);

             //3rd col iteration

	//row=2
	//multiply with the mask
	m0=_mm256_maddubs_epi16(r0,c0_sh2);
	m1=_mm256_maddubs_epi16(r1,c1_sh2);
	m2=_mm256_maddubs_epi16(r2,c2_sh2);
	m3=_mm256_maddubs_epi16(r3,c3_sh2);
	m4=_mm256_maddubs_epi16(r4,c4_sh2);

	//vertical add
	m0=_mm256_add_epi16(m0,m1);
	m2=_mm256_add_epi16(m2,m3);
	m0=_mm256_add_epi16(m0,m2);
	m0=_mm256_add_epi16(m0,m4);


	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);

	//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,_mm256_extract_epi16(m2,8),0,0,0,0,0,0,0);
	m4=_mm256_and_si256(m2,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions

	m1=_mm256_srli_si256(m0,4);
	m2=_mm256_add_epi16(m1,m2);

	//prepei na prosthesw to pixel pu thelw sto r2 kai meta na kanw diairesi me ola mazi

	m2=_mm256_add_epi16(m2,m4);

	//m2 has 16 16bit values now. the results I need are in positions 1,4,7,10,13. keep only those, discard others
	m2 = _mm256_and_si256(m2,output_mask_sh1);
	output_even = _mm256_add_epi16(output_even,m2);

	//row=0
	//multiply with the mask
	m1=_mm256_maddubs_epi16(r1,c0_sh2);
	m2=_mm256_maddubs_epi16(r2,c1_sh2);
	m3=_mm256_maddubs_epi16(r3,c2_sh2);
	m4=_mm256_maddubs_epi16(r4,c3_sh2);

	//vertical add
	m0=_mm256_add_epi16(m1,m2);
	m0=_mm256_add_epi16(m0,m3);
	m0=_mm256_add_epi16(m0,m4);


	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);

	//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,_mm256_extract_epi16(m2,8),0,0,0,0,0,0,0);
	m4=_mm256_and_si256(m2,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions

	m1=_mm256_srli_si256(m0,4);
	m2=_mm256_add_epi16(m1,m2);

	//prepei na prosthesw to pixel pu thelw sto r2 kai meta na kanw diairesi me ola mazi

	m2=_mm256_add_epi16(m2,m4);

	//m2 has 16 16bit values now. the results I need are in positions 1,4,7,10,13. keep only those, discard others
	m2 = _mm256_and_si256(m2,output_mask_sh1);
	output_row0_even = _mm256_add_epi16(output_row0_even,m2);

	//row=1
	//multiply with the mask
	m2=_mm256_maddubs_epi16(r2,c0_sh2);
	m3=_mm256_maddubs_epi16(r3,c1_sh2);
	m4=_mm256_maddubs_epi16(r4,c2_sh2);

	//vertical add
	m0=_mm256_add_epi16(m2,m3);
	m0=_mm256_add_epi16(m0,m4);


	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);

	//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,_mm256_extract_epi16(m2,8),0,0,0,0,0,0,0);
	m4=_mm256_and_si256(m2,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions

	m1=_mm256_srli_si256(m0,4);
	m2=_mm256_add_epi16(m1,m2);

	//prepei na prosthesw to pixel pu thelw sto r2 kai meta na kanw diairesi me ola mazi

	m2=_mm256_add_epi16(m2,m4);

	//m2 has 16 16bit values now. the results I need are in positions 1,4,7,10,13. keep only those, discard others
	m2 = _mm256_and_si256(m2,output_mask_sh1);
	output_row1_even = _mm256_add_epi16(output_row1_even,m2);

	//4th col iteration
	//load the 5 rows
	r0=_mm256_loadu_si256( (__m256i *) &frame1[N-5][col+3-2]);
	r1=_mm256_loadu_si256( (__m256i *) &frame1[N-4][col+3-2]);
	r2=_mm256_loadu_si256( (__m256i *) &frame1[N-3][col+3-2]);
	r3=_mm256_loadu_si256( (__m256i *) &frame1[N-2][col+3-2]);
	r4=_mm256_loadu_si256( (__m256i *) &frame1[N-1][col+3-2]);

	//AND r0-r4 with reminder_mask
	r0=_mm256_and_si256(r0,reminder_mask2);
	r1=_mm256_and_si256(r1,reminder_mask2);
	r2=_mm256_and_si256(r2,reminder_mask2);
	r3=_mm256_and_si256(r3,reminder_mask2);
	r4=_mm256_and_si256(r4,reminder_mask2);

	//row=2
	//multiply with the mask
	m0=_mm256_maddubs_epi16(r0,c0);
	m1=_mm256_maddubs_epi16(r1,c1);
	m2=_mm256_maddubs_epi16(r2,c2);
	m3=_mm256_maddubs_epi16(r3,c3);
	m4=_mm256_maddubs_epi16(r4,c4);

	//vertical add
	m0=_mm256_add_epi16(m0,m1);
	m2=_mm256_add_epi16(m2,m3);
	m0=_mm256_add_epi16(m0,m2);
	m0=_mm256_add_epi16(m0,m4);


	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);

	m1=_mm256_srli_si256(m0,4);
	m2=_mm256_add_epi16(m1,m2);

	//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
	//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
	m4=_mm256_and_si256(m0,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions

	m2=_mm256_add_epi16(m2,m4);

	//m2 has 16 16bit values now. the results I need are in positions 1,4,7,10,13. keep only those, discard others
	m2 = _mm256_and_si256(m2,output_mask);
	m2 = _mm256_slli_si256(m2,2);
	output_odd = _mm256_add_epi16(output_odd,m2);

	//row=0
	//multiply with the mask
	m1=_mm256_maddubs_epi16(r1,c0);
	m2=_mm256_maddubs_epi16(r2,c1);
	m3=_mm256_maddubs_epi16(r3,c2);
	m4=_mm256_maddubs_epi16(r4,c3);

	//vertical add
	m0=_mm256_add_epi16(m1,m2);
	m0=_mm256_add_epi16(m0,m3);
	m0=_mm256_add_epi16(m0,m4);


	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);

	m1=_mm256_srli_si256(m0,4);
	m2=_mm256_add_epi16(m1,m2);

	//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
	//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
	m4=_mm256_and_si256(m0,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions

	m2=_mm256_add_epi16(m2,m4);

	//m2 has 16 16bit values now. the results I need are in positions 1,4,7,10,13. keep only those, discard others
	m2 = _mm256_and_si256(m2,output_mask);
	m2 = _mm256_slli_si256(m2,2);
	output_row0_odd = _mm256_add_epi16(output_row0_odd,m2);

	//row=1
	//multiply with the mask
	m2=_mm256_maddubs_epi16(r2,c0);
	m3=_mm256_maddubs_epi16(r3,c1);
	m4=_mm256_maddubs_epi16(r4,c2);

	//vertical add
	m0=_mm256_add_epi16(m2,m3);
	m0=_mm256_add_epi16(m0,m4);


	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);

	m1=_mm256_srli_si256(m0,4);
	m2=_mm256_add_epi16(m1,m2);

	//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
	//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
	m4=_mm256_and_si256(m0,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions

	m2=_mm256_add_epi16(m2,m4);

	//m2 has 16 16bit values now. the results I need are in positions 1,4,7,10,13. keep only those, discard others
	m2 = _mm256_and_si256(m2,output_mask);
	m2 = _mm256_slli_si256(m2,2);
	output_row1_odd = _mm256_add_epi16(output_row1_odd,m2);

	//5th col iteration

	//row=2
	//multiply with the mask
	m0=_mm256_maddubs_epi16(r0,c0_sh1);
	m1=_mm256_maddubs_epi16(r1,c1_sh1);
	m2=_mm256_maddubs_epi16(r2,c2_sh1);
	m3=_mm256_maddubs_epi16(r3,c3_sh1);
	m4=_mm256_maddubs_epi16(r4,c4_sh1);

	//vertical add
	m0=_mm256_add_epi16(m0,m1);
	m2=_mm256_add_epi16(m2,m3);
	m0=_mm256_add_epi16(m0,m2);
	m0=_mm256_add_epi16(m0,m4);


	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);

	m1=_mm256_srli_si256(m0,4);
	m2=_mm256_add_epi16(m1,m2);

	//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
	//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
	m4=_mm256_and_si256(m0,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions

	m2=_mm256_add_epi16(m2,m4);

	//m2 has 16 16bit values now.
	m2 = _mm256_and_si256(m2,output_mask); //holds results in positions 0,3,6,9,12
	m1 = _mm256_slli_si256(m2,4); //put results in positions 2,5,8,11,14 , but 8 is not written
	m2=_mm256_and_si256(m2,mask_new);//extract only the 6th
	m2=_mm256_permute2f128_si256(m2,m2,1);
	m2=_mm256_srli_si256(m2,12);
	m2 = _mm256_add_epi16(m1,m2);
	output_even = _mm256_add_epi16(output_even,m2);

	//row=0
	//multiply with the mask
	m1=_mm256_maddubs_epi16(r1,c0_sh1);
	m2=_mm256_maddubs_epi16(r2,c1_sh1);
	m3=_mm256_maddubs_epi16(r3,c2_sh1);
	m4=_mm256_maddubs_epi16(r4,c3_sh1);

	//vertical add
	m0=_mm256_add_epi16(m1,m2);
	m0=_mm256_add_epi16(m0,m3);
	m0=_mm256_add_epi16(m0,m4);



	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);

	m1=_mm256_srli_si256(m0,4);
	m2=_mm256_add_epi16(m1,m2);

	//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
	//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
	m4=_mm256_and_si256(m0,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions

	m2=_mm256_add_epi16(m2,m4);

	//m2 has 16 16bit values now.
	m2 = _mm256_and_si256(m2,output_mask); //holds results in positions 0,3,6,9,12
	m1 = _mm256_slli_si256(m2,4); //put results in positions 2,5,8,11,14 , but 8 is not written
	m2=_mm256_and_si256(m2,mask_new);//extract only the 6th
	m2=_mm256_permute2f128_si256(m2,m2,1);
	m2=_mm256_srli_si256(m2,12);
	m2 = _mm256_add_epi16(m1,m2);
	output_row0_even = _mm256_add_epi16(output_row0_even,m2);

	//row=1
	//multiply with the mask
	m2=_mm256_maddubs_epi16(r2,c0_sh1);
	m3=_mm256_maddubs_epi16(r3,c1_sh1);
	m4=_mm256_maddubs_epi16(r4,c2_sh1);

	//vertical add
	m0=_mm256_add_epi16(m2,m3);
	m0=_mm256_add_epi16(m0,m4);


	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);

	m1=_mm256_srli_si256(m0,4);
	m2=_mm256_add_epi16(m1,m2);

	//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
	//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
	m4=_mm256_and_si256(m0,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions

	m2=_mm256_add_epi16(m2,m4);

	//m2 has 16 16bit values now.
	m2 = _mm256_and_si256(m2,output_mask); //holds results in positions 0,3,6,9,12
	m1 = _mm256_slli_si256(m2,4); //put results in positions 2,5,8,11,14 , but 8 is not written
	m2=_mm256_and_si256(m2,mask_new);//extract only the 6th
	m2=_mm256_permute2f128_si256(m2,m2,1);
	m2=_mm256_srli_si256(m2,12);
	m2 = _mm256_add_epi16(m1,m2);
	output_row1_even = _mm256_add_epi16(output_row1_even,m2);

             //6th col iteration
	//row=2
	//multiply with the mask
	m0=_mm256_maddubs_epi16(r0,c0_sh2);
	m1=_mm256_maddubs_epi16(r1,c1_sh2);
	m2=_mm256_maddubs_epi16(r2,c2_sh2);
	m3=_mm256_maddubs_epi16(r3,c3_sh2);
	m4=_mm256_maddubs_epi16(r4,c4_sh2);

	//vertical add
	m0=_mm256_add_epi16(m0,m1);
	m2=_mm256_add_epi16(m2,m3);
	m0=_mm256_add_epi16(m0,m2);
	m0=_mm256_add_epi16(m0,m4);


	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);
	//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,_mm256_extract_epi16(m2,8),0,0,0,0,0,0,0);
	m4=_mm256_and_si256(m2,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions

	m1=_mm256_srli_si256(m0,4);
	m2=_mm256_add_epi16(m1,m2);
	m2=_mm256_add_epi16(m2,m4);

	//m2 has 16 16bit values now.
	m2 = _mm256_and_si256(m2,output_mask_sh1);//holds results in positions 1,4,7,10,13
	m1 = _mm256_slli_si256(m2,2); //put results in positions 2,5,8,11,14 , but 8 is not written
	m2=_mm256_and_si256(m2,mask_new2);//extract only the 7th
	m2=_mm256_permute2f128_si256(m2,m2,1);
	m2=_mm256_srli_si256(m2,14);
	m2 = _mm256_add_epi16(m1,m2);
	output_odd = _mm256_add_epi16(output_odd,m2);


	//now division follows
	output_even=division(division_case,output_even,f);
	output_odd=division(division_case,output_odd,f);

	//shift odd 1 position and add to even
	output_odd = _mm256_slli_si256(output_odd,1);
	output_even = _mm256_add_epi8(output_even,output_odd);

	//row=0
	//multiply with the mask
	m1=_mm256_maddubs_epi16(r1,c0_sh2);
	m2=_mm256_maddubs_epi16(r2,c1_sh2);
	m3=_mm256_maddubs_epi16(r3,c2_sh2);
	m4=_mm256_maddubs_epi16(r4,c3_sh2);

	//vertical add
	m0=_mm256_add_epi16(m1,m2);
	m0=_mm256_add_epi16(m0,m3);
	m0=_mm256_add_epi16(m0,m4);


	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);
	//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,_mm256_extract_epi16(m2,8),0,0,0,0,0,0,0);
	m4=_mm256_and_si256(m2,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions

	m1=_mm256_srli_si256(m0,4);
	m2=_mm256_add_epi16(m1,m2);
	m2=_mm256_add_epi16(m2,m4);

	//m2 has 16 16bit values now.
	m2 = _mm256_and_si256(m2,output_mask_sh1);//holds results in positions 1,4,7,10,13
	m1 = _mm256_slli_si256(m2,2); //put results in positions 2,5,8,11,14 , but 8 is not written
	m2=_mm256_and_si256(m2,mask_new2);//extract only the 7th
	m2=_mm256_permute2f128_si256(m2,m2,1);
	m2=_mm256_srli_si256(m2,14);
	m2 = _mm256_add_epi16(m1,m2);
	output_row0_odd = _mm256_add_epi16(output_row0_odd,m2);


	//now division follows
	output_row0_even=division(division_case,output_row0_even,f);
	output_row0_odd=division(division_case,output_row0_odd,f);

	//shift odd 1 position and add to even
	output_row0_odd = _mm256_slli_si256(output_row0_odd,1);
	output_row0_even = _mm256_add_epi8(output_row0_even,output_row0_odd);

	//row=1
	//multiply with the mask
	m2=_mm256_maddubs_epi16(r2,c0_sh2);
	m3=_mm256_maddubs_epi16(r3,c1_sh2);
	m4=_mm256_maddubs_epi16(r4,c2_sh2);

	//vertical add
	m0=_mm256_add_epi16(m2,m3);
	m0=_mm256_add_epi16(m0,m4);


	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);
	//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,_mm256_extract_epi16(m2,8),0,0,0,0,0,0,0);
	m4=_mm256_and_si256(m2,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions

	m1=_mm256_srli_si256(m0,4);
	m2=_mm256_add_epi16(m1,m2);
	m2=_mm256_add_epi16(m2,m4);

	//m2 has 16 16bit values now.
	m2 = _mm256_and_si256(m2,output_mask_sh1);//holds results in positions 1,4,7,10,13
	m1 = _mm256_slli_si256(m2,2); //put results in positions 2,5,8,11,14 , but 8 is not written
	m2=_mm256_and_si256(m2,mask_new2);//extract only the 7th
	m2=_mm256_permute2f128_si256(m2,m2,1);
	m2=_mm256_srli_si256(m2,14);
	m2 = _mm256_add_epi16(m1,m2);
	output_row1_odd = _mm256_add_epi16(output_row1_odd,m2);


	//now division follows
	output_row1_even=division(division_case,output_row1_even,f);
	output_row1_odd=division(division_case,output_row1_odd,f);

	//shift odd 1 position and add to even
	output_row1_odd = _mm256_slli_si256(output_row1_odd,1);
	output_row1_even = _mm256_add_epi8(output_row1_even,output_row1_odd);




	switch (REMINDER_ITERATIONS){
	case 1:
		filt[N-3][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[N-2][col] = (unsigned char) _mm256_extract_epi8(output_row0_even,0);
		filt[N-1][col] = (unsigned char) _mm256_extract_epi8(output_row1_even,0);
		 break;
	case 2:
		filt[N-3][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[N-3][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);

		filt[N-2][col] = (unsigned char) _mm256_extract_epi8(output_row0_even,0);
		filt[N-2][col+1] = (unsigned char) _mm256_extract_epi8(output_row0_even,1);

		filt[N-1][col] = (unsigned char) _mm256_extract_epi8(output_row1_even,0);
		filt[N-1][col+1] = (unsigned char) _mm256_extract_epi8(output_row1_even,1);
		 break;
	case 3:
		filt[N-3][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[N-3][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[N-3][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);

		filt[N-2][col] = (unsigned char) _mm256_extract_epi8(output_row0_even,0);
		filt[N-2][col+1] = (unsigned char) _mm256_extract_epi8(output_row0_even,1);
		filt[N-2][col+2] = (unsigned char) _mm256_extract_epi8(output_row0_even,2);

		filt[N-1][col] = (unsigned char) _mm256_extract_epi8(output_row1_even,0);
		filt[N-1][col+1] = (unsigned char) _mm256_extract_epi8(output_row1_even,1);
		filt[N-1][col+2] = (unsigned char) _mm256_extract_epi8(output_row1_even,2);
		  break;
	case 4:
		filt[N-3][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[N-3][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[N-3][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		filt[N-3][col+3] = (unsigned char) _mm256_extract_epi8(output_even,3);

		filt[N-2][col] = (unsigned char) _mm256_extract_epi8(output_row0_even,0);
		filt[N-2][col+1] = (unsigned char) _mm256_extract_epi8(output_row0_even,1);
		filt[N-2][col+2] = (unsigned char) _mm256_extract_epi8(output_row0_even,2);
		filt[N-2][col+3] = (unsigned char) _mm256_extract_epi8(output_row0_even,3);

		filt[N-1][col] = (unsigned char) _mm256_extract_epi8(output_row1_even,0);
		filt[N-1][col+1] = (unsigned char) _mm256_extract_epi8(output_row1_even,1);
		filt[N-1][col+2] = (unsigned char) _mm256_extract_epi8(output_row1_even,2);
		filt[N-1][col+3] = (unsigned char) _mm256_extract_epi8(output_row1_even,3);
		  break;
	case 5:
		filt[N-3][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[N-3][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[N-3][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		filt[N-3][col+3] = (unsigned char) _mm256_extract_epi8(output_even,3);
		filt[N-3][col+4] = (unsigned char) _mm256_extract_epi8(output_even,4);

		filt[N-2][col] = (unsigned char) _mm256_extract_epi8(output_row0_even,0);
		filt[N-2][col+1] = (unsigned char) _mm256_extract_epi8(output_row0_even,1);
		filt[N-2][col+2] = (unsigned char) _mm256_extract_epi8(output_row0_even,2);
		filt[N-2][col+3] = (unsigned char) _mm256_extract_epi8(output_row0_even,3);
		filt[N-2][col+4] = (unsigned char) _mm256_extract_epi8(output_row0_even,4);

		filt[N-1][col] = (unsigned char) _mm256_extract_epi8(output_row1_even,0);
		filt[N-1][col+1] = (unsigned char) _mm256_extract_epi8(output_row1_even,1);
		filt[N-1][col+2] = (unsigned char) _mm256_extract_epi8(output_row1_even,2);
		filt[N-1][col+3] = (unsigned char) _mm256_extract_epi8(output_row1_even,3);
		filt[N-1][col+4] = (unsigned char) _mm256_extract_epi8(output_row1_even,4);
		  break;
	case 6:
		filt[N-3][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[N-3][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[N-3][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		filt[N-3][col+3] = (unsigned char) _mm256_extract_epi8(output_even,3);
		filt[N-3][col+4] = (unsigned char) _mm256_extract_epi8(output_even,4);
		filt[N-3][col+5] = (unsigned char) _mm256_extract_epi8(output_even,5);

		filt[N-2][col] = (unsigned char) _mm256_extract_epi8(output_row0_even,0);
		filt[N-2][col+1] = (unsigned char) _mm256_extract_epi8(output_row0_even,1);
		filt[N-2][col+2] = (unsigned char) _mm256_extract_epi8(output_row0_even,2);
		filt[N-2][col+3] = (unsigned char) _mm256_extract_epi8(output_row0_even,3);
		filt[N-2][col+4] = (unsigned char) _mm256_extract_epi8(output_row0_even,4);
		filt[N-2][col+5] = (unsigned char) _mm256_extract_epi8(output_row0_even,5);

		filt[N-1][col] = (unsigned char) _mm256_extract_epi8(output_row1_even,0);
		filt[N-1][col+1] = (unsigned char) _mm256_extract_epi8(output_row1_even,1);
		filt[N-1][col+2] = (unsigned char) _mm256_extract_epi8(output_row1_even,2);
		filt[N-1][col+3] = (unsigned char) _mm256_extract_epi8(output_row1_even,3);
		filt[N-1][col+4] = (unsigned char) _mm256_extract_epi8(output_row1_even,4);
		filt[N-1][col+5] = (unsigned char) _mm256_extract_epi8(output_row1_even,5);
		  break;
	case 7:
		filt[N-3][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[N-3][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[N-3][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		filt[N-3][col+3] = (unsigned char) _mm256_extract_epi8(output_even,3);
		filt[N-3][col+4] = (unsigned char) _mm256_extract_epi8(output_even,4);
		filt[N-3][col+5] = (unsigned char) _mm256_extract_epi8(output_even,5);
		filt[N-3][col+6] = (unsigned char) _mm256_extract_epi8(output_even,6);

		filt[N-2][col] = (unsigned char) _mm256_extract_epi8(output_row0_even,0);
		filt[N-2][col+1] = (unsigned char) _mm256_extract_epi8(output_row0_even,1);
		filt[N-2][col+2] = (unsigned char) _mm256_extract_epi8(output_row0_even,2);
		filt[N-2][col+3] = (unsigned char) _mm256_extract_epi8(output_row0_even,3);
		filt[N-2][col+4] = (unsigned char) _mm256_extract_epi8(output_row0_even,4);
		filt[N-2][col+5] = (unsigned char) _mm256_extract_epi8(output_row0_even,5);
		filt[N-2][col+6] = (unsigned char) _mm256_extract_epi8(output_row0_even,6);

		filt[N-1][col] = (unsigned char) _mm256_extract_epi8(output_row1_even,0);
		filt[N-1][col+1] = (unsigned char) _mm256_extract_epi8(output_row1_even,1);
		filt[N-1][col+2] = (unsigned char) _mm256_extract_epi8(output_row1_even,2);
		filt[N-1][col+3] = (unsigned char) _mm256_extract_epi8(output_row1_even,3);
		filt[N-1][col+4] = (unsigned char) _mm256_extract_epi8(output_row1_even,4);
		filt[N-1][col+5] = (unsigned char) _mm256_extract_epi8(output_row1_even,5);
		filt[N-1][col+6] = (unsigned char) _mm256_extract_epi8(output_row1_even,6);
		  break;
	case 8:
		filt[N-3][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[N-3][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[N-3][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		filt[N-3][col+3] = (unsigned char) _mm256_extract_epi8(output_even,3);
		filt[N-3][col+4] = (unsigned char) _mm256_extract_epi8(output_even,4);
		filt[N-3][col+5] = (unsigned char) _mm256_extract_epi8(output_even,5);
		filt[N-3][col+6] = (unsigned char) _mm256_extract_epi8(output_even,6);
		filt[N-3][col+7] = (unsigned char) _mm256_extract_epi8(output_even,7);

		filt[N-2][col] = (unsigned char) _mm256_extract_epi8(output_row0_even,0);
		filt[N-2][col+1] = (unsigned char) _mm256_extract_epi8(output_row0_even,1);
		filt[N-2][col+2] = (unsigned char) _mm256_extract_epi8(output_row0_even,2);
		filt[N-2][col+3] = (unsigned char) _mm256_extract_epi8(output_row0_even,3);
		filt[N-2][col+4] = (unsigned char) _mm256_extract_epi8(output_row0_even,4);
		filt[N-2][col+5] = (unsigned char) _mm256_extract_epi8(output_row0_even,5);
		filt[N-2][col+6] = (unsigned char) _mm256_extract_epi8(output_row0_even,6);
		filt[N-2][col+7] = (unsigned char) _mm256_extract_epi8(output_row0_even,7);

		filt[N-1][col] = (unsigned char) _mm256_extract_epi8(output_row1_even,0);
		filt[N-1][col+1] = (unsigned char) _mm256_extract_epi8(output_row1_even,1);
		filt[N-1][col+2] = (unsigned char) _mm256_extract_epi8(output_row1_even,2);
		filt[N-1][col+3] = (unsigned char) _mm256_extract_epi8(output_row1_even,3);
		filt[N-1][col+4] = (unsigned char) _mm256_extract_epi8(output_row1_even,4);
		filt[N-1][col+5] = (unsigned char) _mm256_extract_epi8(output_row1_even,5);
		filt[N-1][col+6] = (unsigned char) _mm256_extract_epi8(output_row1_even,6);
		filt[N-1][col+7] = (unsigned char) _mm256_extract_epi8(output_row1_even,7);
		  break;
	case 9:
		filt[N-3][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[N-3][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[N-3][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		filt[N-3][col+3] = (unsigned char) _mm256_extract_epi8(output_even,3);
		filt[N-3][col+4] = (unsigned char) _mm256_extract_epi8(output_even,4);
		filt[N-3][col+5] = (unsigned char) _mm256_extract_epi8(output_even,5);
		filt[N-3][col+6] = (unsigned char) _mm256_extract_epi8(output_even,6);
		filt[N-3][col+7] = (unsigned char) _mm256_extract_epi8(output_even,7);
		filt[N-3][col+8] = (unsigned char) _mm256_extract_epi8(output_even,8);

		filt[N-2][col] = (unsigned char) _mm256_extract_epi8(output_row0_even,0);
		filt[N-2][col+1] = (unsigned char) _mm256_extract_epi8(output_row0_even,1);
		filt[N-2][col+2] = (unsigned char) _mm256_extract_epi8(output_row0_even,2);
		filt[N-2][col+3] = (unsigned char) _mm256_extract_epi8(output_row0_even,3);
		filt[N-2][col+4] = (unsigned char) _mm256_extract_epi8(output_row0_even,4);
		filt[N-2][col+5] = (unsigned char) _mm256_extract_epi8(output_row0_even,5);
		filt[N-2][col+6] = (unsigned char) _mm256_extract_epi8(output_row0_even,6);
		filt[N-2][col+7] = (unsigned char) _mm256_extract_epi8(output_row0_even,7);
		filt[N-2][col+8] = (unsigned char) _mm256_extract_epi8(output_row0_even,8);

		filt[N-1][col] = (unsigned char) _mm256_extract_epi8(output_row1_even,0);
		filt[N-1][col+1] = (unsigned char) _mm256_extract_epi8(output_row1_even,1);
		filt[N-1][col+2] = (unsigned char) _mm256_extract_epi8(output_row1_even,2);
		filt[N-1][col+3] = (unsigned char) _mm256_extract_epi8(output_row1_even,3);
		filt[N-1][col+4] = (unsigned char) _mm256_extract_epi8(output_row1_even,4);
		filt[N-1][col+5] = (unsigned char) _mm256_extract_epi8(output_row1_even,5);
		filt[N-1][col+6] = (unsigned char) _mm256_extract_epi8(output_row1_even,6);
		filt[N-1][col+7] = (unsigned char) _mm256_extract_epi8(output_row1_even,7);
		filt[N-1][col+8] = (unsigned char) _mm256_extract_epi8(output_row1_even,8);
		  break;
	case 10:
		filt[N-3][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[N-3][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[N-3][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		filt[N-3][col+3] = (unsigned char) _mm256_extract_epi8(output_even,3);
		filt[N-3][col+4] = (unsigned char) _mm256_extract_epi8(output_even,4);
		filt[N-3][col+5] = (unsigned char) _mm256_extract_epi8(output_even,5);
		filt[N-3][col+6] = (unsigned char) _mm256_extract_epi8(output_even,6);
		filt[N-3][col+7] = (unsigned char) _mm256_extract_epi8(output_even,7);
		filt[N-3][col+8] = (unsigned char) _mm256_extract_epi8(output_even,8);
		filt[N-3][col+9] = (unsigned char) _mm256_extract_epi8(output_even,9);

		filt[N-2][col] = (unsigned char) _mm256_extract_epi8(output_row0_even,0);
		filt[N-2][col+1] = (unsigned char) _mm256_extract_epi8(output_row0_even,1);
		filt[N-2][col+2] = (unsigned char) _mm256_extract_epi8(output_row0_even,2);
		filt[N-2][col+3] = (unsigned char) _mm256_extract_epi8(output_row0_even,3);
		filt[N-2][col+4] = (unsigned char) _mm256_extract_epi8(output_row0_even,4);
		filt[N-2][col+5] = (unsigned char) _mm256_extract_epi8(output_row0_even,5);
		filt[N-2][col+6] = (unsigned char) _mm256_extract_epi8(output_row0_even,6);
		filt[N-2][col+7] = (unsigned char) _mm256_extract_epi8(output_row0_even,7);
		filt[N-2][col+8] = (unsigned char) _mm256_extract_epi8(output_row0_even,8);
		filt[N-2][col+9] = (unsigned char) _mm256_extract_epi8(output_row0_even,9);

		filt[N-1][col] = (unsigned char) _mm256_extract_epi8(output_row1_even,0);
		filt[N-1][col+1] = (unsigned char) _mm256_extract_epi8(output_row1_even,1);
		filt[N-1][col+2] = (unsigned char) _mm256_extract_epi8(output_row1_even,2);
		filt[N-1][col+3] = (unsigned char) _mm256_extract_epi8(output_row1_even,3);
		filt[N-1][col+4] = (unsigned char) _mm256_extract_epi8(output_row1_even,4);
		filt[N-1][col+5] = (unsigned char) _mm256_extract_epi8(output_row1_even,5);
		filt[N-1][col+6] = (unsigned char) _mm256_extract_epi8(output_row1_even,6);
		filt[N-1][col+7] = (unsigned char) _mm256_extract_epi8(output_row1_even,7);
		filt[N-1][col+8] = (unsigned char) _mm256_extract_epi8(output_row1_even,8);
		filt[N-1][col+9] = (unsigned char) _mm256_extract_epi8(output_row1_even,9);
		  break;
	case 11:
		filt[N-3][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[N-3][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[N-3][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		filt[N-3][col+3] = (unsigned char) _mm256_extract_epi8(output_even,3);
		filt[N-3][col+4] = (unsigned char) _mm256_extract_epi8(output_even,4);
		filt[N-3][col+5] = (unsigned char) _mm256_extract_epi8(output_even,5);
		filt[N-3][col+6] = (unsigned char) _mm256_extract_epi8(output_even,6);
		filt[N-3][col+7] = (unsigned char) _mm256_extract_epi8(output_even,7);
		filt[N-3][col+8] = (unsigned char) _mm256_extract_epi8(output_even,8);
		filt[N-3][col+9] = (unsigned char) _mm256_extract_epi8(output_even,9);
		filt[N-3][col+10] = (unsigned char) _mm256_extract_epi8(output_even,10);

		filt[N-2][col] = (unsigned char) _mm256_extract_epi8(output_row0_even,0);
		filt[N-2][col+1] = (unsigned char) _mm256_extract_epi8(output_row0_even,1);
		filt[N-2][col+2] = (unsigned char) _mm256_extract_epi8(output_row0_even,2);
		filt[N-2][col+3] = (unsigned char) _mm256_extract_epi8(output_row0_even,3);
		filt[N-2][col+4] = (unsigned char) _mm256_extract_epi8(output_row0_even,4);
		filt[N-2][col+5] = (unsigned char) _mm256_extract_epi8(output_row0_even,5);
		filt[N-2][col+6] = (unsigned char) _mm256_extract_epi8(output_row0_even,6);
		filt[N-2][col+7] = (unsigned char) _mm256_extract_epi8(output_row0_even,7);
		filt[N-2][col+8] = (unsigned char) _mm256_extract_epi8(output_row0_even,8);
		filt[N-2][col+9] = (unsigned char) _mm256_extract_epi8(output_row0_even,9);
		filt[N-2][col+10] = (unsigned char) _mm256_extract_epi8(output_row0_even,10);

		filt[N-1][col] = (unsigned char) _mm256_extract_epi8(output_row1_even,0);
		filt[N-1][col+1] = (unsigned char) _mm256_extract_epi8(output_row1_even,1);
		filt[N-1][col+2] = (unsigned char) _mm256_extract_epi8(output_row1_even,2);
		filt[N-1][col+3] = (unsigned char) _mm256_extract_epi8(output_row1_even,3);
		filt[N-1][col+4] = (unsigned char) _mm256_extract_epi8(output_row1_even,4);
		filt[N-1][col+5] = (unsigned char) _mm256_extract_epi8(output_row1_even,5);
		filt[N-1][col+6] = (unsigned char) _mm256_extract_epi8(output_row1_even,6);
		filt[N-1][col+7] = (unsigned char) _mm256_extract_epi8(output_row1_even,7);
		filt[N-1][col+8] = (unsigned char) _mm256_extract_epi8(output_row1_even,8);
		filt[N-1][col+9] = (unsigned char) _mm256_extract_epi8(output_row1_even,9);
		filt[N-1][col+10] = (unsigned char) _mm256_extract_epi8(output_row1_even,10);
		  break;
	case 12:
		filt[N-3][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[N-3][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[N-3][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		filt[N-3][col+3] = (unsigned char) _mm256_extract_epi8(output_even,3);
		filt[N-3][col+4] = (unsigned char) _mm256_extract_epi8(output_even,4);
		filt[N-3][col+5] = (unsigned char) _mm256_extract_epi8(output_even,5);
		filt[N-3][col+6] = (unsigned char) _mm256_extract_epi8(output_even,6);
		filt[N-3][col+7] = (unsigned char) _mm256_extract_epi8(output_even,7);
		filt[N-3][col+8] = (unsigned char) _mm256_extract_epi8(output_even,8);
		filt[N-3][col+9] = (unsigned char) _mm256_extract_epi8(output_even,9);
		filt[N-3][col+10] = (unsigned char) _mm256_extract_epi8(output_even,10);
		filt[N-3][col+11] = (unsigned char) _mm256_extract_epi8(output_even,11);

		filt[N-2][col] = (unsigned char) _mm256_extract_epi8(output_row0_even,0);
		filt[N-2][col+1] = (unsigned char) _mm256_extract_epi8(output_row0_even,1);
		filt[N-2][col+2] = (unsigned char) _mm256_extract_epi8(output_row0_even,2);
		filt[N-2][col+3] = (unsigned char) _mm256_extract_epi8(output_row0_even,3);
		filt[N-2][col+4] = (unsigned char) _mm256_extract_epi8(output_row0_even,4);
		filt[N-2][col+5] = (unsigned char) _mm256_extract_epi8(output_row0_even,5);
		filt[N-2][col+6] = (unsigned char) _mm256_extract_epi8(output_row0_even,6);
		filt[N-2][col+7] = (unsigned char) _mm256_extract_epi8(output_row0_even,7);
		filt[N-2][col+8] = (unsigned char) _mm256_extract_epi8(output_row0_even,8);
		filt[N-2][col+9] = (unsigned char) _mm256_extract_epi8(output_row0_even,9);
		filt[N-2][col+10] = (unsigned char) _mm256_extract_epi8(output_row0_even,10);
		filt[N-2][col+11] = (unsigned char) _mm256_extract_epi8(output_row0_even,11);

		filt[N-1][col] = (unsigned char) _mm256_extract_epi8(output_row1_even,0);
		filt[N-1][col+1] = (unsigned char) _mm256_extract_epi8(output_row1_even,1);
		filt[N-1][col+2] = (unsigned char) _mm256_extract_epi8(output_row1_even,2);
		filt[N-1][col+3] = (unsigned char) _mm256_extract_epi8(output_row1_even,3);
		filt[N-1][col+4] = (unsigned char) _mm256_extract_epi8(output_row1_even,4);
		filt[N-1][col+5] = (unsigned char) _mm256_extract_epi8(output_row1_even,5);
		filt[N-1][col+6] = (unsigned char) _mm256_extract_epi8(output_row1_even,6);
		filt[N-1][col+7] = (unsigned char) _mm256_extract_epi8(output_row1_even,7);
		filt[N-1][col+8] = (unsigned char) _mm256_extract_epi8(output_row1_even,8);
		filt[N-1][col+9] = (unsigned char) _mm256_extract_epi8(output_row1_even,9);
		filt[N-1][col+10] = (unsigned char) _mm256_extract_epi8(output_row1_even,10);
		filt[N-1][col+11] = (unsigned char) _mm256_extract_epi8(output_row1_even,11);
		  break;
	case 13:
		filt[N-3][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[N-3][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[N-3][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		filt[N-3][col+3] = (unsigned char) _mm256_extract_epi8(output_even,3);
		filt[N-3][col+4] = (unsigned char) _mm256_extract_epi8(output_even,4);
		filt[N-3][col+5] = (unsigned char) _mm256_extract_epi8(output_even,5);
		filt[N-3][col+6] = (unsigned char) _mm256_extract_epi8(output_even,6);
		filt[N-3][col+7] = (unsigned char) _mm256_extract_epi8(output_even,7);
		filt[N-3][col+8] = (unsigned char) _mm256_extract_epi8(output_even,8);
		filt[N-3][col+9] = (unsigned char) _mm256_extract_epi8(output_even,9);
		filt[N-3][col+10] = (unsigned char) _mm256_extract_epi8(output_even,10);
		filt[N-3][col+11] = (unsigned char) _mm256_extract_epi8(output_even,11);
		filt[N-3][col+12] = (unsigned char) _mm256_extract_epi8(output_even,12);

		filt[N-2][col] = (unsigned char) _mm256_extract_epi8(output_row0_even,0);
		filt[N-2][col+1] = (unsigned char) _mm256_extract_epi8(output_row0_even,1);
		filt[N-2][col+2] = (unsigned char) _mm256_extract_epi8(output_row0_even,2);
		filt[N-2][col+3] = (unsigned char) _mm256_extract_epi8(output_row0_even,3);
		filt[N-2][col+4] = (unsigned char) _mm256_extract_epi8(output_row0_even,4);
		filt[N-2][col+5] = (unsigned char) _mm256_extract_epi8(output_row0_even,5);
		filt[N-2][col+6] = (unsigned char) _mm256_extract_epi8(output_row0_even,6);
		filt[N-2][col+7] = (unsigned char) _mm256_extract_epi8(output_row0_even,7);
		filt[N-2][col+8] = (unsigned char) _mm256_extract_epi8(output_row0_even,8);
		filt[N-2][col+9] = (unsigned char) _mm256_extract_epi8(output_row0_even,9);
		filt[N-2][col+10] = (unsigned char) _mm256_extract_epi8(output_row0_even,10);
		filt[N-2][col+11] = (unsigned char) _mm256_extract_epi8(output_row0_even,11);
		filt[N-2][col+12] = (unsigned char) _mm256_extract_epi8(output_row0_even,12);

		filt[N-1][col] = (unsigned char) _mm256_extract_epi8(output_row1_even,0);
		filt[N-1][col+1] = (unsigned char) _mm256_extract_epi8(output_row1_even,1);
		filt[N-1][col+2] = (unsigned char) _mm256_extract_epi8(output_row1_even,2);
		filt[N-1][col+3] = (unsigned char) _mm256_extract_epi8(output_row1_even,3);
		filt[N-1][col+4] = (unsigned char) _mm256_extract_epi8(output_row1_even,4);
		filt[N-1][col+5] = (unsigned char) _mm256_extract_epi8(output_row1_even,5);
		filt[N-1][col+6] = (unsigned char) _mm256_extract_epi8(output_row1_even,6);
		filt[N-1][col+7] = (unsigned char) _mm256_extract_epi8(output_row1_even,7);
		filt[N-1][col+8] = (unsigned char) _mm256_extract_epi8(output_row1_even,8);
		filt[N-1][col+9] = (unsigned char) _mm256_extract_epi8(output_row1_even,9);
		filt[N-1][col+10] = (unsigned char) _mm256_extract_epi8(output_row1_even,10);
		filt[N-1][col+11] = (unsigned char) _mm256_extract_epi8(output_row1_even,11);
		filt[N-1][col+12] = (unsigned char) _mm256_extract_epi8(output_row1_even,12);
		  break;
	case 14:
		filt[N-3][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[N-3][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[N-3][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		filt[N-3][col+3] = (unsigned char) _mm256_extract_epi8(output_even,3);
		filt[N-3][col+4] = (unsigned char) _mm256_extract_epi8(output_even,4);
		filt[N-3][col+5] = (unsigned char) _mm256_extract_epi8(output_even,5);
		filt[N-3][col+6] = (unsigned char) _mm256_extract_epi8(output_even,6);
		filt[N-3][col+7] = (unsigned char) _mm256_extract_epi8(output_even,7);
		filt[N-3][col+8] = (unsigned char) _mm256_extract_epi8(output_even,8);
		filt[N-3][col+9] = (unsigned char) _mm256_extract_epi8(output_even,9);
		filt[N-3][col+10] = (unsigned char) _mm256_extract_epi8(output_even,10);
		filt[N-3][col+11] = (unsigned char) _mm256_extract_epi8(output_even,11);
		filt[N-3][col+12] = (unsigned char) _mm256_extract_epi8(output_even,12);
		filt[N-3][col+13] = (unsigned char) _mm256_extract_epi8(output_even,13);

		filt[N-2][col] = (unsigned char) _mm256_extract_epi8(output_row0_even,0);
		filt[N-2][col+1] = (unsigned char) _mm256_extract_epi8(output_row0_even,1);
		filt[N-2][col+2] = (unsigned char) _mm256_extract_epi8(output_row0_even,2);
		filt[N-2][col+3] = (unsigned char) _mm256_extract_epi8(output_row0_even,3);
		filt[N-2][col+4] = (unsigned char) _mm256_extract_epi8(output_row0_even,4);
		filt[N-2][col+5] = (unsigned char) _mm256_extract_epi8(output_row0_even,5);
		filt[N-2][col+6] = (unsigned char) _mm256_extract_epi8(output_row0_even,6);
		filt[N-2][col+7] = (unsigned char) _mm256_extract_epi8(output_row0_even,7);
		filt[N-2][col+8] = (unsigned char) _mm256_extract_epi8(output_row0_even,8);
		filt[N-2][col+9] = (unsigned char) _mm256_extract_epi8(output_row0_even,9);
		filt[N-2][col+10] = (unsigned char) _mm256_extract_epi8(output_row0_even,10);
		filt[N-2][col+11] = (unsigned char) _mm256_extract_epi8(output_row0_even,11);
		filt[N-2][col+12] = (unsigned char) _mm256_extract_epi8(output_row0_even,12);
		filt[N-2][col+13] = (unsigned char) _mm256_extract_epi8(output_row0_even,13);

		filt[N-1][col] = (unsigned char) _mm256_extract_epi8(output_row1_even,0);
		filt[N-1][col+1] = (unsigned char) _mm256_extract_epi8(output_row1_even,1);
		filt[N-1][col+2] = (unsigned char) _mm256_extract_epi8(output_row1_even,2);
		filt[N-1][col+3] = (unsigned char) _mm256_extract_epi8(output_row1_even,3);
		filt[N-1][col+4] = (unsigned char) _mm256_extract_epi8(output_row1_even,4);
		filt[N-1][col+5] = (unsigned char) _mm256_extract_epi8(output_row1_even,5);
		filt[N-1][col+6] = (unsigned char) _mm256_extract_epi8(output_row1_even,6);
		filt[N-1][col+7] = (unsigned char) _mm256_extract_epi8(output_row1_even,7);
		filt[N-1][col+8] = (unsigned char) _mm256_extract_epi8(output_row1_even,8);
		filt[N-1][col+9] = (unsigned char) _mm256_extract_epi8(output_row1_even,9);
		filt[N-1][col+10] = (unsigned char) _mm256_extract_epi8(output_row1_even,10);
		filt[N-1][col+11] = (unsigned char) _mm256_extract_epi8(output_row1_even,11);
		filt[N-1][col+12] = (unsigned char) _mm256_extract_epi8(output_row1_even,12);
		filt[N-1][col+13] = (unsigned char) _mm256_extract_epi8(output_row1_even,13);
		  break;
	case 15:
		filt[N-3][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[N-3][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[N-3][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		filt[N-3][col+3] = (unsigned char) _mm256_extract_epi8(output_even,3);
		filt[N-3][col+4] = (unsigned char) _mm256_extract_epi8(output_even,4);
		filt[N-3][col+5] = (unsigned char) _mm256_extract_epi8(output_even,5);
		filt[N-3][col+6] = (unsigned char) _mm256_extract_epi8(output_even,6);
		filt[N-3][col+7] = (unsigned char) _mm256_extract_epi8(output_even,7);
		filt[N-3][col+8] = (unsigned char) _mm256_extract_epi8(output_even,8);
		filt[N-3][col+9] = (unsigned char) _mm256_extract_epi8(output_even,9);
		filt[N-3][col+10] = (unsigned char) _mm256_extract_epi8(output_even,10);
		filt[N-3][col+11] = (unsigned char) _mm256_extract_epi8(output_even,11);
		filt[N-3][col+12] = (unsigned char) _mm256_extract_epi8(output_even,12);
		filt[N-3][col+13] = (unsigned char) _mm256_extract_epi8(output_even,13);
		filt[N-3][col+14] = (unsigned char) _mm256_extract_epi8(output_even,14);

		filt[N-2][col] = (unsigned char) _mm256_extract_epi8(output_row0_even,0);
		filt[N-2][col+1] = (unsigned char) _mm256_extract_epi8(output_row0_even,1);
		filt[N-2][col+2] = (unsigned char) _mm256_extract_epi8(output_row0_even,2);
		filt[N-2][col+3] = (unsigned char) _mm256_extract_epi8(output_row0_even,3);
		filt[N-2][col+4] = (unsigned char) _mm256_extract_epi8(output_row0_even,4);
		filt[N-2][col+5] = (unsigned char) _mm256_extract_epi8(output_row0_even,5);
		filt[N-2][col+6] = (unsigned char) _mm256_extract_epi8(output_row0_even,6);
		filt[N-2][col+7] = (unsigned char) _mm256_extract_epi8(output_row0_even,7);
		filt[N-2][col+8] = (unsigned char) _mm256_extract_epi8(output_row0_even,8);
		filt[N-2][col+9] = (unsigned char) _mm256_extract_epi8(output_row0_even,9);
		filt[N-2][col+10] = (unsigned char) _mm256_extract_epi8(output_row0_even,10);
		filt[N-2][col+11] = (unsigned char) _mm256_extract_epi8(output_row0_even,11);
		filt[N-2][col+12] = (unsigned char) _mm256_extract_epi8(output_row0_even,12);
		filt[N-2][col+13] = (unsigned char) _mm256_extract_epi8(output_row0_even,13);
		filt[N-2][col+14] = (unsigned char) _mm256_extract_epi8(output_row0_even,14);

		filt[N-1][col] = (unsigned char) _mm256_extract_epi8(output_row1_even,0);
		filt[N-1][col+1] = (unsigned char) _mm256_extract_epi8(output_row1_even,1);
		filt[N-1][col+2] = (unsigned char) _mm256_extract_epi8(output_row1_even,2);
		filt[N-1][col+3] = (unsigned char) _mm256_extract_epi8(output_row1_even,3);
		filt[N-1][col+4] = (unsigned char) _mm256_extract_epi8(output_row1_even,4);
		filt[N-1][col+5] = (unsigned char) _mm256_extract_epi8(output_row1_even,5);
		filt[N-1][col+6] = (unsigned char) _mm256_extract_epi8(output_row1_even,6);
		filt[N-1][col+7] = (unsigned char) _mm256_extract_epi8(output_row1_even,7);
		filt[N-1][col+8] = (unsigned char) _mm256_extract_epi8(output_row1_even,8);
		filt[N-1][col+9] = (unsigned char) _mm256_extract_epi8(output_row1_even,9);
		filt[N-1][col+10] = (unsigned char) _mm256_extract_epi8(output_row1_even,10);
		filt[N-1][col+11] = (unsigned char) _mm256_extract_epi8(output_row1_even,11);
		filt[N-1][col+12] = (unsigned char) _mm256_extract_epi8(output_row1_even,12);
		filt[N-1][col+13] = (unsigned char) _mm256_extract_epi8(output_row1_even,13);
		filt[N-1][col+14] = (unsigned char) _mm256_extract_epi8(output_row1_even,14);
		  break;
	case 16:
		_mm_storeu_si128( (__m128i *) &filt[N-3][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
		_mm_storeu_si128( (__m128i *) &filt[N-2][col],_mm256_extractf128_si256(output_row0_even, 0)); //store low 128bit - 16pixels
		_mm_storeu_si128( (__m128i *) &filt[N-1][col],_mm256_extractf128_si256(output_row1_even, 0)); //store low 128bit - 16pixels
		  break;
	case 17:
		_mm_storeu_si128( (__m128i *) &filt[N-3][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
		filt[N-3][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);

		_mm_storeu_si128( (__m128i *) &filt[N-2][col],_mm256_extractf128_si256(output_row0_even, 0)); //store low 128bit - 16pixels
		filt[N-2][col+16] = (unsigned char) _mm256_extract_epi8(output_row0_even,16);

		_mm_storeu_si128( (__m128i *) &filt[N-1][col],_mm256_extractf128_si256(output_row1_even, 0)); //store low 128bit - 16pixels
		filt[N-1][col+16] = (unsigned char) _mm256_extract_epi8(output_row1_even,16);
		  break;
	case 18:
		_mm_storeu_si128( (__m128i *) &filt[N-3][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
		filt[N-3][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
		filt[N-3][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);

		_mm_storeu_si128( (__m128i *) &filt[N-2][col],_mm256_extractf128_si256(output_row0_even, 0)); //store low 128bit - 16pixels
		filt[N-2][col+16] = (unsigned char) _mm256_extract_epi8(output_row0_even,16);
		filt[N-2][col+17] = (unsigned char) _mm256_extract_epi8(output_row0_even,17);

		_mm_storeu_si128( (__m128i *) &filt[N-1][col],_mm256_extractf128_si256(output_row1_even, 0)); //store low 128bit - 16pixels
		filt[N-1][col+16] = (unsigned char) _mm256_extract_epi8(output_row1_even,16);
		filt[N-1][col+17] = (unsigned char) _mm256_extract_epi8(output_row1_even,17);
		  break;
	case 19:
		_mm_storeu_si128( (__m128i *) &filt[N-3][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
		filt[N-3][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
		filt[N-3][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
		filt[N-3][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);

		_mm_storeu_si128( (__m128i *) &filt[N-2][col],_mm256_extractf128_si256(output_row0_even, 0)); //store low 128bit - 16pixels
		filt[N-2][col+16] = (unsigned char) _mm256_extract_epi8(output_row0_even,16);
		filt[N-2][col+17] = (unsigned char) _mm256_extract_epi8(output_row0_even,17);
		filt[N-2][col+18] = (unsigned char) _mm256_extract_epi8(output_row0_even,18);

		_mm_storeu_si128( (__m128i *) &filt[N-1][col],_mm256_extractf128_si256(output_row1_even, 0)); //store low 128bit - 16pixels
		filt[N-1][col+16] = (unsigned char) _mm256_extract_epi8(output_row1_even,16);
		filt[N-1][col+17] = (unsigned char) _mm256_extract_epi8(output_row1_even,17);
		filt[N-1][col+18] = (unsigned char) _mm256_extract_epi8(output_row1_even,18);
		  break;
	case 20:
		_mm_storeu_si128( (__m128i *) &filt[N-3][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
		filt[N-3][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
		filt[N-3][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
		filt[N-3][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
		filt[N-3][col+19] = (unsigned char) _mm256_extract_epi8(output_even,19);

		_mm_storeu_si128( (__m128i *) &filt[N-2][col],_mm256_extractf128_si256(output_row0_even, 0)); //store low 128bit - 16pixels
		filt[N-2][col+16] = (unsigned char) _mm256_extract_epi8(output_row0_even,16);
		filt[N-2][col+17] = (unsigned char) _mm256_extract_epi8(output_row0_even,17);
		filt[N-2][col+18] = (unsigned char) _mm256_extract_epi8(output_row0_even,18);
		filt[N-2][col+19] = (unsigned char) _mm256_extract_epi8(output_row0_even,19);

		_mm_storeu_si128( (__m128i *) &filt[N-1][col],_mm256_extractf128_si256(output_row1_even, 0)); //store low 128bit - 16pixels
		filt[N-1][col+16] = (unsigned char) _mm256_extract_epi8(output_row1_even,16);
		filt[N-1][col+17] = (unsigned char) _mm256_extract_epi8(output_row1_even,17);
		filt[N-1][col+18] = (unsigned char) _mm256_extract_epi8(output_row1_even,18);
		filt[N-1][col+19] = (unsigned char) _mm256_extract_epi8(output_row1_even,19);
		  break;
	case 21:
		_mm_storeu_si128( (__m128i *) &filt[N-3][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
		filt[N-3][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
		filt[N-3][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
		filt[N-3][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
		filt[N-3][col+19] = (unsigned char) _mm256_extract_epi8(output_even,19);
		filt[N-3][col+20] = (unsigned char) _mm256_extract_epi8(output_even,20);

		_mm_storeu_si128( (__m128i *) &filt[N-2][col],_mm256_extractf128_si256(output_row0_even, 0)); //store low 128bit - 16pixels
		filt[N-2][col+16] = (unsigned char) _mm256_extract_epi8(output_row0_even,16);
		filt[N-2][col+17] = (unsigned char) _mm256_extract_epi8(output_row0_even,17);
		filt[N-2][col+18] = (unsigned char) _mm256_extract_epi8(output_row0_even,18);
		filt[N-2][col+19] = (unsigned char) _mm256_extract_epi8(output_row0_even,19);
		filt[N-2][col+20] = (unsigned char) _mm256_extract_epi8(output_row0_even,20);

		_mm_storeu_si128( (__m128i *) &filt[N-1][col],_mm256_extractf128_si256(output_row1_even, 0)); //store low 128bit - 16pixels
		filt[N-1][col+16] = (unsigned char) _mm256_extract_epi8(output_row1_even,16);
		filt[N-1][col+17] = (unsigned char) _mm256_extract_epi8(output_row1_even,17);
		filt[N-1][col+18] = (unsigned char) _mm256_extract_epi8(output_row1_even,18);
		filt[N-1][col+19] = (unsigned char) _mm256_extract_epi8(output_row1_even,19);
		filt[N-1][col+20] = (unsigned char) _mm256_extract_epi8(output_row1_even,20);
		  break;
	case 22:
		_mm_storeu_si128( (__m128i *) &filt[N-3][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
		filt[N-3][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
		filt[N-3][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
		filt[N-3][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
		filt[N-3][col+19] = (unsigned char) _mm256_extract_epi8(output_even,19);
		filt[N-3][col+20] = (unsigned char) _mm256_extract_epi8(output_even,20);
		filt[N-3][col+21] = (unsigned char) _mm256_extract_epi8(output_even,21);

		_mm_storeu_si128( (__m128i *) &filt[N-2][col],_mm256_extractf128_si256(output_row0_even, 0)); //store low 128bit - 16pixels
		filt[N-2][col+16] = (unsigned char) _mm256_extract_epi8(output_row0_even,16);
		filt[N-2][col+17] = (unsigned char) _mm256_extract_epi8(output_row0_even,17);
		filt[N-2][col+18] = (unsigned char) _mm256_extract_epi8(output_row0_even,18);
		filt[N-2][col+19] = (unsigned char) _mm256_extract_epi8(output_row0_even,19);
		filt[N-2][col+20] = (unsigned char) _mm256_extract_epi8(output_row0_even,20);
		filt[N-2][col+21] = (unsigned char) _mm256_extract_epi8(output_row0_even,21);

		_mm_storeu_si128( (__m128i *) &filt[N-1][col],_mm256_extractf128_si256(output_row1_even, 0)); //store low 128bit - 16pixels
		filt[N-1][col+16] = (unsigned char) _mm256_extract_epi8(output_row1_even,16);
		filt[N-1][col+17] = (unsigned char) _mm256_extract_epi8(output_row1_even,17);
		filt[N-1][col+18] = (unsigned char) _mm256_extract_epi8(output_row1_even,18);
		filt[N-1][col+19] = (unsigned char) _mm256_extract_epi8(output_row1_even,19);
		filt[N-1][col+20] = (unsigned char) _mm256_extract_epi8(output_row1_even,20);
		filt[N-1][col+21] = (unsigned char) _mm256_extract_epi8(output_row1_even,21);
		  break;
	case 23:
		_mm_storeu_si128( (__m128i *) &filt[N-3][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
		filt[N-3][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
		filt[N-3][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
		filt[N-3][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
		filt[N-3][col+19] = (unsigned char) _mm256_extract_epi8(output_even,19);
		filt[N-3][col+20] = (unsigned char) _mm256_extract_epi8(output_even,20);
		filt[N-3][col+21] = (unsigned char) _mm256_extract_epi8(output_even,21);
		filt[N-3][col+22] = (unsigned char) _mm256_extract_epi8(output_even,22);


		_mm_storeu_si128( (__m128i *) &filt[N-2][col],_mm256_extractf128_si256(output_row0_even, 0)); //store low 128bit - 16pixels
		filt[N-2][col+16] = (unsigned char) _mm256_extract_epi8(output_row0_even,16);
		filt[N-2][col+17] = (unsigned char) _mm256_extract_epi8(output_row0_even,17);
		filt[N-2][col+18] = (unsigned char) _mm256_extract_epi8(output_row0_even,18);
		filt[N-2][col+19] = (unsigned char) _mm256_extract_epi8(output_row0_even,19);
		filt[N-2][col+20] = (unsigned char) _mm256_extract_epi8(output_row0_even,20);
		filt[N-2][col+21] = (unsigned char) _mm256_extract_epi8(output_row0_even,21);
		filt[N-2][col+22] = (unsigned char) _mm256_extract_epi8(output_row0_even,22);

		_mm_storeu_si128( (__m128i *) &filt[N-1][col],_mm256_extractf128_si256(output_row1_even, 0)); //store low 128bit - 16pixels
		filt[N-1][col+16] = (unsigned char) _mm256_extract_epi8(output_row1_even,16);
		filt[N-1][col+17] = (unsigned char) _mm256_extract_epi8(output_row1_even,17);
		filt[N-1][col+18] = (unsigned char) _mm256_extract_epi8(output_row1_even,18);
		filt[N-1][col+19] = (unsigned char) _mm256_extract_epi8(output_row1_even,19);
		filt[N-1][col+20] = (unsigned char) _mm256_extract_epi8(output_row1_even,20);
		filt[N-1][col+21] = (unsigned char) _mm256_extract_epi8(output_row1_even,21);
		filt[N-1][col+22] = (unsigned char) _mm256_extract_epi8(output_row1_even,22);
		  break;
	case 24:
		_mm_storeu_si128( (__m128i *) &filt[N-3][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
		filt[N-3][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
		filt[N-3][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
		filt[N-3][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
		filt[N-3][col+19] = (unsigned char) _mm256_extract_epi8(output_even,19);
		filt[N-3][col+20] = (unsigned char) _mm256_extract_epi8(output_even,20);
		filt[N-3][col+21] = (unsigned char) _mm256_extract_epi8(output_even,21);
		filt[N-3][col+22] = (unsigned char) _mm256_extract_epi8(output_even,22);
		filt[N-3][col+23] = (unsigned char) _mm256_extract_epi8(output_even,23);

		_mm_storeu_si128( (__m128i *) &filt[N-2][col],_mm256_extractf128_si256(output_row0_even, 0)); //store low 128bit - 16pixels
		filt[N-2][col+16] = (unsigned char) _mm256_extract_epi8(output_row0_even,16);
		filt[N-2][col+17] = (unsigned char) _mm256_extract_epi8(output_row0_even,17);
		filt[N-2][col+18] = (unsigned char) _mm256_extract_epi8(output_row0_even,18);
		filt[N-2][col+19] = (unsigned char) _mm256_extract_epi8(output_row0_even,19);
		filt[N-2][col+20] = (unsigned char) _mm256_extract_epi8(output_row0_even,20);
		filt[N-2][col+21] = (unsigned char) _mm256_extract_epi8(output_row0_even,21);
		filt[N-2][col+22] = (unsigned char) _mm256_extract_epi8(output_row0_even,22);
		filt[N-2][col+23] = (unsigned char) _mm256_extract_epi8(output_row0_even,23);

		_mm_storeu_si128( (__m128i *) &filt[N-1][col],_mm256_extractf128_si256(output_row1_even, 0)); //store low 128bit - 16pixels
		filt[N-1][col+16] = (unsigned char) _mm256_extract_epi8(output_row1_even,16);
		filt[N-1][col+17] = (unsigned char) _mm256_extract_epi8(output_row1_even,17);
		filt[N-1][col+18] = (unsigned char) _mm256_extract_epi8(output_row1_even,18);
		filt[N-1][col+19] = (unsigned char) _mm256_extract_epi8(output_row1_even,19);
		filt[N-1][col+20] = (unsigned char) _mm256_extract_epi8(output_row1_even,20);
		filt[N-1][col+21] = (unsigned char) _mm256_extract_epi8(output_row1_even,21);
		filt[N-1][col+22] = (unsigned char) _mm256_extract_epi8(output_row1_even,22);
		filt[N-1][col+23] = (unsigned char) _mm256_extract_epi8(output_row1_even,23);
		  break;
	case 25:
		_mm_storeu_si128( (__m128i *) &filt[N-3][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
		filt[N-3][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
		filt[N-3][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
		filt[N-3][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
		filt[N-3][col+19] = (unsigned char) _mm256_extract_epi8(output_even,19);
		filt[N-3][col+20] = (unsigned char) _mm256_extract_epi8(output_even,20);
		filt[N-3][col+21] = (unsigned char) _mm256_extract_epi8(output_even,21);
		filt[N-3][col+22] = (unsigned char) _mm256_extract_epi8(output_even,22);
		filt[N-3][col+23] = (unsigned char) _mm256_extract_epi8(output_even,23);
		filt[N-3][col+24] = (unsigned char) _mm256_extract_epi8(output_even,24);

		_mm_storeu_si128( (__m128i *) &filt[N-2][col],_mm256_extractf128_si256(output_row0_even, 0)); //store low 128bit - 16pixels
		filt[N-2][col+16] = (unsigned char) _mm256_extract_epi8(output_row0_even,16);
		filt[N-2][col+17] = (unsigned char) _mm256_extract_epi8(output_row0_even,17);
		filt[N-2][col+18] = (unsigned char) _mm256_extract_epi8(output_row0_even,18);
		filt[N-2][col+19] = (unsigned char) _mm256_extract_epi8(output_row0_even,19);
		filt[N-2][col+20] = (unsigned char) _mm256_extract_epi8(output_row0_even,20);
		filt[N-2][col+21] = (unsigned char) _mm256_extract_epi8(output_row0_even,21);
		filt[N-2][col+22] = (unsigned char) _mm256_extract_epi8(output_row0_even,22);
		filt[N-2][col+23] = (unsigned char) _mm256_extract_epi8(output_row0_even,23);
		filt[N-2][col+24] = (unsigned char) _mm256_extract_epi8(output_row0_even,24);

		_mm_storeu_si128( (__m128i *) &filt[N-1][col],_mm256_extractf128_si256(output_row1_even, 0)); //store low 128bit - 16pixels
		filt[N-1][col+16] = (unsigned char) _mm256_extract_epi8(output_row1_even,16);
		filt[N-1][col+17] = (unsigned char) _mm256_extract_epi8(output_row1_even,17);
		filt[N-1][col+18] = (unsigned char) _mm256_extract_epi8(output_row1_even,18);
		filt[N-1][col+19] = (unsigned char) _mm256_extract_epi8(output_row1_even,19);
		filt[N-1][col+20] = (unsigned char) _mm256_extract_epi8(output_row1_even,20);
		filt[N-1][col+21] = (unsigned char) _mm256_extract_epi8(output_row1_even,21);
		filt[N-1][col+22] = (unsigned char) _mm256_extract_epi8(output_row1_even,22);
		filt[N-1][col+23] = (unsigned char) _mm256_extract_epi8(output_row1_even,23);
		filt[N-1][col+24] = (unsigned char) _mm256_extract_epi8(output_row1_even,24);
		  break;
	case 26:
		_mm_storeu_si128( (__m128i *) &filt[N-3][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
		filt[N-3][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
		filt[N-3][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
		filt[N-3][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
		filt[N-3][col+19] = (unsigned char) _mm256_extract_epi8(output_even,19);
		filt[N-3][col+20] = (unsigned char) _mm256_extract_epi8(output_even,20);
		filt[N-3][col+21] = (unsigned char) _mm256_extract_epi8(output_even,21);
		filt[N-3][col+22] = (unsigned char) _mm256_extract_epi8(output_even,22);
		filt[N-3][col+23] = (unsigned char) _mm256_extract_epi8(output_even,23);
		filt[N-3][col+24] = (unsigned char) _mm256_extract_epi8(output_even,24);
		filt[N-3][col+25] = (unsigned char) _mm256_extract_epi8(output_even,25);

		_mm_storeu_si128( (__m128i *) &filt[N-2][col],_mm256_extractf128_si256(output_row0_even, 0)); //store low 128bit - 16pixels
		filt[N-2][col+16] = (unsigned char) _mm256_extract_epi8(output_row0_even,16);
		filt[N-2][col+17] = (unsigned char) _mm256_extract_epi8(output_row0_even,17);
		filt[N-2][col+18] = (unsigned char) _mm256_extract_epi8(output_row0_even,18);
		filt[N-2][col+19] = (unsigned char) _mm256_extract_epi8(output_row0_even,19);
		filt[N-2][col+20] = (unsigned char) _mm256_extract_epi8(output_row0_even,20);
		filt[N-2][col+21] = (unsigned char) _mm256_extract_epi8(output_row0_even,21);
		filt[N-2][col+22] = (unsigned char) _mm256_extract_epi8(output_row0_even,22);
		filt[N-2][col+23] = (unsigned char) _mm256_extract_epi8(output_row0_even,23);
		filt[N-2][col+24] = (unsigned char) _mm256_extract_epi8(output_row0_even,24);
		filt[N-2][col+25] = (unsigned char) _mm256_extract_epi8(output_row0_even,25);

		_mm_storeu_si128( (__m128i *) &filt[N-1][col],_mm256_extractf128_si256(output_row1_even, 0)); //store low 128bit - 16pixels
		filt[N-1][col+16] = (unsigned char) _mm256_extract_epi8(output_row1_even,16);
		filt[N-1][col+17] = (unsigned char) _mm256_extract_epi8(output_row1_even,17);
		filt[N-1][col+18] = (unsigned char) _mm256_extract_epi8(output_row1_even,18);
		filt[N-1][col+19] = (unsigned char) _mm256_extract_epi8(output_row1_even,19);
		filt[N-1][col+20] = (unsigned char) _mm256_extract_epi8(output_row1_even,20);
		filt[N-1][col+21] = (unsigned char) _mm256_extract_epi8(output_row1_even,21);
		filt[N-1][col+22] = (unsigned char) _mm256_extract_epi8(output_row1_even,22);
		filt[N-1][col+23] = (unsigned char) _mm256_extract_epi8(output_row1_even,23);
		filt[N-1][col+24] = (unsigned char) _mm256_extract_epi8(output_row1_even,24);
		filt[N-1][col+25] = (unsigned char) _mm256_extract_epi8(output_row1_even,25);
		  break;
	case 27:
		_mm_storeu_si128( (__m128i *) &filt[N-3][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
		filt[N-3][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
		filt[N-3][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
		filt[N-3][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
		filt[N-3][col+19] = (unsigned char) _mm256_extract_epi8(output_even,19);
		filt[N-3][col+20] = (unsigned char) _mm256_extract_epi8(output_even,20);
		filt[N-3][col+21] = (unsigned char) _mm256_extract_epi8(output_even,21);
		filt[N-3][col+22] = (unsigned char) _mm256_extract_epi8(output_even,22);
		filt[N-3][col+23] = (unsigned char) _mm256_extract_epi8(output_even,23);
		filt[N-3][col+24] = (unsigned char) _mm256_extract_epi8(output_even,24);
		filt[N-3][col+25] = (unsigned char) _mm256_extract_epi8(output_even,25);
		filt[N-3][col+26] = (unsigned char) _mm256_extract_epi8(output_even,26);

		_mm_storeu_si128( (__m128i *) &filt[N-2][col],_mm256_extractf128_si256(output_row0_even, 0)); //store low 128bit - 16pixels
		filt[N-2][col+16] = (unsigned char) _mm256_extract_epi8(output_row0_even,16);
		filt[N-2][col+17] = (unsigned char) _mm256_extract_epi8(output_row0_even,17);
		filt[N-2][col+18] = (unsigned char) _mm256_extract_epi8(output_row0_even,18);
		filt[N-2][col+19] = (unsigned char) _mm256_extract_epi8(output_row0_even,19);
		filt[N-2][col+20] = (unsigned char) _mm256_extract_epi8(output_row0_even,20);
		filt[N-2][col+21] = (unsigned char) _mm256_extract_epi8(output_row0_even,21);
		filt[N-2][col+22] = (unsigned char) _mm256_extract_epi8(output_row0_even,22);
		filt[N-2][col+23] = (unsigned char) _mm256_extract_epi8(output_row0_even,23);
		filt[N-2][col+24] = (unsigned char) _mm256_extract_epi8(output_row0_even,24);
		filt[N-2][col+25] = (unsigned char) _mm256_extract_epi8(output_row0_even,25);
		filt[N-2][col+26] = (unsigned char) _mm256_extract_epi8(output_row0_even,26);

		_mm_storeu_si128( (__m128i *) &filt[N-1][col],_mm256_extractf128_si256(output_row1_even, 0)); //store low 128bit - 16pixels
		filt[N-1][col+16] = (unsigned char) _mm256_extract_epi8(output_row1_even,16);
		filt[N-1][col+17] = (unsigned char) _mm256_extract_epi8(output_row1_even,17);
		filt[N-1][col+18] = (unsigned char) _mm256_extract_epi8(output_row1_even,18);
		filt[N-1][col+19] = (unsigned char) _mm256_extract_epi8(output_row1_even,19);
		filt[N-1][col+20] = (unsigned char) _mm256_extract_epi8(output_row1_even,20);
		filt[N-1][col+21] = (unsigned char) _mm256_extract_epi8(output_row1_even,21);
		filt[N-1][col+22] = (unsigned char) _mm256_extract_epi8(output_row1_even,22);
		filt[N-1][col+23] = (unsigned char) _mm256_extract_epi8(output_row1_even,23);
		filt[N-1][col+24] = (unsigned char) _mm256_extract_epi8(output_row1_even,24);
		filt[N-1][col+25] = (unsigned char) _mm256_extract_epi8(output_row1_even,25);
		filt[N-1][col+26] = (unsigned char) _mm256_extract_epi8(output_row1_even,26);
		  break;
	case 28:
		_mm_storeu_si128( (__m128i *) &filt[N-3][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
		filt[N-3][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
		filt[N-3][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
		filt[N-3][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
		filt[N-3][col+19] = (unsigned char) _mm256_extract_epi8(output_even,19);
		filt[N-3][col+20] = (unsigned char) _mm256_extract_epi8(output_even,20);
		filt[N-3][col+21] = (unsigned char) _mm256_extract_epi8(output_even,21);
		filt[N-3][col+22] = (unsigned char) _mm256_extract_epi8(output_even,22);
		filt[N-3][col+23] = (unsigned char) _mm256_extract_epi8(output_even,23);
		filt[N-3][col+24] = (unsigned char) _mm256_extract_epi8(output_even,24);
		filt[N-3][col+25] = (unsigned char) _mm256_extract_epi8(output_even,25);
		filt[N-3][col+26] = (unsigned char) _mm256_extract_epi8(output_even,26);
		filt[N-3][col+27] = (unsigned char) _mm256_extract_epi8(output_even,27);

		_mm_storeu_si128( (__m128i *) &filt[N-2][col],_mm256_extractf128_si256(output_row0_even, 0)); //store low 128bit - 16pixels
		filt[N-2][col+16] = (unsigned char) _mm256_extract_epi8(output_row0_even,16);
		filt[N-2][col+17] = (unsigned char) _mm256_extract_epi8(output_row0_even,17);
		filt[N-2][col+18] = (unsigned char) _mm256_extract_epi8(output_row0_even,18);
		filt[N-2][col+19] = (unsigned char) _mm256_extract_epi8(output_row0_even,19);
		filt[N-2][col+20] = (unsigned char) _mm256_extract_epi8(output_row0_even,20);
		filt[N-2][col+21] = (unsigned char) _mm256_extract_epi8(output_row0_even,21);
		filt[N-2][col+22] = (unsigned char) _mm256_extract_epi8(output_row0_even,22);
		filt[N-2][col+23] = (unsigned char) _mm256_extract_epi8(output_row0_even,23);
		filt[N-2][col+24] = (unsigned char) _mm256_extract_epi8(output_row0_even,24);
		filt[N-2][col+25] = (unsigned char) _mm256_extract_epi8(output_row0_even,25);
		filt[N-2][col+26] = (unsigned char) _mm256_extract_epi8(output_row0_even,26);
		filt[N-2][col+27] = (unsigned char) _mm256_extract_epi8(output_row0_even,27);

		_mm_storeu_si128( (__m128i *) &filt[N-1][col],_mm256_extractf128_si256(output_row1_even, 0)); //store low 128bit - 16pixels
		filt[N-1][col+16] = (unsigned char) _mm256_extract_epi8(output_row1_even,16);
		filt[N-1][col+17] = (unsigned char) _mm256_extract_epi8(output_row1_even,17);
		filt[N-1][col+18] = (unsigned char) _mm256_extract_epi8(output_row1_even,18);
		filt[N-1][col+19] = (unsigned char) _mm256_extract_epi8(output_row1_even,19);
		filt[N-1][col+20] = (unsigned char) _mm256_extract_epi8(output_row1_even,20);
		filt[N-1][col+21] = (unsigned char) _mm256_extract_epi8(output_row1_even,21);
		filt[N-1][col+22] = (unsigned char) _mm256_extract_epi8(output_row1_even,22);
		filt[N-1][col+23] = (unsigned char) _mm256_extract_epi8(output_row1_even,23);
		filt[N-1][col+24] = (unsigned char) _mm256_extract_epi8(output_row1_even,24);
		filt[N-1][col+25] = (unsigned char) _mm256_extract_epi8(output_row1_even,25);
		filt[N-1][col+26] = (unsigned char) _mm256_extract_epi8(output_row1_even,26);
		filt[N-1][col+27] = (unsigned char) _mm256_extract_epi8(output_row1_even,27);
		  break;
	case 29:
		_mm_storeu_si128( (__m128i *) &filt[N-3][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
		filt[N-3][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
		filt[N-3][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
		filt[N-3][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
		filt[N-3][col+19] = (unsigned char) _mm256_extract_epi8(output_even,19);
		filt[N-3][col+20] = (unsigned char) _mm256_extract_epi8(output_even,20);
		filt[N-3][col+21] = (unsigned char) _mm256_extract_epi8(output_even,21);
		filt[N-3][col+22] = (unsigned char) _mm256_extract_epi8(output_even,22);
		filt[N-3][col+23] = (unsigned char) _mm256_extract_epi8(output_even,23);
		filt[N-3][col+24] = (unsigned char) _mm256_extract_epi8(output_even,24);
		filt[N-3][col+25] = (unsigned char) _mm256_extract_epi8(output_even,25);
		filt[N-3][col+26] = (unsigned char) _mm256_extract_epi8(output_even,26);
		filt[N-3][col+27] = (unsigned char) _mm256_extract_epi8(output_even,27);
		filt[N-3][col+28] = (unsigned char) _mm256_extract_epi8(output_even,28);

		_mm_storeu_si128( (__m128i *) &filt[N-2][col],_mm256_extractf128_si256(output_row0_even, 0)); //store low 128bit - 16pixels
		filt[N-2][col+16] = (unsigned char) _mm256_extract_epi8(output_row0_even,16);
		filt[N-2][col+17] = (unsigned char) _mm256_extract_epi8(output_row0_even,17);
		filt[N-2][col+18] = (unsigned char) _mm256_extract_epi8(output_row0_even,18);
		filt[N-2][col+19] = (unsigned char) _mm256_extract_epi8(output_row0_even,19);
		filt[N-2][col+20] = (unsigned char) _mm256_extract_epi8(output_row0_even,20);
		filt[N-2][col+21] = (unsigned char) _mm256_extract_epi8(output_row0_even,21);
		filt[N-2][col+22] = (unsigned char) _mm256_extract_epi8(output_row0_even,22);
		filt[N-2][col+23] = (unsigned char) _mm256_extract_epi8(output_row0_even,23);
		filt[N-2][col+24] = (unsigned char) _mm256_extract_epi8(output_row0_even,24);
		filt[N-2][col+25] = (unsigned char) _mm256_extract_epi8(output_row0_even,25);
		filt[N-2][col+26] = (unsigned char) _mm256_extract_epi8(output_row0_even,26);
		filt[N-2][col+27] = (unsigned char) _mm256_extract_epi8(output_row0_even,27);
		filt[N-2][col+28] = (unsigned char) _mm256_extract_epi8(output_row0_even,28);

		_mm_storeu_si128( (__m128i *) &filt[N-1][col],_mm256_extractf128_si256(output_row1_even, 0)); //store low 128bit - 16pixels
		filt[N-1][col+16] = (unsigned char) _mm256_extract_epi8(output_row1_even,16);
		filt[N-1][col+17] = (unsigned char) _mm256_extract_epi8(output_row1_even,17);
		filt[N-1][col+18] = (unsigned char) _mm256_extract_epi8(output_row1_even,18);
		filt[N-1][col+19] = (unsigned char) _mm256_extract_epi8(output_row1_even,19);
		filt[N-1][col+20] = (unsigned char) _mm256_extract_epi8(output_row1_even,20);
		filt[N-1][col+21] = (unsigned char) _mm256_extract_epi8(output_row1_even,21);
		filt[N-1][col+22] = (unsigned char) _mm256_extract_epi8(output_row1_even,22);
		filt[N-1][col+23] = (unsigned char) _mm256_extract_epi8(output_row1_even,23);
		filt[N-1][col+24] = (unsigned char) _mm256_extract_epi8(output_row1_even,24);
		filt[N-1][col+25] = (unsigned char) _mm256_extract_epi8(output_row1_even,25);
		filt[N-1][col+26] = (unsigned char) _mm256_extract_epi8(output_row1_even,26);
		filt[N-1][col+27] = (unsigned char) _mm256_extract_epi8(output_row1_even,27);
		filt[N-1][col+28] = (unsigned char) _mm256_extract_epi8(output_row1_even,28);
		  break;
	case 30:
		_mm_storeu_si128( (__m128i *) &filt[N-3][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
		filt[N-3][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
		filt[N-3][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
		filt[N-3][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
		filt[N-3][col+19] = (unsigned char) _mm256_extract_epi8(output_even,19);
		filt[N-3][col+20] = (unsigned char) _mm256_extract_epi8(output_even,20);
		filt[N-3][col+21] = (unsigned char) _mm256_extract_epi8(output_even,21);
		filt[N-3][col+22] = (unsigned char) _mm256_extract_epi8(output_even,22);
		filt[N-3][col+23] = (unsigned char) _mm256_extract_epi8(output_even,23);
		filt[N-3][col+24] = (unsigned char) _mm256_extract_epi8(output_even,24);
		filt[N-3][col+25] = (unsigned char) _mm256_extract_epi8(output_even,25);
		filt[N-3][col+26] = (unsigned char) _mm256_extract_epi8(output_even,26);
		filt[N-3][col+27] = (unsigned char) _mm256_extract_epi8(output_even,27);
		filt[N-3][col+28] = (unsigned char) _mm256_extract_epi8(output_even,28);
		filt[N-3][col+29] = (unsigned char) _mm256_extract_epi8(output_even,29);

		_mm_storeu_si128( (__m128i *) &filt[N-2][col],_mm256_extractf128_si256(output_row0_even, 0)); //store low 128bit - 16pixels
		filt[N-2][col+16] = (unsigned char) _mm256_extract_epi8(output_row0_even,16);
		filt[N-2][col+17] = (unsigned char) _mm256_extract_epi8(output_row0_even,17);
		filt[N-2][col+18] = (unsigned char) _mm256_extract_epi8(output_row0_even,18);
		filt[N-2][col+19] = (unsigned char) _mm256_extract_epi8(output_row0_even,19);
		filt[N-2][col+20] = (unsigned char) _mm256_extract_epi8(output_row0_even,20);
		filt[N-2][col+21] = (unsigned char) _mm256_extract_epi8(output_row0_even,21);
		filt[N-2][col+22] = (unsigned char) _mm256_extract_epi8(output_row0_even,22);
		filt[N-2][col+23] = (unsigned char) _mm256_extract_epi8(output_row0_even,23);
		filt[N-2][col+24] = (unsigned char) _mm256_extract_epi8(output_row0_even,24);
		filt[N-2][col+25] = (unsigned char) _mm256_extract_epi8(output_row0_even,25);
		filt[N-2][col+26] = (unsigned char) _mm256_extract_epi8(output_row0_even,26);
		filt[N-2][col+27] = (unsigned char) _mm256_extract_epi8(output_row0_even,27);
		filt[N-2][col+28] = (unsigned char) _mm256_extract_epi8(output_row0_even,28);
		filt[N-2][col+29] = (unsigned char) _mm256_extract_epi8(output_row0_even,29);


		_mm_storeu_si128( (__m128i *) &filt[N-1][col],_mm256_extractf128_si256(output_row1_even, 0)); //store low 128bit - 16pixels
		filt[N-1][col+16] = (unsigned char) _mm256_extract_epi8(output_row1_even,16);
		filt[N-1][col+17] = (unsigned char) _mm256_extract_epi8(output_row1_even,17);
		filt[N-1][col+18] = (unsigned char) _mm256_extract_epi8(output_row1_even,18);
		filt[N-1][col+19] = (unsigned char) _mm256_extract_epi8(output_row1_even,19);
		filt[N-1][col+20] = (unsigned char) _mm256_extract_epi8(output_row1_even,20);
		filt[N-1][col+21] = (unsigned char) _mm256_extract_epi8(output_row1_even,21);
		filt[N-1][col+22] = (unsigned char) _mm256_extract_epi8(output_row1_even,22);
		filt[N-1][col+23] = (unsigned char) _mm256_extract_epi8(output_row1_even,23);
		filt[N-1][col+24] = (unsigned char) _mm256_extract_epi8(output_row1_even,24);
		filt[N-1][col+25] = (unsigned char) _mm256_extract_epi8(output_row1_even,25);
		filt[N-1][col+26] = (unsigned char) _mm256_extract_epi8(output_row1_even,26);
		filt[N-1][col+27] = (unsigned char) _mm256_extract_epi8(output_row1_even,27);
		filt[N-1][col+28] = (unsigned char) _mm256_extract_epi8(output_row1_even,28);
		filt[N-1][col+29] = (unsigned char) _mm256_extract_epi8(output_row1_even,29);
		  break;
	case 31:
		_mm_storeu_si128( (__m128i *) &filt[N-3][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
		filt[N-3][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
		filt[N-3][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
		filt[N-3][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
		filt[N-3][col+19] = (unsigned char) _mm256_extract_epi8(output_even,19);
		filt[N-3][col+20] = (unsigned char) _mm256_extract_epi8(output_even,20);
		filt[N-3][col+21] = (unsigned char) _mm256_extract_epi8(output_even,21);
		filt[N-3][col+22] = (unsigned char) _mm256_extract_epi8(output_even,22);
		filt[N-3][col+23] = (unsigned char) _mm256_extract_epi8(output_even,23);
		filt[N-3][col+24] = (unsigned char) _mm256_extract_epi8(output_even,24);
		filt[N-3][col+25] = (unsigned char) _mm256_extract_epi8(output_even,25);
		filt[N-3][col+26] = (unsigned char) _mm256_extract_epi8(output_even,26);
		filt[N-3][col+27] = (unsigned char) _mm256_extract_epi8(output_even,27);
		filt[N-3][col+28] = (unsigned char) _mm256_extract_epi8(output_even,28);
		filt[N-3][col+29] = (unsigned char) _mm256_extract_epi8(output_even,29);

		_mm_storeu_si128( (__m128i *) &filt[N-2][col],_mm256_extractf128_si256(output_row0_even, 0)); //store low 128bit - 16pixels
		filt[N-2][col+16] = (unsigned char) _mm256_extract_epi8(output_row0_even,16);
		filt[N-2][col+17] = (unsigned char) _mm256_extract_epi8(output_row0_even,17);
		filt[N-2][col+18] = (unsigned char) _mm256_extract_epi8(output_row0_even,18);
		filt[N-2][col+19] = (unsigned char) _mm256_extract_epi8(output_row0_even,19);
		filt[N-2][col+20] = (unsigned char) _mm256_extract_epi8(output_row0_even,20);
		filt[N-2][col+21] = (unsigned char) _mm256_extract_epi8(output_row0_even,21);
		filt[N-2][col+22] = (unsigned char) _mm256_extract_epi8(output_row0_even,22);
		filt[N-2][col+23] = (unsigned char) _mm256_extract_epi8(output_row0_even,23);
		filt[N-2][col+24] = (unsigned char) _mm256_extract_epi8(output_row0_even,24);
		filt[N-2][col+25] = (unsigned char) _mm256_extract_epi8(output_row0_even,25);
		filt[N-2][col+26] = (unsigned char) _mm256_extract_epi8(output_row0_even,26);
		filt[N-2][col+27] = (unsigned char) _mm256_extract_epi8(output_row0_even,27);
		filt[N-2][col+28] = (unsigned char) _mm256_extract_epi8(output_row0_even,28);
		filt[N-2][col+29] = (unsigned char) _mm256_extract_epi8(output_row0_even,29);


		_mm_storeu_si128( (__m128i *) &filt[N-1][col],_mm256_extractf128_si256(output_row1_even, 0)); //store low 128bit - 16pixels
		filt[N-1][col+16] = (unsigned char) _mm256_extract_epi8(output_row1_even,16);
		filt[N-1][col+17] = (unsigned char) _mm256_extract_epi8(output_row1_even,17);
		filt[N-1][col+18] = (unsigned char) _mm256_extract_epi8(output_row1_even,18);
		filt[N-1][col+19] = (unsigned char) _mm256_extract_epi8(output_row1_even,19);
		filt[N-1][col+20] = (unsigned char) _mm256_extract_epi8(output_row1_even,20);
		filt[N-1][col+21] = (unsigned char) _mm256_extract_epi8(output_row1_even,21);
		filt[N-1][col+22] = (unsigned char) _mm256_extract_epi8(output_row1_even,22);
		filt[N-1][col+23] = (unsigned char) _mm256_extract_epi8(output_row1_even,23);
		filt[N-1][col+24] = (unsigned char) _mm256_extract_epi8(output_row1_even,24);
		filt[N-1][col+25] = (unsigned char) _mm256_extract_epi8(output_row1_even,25);
		filt[N-1][col+26] = (unsigned char) _mm256_extract_epi8(output_row1_even,26);
		filt[N-1][col+27] = (unsigned char) _mm256_extract_epi8(output_row1_even,27);
		filt[N-1][col+28] = (unsigned char) _mm256_extract_epi8(output_row1_even,28);
		filt[N-1][col+29] = (unsigned char) _mm256_extract_epi8(output_row1_even,29);

		//the filt[0:2][M-1] pixel will be computed without vectorization
		int newPixel;
		for (int row=N-3;row<=N-1;row++){
		 newPixel = 0;
		for (int rowOffset=-2; rowOffset<=2; rowOffset++) {
			for (int colOffset=-2; colOffset<=2; colOffset++) {

			   if ( ((row+rowOffset)<N) && ((M-1+colOffset)<M) && ((row+rowOffset)>=0) && ((M-1+colOffset)>=0) )
	              newPixel += frame1[row+rowOffset][M-1+colOffset] * filter[2 + rowOffset][2 + colOffset];

			      }
		        }
	   filt[row][M-1] = (unsigned char) (newPixel / divisor);
		}

		  break;
	default:
		printf("\n something went wrong");
	}

return 0;

}







//for row=N-2,N-1 only
//row0 refers to N-2, while row1 refers to N-1
int conv_loop_reminder_last_less_div_special_case(unsigned char **frame1,unsigned char **filt,const unsigned int M, const unsigned int N, const unsigned int col, const unsigned int REMINDER_ITERATIONS,const unsigned int division_case,const signed char mask_vector[][32], const __m256i f, const unsigned short int divisor,signed char **filter ){

__m256i r1,r2,r3,r4,m0,m1,m2,m3,m4;
__m256i output_row0_even,output_row1_even,output_row0_odd,output_row1_odd;

		//const __m256i f  = _mm256_set_epi16(52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759);
		//const __m256i mask  = _mm256_set_epi8(0,0,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255);
		//const __m256i mask2  = _mm256_set_epi8(0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0);
		const __m256i mask3  = _mm256_set_epi16(0,0,0,0,0,0,0,65535,0,0,0,0,0,0,0,0);
		//const __m256i mask4  = _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255,0,0,0,0,0,0,0,0,0,0,0,0);
		//const __m256i mask5  = _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0);

		const __m256i output_mask      = _mm256_set_epi16(0,0,0,65535,0,0,65535,0,0,65535,0,0,65535,0,0,65535);
		const __m256i output_mask_sh1  = _mm256_set_epi16(0,0,65535,0,0,65535,0,0,65535,0,0,65535,0,0,65535,0);
		//const __m256i output_mask_sh2  = _mm256_set_epi16(0,0,0,    0,65535,0,0,65535,0,0,65535,0,0,65535,0,0);
		const __m256i mask_new      = _mm256_set_epi16(0,0,0,0,0,0,0,0,0,65535,0,0,0,0,0,0);
		const __m256i mask_new2      = _mm256_set_epi16(0,0,0,0,0,0,0,0,65535,0,0,0,0,0,0,0);


		const __m256i c0=_mm256_load_si256( (__m256i *) &mask_vector[0][0]);
			const __m256i c1=_mm256_load_si256( (__m256i *) &mask_vector[1][0]);
			const __m256i c2=_mm256_load_si256( (__m256i *) &mask_vector[2][0]);
			const __m256i c3=_mm256_load_si256( (__m256i *) &mask_vector[3][0]);
			//const __m256i c4=_mm256_load_si256( (__m256i *) &mask_vector[4][0]);

			const __m256i c0_sh1=_mm256_load_si256( (__m256i *) &mask_vector[5][0]);
			const __m256i c1_sh1=_mm256_load_si256( (__m256i *) &mask_vector[6][0]);
			const __m256i c2_sh1=_mm256_load_si256( (__m256i *) &mask_vector[7][0]);
			const __m256i c3_sh1=_mm256_load_si256( (__m256i *) &mask_vector[8][0]);
			//const __m256i c4_sh1=_mm256_load_si256( (__m256i *) &mask_vector[9][0]);

			const __m256i c0_sh2=_mm256_load_si256( (__m256i *) &mask_vector[10][0]);
			const __m256i c1_sh2=_mm256_load_si256( (__m256i *) &mask_vector[11][0]);
			const __m256i c2_sh2=_mm256_load_si256( (__m256i *) &mask_vector[12][0]);
			const __m256i c3_sh2=_mm256_load_si256( (__m256i *) &mask_vector[13][0]);
			//const __m256i c4_sh2=_mm256_load_si256( (__m256i *) &mask_vector[14][0]);



 __m256i reminder_mask1,reminder_mask2;

	reminder_mask1=_mm256_load_si256( (__m256i *) &reminder_msk1[REMINDER_ITERATIONS-1][0]);
	reminder_mask2=_mm256_load_si256( (__m256i *) &reminder_msk2[REMINDER_ITERATIONS-1][0]);


	 //1st col iteration
	//load the 4 rows
	r1=_mm256_loadu_si256( (__m256i *) &frame1[N-4][col-2]);
	r2=_mm256_loadu_si256( (__m256i *) &frame1[N-3][col-2]);
	r3=_mm256_loadu_si256( (__m256i *) &frame1[N-2][col-2]);
	r4=_mm256_loadu_si256( (__m256i *) &frame1[N-1][col-2]);

	/*
	 * The above load operations load outside of the array bounds; these elements are filled with zeros just after
	 * Furthermore, I add two extra zeros in the end to compute col=N-2 and col=N-1. The last value of col is N-1, not N
	 */

	//AND r0-r4 with reminder_mask
	r1=_mm256_and_si256(r1,reminder_mask1);
	r2=_mm256_and_si256(r2,reminder_mask1);
	r3=_mm256_and_si256(r3,reminder_mask1);
	r4=_mm256_and_si256(r4,reminder_mask1);



	//row=0
	//multiply with the mask
	m1=_mm256_maddubs_epi16(r1,c0);
	m2=_mm256_maddubs_epi16(r2,c1);
	m3=_mm256_maddubs_epi16(r3,c2);
	m4=_mm256_maddubs_epi16(r4,c3);

	//vertical add
	m0=_mm256_add_epi16(m1,m2);
	m0=_mm256_add_epi16(m0,m3);
	m0=_mm256_add_epi16(m0,m4);


	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);

	m1=_mm256_srli_si256(m0,4);
	m2=_mm256_add_epi16(m1,m2);

	//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
	//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
	m4=_mm256_and_si256(m0,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions

	m2=_mm256_add_epi16(m2,m4);

	//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
	output_row0_even=_mm256_and_si256(m2,output_mask);


	//row=1
	//multiply with the mask
	m2=_mm256_maddubs_epi16(r2,c0);
	m3=_mm256_maddubs_epi16(r3,c1);
	m4=_mm256_maddubs_epi16(r4,c2);

	//vertical add
	m0=_mm256_add_epi16(m2,m3);
	m0=_mm256_add_epi16(m0,m4);


	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);

	m1=_mm256_srli_si256(m0,4);
	m2=_mm256_add_epi16(m1,m2);

	//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
	//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
	m4=_mm256_and_si256(m0,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions

	m2=_mm256_add_epi16(m2,m4);

	//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
	output_row1_even=_mm256_and_si256(m2,output_mask);

	//2nd col iteration



	//row=0
	//multiply with the mask
	m1=_mm256_maddubs_epi16(r1,c0_sh1);
	m2=_mm256_maddubs_epi16(r2,c1_sh1);
	m3=_mm256_maddubs_epi16(r3,c2_sh1);
	m4=_mm256_maddubs_epi16(r4,c3_sh1);

	//vertical add
	m0=_mm256_add_epi16(m1,m2);
	m0=_mm256_add_epi16(m0,m3);
	m0=_mm256_add_epi16(m0,m4);


	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);

	m1=_mm256_srli_si256(m0,4);
	m2=_mm256_add_epi16(m1,m2);

	//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
	//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
	m4=_mm256_and_si256(m0,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions
	m2=_mm256_add_epi16(m2,m4);

	//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
	output_row0_odd = _mm256_and_si256(m2,output_mask);

	//row=1
	//multiply with the mask
	m2=_mm256_maddubs_epi16(r2,c0_sh1);
	m3=_mm256_maddubs_epi16(r3,c1_sh1);
	m4=_mm256_maddubs_epi16(r4,c2_sh1);

	//vertical add
	m0=_mm256_add_epi16(m2,m3);
	m0=_mm256_add_epi16(m0,m4);

	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);

	m1=_mm256_srli_si256(m0,4);
	m2=_mm256_add_epi16(m1,m2);

	//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
	//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
	m4=_mm256_and_si256(m0,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions
	m2=_mm256_add_epi16(m2,m4);

	//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
	output_row1_odd = _mm256_and_si256(m2,output_mask);

             //3rd col iteration


	//row=0
	//multiply with the mask
	m1=_mm256_maddubs_epi16(r1,c0_sh2);
	m2=_mm256_maddubs_epi16(r2,c1_sh2);
	m3=_mm256_maddubs_epi16(r3,c2_sh2);
	m4=_mm256_maddubs_epi16(r4,c3_sh2);

	//vertical add
	m0=_mm256_add_epi16(m1,m2);
	m0=_mm256_add_epi16(m0,m3);
	m0=_mm256_add_epi16(m0,m4);


	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);

	//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,_mm256_extract_epi16(m2,8),0,0,0,0,0,0,0);
	m4=_mm256_and_si256(m2,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions

	m1=_mm256_srli_si256(m0,4);
	m2=_mm256_add_epi16(m1,m2);

	//prepei na prosthesw to pixel pu thelw sto r2 kai meta na kanw diairesi me ola mazi

	m2=_mm256_add_epi16(m2,m4);

	//m2 has 16 16bit values now. the results I need are in positions 1,4,7,10,13. keep only those, discard others
	m2 = _mm256_and_si256(m2,output_mask_sh1);
	output_row0_even = _mm256_add_epi16(output_row0_even,m2);

	//row=1
	//multiply with the mask
	m2=_mm256_maddubs_epi16(r2,c0_sh2);
	m3=_mm256_maddubs_epi16(r3,c1_sh2);
	m4=_mm256_maddubs_epi16(r4,c2_sh2);

	//vertical add
	m0=_mm256_add_epi16(m2,m3);
	m0=_mm256_add_epi16(m0,m4);


	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);

	//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,_mm256_extract_epi16(m2,8),0,0,0,0,0,0,0);
	m4=_mm256_and_si256(m2,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions

	m1=_mm256_srli_si256(m0,4);
	m2=_mm256_add_epi16(m1,m2);

	//prepei na prosthesw to pixel pu thelw sto r2 kai meta na kanw diairesi me ola mazi

	m2=_mm256_add_epi16(m2,m4);

	//m2 has 16 16bit values now. the results I need are in positions 1,4,7,10,13. keep only those, discard others
	m2 = _mm256_and_si256(m2,output_mask_sh1);
	output_row1_even = _mm256_add_epi16(output_row1_even,m2);

	//4th col iteration
	//load the 5 rows
	r1=_mm256_loadu_si256( (__m256i *) &frame1[N-4][col+3-2]);
	r2=_mm256_loadu_si256( (__m256i *) &frame1[N-3][col+3-2]);
	r3=_mm256_loadu_si256( (__m256i *) &frame1[N-2][col+3-2]);
	r4=_mm256_loadu_si256( (__m256i *) &frame1[N-1][col+3-2]);

	//AND r0-r4 with reminder_mask
	r1=_mm256_and_si256(r1,reminder_mask2);
	r2=_mm256_and_si256(r2,reminder_mask2);
	r3=_mm256_and_si256(r3,reminder_mask2);
	r4=_mm256_and_si256(r4,reminder_mask2);



	//row=0
	//multiply with the mask
	m1=_mm256_maddubs_epi16(r1,c0);
	m2=_mm256_maddubs_epi16(r2,c1);
	m3=_mm256_maddubs_epi16(r3,c2);
	m4=_mm256_maddubs_epi16(r4,c3);

	//vertical add
	m0=_mm256_add_epi16(m1,m2);
	m0=_mm256_add_epi16(m0,m3);
	m0=_mm256_add_epi16(m0,m4);


	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);

	m1=_mm256_srli_si256(m0,4);
	m2=_mm256_add_epi16(m1,m2);

	//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
	//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
	m4=_mm256_and_si256(m0,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions

	m2=_mm256_add_epi16(m2,m4);

	//m2 has 16 16bit values now. the results I need are in positions 1,4,7,10,13. keep only those, discard others
	m2 = _mm256_and_si256(m2,output_mask);
	m2 = _mm256_slli_si256(m2,2);
	output_row0_odd = _mm256_add_epi16(output_row0_odd,m2);

	//row=1
	//multiply with the mask
	m2=_mm256_maddubs_epi16(r2,c0);
	m3=_mm256_maddubs_epi16(r3,c1);
	m4=_mm256_maddubs_epi16(r4,c2);

	//vertical add
	m0=_mm256_add_epi16(m2,m3);
	m0=_mm256_add_epi16(m0,m4);


	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);

	m1=_mm256_srli_si256(m0,4);
	m2=_mm256_add_epi16(m1,m2);

	//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
	//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
	m4=_mm256_and_si256(m0,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions

	m2=_mm256_add_epi16(m2,m4);

	//m2 has 16 16bit values now. the results I need are in positions 1,4,7,10,13. keep only those, discard others
	m2 = _mm256_and_si256(m2,output_mask);
	m2 = _mm256_slli_si256(m2,2);
	output_row1_odd = _mm256_add_epi16(output_row1_odd,m2);

	//5th col iteration



	//row=0
	//multiply with the mask
	m1=_mm256_maddubs_epi16(r1,c0_sh1);
	m2=_mm256_maddubs_epi16(r2,c1_sh1);
	m3=_mm256_maddubs_epi16(r3,c2_sh1);
	m4=_mm256_maddubs_epi16(r4,c3_sh1);

	//vertical add
	m0=_mm256_add_epi16(m1,m2);
	m0=_mm256_add_epi16(m0,m3);
	m0=_mm256_add_epi16(m0,m4);



	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);

	m1=_mm256_srli_si256(m0,4);
	m2=_mm256_add_epi16(m1,m2);

	//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
	//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
	m4=_mm256_and_si256(m0,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions

	m2=_mm256_add_epi16(m2,m4);

	//m2 has 16 16bit values now.
	m2 = _mm256_and_si256(m2,output_mask); //holds results in positions 0,3,6,9,12
	m1 = _mm256_slli_si256(m2,4); //put results in positions 2,5,8,11,14 , but 8 is not written
	m2=_mm256_and_si256(m2,mask_new);//extract only the 6th
	m2=_mm256_permute2f128_si256(m2,m2,1);
	m2=_mm256_srli_si256(m2,12);
	m2 = _mm256_add_epi16(m1,m2);
	output_row0_even = _mm256_add_epi16(output_row0_even,m2);

	//row=1
	//multiply with the mask
	m2=_mm256_maddubs_epi16(r2,c0_sh1);
	m3=_mm256_maddubs_epi16(r3,c1_sh1);
	m4=_mm256_maddubs_epi16(r4,c2_sh1);

	//vertical add
	m0=_mm256_add_epi16(m2,m3);
	m0=_mm256_add_epi16(m0,m4);


	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);

	m1=_mm256_srli_si256(m0,4);
	m2=_mm256_add_epi16(m1,m2);

	//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
	//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
	m4=_mm256_and_si256(m0,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions

	m2=_mm256_add_epi16(m2,m4);

	//m2 has 16 16bit values now.
	m2 = _mm256_and_si256(m2,output_mask); //holds results in positions 0,3,6,9,12
	m1 = _mm256_slli_si256(m2,4); //put results in positions 2,5,8,11,14 , but 8 is not written
	m2=_mm256_and_si256(m2,mask_new);//extract only the 6th
	m2=_mm256_permute2f128_si256(m2,m2,1);
	m2=_mm256_srli_si256(m2,12);
	m2 = _mm256_add_epi16(m1,m2);
	output_row1_even = _mm256_add_epi16(output_row1_even,m2);

             //6th col iteration

	//row=0
	//multiply with the mask
	m1=_mm256_maddubs_epi16(r1,c0_sh2);
	m2=_mm256_maddubs_epi16(r2,c1_sh2);
	m3=_mm256_maddubs_epi16(r3,c2_sh2);
	m4=_mm256_maddubs_epi16(r4,c3_sh2);

	//vertical add
	m0=_mm256_add_epi16(m1,m2);
	m0=_mm256_add_epi16(m0,m3);
	m0=_mm256_add_epi16(m0,m4);


	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);
	//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,_mm256_extract_epi16(m2,8),0,0,0,0,0,0,0);
	m4=_mm256_and_si256(m2,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions

	m1=_mm256_srli_si256(m0,4);
	m2=_mm256_add_epi16(m1,m2);
	m2=_mm256_add_epi16(m2,m4);

	//m2 has 16 16bit values now.
	m2 = _mm256_and_si256(m2,output_mask_sh1);//holds results in positions 1,4,7,10,13
	m1 = _mm256_slli_si256(m2,2); //put results in positions 2,5,8,11,14 , but 8 is not written
	m2=_mm256_and_si256(m2,mask_new2);//extract only the 7th
	m2=_mm256_permute2f128_si256(m2,m2,1);
	m2=_mm256_srli_si256(m2,14);
	m2 = _mm256_add_epi16(m1,m2);
	output_row0_odd = _mm256_add_epi16(output_row0_odd,m2);


	//now division follows
	output_row0_even=division(division_case,output_row0_even,f);
	output_row0_odd=division(division_case,output_row0_odd,f);

	//shift odd 1 position and add to even
	output_row0_odd = _mm256_slli_si256(output_row0_odd,1);
	output_row0_even = _mm256_add_epi8(output_row0_even,output_row0_odd);

	//row=1
	//multiply with the mask
	m2=_mm256_maddubs_epi16(r2,c0_sh2);
	m3=_mm256_maddubs_epi16(r3,c1_sh2);
	m4=_mm256_maddubs_epi16(r4,c2_sh2);

	//vertical add
	m0=_mm256_add_epi16(m2,m3);
	m0=_mm256_add_epi16(m0,m4);


	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);
	//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,_mm256_extract_epi16(m2,8),0,0,0,0,0,0,0);
	m4=_mm256_and_si256(m2,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions

	m1=_mm256_srli_si256(m0,4);
	m2=_mm256_add_epi16(m1,m2);
	m2=_mm256_add_epi16(m2,m4);

	//m2 has 16 16bit values now.
	m2 = _mm256_and_si256(m2,output_mask_sh1);//holds results in positions 1,4,7,10,13
	m1 = _mm256_slli_si256(m2,2); //put results in positions 2,5,8,11,14 , but 8 is not written
	m2=_mm256_and_si256(m2,mask_new2);//extract only the 7th
	m2=_mm256_permute2f128_si256(m2,m2,1);
	m2=_mm256_srli_si256(m2,14);
	m2 = _mm256_add_epi16(m1,m2);
	output_row1_odd = _mm256_add_epi16(output_row1_odd,m2);


	//now division follows
	output_row1_even=division(division_case,output_row1_even,f);
	output_row1_odd=division(division_case,output_row1_odd,f);

	//shift odd 1 position and add to even
	output_row1_odd = _mm256_slli_si256(output_row1_odd,1);
	output_row1_even = _mm256_add_epi8(output_row1_even,output_row1_odd);




	switch (REMINDER_ITERATIONS){
	case 1:
		filt[N-2][col] = (unsigned char) _mm256_extract_epi8(output_row0_even,0);
		filt[N-1][col] = (unsigned char) _mm256_extract_epi8(output_row1_even,0);
		 break;
	case 2:

		filt[N-2][col] = (unsigned char) _mm256_extract_epi8(output_row0_even,0);
		filt[N-2][col+1] = (unsigned char) _mm256_extract_epi8(output_row0_even,1);

		filt[N-1][col] = (unsigned char) _mm256_extract_epi8(output_row1_even,0);
		filt[N-1][col+1] = (unsigned char) _mm256_extract_epi8(output_row1_even,1);
		 break;
	case 3:

		filt[N-2][col] = (unsigned char) _mm256_extract_epi8(output_row0_even,0);
		filt[N-2][col+1] = (unsigned char) _mm256_extract_epi8(output_row0_even,1);
		filt[N-2][col+2] = (unsigned char) _mm256_extract_epi8(output_row0_even,2);

		filt[N-1][col] = (unsigned char) _mm256_extract_epi8(output_row1_even,0);
		filt[N-1][col+1] = (unsigned char) _mm256_extract_epi8(output_row1_even,1);
		filt[N-1][col+2] = (unsigned char) _mm256_extract_epi8(output_row1_even,2);
		  break;
	case 4:

		filt[N-2][col] = (unsigned char) _mm256_extract_epi8(output_row0_even,0);
		filt[N-2][col+1] = (unsigned char) _mm256_extract_epi8(output_row0_even,1);
		filt[N-2][col+2] = (unsigned char) _mm256_extract_epi8(output_row0_even,2);
		filt[N-2][col+3] = (unsigned char) _mm256_extract_epi8(output_row0_even,3);

		filt[N-1][col] = (unsigned char) _mm256_extract_epi8(output_row1_even,0);
		filt[N-1][col+1] = (unsigned char) _mm256_extract_epi8(output_row1_even,1);
		filt[N-1][col+2] = (unsigned char) _mm256_extract_epi8(output_row1_even,2);
		filt[N-1][col+3] = (unsigned char) _mm256_extract_epi8(output_row1_even,3);
		  break;
	case 5:

		filt[N-2][col] = (unsigned char) _mm256_extract_epi8(output_row0_even,0);
		filt[N-2][col+1] = (unsigned char) _mm256_extract_epi8(output_row0_even,1);
		filt[N-2][col+2] = (unsigned char) _mm256_extract_epi8(output_row0_even,2);
		filt[N-2][col+3] = (unsigned char) _mm256_extract_epi8(output_row0_even,3);
		filt[N-2][col+4] = (unsigned char) _mm256_extract_epi8(output_row0_even,4);

		filt[N-1][col] = (unsigned char) _mm256_extract_epi8(output_row1_even,0);
		filt[N-1][col+1] = (unsigned char) _mm256_extract_epi8(output_row1_even,1);
		filt[N-1][col+2] = (unsigned char) _mm256_extract_epi8(output_row1_even,2);
		filt[N-1][col+3] = (unsigned char) _mm256_extract_epi8(output_row1_even,3);
		filt[N-1][col+4] = (unsigned char) _mm256_extract_epi8(output_row1_even,4);
		  break;
	case 6:
		filt[N-2][col] = (unsigned char) _mm256_extract_epi8(output_row0_even,0);
		filt[N-2][col+1] = (unsigned char) _mm256_extract_epi8(output_row0_even,1);
		filt[N-2][col+2] = (unsigned char) _mm256_extract_epi8(output_row0_even,2);
		filt[N-2][col+3] = (unsigned char) _mm256_extract_epi8(output_row0_even,3);
		filt[N-2][col+4] = (unsigned char) _mm256_extract_epi8(output_row0_even,4);
		filt[N-2][col+5] = (unsigned char) _mm256_extract_epi8(output_row0_even,5);

		filt[N-1][col] = (unsigned char) _mm256_extract_epi8(output_row1_even,0);
		filt[N-1][col+1] = (unsigned char) _mm256_extract_epi8(output_row1_even,1);
		filt[N-1][col+2] = (unsigned char) _mm256_extract_epi8(output_row1_even,2);
		filt[N-1][col+3] = (unsigned char) _mm256_extract_epi8(output_row1_even,3);
		filt[N-1][col+4] = (unsigned char) _mm256_extract_epi8(output_row1_even,4);
		filt[N-1][col+5] = (unsigned char) _mm256_extract_epi8(output_row1_even,5);
		  break;
	case 7:
		filt[N-2][col] = (unsigned char) _mm256_extract_epi8(output_row0_even,0);
		filt[N-2][col+1] = (unsigned char) _mm256_extract_epi8(output_row0_even,1);
		filt[N-2][col+2] = (unsigned char) _mm256_extract_epi8(output_row0_even,2);
		filt[N-2][col+3] = (unsigned char) _mm256_extract_epi8(output_row0_even,3);
		filt[N-2][col+4] = (unsigned char) _mm256_extract_epi8(output_row0_even,4);
		filt[N-2][col+5] = (unsigned char) _mm256_extract_epi8(output_row0_even,5);
		filt[N-2][col+6] = (unsigned char) _mm256_extract_epi8(output_row0_even,6);

		filt[N-1][col] = (unsigned char) _mm256_extract_epi8(output_row1_even,0);
		filt[N-1][col+1] = (unsigned char) _mm256_extract_epi8(output_row1_even,1);
		filt[N-1][col+2] = (unsigned char) _mm256_extract_epi8(output_row1_even,2);
		filt[N-1][col+3] = (unsigned char) _mm256_extract_epi8(output_row1_even,3);
		filt[N-1][col+4] = (unsigned char) _mm256_extract_epi8(output_row1_even,4);
		filt[N-1][col+5] = (unsigned char) _mm256_extract_epi8(output_row1_even,5);
		filt[N-1][col+6] = (unsigned char) _mm256_extract_epi8(output_row1_even,6);
		  break;
	case 8:
		filt[N-2][col] = (unsigned char) _mm256_extract_epi8(output_row0_even,0);
		filt[N-2][col+1] = (unsigned char) _mm256_extract_epi8(output_row0_even,1);
		filt[N-2][col+2] = (unsigned char) _mm256_extract_epi8(output_row0_even,2);
		filt[N-2][col+3] = (unsigned char) _mm256_extract_epi8(output_row0_even,3);
		filt[N-2][col+4] = (unsigned char) _mm256_extract_epi8(output_row0_even,4);
		filt[N-2][col+5] = (unsigned char) _mm256_extract_epi8(output_row0_even,5);
		filt[N-2][col+6] = (unsigned char) _mm256_extract_epi8(output_row0_even,6);
		filt[N-2][col+7] = (unsigned char) _mm256_extract_epi8(output_row0_even,7);

		filt[N-1][col] = (unsigned char) _mm256_extract_epi8(output_row1_even,0);
		filt[N-1][col+1] = (unsigned char) _mm256_extract_epi8(output_row1_even,1);
		filt[N-1][col+2] = (unsigned char) _mm256_extract_epi8(output_row1_even,2);
		filt[N-1][col+3] = (unsigned char) _mm256_extract_epi8(output_row1_even,3);
		filt[N-1][col+4] = (unsigned char) _mm256_extract_epi8(output_row1_even,4);
		filt[N-1][col+5] = (unsigned char) _mm256_extract_epi8(output_row1_even,5);
		filt[N-1][col+6] = (unsigned char) _mm256_extract_epi8(output_row1_even,6);
		filt[N-1][col+7] = (unsigned char) _mm256_extract_epi8(output_row1_even,7);
		  break;
	case 9:

		filt[N-2][col] = (unsigned char) _mm256_extract_epi8(output_row0_even,0);
		filt[N-2][col+1] = (unsigned char) _mm256_extract_epi8(output_row0_even,1);
		filt[N-2][col+2] = (unsigned char) _mm256_extract_epi8(output_row0_even,2);
		filt[N-2][col+3] = (unsigned char) _mm256_extract_epi8(output_row0_even,3);
		filt[N-2][col+4] = (unsigned char) _mm256_extract_epi8(output_row0_even,4);
		filt[N-2][col+5] = (unsigned char) _mm256_extract_epi8(output_row0_even,5);
		filt[N-2][col+6] = (unsigned char) _mm256_extract_epi8(output_row0_even,6);
		filt[N-2][col+7] = (unsigned char) _mm256_extract_epi8(output_row0_even,7);
		filt[N-2][col+8] = (unsigned char) _mm256_extract_epi8(output_row0_even,8);

		filt[N-1][col] = (unsigned char) _mm256_extract_epi8(output_row1_even,0);
		filt[N-1][col+1] = (unsigned char) _mm256_extract_epi8(output_row1_even,1);
		filt[N-1][col+2] = (unsigned char) _mm256_extract_epi8(output_row1_even,2);
		filt[N-1][col+3] = (unsigned char) _mm256_extract_epi8(output_row1_even,3);
		filt[N-1][col+4] = (unsigned char) _mm256_extract_epi8(output_row1_even,4);
		filt[N-1][col+5] = (unsigned char) _mm256_extract_epi8(output_row1_even,5);
		filt[N-1][col+6] = (unsigned char) _mm256_extract_epi8(output_row1_even,6);
		filt[N-1][col+7] = (unsigned char) _mm256_extract_epi8(output_row1_even,7);
		filt[N-1][col+8] = (unsigned char) _mm256_extract_epi8(output_row1_even,8);
		  break;
	case 10:

		filt[N-2][col] = (unsigned char) _mm256_extract_epi8(output_row0_even,0);
		filt[N-2][col+1] = (unsigned char) _mm256_extract_epi8(output_row0_even,1);
		filt[N-2][col+2] = (unsigned char) _mm256_extract_epi8(output_row0_even,2);
		filt[N-2][col+3] = (unsigned char) _mm256_extract_epi8(output_row0_even,3);
		filt[N-2][col+4] = (unsigned char) _mm256_extract_epi8(output_row0_even,4);
		filt[N-2][col+5] = (unsigned char) _mm256_extract_epi8(output_row0_even,5);
		filt[N-2][col+6] = (unsigned char) _mm256_extract_epi8(output_row0_even,6);
		filt[N-2][col+7] = (unsigned char) _mm256_extract_epi8(output_row0_even,7);
		filt[N-2][col+8] = (unsigned char) _mm256_extract_epi8(output_row0_even,8);
		filt[N-2][col+9] = (unsigned char) _mm256_extract_epi8(output_row0_even,9);

		filt[N-1][col] = (unsigned char) _mm256_extract_epi8(output_row1_even,0);
		filt[N-1][col+1] = (unsigned char) _mm256_extract_epi8(output_row1_even,1);
		filt[N-1][col+2] = (unsigned char) _mm256_extract_epi8(output_row1_even,2);
		filt[N-1][col+3] = (unsigned char) _mm256_extract_epi8(output_row1_even,3);
		filt[N-1][col+4] = (unsigned char) _mm256_extract_epi8(output_row1_even,4);
		filt[N-1][col+5] = (unsigned char) _mm256_extract_epi8(output_row1_even,5);
		filt[N-1][col+6] = (unsigned char) _mm256_extract_epi8(output_row1_even,6);
		filt[N-1][col+7] = (unsigned char) _mm256_extract_epi8(output_row1_even,7);
		filt[N-1][col+8] = (unsigned char) _mm256_extract_epi8(output_row1_even,8);
		filt[N-1][col+9] = (unsigned char) _mm256_extract_epi8(output_row1_even,9);
		  break;
	case 11:

		filt[N-2][col] = (unsigned char) _mm256_extract_epi8(output_row0_even,0);
		filt[N-2][col+1] = (unsigned char) _mm256_extract_epi8(output_row0_even,1);
		filt[N-2][col+2] = (unsigned char) _mm256_extract_epi8(output_row0_even,2);
		filt[N-2][col+3] = (unsigned char) _mm256_extract_epi8(output_row0_even,3);
		filt[N-2][col+4] = (unsigned char) _mm256_extract_epi8(output_row0_even,4);
		filt[N-2][col+5] = (unsigned char) _mm256_extract_epi8(output_row0_even,5);
		filt[N-2][col+6] = (unsigned char) _mm256_extract_epi8(output_row0_even,6);
		filt[N-2][col+7] = (unsigned char) _mm256_extract_epi8(output_row0_even,7);
		filt[N-2][col+8] = (unsigned char) _mm256_extract_epi8(output_row0_even,8);
		filt[N-2][col+9] = (unsigned char) _mm256_extract_epi8(output_row0_even,9);
		filt[N-2][col+10] = (unsigned char) _mm256_extract_epi8(output_row0_even,10);

		filt[N-1][col] = (unsigned char) _mm256_extract_epi8(output_row1_even,0);
		filt[N-1][col+1] = (unsigned char) _mm256_extract_epi8(output_row1_even,1);
		filt[N-1][col+2] = (unsigned char) _mm256_extract_epi8(output_row1_even,2);
		filt[N-1][col+3] = (unsigned char) _mm256_extract_epi8(output_row1_even,3);
		filt[N-1][col+4] = (unsigned char) _mm256_extract_epi8(output_row1_even,4);
		filt[N-1][col+5] = (unsigned char) _mm256_extract_epi8(output_row1_even,5);
		filt[N-1][col+6] = (unsigned char) _mm256_extract_epi8(output_row1_even,6);
		filt[N-1][col+7] = (unsigned char) _mm256_extract_epi8(output_row1_even,7);
		filt[N-1][col+8] = (unsigned char) _mm256_extract_epi8(output_row1_even,8);
		filt[N-1][col+9] = (unsigned char) _mm256_extract_epi8(output_row1_even,9);
		filt[N-1][col+10] = (unsigned char) _mm256_extract_epi8(output_row1_even,10);
		  break;
	case 12:

		filt[N-2][col] = (unsigned char) _mm256_extract_epi8(output_row0_even,0);
		filt[N-2][col+1] = (unsigned char) _mm256_extract_epi8(output_row0_even,1);
		filt[N-2][col+2] = (unsigned char) _mm256_extract_epi8(output_row0_even,2);
		filt[N-2][col+3] = (unsigned char) _mm256_extract_epi8(output_row0_even,3);
		filt[N-2][col+4] = (unsigned char) _mm256_extract_epi8(output_row0_even,4);
		filt[N-2][col+5] = (unsigned char) _mm256_extract_epi8(output_row0_even,5);
		filt[N-2][col+6] = (unsigned char) _mm256_extract_epi8(output_row0_even,6);
		filt[N-2][col+7] = (unsigned char) _mm256_extract_epi8(output_row0_even,7);
		filt[N-2][col+8] = (unsigned char) _mm256_extract_epi8(output_row0_even,8);
		filt[N-2][col+9] = (unsigned char) _mm256_extract_epi8(output_row0_even,9);
		filt[N-2][col+10] = (unsigned char) _mm256_extract_epi8(output_row0_even,10);
		filt[N-2][col+11] = (unsigned char) _mm256_extract_epi8(output_row0_even,11);

		filt[N-1][col] = (unsigned char) _mm256_extract_epi8(output_row1_even,0);
		filt[N-1][col+1] = (unsigned char) _mm256_extract_epi8(output_row1_even,1);
		filt[N-1][col+2] = (unsigned char) _mm256_extract_epi8(output_row1_even,2);
		filt[N-1][col+3] = (unsigned char) _mm256_extract_epi8(output_row1_even,3);
		filt[N-1][col+4] = (unsigned char) _mm256_extract_epi8(output_row1_even,4);
		filt[N-1][col+5] = (unsigned char) _mm256_extract_epi8(output_row1_even,5);
		filt[N-1][col+6] = (unsigned char) _mm256_extract_epi8(output_row1_even,6);
		filt[N-1][col+7] = (unsigned char) _mm256_extract_epi8(output_row1_even,7);
		filt[N-1][col+8] = (unsigned char) _mm256_extract_epi8(output_row1_even,8);
		filt[N-1][col+9] = (unsigned char) _mm256_extract_epi8(output_row1_even,9);
		filt[N-1][col+10] = (unsigned char) _mm256_extract_epi8(output_row1_even,10);
		filt[N-1][col+11] = (unsigned char) _mm256_extract_epi8(output_row1_even,11);
		  break;
	case 13:

		filt[N-2][col] = (unsigned char) _mm256_extract_epi8(output_row0_even,0);
		filt[N-2][col+1] = (unsigned char) _mm256_extract_epi8(output_row0_even,1);
		filt[N-2][col+2] = (unsigned char) _mm256_extract_epi8(output_row0_even,2);
		filt[N-2][col+3] = (unsigned char) _mm256_extract_epi8(output_row0_even,3);
		filt[N-2][col+4] = (unsigned char) _mm256_extract_epi8(output_row0_even,4);
		filt[N-2][col+5] = (unsigned char) _mm256_extract_epi8(output_row0_even,5);
		filt[N-2][col+6] = (unsigned char) _mm256_extract_epi8(output_row0_even,6);
		filt[N-2][col+7] = (unsigned char) _mm256_extract_epi8(output_row0_even,7);
		filt[N-2][col+8] = (unsigned char) _mm256_extract_epi8(output_row0_even,8);
		filt[N-2][col+9] = (unsigned char) _mm256_extract_epi8(output_row0_even,9);
		filt[N-2][col+10] = (unsigned char) _mm256_extract_epi8(output_row0_even,10);
		filt[N-2][col+11] = (unsigned char) _mm256_extract_epi8(output_row0_even,11);
		filt[N-2][col+12] = (unsigned char) _mm256_extract_epi8(output_row0_even,12);

		filt[N-1][col] = (unsigned char) _mm256_extract_epi8(output_row1_even,0);
		filt[N-1][col+1] = (unsigned char) _mm256_extract_epi8(output_row1_even,1);
		filt[N-1][col+2] = (unsigned char) _mm256_extract_epi8(output_row1_even,2);
		filt[N-1][col+3] = (unsigned char) _mm256_extract_epi8(output_row1_even,3);
		filt[N-1][col+4] = (unsigned char) _mm256_extract_epi8(output_row1_even,4);
		filt[N-1][col+5] = (unsigned char) _mm256_extract_epi8(output_row1_even,5);
		filt[N-1][col+6] = (unsigned char) _mm256_extract_epi8(output_row1_even,6);
		filt[N-1][col+7] = (unsigned char) _mm256_extract_epi8(output_row1_even,7);
		filt[N-1][col+8] = (unsigned char) _mm256_extract_epi8(output_row1_even,8);
		filt[N-1][col+9] = (unsigned char) _mm256_extract_epi8(output_row1_even,9);
		filt[N-1][col+10] = (unsigned char) _mm256_extract_epi8(output_row1_even,10);
		filt[N-1][col+11] = (unsigned char) _mm256_extract_epi8(output_row1_even,11);
		filt[N-1][col+12] = (unsigned char) _mm256_extract_epi8(output_row1_even,12);
		  break;
	case 14:

		filt[N-2][col] = (unsigned char) _mm256_extract_epi8(output_row0_even,0);
		filt[N-2][col+1] = (unsigned char) _mm256_extract_epi8(output_row0_even,1);
		filt[N-2][col+2] = (unsigned char) _mm256_extract_epi8(output_row0_even,2);
		filt[N-2][col+3] = (unsigned char) _mm256_extract_epi8(output_row0_even,3);
		filt[N-2][col+4] = (unsigned char) _mm256_extract_epi8(output_row0_even,4);
		filt[N-2][col+5] = (unsigned char) _mm256_extract_epi8(output_row0_even,5);
		filt[N-2][col+6] = (unsigned char) _mm256_extract_epi8(output_row0_even,6);
		filt[N-2][col+7] = (unsigned char) _mm256_extract_epi8(output_row0_even,7);
		filt[N-2][col+8] = (unsigned char) _mm256_extract_epi8(output_row0_even,8);
		filt[N-2][col+9] = (unsigned char) _mm256_extract_epi8(output_row0_even,9);
		filt[N-2][col+10] = (unsigned char) _mm256_extract_epi8(output_row0_even,10);
		filt[N-2][col+11] = (unsigned char) _mm256_extract_epi8(output_row0_even,11);
		filt[N-2][col+12] = (unsigned char) _mm256_extract_epi8(output_row0_even,12);
		filt[N-2][col+13] = (unsigned char) _mm256_extract_epi8(output_row0_even,13);

		filt[N-1][col] = (unsigned char) _mm256_extract_epi8(output_row1_even,0);
		filt[N-1][col+1] = (unsigned char) _mm256_extract_epi8(output_row1_even,1);
		filt[N-1][col+2] = (unsigned char) _mm256_extract_epi8(output_row1_even,2);
		filt[N-1][col+3] = (unsigned char) _mm256_extract_epi8(output_row1_even,3);
		filt[N-1][col+4] = (unsigned char) _mm256_extract_epi8(output_row1_even,4);
		filt[N-1][col+5] = (unsigned char) _mm256_extract_epi8(output_row1_even,5);
		filt[N-1][col+6] = (unsigned char) _mm256_extract_epi8(output_row1_even,6);
		filt[N-1][col+7] = (unsigned char) _mm256_extract_epi8(output_row1_even,7);
		filt[N-1][col+8] = (unsigned char) _mm256_extract_epi8(output_row1_even,8);
		filt[N-1][col+9] = (unsigned char) _mm256_extract_epi8(output_row1_even,9);
		filt[N-1][col+10] = (unsigned char) _mm256_extract_epi8(output_row1_even,10);
		filt[N-1][col+11] = (unsigned char) _mm256_extract_epi8(output_row1_even,11);
		filt[N-1][col+12] = (unsigned char) _mm256_extract_epi8(output_row1_even,12);
		filt[N-1][col+13] = (unsigned char) _mm256_extract_epi8(output_row1_even,13);
		  break;
	case 15:

		filt[N-2][col] = (unsigned char) _mm256_extract_epi8(output_row0_even,0);
		filt[N-2][col+1] = (unsigned char) _mm256_extract_epi8(output_row0_even,1);
		filt[N-2][col+2] = (unsigned char) _mm256_extract_epi8(output_row0_even,2);
		filt[N-2][col+3] = (unsigned char) _mm256_extract_epi8(output_row0_even,3);
		filt[N-2][col+4] = (unsigned char) _mm256_extract_epi8(output_row0_even,4);
		filt[N-2][col+5] = (unsigned char) _mm256_extract_epi8(output_row0_even,5);
		filt[N-2][col+6] = (unsigned char) _mm256_extract_epi8(output_row0_even,6);
		filt[N-2][col+7] = (unsigned char) _mm256_extract_epi8(output_row0_even,7);
		filt[N-2][col+8] = (unsigned char) _mm256_extract_epi8(output_row0_even,8);
		filt[N-2][col+9] = (unsigned char) _mm256_extract_epi8(output_row0_even,9);
		filt[N-2][col+10] = (unsigned char) _mm256_extract_epi8(output_row0_even,10);
		filt[N-2][col+11] = (unsigned char) _mm256_extract_epi8(output_row0_even,11);
		filt[N-2][col+12] = (unsigned char) _mm256_extract_epi8(output_row0_even,12);
		filt[N-2][col+13] = (unsigned char) _mm256_extract_epi8(output_row0_even,13);
		filt[N-2][col+14] = (unsigned char) _mm256_extract_epi8(output_row0_even,14);

		filt[N-1][col] = (unsigned char) _mm256_extract_epi8(output_row1_even,0);
		filt[N-1][col+1] = (unsigned char) _mm256_extract_epi8(output_row1_even,1);
		filt[N-1][col+2] = (unsigned char) _mm256_extract_epi8(output_row1_even,2);
		filt[N-1][col+3] = (unsigned char) _mm256_extract_epi8(output_row1_even,3);
		filt[N-1][col+4] = (unsigned char) _mm256_extract_epi8(output_row1_even,4);
		filt[N-1][col+5] = (unsigned char) _mm256_extract_epi8(output_row1_even,5);
		filt[N-1][col+6] = (unsigned char) _mm256_extract_epi8(output_row1_even,6);
		filt[N-1][col+7] = (unsigned char) _mm256_extract_epi8(output_row1_even,7);
		filt[N-1][col+8] = (unsigned char) _mm256_extract_epi8(output_row1_even,8);
		filt[N-1][col+9] = (unsigned char) _mm256_extract_epi8(output_row1_even,9);
		filt[N-1][col+10] = (unsigned char) _mm256_extract_epi8(output_row1_even,10);
		filt[N-1][col+11] = (unsigned char) _mm256_extract_epi8(output_row1_even,11);
		filt[N-1][col+12] = (unsigned char) _mm256_extract_epi8(output_row1_even,12);
		filt[N-1][col+13] = (unsigned char) _mm256_extract_epi8(output_row1_even,13);
		filt[N-1][col+14] = (unsigned char) _mm256_extract_epi8(output_row1_even,14);
		  break;
	case 16:
		_mm_storeu_si128( (__m128i *) &filt[N-2][col],_mm256_extractf128_si256(output_row0_even, 0)); //store low 128bit - 16pixels
		_mm_storeu_si128( (__m128i *) &filt[N-1][col],_mm256_extractf128_si256(output_row1_even, 0)); //store low 128bit - 16pixels
		  break;
	case 17:
		_mm_storeu_si128( (__m128i *) &filt[N-2][col],_mm256_extractf128_si256(output_row0_even, 0)); //store low 128bit - 16pixels
		filt[N-2][col+16] = (unsigned char) _mm256_extract_epi8(output_row0_even,16);

		_mm_storeu_si128( (__m128i *) &filt[N-1][col],_mm256_extractf128_si256(output_row1_even, 0)); //store low 128bit - 16pixels
		filt[N-1][col+16] = (unsigned char) _mm256_extract_epi8(output_row1_even,16);
		  break;
	case 18:

		_mm_storeu_si128( (__m128i *) &filt[N-2][col],_mm256_extractf128_si256(output_row0_even, 0)); //store low 128bit - 16pixels
		filt[N-2][col+16] = (unsigned char) _mm256_extract_epi8(output_row0_even,16);
		filt[N-2][col+17] = (unsigned char) _mm256_extract_epi8(output_row0_even,17);

		_mm_storeu_si128( (__m128i *) &filt[N-1][col],_mm256_extractf128_si256(output_row1_even, 0)); //store low 128bit - 16pixels
		filt[N-1][col+16] = (unsigned char) _mm256_extract_epi8(output_row1_even,16);
		filt[N-1][col+17] = (unsigned char) _mm256_extract_epi8(output_row1_even,17);
		  break;
	case 19:

		_mm_storeu_si128( (__m128i *) &filt[N-2][col],_mm256_extractf128_si256(output_row0_even, 0)); //store low 128bit - 16pixels
		filt[N-2][col+16] = (unsigned char) _mm256_extract_epi8(output_row0_even,16);
		filt[N-2][col+17] = (unsigned char) _mm256_extract_epi8(output_row0_even,17);
		filt[N-2][col+18] = (unsigned char) _mm256_extract_epi8(output_row0_even,18);

		_mm_storeu_si128( (__m128i *) &filt[N-1][col],_mm256_extractf128_si256(output_row1_even, 0)); //store low 128bit - 16pixels
		filt[N-1][col+16] = (unsigned char) _mm256_extract_epi8(output_row1_even,16);
		filt[N-1][col+17] = (unsigned char) _mm256_extract_epi8(output_row1_even,17);
		filt[N-1][col+18] = (unsigned char) _mm256_extract_epi8(output_row1_even,18);
		  break;
	case 20:

		_mm_storeu_si128( (__m128i *) &filt[N-2][col],_mm256_extractf128_si256(output_row0_even, 0)); //store low 128bit - 16pixels
		filt[N-2][col+16] = (unsigned char) _mm256_extract_epi8(output_row0_even,16);
		filt[N-2][col+17] = (unsigned char) _mm256_extract_epi8(output_row0_even,17);
		filt[N-2][col+18] = (unsigned char) _mm256_extract_epi8(output_row0_even,18);
		filt[N-2][col+19] = (unsigned char) _mm256_extract_epi8(output_row0_even,19);

		_mm_storeu_si128( (__m128i *) &filt[N-1][col],_mm256_extractf128_si256(output_row1_even, 0)); //store low 128bit - 16pixels
		filt[N-1][col+16] = (unsigned char) _mm256_extract_epi8(output_row1_even,16);
		filt[N-1][col+17] = (unsigned char) _mm256_extract_epi8(output_row1_even,17);
		filt[N-1][col+18] = (unsigned char) _mm256_extract_epi8(output_row1_even,18);
		filt[N-1][col+19] = (unsigned char) _mm256_extract_epi8(output_row1_even,19);
		  break;
	case 21:
		_mm_storeu_si128( (__m128i *) &filt[N-2][col],_mm256_extractf128_si256(output_row0_even, 0)); //store low 128bit - 16pixels
		filt[N-2][col+16] = (unsigned char) _mm256_extract_epi8(output_row0_even,16);
		filt[N-2][col+17] = (unsigned char) _mm256_extract_epi8(output_row0_even,17);
		filt[N-2][col+18] = (unsigned char) _mm256_extract_epi8(output_row0_even,18);
		filt[N-2][col+19] = (unsigned char) _mm256_extract_epi8(output_row0_even,19);
		filt[N-2][col+20] = (unsigned char) _mm256_extract_epi8(output_row0_even,20);

		_mm_storeu_si128( (__m128i *) &filt[N-1][col],_mm256_extractf128_si256(output_row1_even, 0)); //store low 128bit - 16pixels
		filt[N-1][col+16] = (unsigned char) _mm256_extract_epi8(output_row1_even,16);
		filt[N-1][col+17] = (unsigned char) _mm256_extract_epi8(output_row1_even,17);
		filt[N-1][col+18] = (unsigned char) _mm256_extract_epi8(output_row1_even,18);
		filt[N-1][col+19] = (unsigned char) _mm256_extract_epi8(output_row1_even,19);
		filt[N-1][col+20] = (unsigned char) _mm256_extract_epi8(output_row1_even,20);
		  break;
	case 22:
		_mm_storeu_si128( (__m128i *) &filt[N-2][col],_mm256_extractf128_si256(output_row0_even, 0)); //store low 128bit - 16pixels
		filt[N-2][col+16] = (unsigned char) _mm256_extract_epi8(output_row0_even,16);
		filt[N-2][col+17] = (unsigned char) _mm256_extract_epi8(output_row0_even,17);
		filt[N-2][col+18] = (unsigned char) _mm256_extract_epi8(output_row0_even,18);
		filt[N-2][col+19] = (unsigned char) _mm256_extract_epi8(output_row0_even,19);
		filt[N-2][col+20] = (unsigned char) _mm256_extract_epi8(output_row0_even,20);
		filt[N-2][col+21] = (unsigned char) _mm256_extract_epi8(output_row0_even,21);

		_mm_storeu_si128( (__m128i *) &filt[N-1][col],_mm256_extractf128_si256(output_row1_even, 0)); //store low 128bit - 16pixels
		filt[N-1][col+16] = (unsigned char) _mm256_extract_epi8(output_row1_even,16);
		filt[N-1][col+17] = (unsigned char) _mm256_extract_epi8(output_row1_even,17);
		filt[N-1][col+18] = (unsigned char) _mm256_extract_epi8(output_row1_even,18);
		filt[N-1][col+19] = (unsigned char) _mm256_extract_epi8(output_row1_even,19);
		filt[N-1][col+20] = (unsigned char) _mm256_extract_epi8(output_row1_even,20);
		filt[N-1][col+21] = (unsigned char) _mm256_extract_epi8(output_row1_even,21);
		  break;
	case 23:
		_mm_storeu_si128( (__m128i *) &filt[N-2][col],_mm256_extractf128_si256(output_row0_even, 0)); //store low 128bit - 16pixels
		filt[N-2][col+16] = (unsigned char) _mm256_extract_epi8(output_row0_even,16);
		filt[N-2][col+17] = (unsigned char) _mm256_extract_epi8(output_row0_even,17);
		filt[N-2][col+18] = (unsigned char) _mm256_extract_epi8(output_row0_even,18);
		filt[N-2][col+19] = (unsigned char) _mm256_extract_epi8(output_row0_even,19);
		filt[N-2][col+20] = (unsigned char) _mm256_extract_epi8(output_row0_even,20);
		filt[N-2][col+21] = (unsigned char) _mm256_extract_epi8(output_row0_even,21);
		filt[N-2][col+22] = (unsigned char) _mm256_extract_epi8(output_row0_even,22);

		_mm_storeu_si128( (__m128i *) &filt[N-1][col],_mm256_extractf128_si256(output_row1_even, 0)); //store low 128bit - 16pixels
		filt[N-1][col+16] = (unsigned char) _mm256_extract_epi8(output_row1_even,16);
		filt[N-1][col+17] = (unsigned char) _mm256_extract_epi8(output_row1_even,17);
		filt[N-1][col+18] = (unsigned char) _mm256_extract_epi8(output_row1_even,18);
		filt[N-1][col+19] = (unsigned char) _mm256_extract_epi8(output_row1_even,19);
		filt[N-1][col+20] = (unsigned char) _mm256_extract_epi8(output_row1_even,20);
		filt[N-1][col+21] = (unsigned char) _mm256_extract_epi8(output_row1_even,21);
		filt[N-1][col+22] = (unsigned char) _mm256_extract_epi8(output_row1_even,22);
		  break;
	case 24:
		_mm_storeu_si128( (__m128i *) &filt[N-2][col],_mm256_extractf128_si256(output_row0_even, 0)); //store low 128bit - 16pixels
		filt[N-2][col+16] = (unsigned char) _mm256_extract_epi8(output_row0_even,16);
		filt[N-2][col+17] = (unsigned char) _mm256_extract_epi8(output_row0_even,17);
		filt[N-2][col+18] = (unsigned char) _mm256_extract_epi8(output_row0_even,18);
		filt[N-2][col+19] = (unsigned char) _mm256_extract_epi8(output_row0_even,19);
		filt[N-2][col+20] = (unsigned char) _mm256_extract_epi8(output_row0_even,20);
		filt[N-2][col+21] = (unsigned char) _mm256_extract_epi8(output_row0_even,21);
		filt[N-2][col+22] = (unsigned char) _mm256_extract_epi8(output_row0_even,22);
		filt[N-2][col+23] = (unsigned char) _mm256_extract_epi8(output_row0_even,23);

		_mm_storeu_si128( (__m128i *) &filt[N-1][col],_mm256_extractf128_si256(output_row1_even, 0)); //store low 128bit - 16pixels
		filt[N-1][col+16] = (unsigned char) _mm256_extract_epi8(output_row1_even,16);
		filt[N-1][col+17] = (unsigned char) _mm256_extract_epi8(output_row1_even,17);
		filt[N-1][col+18] = (unsigned char) _mm256_extract_epi8(output_row1_even,18);
		filt[N-1][col+19] = (unsigned char) _mm256_extract_epi8(output_row1_even,19);
		filt[N-1][col+20] = (unsigned char) _mm256_extract_epi8(output_row1_even,20);
		filt[N-1][col+21] = (unsigned char) _mm256_extract_epi8(output_row1_even,21);
		filt[N-1][col+22] = (unsigned char) _mm256_extract_epi8(output_row1_even,22);
		filt[N-1][col+23] = (unsigned char) _mm256_extract_epi8(output_row1_even,23);
		  break;
	case 25:
		_mm_storeu_si128( (__m128i *) &filt[N-2][col],_mm256_extractf128_si256(output_row0_even, 0)); //store low 128bit - 16pixels
		filt[N-2][col+16] = (unsigned char) _mm256_extract_epi8(output_row0_even,16);
		filt[N-2][col+17] = (unsigned char) _mm256_extract_epi8(output_row0_even,17);
		filt[N-2][col+18] = (unsigned char) _mm256_extract_epi8(output_row0_even,18);
		filt[N-2][col+19] = (unsigned char) _mm256_extract_epi8(output_row0_even,19);
		filt[N-2][col+20] = (unsigned char) _mm256_extract_epi8(output_row0_even,20);
		filt[N-2][col+21] = (unsigned char) _mm256_extract_epi8(output_row0_even,21);
		filt[N-2][col+22] = (unsigned char) _mm256_extract_epi8(output_row0_even,22);
		filt[N-2][col+23] = (unsigned char) _mm256_extract_epi8(output_row0_even,23);
		filt[N-2][col+24] = (unsigned char) _mm256_extract_epi8(output_row0_even,24);

		_mm_storeu_si128( (__m128i *) &filt[N-1][col],_mm256_extractf128_si256(output_row1_even, 0)); //store low 128bit - 16pixels
		filt[N-1][col+16] = (unsigned char) _mm256_extract_epi8(output_row1_even,16);
		filt[N-1][col+17] = (unsigned char) _mm256_extract_epi8(output_row1_even,17);
		filt[N-1][col+18] = (unsigned char) _mm256_extract_epi8(output_row1_even,18);
		filt[N-1][col+19] = (unsigned char) _mm256_extract_epi8(output_row1_even,19);
		filt[N-1][col+20] = (unsigned char) _mm256_extract_epi8(output_row1_even,20);
		filt[N-1][col+21] = (unsigned char) _mm256_extract_epi8(output_row1_even,21);
		filt[N-1][col+22] = (unsigned char) _mm256_extract_epi8(output_row1_even,22);
		filt[N-1][col+23] = (unsigned char) _mm256_extract_epi8(output_row1_even,23);
		filt[N-1][col+24] = (unsigned char) _mm256_extract_epi8(output_row1_even,24);
		  break;
	case 26:
		_mm_storeu_si128( (__m128i *) &filt[N-2][col],_mm256_extractf128_si256(output_row0_even, 0)); //store low 128bit - 16pixels
		filt[N-2][col+16] = (unsigned char) _mm256_extract_epi8(output_row0_even,16);
		filt[N-2][col+17] = (unsigned char) _mm256_extract_epi8(output_row0_even,17);
		filt[N-2][col+18] = (unsigned char) _mm256_extract_epi8(output_row0_even,18);
		filt[N-2][col+19] = (unsigned char) _mm256_extract_epi8(output_row0_even,19);
		filt[N-2][col+20] = (unsigned char) _mm256_extract_epi8(output_row0_even,20);
		filt[N-2][col+21] = (unsigned char) _mm256_extract_epi8(output_row0_even,21);
		filt[N-2][col+22] = (unsigned char) _mm256_extract_epi8(output_row0_even,22);
		filt[N-2][col+23] = (unsigned char) _mm256_extract_epi8(output_row0_even,23);
		filt[N-2][col+24] = (unsigned char) _mm256_extract_epi8(output_row0_even,24);
		filt[N-2][col+25] = (unsigned char) _mm256_extract_epi8(output_row0_even,25);

		_mm_storeu_si128( (__m128i *) &filt[N-1][col],_mm256_extractf128_si256(output_row1_even, 0)); //store low 128bit - 16pixels
		filt[N-1][col+16] = (unsigned char) _mm256_extract_epi8(output_row1_even,16);
		filt[N-1][col+17] = (unsigned char) _mm256_extract_epi8(output_row1_even,17);
		filt[N-1][col+18] = (unsigned char) _mm256_extract_epi8(output_row1_even,18);
		filt[N-1][col+19] = (unsigned char) _mm256_extract_epi8(output_row1_even,19);
		filt[N-1][col+20] = (unsigned char) _mm256_extract_epi8(output_row1_even,20);
		filt[N-1][col+21] = (unsigned char) _mm256_extract_epi8(output_row1_even,21);
		filt[N-1][col+22] = (unsigned char) _mm256_extract_epi8(output_row1_even,22);
		filt[N-1][col+23] = (unsigned char) _mm256_extract_epi8(output_row1_even,23);
		filt[N-1][col+24] = (unsigned char) _mm256_extract_epi8(output_row1_even,24);
		filt[N-1][col+25] = (unsigned char) _mm256_extract_epi8(output_row1_even,25);
		  break;
	case 27:
		_mm_storeu_si128( (__m128i *) &filt[N-2][col],_mm256_extractf128_si256(output_row0_even, 0)); //store low 128bit - 16pixels
		filt[N-2][col+16] = (unsigned char) _mm256_extract_epi8(output_row0_even,16);
		filt[N-2][col+17] = (unsigned char) _mm256_extract_epi8(output_row0_even,17);
		filt[N-2][col+18] = (unsigned char) _mm256_extract_epi8(output_row0_even,18);
		filt[N-2][col+19] = (unsigned char) _mm256_extract_epi8(output_row0_even,19);
		filt[N-2][col+20] = (unsigned char) _mm256_extract_epi8(output_row0_even,20);
		filt[N-2][col+21] = (unsigned char) _mm256_extract_epi8(output_row0_even,21);
		filt[N-2][col+22] = (unsigned char) _mm256_extract_epi8(output_row0_even,22);
		filt[N-2][col+23] = (unsigned char) _mm256_extract_epi8(output_row0_even,23);
		filt[N-2][col+24] = (unsigned char) _mm256_extract_epi8(output_row0_even,24);
		filt[N-2][col+25] = (unsigned char) _mm256_extract_epi8(output_row0_even,25);
		filt[N-2][col+26] = (unsigned char) _mm256_extract_epi8(output_row0_even,26);

		_mm_storeu_si128( (__m128i *) &filt[N-1][col],_mm256_extractf128_si256(output_row1_even, 0)); //store low 128bit - 16pixels
		filt[N-1][col+16] = (unsigned char) _mm256_extract_epi8(output_row1_even,16);
		filt[N-1][col+17] = (unsigned char) _mm256_extract_epi8(output_row1_even,17);
		filt[N-1][col+18] = (unsigned char) _mm256_extract_epi8(output_row1_even,18);
		filt[N-1][col+19] = (unsigned char) _mm256_extract_epi8(output_row1_even,19);
		filt[N-1][col+20] = (unsigned char) _mm256_extract_epi8(output_row1_even,20);
		filt[N-1][col+21] = (unsigned char) _mm256_extract_epi8(output_row1_even,21);
		filt[N-1][col+22] = (unsigned char) _mm256_extract_epi8(output_row1_even,22);
		filt[N-1][col+23] = (unsigned char) _mm256_extract_epi8(output_row1_even,23);
		filt[N-1][col+24] = (unsigned char) _mm256_extract_epi8(output_row1_even,24);
		filt[N-1][col+25] = (unsigned char) _mm256_extract_epi8(output_row1_even,25);
		filt[N-1][col+26] = (unsigned char) _mm256_extract_epi8(output_row1_even,26);
		  break;
	case 28:
		_mm_storeu_si128( (__m128i *) &filt[N-2][col],_mm256_extractf128_si256(output_row0_even, 0)); //store low 128bit - 16pixels
		filt[N-2][col+16] = (unsigned char) _mm256_extract_epi8(output_row0_even,16);
		filt[N-2][col+17] = (unsigned char) _mm256_extract_epi8(output_row0_even,17);
		filt[N-2][col+18] = (unsigned char) _mm256_extract_epi8(output_row0_even,18);
		filt[N-2][col+19] = (unsigned char) _mm256_extract_epi8(output_row0_even,19);
		filt[N-2][col+20] = (unsigned char) _mm256_extract_epi8(output_row0_even,20);
		filt[N-2][col+21] = (unsigned char) _mm256_extract_epi8(output_row0_even,21);
		filt[N-2][col+22] = (unsigned char) _mm256_extract_epi8(output_row0_even,22);
		filt[N-2][col+23] = (unsigned char) _mm256_extract_epi8(output_row0_even,23);
		filt[N-2][col+24] = (unsigned char) _mm256_extract_epi8(output_row0_even,24);
		filt[N-2][col+25] = (unsigned char) _mm256_extract_epi8(output_row0_even,25);
		filt[N-2][col+26] = (unsigned char) _mm256_extract_epi8(output_row0_even,26);
		filt[N-2][col+27] = (unsigned char) _mm256_extract_epi8(output_row0_even,27);

		_mm_storeu_si128( (__m128i *) &filt[N-1][col],_mm256_extractf128_si256(output_row1_even, 0)); //store low 128bit - 16pixels
		filt[N-1][col+16] = (unsigned char) _mm256_extract_epi8(output_row1_even,16);
		filt[N-1][col+17] = (unsigned char) _mm256_extract_epi8(output_row1_even,17);
		filt[N-1][col+18] = (unsigned char) _mm256_extract_epi8(output_row1_even,18);
		filt[N-1][col+19] = (unsigned char) _mm256_extract_epi8(output_row1_even,19);
		filt[N-1][col+20] = (unsigned char) _mm256_extract_epi8(output_row1_even,20);
		filt[N-1][col+21] = (unsigned char) _mm256_extract_epi8(output_row1_even,21);
		filt[N-1][col+22] = (unsigned char) _mm256_extract_epi8(output_row1_even,22);
		filt[N-1][col+23] = (unsigned char) _mm256_extract_epi8(output_row1_even,23);
		filt[N-1][col+24] = (unsigned char) _mm256_extract_epi8(output_row1_even,24);
		filt[N-1][col+25] = (unsigned char) _mm256_extract_epi8(output_row1_even,25);
		filt[N-1][col+26] = (unsigned char) _mm256_extract_epi8(output_row1_even,26);
		filt[N-1][col+27] = (unsigned char) _mm256_extract_epi8(output_row1_even,27);
		  break;
	case 29:
		_mm_storeu_si128( (__m128i *) &filt[N-2][col],_mm256_extractf128_si256(output_row0_even, 0)); //store low 128bit - 16pixels
		filt[N-2][col+16] = (unsigned char) _mm256_extract_epi8(output_row0_even,16);
		filt[N-2][col+17] = (unsigned char) _mm256_extract_epi8(output_row0_even,17);
		filt[N-2][col+18] = (unsigned char) _mm256_extract_epi8(output_row0_even,18);
		filt[N-2][col+19] = (unsigned char) _mm256_extract_epi8(output_row0_even,19);
		filt[N-2][col+20] = (unsigned char) _mm256_extract_epi8(output_row0_even,20);
		filt[N-2][col+21] = (unsigned char) _mm256_extract_epi8(output_row0_even,21);
		filt[N-2][col+22] = (unsigned char) _mm256_extract_epi8(output_row0_even,22);
		filt[N-2][col+23] = (unsigned char) _mm256_extract_epi8(output_row0_even,23);
		filt[N-2][col+24] = (unsigned char) _mm256_extract_epi8(output_row0_even,24);
		filt[N-2][col+25] = (unsigned char) _mm256_extract_epi8(output_row0_even,25);
		filt[N-2][col+26] = (unsigned char) _mm256_extract_epi8(output_row0_even,26);
		filt[N-2][col+27] = (unsigned char) _mm256_extract_epi8(output_row0_even,27);
		filt[N-2][col+28] = (unsigned char) _mm256_extract_epi8(output_row0_even,28);

		_mm_storeu_si128( (__m128i *) &filt[N-1][col],_mm256_extractf128_si256(output_row1_even, 0)); //store low 128bit - 16pixels
		filt[N-1][col+16] = (unsigned char) _mm256_extract_epi8(output_row1_even,16);
		filt[N-1][col+17] = (unsigned char) _mm256_extract_epi8(output_row1_even,17);
		filt[N-1][col+18] = (unsigned char) _mm256_extract_epi8(output_row1_even,18);
		filt[N-1][col+19] = (unsigned char) _mm256_extract_epi8(output_row1_even,19);
		filt[N-1][col+20] = (unsigned char) _mm256_extract_epi8(output_row1_even,20);
		filt[N-1][col+21] = (unsigned char) _mm256_extract_epi8(output_row1_even,21);
		filt[N-1][col+22] = (unsigned char) _mm256_extract_epi8(output_row1_even,22);
		filt[N-1][col+23] = (unsigned char) _mm256_extract_epi8(output_row1_even,23);
		filt[N-1][col+24] = (unsigned char) _mm256_extract_epi8(output_row1_even,24);
		filt[N-1][col+25] = (unsigned char) _mm256_extract_epi8(output_row1_even,25);
		filt[N-1][col+26] = (unsigned char) _mm256_extract_epi8(output_row1_even,26);
		filt[N-1][col+27] = (unsigned char) _mm256_extract_epi8(output_row1_even,27);
		filt[N-1][col+28] = (unsigned char) _mm256_extract_epi8(output_row1_even,28);
		  break;
	case 30:

		_mm_storeu_si128( (__m128i *) &filt[N-2][col],_mm256_extractf128_si256(output_row0_even, 0)); //store low 128bit - 16pixels
		filt[N-2][col+16] = (unsigned char) _mm256_extract_epi8(output_row0_even,16);
		filt[N-2][col+17] = (unsigned char) _mm256_extract_epi8(output_row0_even,17);
		filt[N-2][col+18] = (unsigned char) _mm256_extract_epi8(output_row0_even,18);
		filt[N-2][col+19] = (unsigned char) _mm256_extract_epi8(output_row0_even,19);
		filt[N-2][col+20] = (unsigned char) _mm256_extract_epi8(output_row0_even,20);
		filt[N-2][col+21] = (unsigned char) _mm256_extract_epi8(output_row0_even,21);
		filt[N-2][col+22] = (unsigned char) _mm256_extract_epi8(output_row0_even,22);
		filt[N-2][col+23] = (unsigned char) _mm256_extract_epi8(output_row0_even,23);
		filt[N-2][col+24] = (unsigned char) _mm256_extract_epi8(output_row0_even,24);
		filt[N-2][col+25] = (unsigned char) _mm256_extract_epi8(output_row0_even,25);
		filt[N-2][col+26] = (unsigned char) _mm256_extract_epi8(output_row0_even,26);
		filt[N-2][col+27] = (unsigned char) _mm256_extract_epi8(output_row0_even,27);
		filt[N-2][col+28] = (unsigned char) _mm256_extract_epi8(output_row0_even,28);
		filt[N-2][col+29] = (unsigned char) _mm256_extract_epi8(output_row0_even,29);


		_mm_storeu_si128( (__m128i *) &filt[N-1][col],_mm256_extractf128_si256(output_row1_even, 0)); //store low 128bit - 16pixels
		filt[N-1][col+16] = (unsigned char) _mm256_extract_epi8(output_row1_even,16);
		filt[N-1][col+17] = (unsigned char) _mm256_extract_epi8(output_row1_even,17);
		filt[N-1][col+18] = (unsigned char) _mm256_extract_epi8(output_row1_even,18);
		filt[N-1][col+19] = (unsigned char) _mm256_extract_epi8(output_row1_even,19);
		filt[N-1][col+20] = (unsigned char) _mm256_extract_epi8(output_row1_even,20);
		filt[N-1][col+21] = (unsigned char) _mm256_extract_epi8(output_row1_even,21);
		filt[N-1][col+22] = (unsigned char) _mm256_extract_epi8(output_row1_even,22);
		filt[N-1][col+23] = (unsigned char) _mm256_extract_epi8(output_row1_even,23);
		filt[N-1][col+24] = (unsigned char) _mm256_extract_epi8(output_row1_even,24);
		filt[N-1][col+25] = (unsigned char) _mm256_extract_epi8(output_row1_even,25);
		filt[N-1][col+26] = (unsigned char) _mm256_extract_epi8(output_row1_even,26);
		filt[N-1][col+27] = (unsigned char) _mm256_extract_epi8(output_row1_even,27);
		filt[N-1][col+28] = (unsigned char) _mm256_extract_epi8(output_row1_even,28);
		filt[N-1][col+29] = (unsigned char) _mm256_extract_epi8(output_row1_even,29);
		  break;
	case 31:
		_mm_storeu_si128( (__m128i *) &filt[N-2][col],_mm256_extractf128_si256(output_row0_even, 0)); //store low 128bit - 16pixels
		filt[N-2][col+16] = (unsigned char) _mm256_extract_epi8(output_row0_even,16);
		filt[N-2][col+17] = (unsigned char) _mm256_extract_epi8(output_row0_even,17);
		filt[N-2][col+18] = (unsigned char) _mm256_extract_epi8(output_row0_even,18);
		filt[N-2][col+19] = (unsigned char) _mm256_extract_epi8(output_row0_even,19);
		filt[N-2][col+20] = (unsigned char) _mm256_extract_epi8(output_row0_even,20);
		filt[N-2][col+21] = (unsigned char) _mm256_extract_epi8(output_row0_even,21);
		filt[N-2][col+22] = (unsigned char) _mm256_extract_epi8(output_row0_even,22);
		filt[N-2][col+23] = (unsigned char) _mm256_extract_epi8(output_row0_even,23);
		filt[N-2][col+24] = (unsigned char) _mm256_extract_epi8(output_row0_even,24);
		filt[N-2][col+25] = (unsigned char) _mm256_extract_epi8(output_row0_even,25);
		filt[N-2][col+26] = (unsigned char) _mm256_extract_epi8(output_row0_even,26);
		filt[N-2][col+27] = (unsigned char) _mm256_extract_epi8(output_row0_even,27);
		filt[N-2][col+28] = (unsigned char) _mm256_extract_epi8(output_row0_even,28);
		filt[N-2][col+29] = (unsigned char) _mm256_extract_epi8(output_row0_even,29);


		_mm_storeu_si128( (__m128i *) &filt[N-1][col],_mm256_extractf128_si256(output_row1_even, 0)); //store low 128bit - 16pixels
		filt[N-1][col+16] = (unsigned char) _mm256_extract_epi8(output_row1_even,16);
		filt[N-1][col+17] = (unsigned char) _mm256_extract_epi8(output_row1_even,17);
		filt[N-1][col+18] = (unsigned char) _mm256_extract_epi8(output_row1_even,18);
		filt[N-1][col+19] = (unsigned char) _mm256_extract_epi8(output_row1_even,19);
		filt[N-1][col+20] = (unsigned char) _mm256_extract_epi8(output_row1_even,20);
		filt[N-1][col+21] = (unsigned char) _mm256_extract_epi8(output_row1_even,21);
		filt[N-1][col+22] = (unsigned char) _mm256_extract_epi8(output_row1_even,22);
		filt[N-1][col+23] = (unsigned char) _mm256_extract_epi8(output_row1_even,23);
		filt[N-1][col+24] = (unsigned char) _mm256_extract_epi8(output_row1_even,24);
		filt[N-1][col+25] = (unsigned char) _mm256_extract_epi8(output_row1_even,25);
		filt[N-1][col+26] = (unsigned char) _mm256_extract_epi8(output_row1_even,26);
		filt[N-1][col+27] = (unsigned char) _mm256_extract_epi8(output_row1_even,27);
		filt[N-1][col+28] = (unsigned char) _mm256_extract_epi8(output_row1_even,28);
		filt[N-1][col+29] = (unsigned char) _mm256_extract_epi8(output_row1_even,29);

		//the filt[0:2][M-1] pixel will be computed without vectorization
		int newPixel;
		for (int row=N-2;row<=N-1;row++){
		 newPixel = 0;
		for (int rowOffset=-2; rowOffset<=2; rowOffset++) {
			for (int colOffset=-2; colOffset<=2; colOffset++) {

			   if ( ((row+rowOffset)<N) && ((M-1+colOffset)<M) && ((row+rowOffset)>=0) && ((M-1+colOffset)>=0) )
	              newPixel += frame1[row+rowOffset][M-1+colOffset] * filter[2 + rowOffset][2 + colOffset];

			      }
		        }
	   filt[row][M-1] = (unsigned char) (newPixel / divisor);
		}

		  break;
	default:
		printf("\n something went wrong");
	}

return 0;

}





void convolution_old_7x7(unsigned char **frame1,unsigned char **filt,const unsigned int M, const unsigned int N, const unsigned short int divisor, signed char **kernel){

const signed char f00=kernel[0][0];
const signed char f01=kernel[0][1];
const signed char f02=kernel[0][2];
const signed char f03=kernel[0][3];
const signed char f04=kernel[0][4];
const signed char f05=kernel[0][5];
const signed char f06=kernel[0][6];

const signed char f10=kernel[1][0];
const signed char f11=kernel[1][1];
const signed char f12=kernel[1][2];
const signed char f13=kernel[1][3];
const signed char f14=kernel[1][4];
const signed char f15=kernel[1][5];
const signed char f16=kernel[1][6];

const signed char f20=kernel[2][0];
const signed char f21=kernel[2][1];
const signed char f22=kernel[2][2];
const signed char f23=kernel[2][3];
const signed char f24=kernel[2][4];
const signed char f25=kernel[2][5];
const signed char f26=kernel[2][6];

const signed char f30=kernel[3][0];
const signed char f31=kernel[3][1];
const signed char f32=kernel[3][2];
const signed char f33=kernel[3][3];
const signed char f34=kernel[3][4];
const signed char f35=kernel[3][5];
const signed char f36=kernel[3][6];

const signed char f40=kernel[4][0];
const signed char f41=kernel[4][1];
const signed char f42=kernel[4][2];
const signed char f43=kernel[4][3];
const signed char f44=kernel[4][4];
const signed char f45=kernel[4][5];
const signed char f46=kernel[4][6];

const signed char f50=kernel[5][0];
const signed char f51=kernel[5][1];
const signed char f52=kernel[5][2];
const signed char f53=kernel[5][3];
const signed char f54=kernel[5][4];
const signed char f55=kernel[5][5];
const signed char f56=kernel[5][6];

const signed char f60=kernel[6][0];
const signed char f61=kernel[6][1];
const signed char f62=kernel[6][2];
const signed char f63=kernel[6][3];
const signed char f64=kernel[6][4];
const signed char f65=kernel[6][5];
const signed char f66=kernel[6][6];

const signed char mask_vector[56][32] __attribute__((aligned(64))) ={
	{f00,f01,f02,f03,f04,f05,f06,0,f00,f01,f02,f03,f04,f05,f06,0,f00,f01,f02,f03,f04,f05,f06,0,f00,f01,f02,f03,f04,f05,f06,0},
	{f10,f11,f12,f13,f14,f15,f16,0,f10,f11,f12,f13,f14,f15,f16,0,f10,f11,f12,f13,f14,f15,f16,0,f10,f11,f12,f13,f14,f15,f16,0},
	{f20,f21,f22,f23,f24,f25,f26,0,f20,f21,f22,f23,f24,f25,f26,0,f20,f21,f22,f23,f24,f25,f26,0,f20,f21,f22,f23,f24,f25,f26,0},
	{f30,f31,f32,f33,f34,f35,f36,0,f30,f31,f32,f33,f34,f35,f36,0,f30,f31,f32,f33,f34,f35,f36,0,f30,f31,f32,f33,f34,f35,f36,0},
	{f40,f41,f42,f43,f44,f45,f46,0,f40,f41,f42,f43,f44,f45,f46,0,f40,f41,f42,f43,f44,f45,f46,0,f40,f41,f42,f43,f44,f45,f46,0},
	{f50,f51,f52,f53,f54,f55,f56,0,f50,f51,f52,f53,f54,f55,f56,0,f50,f51,f52,f53,f54,f55,f56,0,f50,f51,f52,f53,f54,f55,f56,0},
	{f60,f61,f62,f63,f64,f65,f66,0,f60,f61,f62,f63,f64,f65,f66,0,f60,f61,f62,f63,f64,f65,f66,0,f60,f61,f62,f63,f64,f65,f66,0},

	{0,f00,f01,f02,f03,f04,f05,f06,0,f00,f01,f02,f03,f04,f05,f06,0,f00,f01,f02,f03,f04,f05,f06,0,f00,f01,f02,f03,f04,f05,f06},
	{0,f10,f11,f12,f13,f14,f15,f16,0,f10,f11,f12,f13,f14,f15,f16,0,f10,f11,f12,f13,f14,f15,f16,0,f10,f11,f12,f13,f14,f15,f16},
	{0,f20,f21,f22,f23,f24,f25,f26,0,f20,f21,f22,f23,f24,f25,f26,0,f20,f21,f22,f23,f24,f25,f26,0,f20,f21,f22,f23,f24,f25,f26},
	{0,f30,f31,f32,f33,f34,f35,f36,0,f30,f31,f32,f33,f34,f35,f36,0,f30,f31,f32,f33,f34,f35,f36,0,f30,f31,f32,f33,f34,f35,f36},
	{0,f40,f41,f42,f43,f44,f45,f46,0,f40,f41,f42,f43,f44,f45,f46,0,f40,f41,f42,f43,f44,f45,f46,0,f40,f41,f42,f43,f44,f45,f46},
	{0,f50,f51,f52,f53,f54,f55,f56,0,f50,f51,f52,f53,f54,f55,f56,0,f50,f51,f52,f53,f54,f55,f56,0,f50,f51,f52,f53,f54,f55,f56},
	{0,f60,f61,f62,f63,f64,f65,f66,0,f60,f61,f62,f63,f64,f65,f66,0,f60,f61,f62,f63,f64,f65,f66,0,f60,f61,f62,f63,f64,f65,f66},

	{0,0,f00,f01,f02,f03,f04,f05,f06,0,f00,f01,f02,f03,f04,f05,f06,0,f00,f01,f02,f03,f04,f05,f06,0,0,0,0,0,0,0},
	{0,0,f10,f11,f12,f13,f14,f15,f16,0,f10,f11,f12,f13,f14,f15,f16,0,f10,f11,f12,f13,f14,f15,f16,0,0,0,0,0,0,0},
	{0,0,f20,f21,f22,f23,f24,f25,f26,0,f20,f21,f22,f23,f24,f25,f26,0,f20,f21,f22,f23,f24,f25,f26,0,0,0,0,0,0,0},
	{0,0,f30,f31,f32,f33,f34,f35,f36,0,f30,f31,f32,f33,f34,f35,f36,0,f30,f31,f32,f33,f34,f35,f36,0,0,0,0,0,0,0},
	{0,0,f40,f41,f42,f43,f44,f45,f46,0,f40,f41,f42,f43,f44,f45,f46,0,f40,f41,f42,f43,f44,f45,f46,0,0,0,0,0,0,0},
	{0,0,f50,f51,f52,f53,f54,f55,f56,0,f50,f51,f52,f53,f54,f55,f56,0,f50,f51,f52,f53,f54,f55,f56,0,0,0,0,0,0,0},
	{0,0,f60,f61,f62,f63,f64,f65,f66,0,f60,f61,f62,f63,f64,f65,f66,0,f60,f61,f62,f63,f64,f65,f66,0,0,0,0,0,0,0},

	{0,0,0,f00,f01,f02,f03,f04,f05,f06,0,f00,f01,f02,f03,f04,f05,f06,0,f00,f01,f02,f03,f04,f05,f06,0,0,0,0,0,0},
	{0,0,0,f10,f11,f12,f13,f14,f15,f16,0,f10,f11,f12,f13,f14,f15,f16,0,f10,f11,f12,f13,f14,f15,f16,0,0,0,0,0,0},
	{0,0,0,f20,f21,f22,f23,f24,f25,f26,0,f20,f21,f22,f23,f24,f25,f26,0,f20,f21,f22,f23,f24,f25,f26,0,0,0,0,0,0},
	{0,0,0,f30,f31,f32,f33,f34,f35,f36,0,f30,f31,f32,f33,f34,f35,f36,0,f30,f31,f32,f33,f34,f35,f36,0,0,0,0,0,0},
	{0,0,0,f40,f41,f42,f43,f44,f45,f46,0,f40,f41,f42,f43,f44,f45,f46,0,f40,f41,f42,f43,f44,f45,f46,0,0,0,0,0,0},
	{0,0,0,f50,f51,f52,f53,f54,f55,f56,0,f50,f51,f52,f53,f54,f55,f56,0,f50,f51,f52,f53,f54,f55,f56,0,0,0,0,0,0},
	{0,0,0,f60,f61,f62,f63,f64,f65,f66,0,f60,f61,f62,f63,f64,f65,f66,0,f60,f61,f62,f63,f64,f65,f66,0,0,0,0,0,0},

	{0,0,0,0,f00,f01,f02,f03,f04,f05,f06,0,f00,f01,f02,f03,f04,f05,f06,0,f00,f01,f02,f03,f04,f05,f06,0,0,0,0,0},
	{0,0,0,0,f10,f11,f12,f13,f14,f15,f16,0,f10,f11,f12,f13,f14,f15,f16,0,f10,f11,f12,f13,f14,f15,f16,0,0,0,0,0},
	{0,0,0,0,f20,f21,f22,f23,f24,f25,f26,0,f20,f21,f22,f23,f24,f25,f26,0,f20,f21,f22,f23,f24,f25,f26,0,0,0,0,0},
	{0,0,0,0,f30,f31,f32,f33,f34,f35,f36,0,f30,f31,f32,f33,f34,f35,f36,0,f30,f31,f32,f33,f34,f35,f36,0,0,0,0,0},
	{0,0,0,0,f40,f41,f42,f43,f44,f45,f46,0,f40,f41,f42,f43,f44,f45,f46,0,f40,f41,f42,f43,f44,f45,f46,0,0,0,0,0},
	{0,0,0,0,f50,f51,f52,f53,f54,f55,f56,0,f50,f51,f52,f53,f54,f55,f56,0,f50,f51,f52,f53,f54,f55,f56,0,0,0,0,0},
	{0,0,0,0,f60,f61,f62,f63,f64,f65,f66,0,f60,f61,f62,f63,f64,f65,f66,0,f60,f61,f62,f63,f64,f65,f66,0,0,0,0,0},

	{0,0,0,0,0,f00,f01,f02,f03,f04,f05,f06,0,f00,f01,f02,f03,f04,f05,f06,0,f00,f01,f02,f03,f04,f05,f06,0,0,0,0},
	{0,0,0,0,0,f10,f11,f12,f13,f14,f15,f16,0,f10,f11,f12,f13,f14,f15,f16,0,f10,f11,f12,f13,f14,f15,f16,0,0,0,0},
	{0,0,0,0,0,f20,f21,f22,f23,f24,f25,f26,0,f20,f21,f22,f23,f24,f25,f26,0,f20,f21,f22,f23,f24,f25,f26,0,0,0,0},
	{0,0,0,0,0,f30,f31,f32,f33,f34,f35,f36,0,f30,f31,f32,f33,f34,f35,f36,0,f30,f31,f32,f33,f34,f35,f36,0,0,0,0},
	{0,0,0,0,0,f40,f41,f42,f43,f44,f45,f46,0,f40,f41,f42,f43,f44,f45,f46,0,f40,f41,f42,f43,f44,f45,f46,0,0,0,0},
	{0,0,0,0,0,f50,f51,f52,f53,f54,f55,f56,0,f50,f51,f52,f53,f54,f55,f56,0,f50,f51,f52,f53,f54,f55,f56,0,0,0,0},
	{0,0,0,0,0,f60,f61,f62,f63,f64,f65,f66,0,f60,f61,f62,f63,f64,f65,f66,0,f60,f61,f62,f63,f64,f65,f66,0,0,0,0},

	{0,0,0,0,0,0,f00,f01,f02,f03,f04,f05,f06,0,f00,f01,f02,f03,f04,f05,f06,0,f00,f01,f02,f03,f04,f05,f06,0,0,0},
	{0,0,0,0,0,0,f10,f11,f12,f13,f14,f15,f16,0,f10,f11,f12,f13,f14,f15,f16,0,f10,f11,f12,f13,f14,f15,f16,0,0,0},
	{0,0,0,0,0,0,f20,f21,f22,f23,f24,f25,f26,0,f20,f21,f22,f23,f24,f25,f26,0,f20,f21,f22,f23,f24,f25,f26,0,0,0},
	{0,0,0,0,0,0,f30,f31,f32,f33,f34,f35,f36,0,f30,f31,f32,f33,f34,f35,f36,0,f30,f31,f32,f33,f34,f35,f36,0,0,0},
	{0,0,0,0,0,0,f40,f41,f42,f43,f44,f45,f46,0,f40,f41,f42,f43,f44,f45,f46,0,f40,f41,f42,f43,f44,f45,f46,0,0,0},
	{0,0,0,0,0,0,f50,f51,f52,f53,f54,f55,f56,0,f50,f51,f52,f53,f54,f55,f56,0,f50,f51,f52,f53,f54,f55,f56,0,0,0},
	{0,0,0,0,0,0,f60,f61,f62,f63,f64,f65,f66,0,f60,f61,f62,f63,f64,f65,f66,0,f60,f61,f62,f63,f64,f65,f66,0,0,0},

	{0,0,0,0,0,0,0,f00,f01,f02,f03,f04,f05,f06,0,f00,f01,f02,f03,f04,f05,f06,0,f00,f01,f02,f03,f04,f05,f06,0,0},
	{0,0,0,0,0,0,0,f10,f11,f12,f13,f14,f15,f16,0,f10,f11,f12,f13,f14,f15,f16,0,f10,f11,f12,f13,f14,f15,f16,0,0},
	{0,0,0,0,0,0,0,f20,f21,f22,f23,f24,f25,f26,0,f20,f21,f22,f23,f24,f25,f26,0,f20,f21,f22,f23,f24,f25,f26,0,0},
	{0,0,0,0,0,0,0,f30,f31,f32,f33,f34,f35,f36,0,f30,f31,f32,f33,f34,f35,f36,0,f30,f31,f32,f33,f34,f35,f36,0,0},
	{0,0,0,0,0,0,0,f40,f41,f42,f43,f44,f45,f46,0,f40,f41,f42,f43,f44,f45,f46,0,f40,f41,f42,f43,f44,f45,f46,0,0},
	{0,0,0,0,0,0,0,f50,f51,f52,f53,f54,f55,f56,0,f50,f51,f52,f53,f54,f55,f56,0,f50,f51,f52,f53,f54,f55,f56,0,0},
	{0,0,0,0,0,0,0,f60,f61,f62,f63,f64,f65,f66,0,f60,f61,f62,f63,f64,f65,f66,0,f60,f61,f62,f63,f64,f65,f66,0,0},
};


	const __m256i c0=_mm256_load_si256( (__m256i *) &mask_vector[0][0]);
	const __m256i c1=_mm256_load_si256( (__m256i *) &mask_vector[1][0]);
	const __m256i c2=_mm256_load_si256( (__m256i *) &mask_vector[2][0]);
	const __m256i c3=_mm256_load_si256( (__m256i *) &mask_vector[3][0]);
	const __m256i c4=_mm256_load_si256( (__m256i *) &mask_vector[4][0]);
	const __m256i c5=_mm256_load_si256( (__m256i *) &mask_vector[5][0]);
	const __m256i c6=_mm256_load_si256( (__m256i *) &mask_vector[6][0]);

	const __m256i c0_sh1=_mm256_load_si256( (__m256i *) &mask_vector[7][0]);
	const __m256i c1_sh1=_mm256_load_si256( (__m256i *) &mask_vector[8][0]);
	const __m256i c2_sh1=_mm256_load_si256( (__m256i *) &mask_vector[9][0]);
	const __m256i c3_sh1=_mm256_load_si256( (__m256i *) &mask_vector[10][0]);
	const __m256i c4_sh1=_mm256_load_si256( (__m256i *) &mask_vector[11][0]);
	const __m256i c5_sh1=_mm256_load_si256( (__m256i *) &mask_vector[12][0]);
	const __m256i c6_sh1=_mm256_load_si256( (__m256i *) &mask_vector[13][0]);

	const __m256i c0_sh2=_mm256_load_si256( (__m256i *) &mask_vector[14][0]);
	const __m256i c1_sh2=_mm256_load_si256( (__m256i *) &mask_vector[15][0]);
	const __m256i c2_sh2=_mm256_load_si256( (__m256i *) &mask_vector[16][0]);
	const __m256i c3_sh2=_mm256_load_si256( (__m256i *) &mask_vector[17][0]);
	const __m256i c4_sh2=_mm256_load_si256( (__m256i *) &mask_vector[18][0]);
	const __m256i c5_sh2=_mm256_load_si256( (__m256i *) &mask_vector[19][0]);
	const __m256i c6_sh2=_mm256_load_si256( (__m256i *) &mask_vector[20][0]);


	const __m256i c0_sh3=_mm256_load_si256( (__m256i *) &mask_vector[21][0]);
	const __m256i c1_sh3=_mm256_load_si256( (__m256i *) &mask_vector[22][0]);
	const __m256i c2_sh3=_mm256_load_si256( (__m256i *) &mask_vector[23][0]);
	const __m256i c3_sh3=_mm256_load_si256( (__m256i *) &mask_vector[24][0]);
	const __m256i c4_sh3=_mm256_load_si256( (__m256i *) &mask_vector[25][0]);
	const __m256i c5_sh3=_mm256_load_si256( (__m256i *) &mask_vector[26][0]);
	const __m256i c6_sh3=_mm256_load_si256( (__m256i *) &mask_vector[27][0]);


	const __m256i c0_sh4=_mm256_load_si256( (__m256i *) &mask_vector[28][0]);
	const __m256i c1_sh4=_mm256_load_si256( (__m256i *) &mask_vector[29][0]);
	const __m256i c2_sh4=_mm256_load_si256( (__m256i *) &mask_vector[30][0]);
	const __m256i c3_sh4=_mm256_load_si256( (__m256i *) &mask_vector[31][0]);
	const __m256i c4_sh4=_mm256_load_si256( (__m256i *) &mask_vector[32][0]);
	const __m256i c5_sh4=_mm256_load_si256( (__m256i *) &mask_vector[33][0]);
	const __m256i c6_sh4=_mm256_load_si256( (__m256i *) &mask_vector[34][0]);


	const __m256i c0_sh5=_mm256_load_si256( (__m256i *) &mask_vector[35][0]);
	const __m256i c1_sh5=_mm256_load_si256( (__m256i *) &mask_vector[36][0]);
	const __m256i c2_sh5=_mm256_load_si256( (__m256i *) &mask_vector[37][0]);
	const __m256i c3_sh5=_mm256_load_si256( (__m256i *) &mask_vector[38][0]);
	const __m256i c4_sh5=_mm256_load_si256( (__m256i *) &mask_vector[39][0]);
	const __m256i c5_sh5=_mm256_load_si256( (__m256i *) &mask_vector[40][0]);
	const __m256i c6_sh5=_mm256_load_si256( (__m256i *) &mask_vector[41][0]);

	const __m256i c0_sh6=_mm256_load_si256( (__m256i *) &mask_vector[42][0]);
	const __m256i c1_sh6=_mm256_load_si256( (__m256i *) &mask_vector[43][0]);
	const __m256i c2_sh6=_mm256_load_si256( (__m256i *) &mask_vector[44][0]);
	const __m256i c3_sh6=_mm256_load_si256( (__m256i *) &mask_vector[45][0]);
	const __m256i c4_sh6=_mm256_load_si256( (__m256i *) &mask_vector[46][0]);
	const __m256i c5_sh6=_mm256_load_si256( (__m256i *) &mask_vector[47][0]);
	const __m256i c6_sh6=_mm256_load_si256( (__m256i *) &mask_vector[48][0]);

	const __m256i c0_sh7=_mm256_load_si256( (__m256i *) &mask_vector[49][0]);
	const __m256i c1_sh7=_mm256_load_si256( (__m256i *) &mask_vector[50][0]);
	const __m256i c2_sh7=_mm256_load_si256( (__m256i *) &mask_vector[51][0]);
	const __m256i c3_sh7=_mm256_load_si256( (__m256i *) &mask_vector[52][0]);
	const __m256i c4_sh7=_mm256_load_si256( (__m256i *) &mask_vector[53][0]);
	const __m256i c5_sh7=_mm256_load_si256( (__m256i *) &mask_vector[54][0]);
	const __m256i c6_sh7=_mm256_load_si256( (__m256i *) &mask_vector[55][0]);

	//const __m256i f  = _mm256_set_epi16(52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759);
	//const __m256i mask  = _mm256_set_epi8(0,0,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255);
	//const __m256i mask2  = _mm256_set_epi8(0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0);
	//const __m256i mask6  = _mm256_set_epi8(0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0);

	const __m256i mask3  = _mm256_set_epi16(0,0,0,0,0,0,0,65535,0,0,0,0,0,0,0,0);
	const __m256i mask_prelude  = _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0);


	const __m256i mask_even_32  = _mm256_set_epi32(0,0xffffffff,0,0xffffffff,0,0xffffffff,0,0xffffffff);
	const __m256i mask_odd_32  = _mm256_set_epi32(0xffffffff,0,0xffffffff,0,0xffffffff,0,0xffffffff,0);
	const __m256i mask4  = _mm256_set_epi32(0,0,0,0xffffffff,0,0,0,0);



	//const unsigned int REMINDER_ITERATIONS =  (M-((((M-32)/26)*26)+26)); //M-(last_col_value+26)
	//printf("\n%d",REMINDER_ITERATIONS);


	const unsigned int division_case=prepare_for_division_32(divisor); //determine which is the division case (A, B or C)
	const __m256i f = _mm256_load_si256( (__m256i *) &f_vector[0]);// initialize the division vector

//printf("\n%d %d",division_case,b);


#pragma omp parallel
{

unsigned int row,col;
__m256i r0,r1,r2,r3,r4,r5,r6,m0,m1,m2,m3,m4,m5,m6;
__m256i output1,output2,output3,output4;

const __m256i ones=_mm256_set1_epi16(1);

/*---------------------- Gaussian Blur ---------------------------------*/

    #pragma omp for schedule(static)
	for (row = 3; row < N-3; row++) {

		if (row==3){
			;
		}

		else if (row==N-4){
			;
		}

	    else {


	  for (col = 0; col <= M-32; col+=26){

		  if (col==0){

				//load the 5 rows
				r0=_mm256_loadu_si256( (__m256i *) &frame1[row-3][0]);
				r1=_mm256_loadu_si256( (__m256i *) &frame1[row-2][0]);
				r2=_mm256_loadu_si256( (__m256i *) &frame1[row-1][0]);
				r3=_mm256_loadu_si256( (__m256i *) &frame1[row][0]);
				r4=_mm256_loadu_si256( (__m256i *) &frame1[row+1][0]);
				r5=_mm256_loadu_si256( (__m256i *) &frame1[row+2][0]);
				r6=_mm256_loadu_si256( (__m256i *) &frame1[row+3][0]);


		  			//START - extra code needed for prelude
		  					m0=_mm256_slli_si256(r0,3);//shift 3 elements left - equivalent to filling with two zeros inthe beginning
		  					m1=_mm256_slli_si256(r1,3);//shift 3 elements left - equivalent to filling with two zeros inthe beginning
		  					m2=_mm256_slli_si256(r2,3);//shift 3 elements left - equivalent to filling with two zeros inthe beginning
		  					m3=_mm256_slli_si256(r3,3);//shift 3 elements left - equivalent to filling with two zeros inthe beginning
		  					m4=_mm256_slli_si256(r4,3);//shift 3 elements left - equivalent to filling with two zeros inthe beginning
		  					m5=_mm256_slli_si256(r5,3);//shift 3 elements left - equivalent to filling with two zeros inthe beginning
		  					m6=_mm256_slli_si256(r6,3);//shift 3 elements left - equivalent to filling with two zeros inthe beginning

		  					r0=_mm256_and_si256(r0,mask_prelude);
		  					r0=_mm256_permute2f128_si256(r0,r0,1);
		  					r0=_mm256_srli_si256(r0,13);//shift 14 elements
		  					r0=_mm256_add_epi16(m0,r0);

		  					r1=_mm256_and_si256(r1,mask_prelude);
		  					r1=_mm256_permute2f128_si256(r1,r1,1);
		  					r1=_mm256_srli_si256(r1,13);//shift 14 elements
		  					r1=_mm256_add_epi16(m1,r1);

		  					r2=_mm256_and_si256(r2,mask_prelude);
		  					r2=_mm256_permute2f128_si256(r2,r2,1);
		  					r2=_mm256_srli_si256(r2,13);//shift 14 elements
		  					r2=_mm256_add_epi16(m2,r2);

		  					r3=_mm256_and_si256(r3,mask_prelude);
		  					r3=_mm256_permute2f128_si256(r3,r3,1);
		  					r3=_mm256_srli_si256(r3,13);//shift 14 elements
		  					r3=_mm256_add_epi16(m3,r3);

		  					r4=_mm256_and_si256(r4,mask_prelude);
		  					r4=_mm256_permute2f128_si256(r4,r4,1);
		  					r4=_mm256_srli_si256(r4,13);//shift 14 elements
		  					r4=_mm256_add_epi16(m4,r4);

		  					r5=_mm256_and_si256(r5,mask_prelude);
		  					r5=_mm256_permute2f128_si256(r5,r5,1);
		  					r5=_mm256_srli_si256(r5,13);//shift 14 elements
		  					r5=_mm256_add_epi16(m5,r5);

		  					r6=_mm256_and_si256(r6,mask_prelude);
		  					r6=_mm256_permute2f128_si256(r6,r6,1);
		  					r6=_mm256_srli_si256(r6,13);//shift 14 elements
		  					r6=_mm256_add_epi16(m6,r6);

		  			//END - extra code needed for prelude
		  		}
		  else {
				//load the 5 rows
				r0=_mm256_loadu_si256( (__m256i *) &frame1[row-3][col-3]);
				r1=_mm256_loadu_si256( (__m256i *) &frame1[row-2][col-3]);
				r2=_mm256_loadu_si256( (__m256i *) &frame1[row-1][col-3]);
				r3=_mm256_loadu_si256( (__m256i *) &frame1[row][col-3]);
				r4=_mm256_loadu_si256( (__m256i *) &frame1[row+1][col-3]);
				r5=_mm256_loadu_si256( (__m256i *) &frame1[row+2][col-3]);
				r6=_mm256_loadu_si256( (__m256i *) &frame1[row+3][col-3]);

		  }

		// col iteration computes output pixels of   0,8,16,24
		// col+1 iteration computes output pixels of 1,9,17,25
		// col+2 iteration computes output pixels of 2,10,18
		// col+3 iteration computes output pixels of 3,11,19
		// col+4 iteration computes output pixels of 4,12,20
		// col+5 iteration computes output pixels of 5,13,21
		// col+6 iteration computes output pixels of 6,14,22
		// col+7 iteration computes output pixels of 7,15,23
		//afterwards, col becomes 26 and repeat the above process

			//1st col iteration

			//multiply by the mask
			m0=_mm256_maddubs_epi16(r0,c0);
			m1=_mm256_maddubs_epi16(r1,c1);
			m2=_mm256_maddubs_epi16(r2,c2);
			m3=_mm256_maddubs_epi16(r3,c3);
			m4=_mm256_maddubs_epi16(r4,c4);
			m5=_mm256_maddubs_epi16(r5,c5);
			m6=_mm256_maddubs_epi16(r6,c6);


			//make 16-bit values 32-bit (horizontal add)
			m0=_mm256_madd_epi16(m0,ones);
			m1=_mm256_madd_epi16(m1,ones);
			m2=_mm256_madd_epi16(m2,ones);
			m3=_mm256_madd_epi16(m3,ones);
			m4=_mm256_madd_epi16(m4,ones);
			m5=_mm256_madd_epi16(m5,ones);
			m6=_mm256_madd_epi16(m6,ones);


			//vertical add
			m0=_mm256_add_epi32(m0,m1);
			m0=_mm256_add_epi32(m0,m2);
			m0=_mm256_add_epi32(m0,m3);
			m0=_mm256_add_epi32(m0,m4);
			m0=_mm256_add_epi32(m0,m5);
			m0=_mm256_add_epi32(m0,m6);


			//last horizontal add
			m1=_mm256_slli_si256(m0,4);//shift by one integer
			m0=_mm256_add_epi32(m0,m1);//add
			m1=_mm256_srli_si256(m0,4);//shift by one integer
			output1=_mm256_and_si256(m1,mask_even_32);//keep only 0,2,4,6 and discard 1,3,5,7


			//2nd col iteration

			//multiply by the mask
			m0=_mm256_maddubs_epi16(r0,c0_sh1);
			m1=_mm256_maddubs_epi16(r1,c1_sh1);
			m2=_mm256_maddubs_epi16(r2,c2_sh1);
			m3=_mm256_maddubs_epi16(r3,c3_sh1);
			m4=_mm256_maddubs_epi16(r4,c4_sh1);
			m5=_mm256_maddubs_epi16(r5,c5_sh1);
			m6=_mm256_maddubs_epi16(r6,c6_sh1);

			//make 16-bit values 32-bit (horizontal add)
			m0=_mm256_madd_epi16(m0,ones);
			m1=_mm256_madd_epi16(m1,ones);
			m2=_mm256_madd_epi16(m2,ones);
			m3=_mm256_madd_epi16(m3,ones);
			m4=_mm256_madd_epi16(m4,ones);
			m5=_mm256_madd_epi16(m5,ones);
			m6=_mm256_madd_epi16(m6,ones);

			//vertical add
			m0=_mm256_add_epi32(m0,m1);
			m0=_mm256_add_epi32(m0,m2);
			m0=_mm256_add_epi32(m0,m3);
			m0=_mm256_add_epi32(m0,m4);
			m0=_mm256_add_epi32(m0,m5);
			m0=_mm256_add_epi32(m0,m6);

			//last horizontal add
			m1=_mm256_slli_si256(m0,4);//shift by one integer
			m0=_mm256_add_epi32(m0,m1);//add
			m1=_mm256_srli_si256(m0,4);//shift by one integer
			output2=_mm256_and_si256(m1,mask_even_32);//keep only 0,2,4,6 and discard 1,3,5,7


			//3rd col iteration

			//multiply by the mask
			m0=_mm256_maddubs_epi16(r0,c0_sh2);
			m1=_mm256_maddubs_epi16(r1,c1_sh2);
			m2=_mm256_maddubs_epi16(r2,c2_sh2);
			m3=_mm256_maddubs_epi16(r3,c3_sh2);
			m4=_mm256_maddubs_epi16(r4,c4_sh2);
			m5=_mm256_maddubs_epi16(r5,c5_sh2);
			m6=_mm256_maddubs_epi16(r6,c6_sh2);

			//shift m0-m6 by one position because hadd follows
			m0=my_rshift16_by_one(m0,mask3);
			m1=my_rshift16_by_one(m1,mask3);
			m2=my_rshift16_by_one(m2,mask3);
			m3=my_rshift16_by_one(m3,mask3);
			m4=my_rshift16_by_one(m4,mask3);
			m5=my_rshift16_by_one(m5,mask3);
			m6=my_rshift16_by_one(m6,mask3);

			//make 16-bit values 32-bit (horizontal add)
			m0=_mm256_madd_epi16(m0,ones);
			m1=_mm256_madd_epi16(m1,ones);
			m2=_mm256_madd_epi16(m2,ones);
			m3=_mm256_madd_epi16(m3,ones);
			m4=_mm256_madd_epi16(m4,ones);
			m5=_mm256_madd_epi16(m5,ones);
			m6=_mm256_madd_epi16(m6,ones);

			//vertical add
			m0=_mm256_add_epi32(m0,m1);
			m0=_mm256_add_epi32(m0,m2);
			m0=_mm256_add_epi32(m0,m3);
			m0=_mm256_add_epi32(m0,m4);
			m0=_mm256_add_epi32(m0,m5);
			m0=_mm256_add_epi32(m0,m6);

			//last horizontal add
			m1=_mm256_slli_si256(m0,4);//shift by one integer
			m0=_mm256_add_epi32(m0,m1);//add
			m1=_mm256_srli_si256(m0,4);//shift by one integer
			output3=_mm256_and_si256(m1,mask_even_32);//keep only 0,2,4,6 and discard 1,3,5,7


			//4th col iteration

			//multiply by the mask
			m0=_mm256_maddubs_epi16(r0,c0_sh3);
			m1=_mm256_maddubs_epi16(r1,c1_sh3);
			m2=_mm256_maddubs_epi16(r2,c2_sh3);
			m3=_mm256_maddubs_epi16(r3,c3_sh3);
			m4=_mm256_maddubs_epi16(r4,c4_sh3);
			m5=_mm256_maddubs_epi16(r5,c5_sh3);
			m6=_mm256_maddubs_epi16(r6,c6_sh3);

			//shift m0-m6 by one position because hadd follows
			m0=my_rshift16_by_one(m0,mask3);
			m1=my_rshift16_by_one(m1,mask3);
			m2=my_rshift16_by_one(m2,mask3);
			m3=my_rshift16_by_one(m3,mask3);
			m4=my_rshift16_by_one(m4,mask3);
			m5=my_rshift16_by_one(m5,mask3);
			m6=my_rshift16_by_one(m6,mask3);

			//make 16-bit values 32-bit (horizontal add)
			m0=_mm256_madd_epi16(m0,ones);
			m1=_mm256_madd_epi16(m1,ones);
			m2=_mm256_madd_epi16(m2,ones);
			m3=_mm256_madd_epi16(m3,ones);
			m4=_mm256_madd_epi16(m4,ones);
			m5=_mm256_madd_epi16(m5,ones);
			m6=_mm256_madd_epi16(m6,ones);

			//vertical add
			m0=_mm256_add_epi32(m0,m1);
			m0=_mm256_add_epi32(m0,m2);
			m0=_mm256_add_epi32(m0,m3);
			m0=_mm256_add_epi32(m0,m4);
			m0=_mm256_add_epi32(m0,m5);
			m0=_mm256_add_epi32(m0,m6);

			//last horizontal add
			m1=_mm256_slli_si256(m0,4);//shift by one integer
			m0=_mm256_add_epi32(m0,m1);//add
			m1=_mm256_srli_si256(m0,4);//shift by one integer
			output4=_mm256_and_si256(m1,mask_even_32);//keep only 0,2,4,6 and discard 1,3,5,7


			//5th col iteration

			//multiply by the mask
			m0=_mm256_maddubs_epi16(r0,c0_sh4);
			m1=_mm256_maddubs_epi16(r1,c1_sh4);
			m2=_mm256_maddubs_epi16(r2,c2_sh4);
			m3=_mm256_maddubs_epi16(r3,c3_sh4);
			m4=_mm256_maddubs_epi16(r4,c4_sh4);
			m5=_mm256_maddubs_epi16(r5,c5_sh4);
			m6=_mm256_maddubs_epi16(r6,c6_sh4);

			//No need for shift m0-m6 now

			//make 16-bit values 32-bit (horizontal add)
			m0=_mm256_madd_epi16(m0,ones);
			m1=_mm256_madd_epi16(m1,ones);
			m2=_mm256_madd_epi16(m2,ones);
			m3=_mm256_madd_epi16(m3,ones);
			m4=_mm256_madd_epi16(m4,ones);
			m5=_mm256_madd_epi16(m5,ones);
			m6=_mm256_madd_epi16(m6,ones);

			//vertical add
			m0=_mm256_add_epi32(m0,m1);
			m0=_mm256_add_epi32(m0,m2);
			m0=_mm256_add_epi32(m0,m3);
			m0=_mm256_add_epi32(m0,m4);
			m0=_mm256_add_epi32(m0,m5);
			m0=_mm256_add_epi32(m0,m6);


			//last horizontal add
			m3=_mm256_srli_si256(m0,4);
			m4=_mm256_and_si256(m0,mask4);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_slli_si256(m4,12);//shift 3 int positions
			m3=_mm256_add_epi32(m3,m4);
			m0=_mm256_add_epi32(m0,m3);
			m0=_mm256_and_si256(m0,mask_odd_32);//keep only 1,3,5,7 and discard 0,2,4,6

			//MERGE output1 with output5 and DIVIDE
			output1=_mm256_add_epi32(m0,output1);
			output1 = division_32(division_case,output1,f);


			//6th col iteration

			//multiply by the mask
			m0=_mm256_maddubs_epi16(r0,c0_sh5);
			m1=_mm256_maddubs_epi16(r1,c1_sh5);
			m2=_mm256_maddubs_epi16(r2,c2_sh5);
			m3=_mm256_maddubs_epi16(r3,c3_sh5);
			m4=_mm256_maddubs_epi16(r4,c4_sh5);
			m5=_mm256_maddubs_epi16(r5,c5_sh5);
			m6=_mm256_maddubs_epi16(r6,c6_sh5);

			//No need for shift m0-m6 now

			//make 16-bit values 32-bit (horizontal add)
			m0=_mm256_madd_epi16(m0,ones);
			m1=_mm256_madd_epi16(m1,ones);
			m2=_mm256_madd_epi16(m2,ones);
			m3=_mm256_madd_epi16(m3,ones);
			m4=_mm256_madd_epi16(m4,ones);
			m5=_mm256_madd_epi16(m5,ones);
			m6=_mm256_madd_epi16(m6,ones);

			//vertical add
			m0=_mm256_add_epi32(m0,m1);
			m0=_mm256_add_epi32(m0,m2);
			m0=_mm256_add_epi32(m0,m3);
			m0=_mm256_add_epi32(m0,m4);
			m0=_mm256_add_epi32(m0,m5);
			m0=_mm256_add_epi32(m0,m6);

			//last horizontal add
			m3=_mm256_srli_si256(m0,4);
			m4=_mm256_and_si256(m0,mask4);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_slli_si256(m4,12);//shift 3 int positions
			m3=_mm256_add_epi32(m3,m4);
			m0=_mm256_add_epi32(m0,m3);
			m0=_mm256_and_si256(m0,mask_odd_32);//keep only 1,3,5,7 and discard 0,2,4,6

			//MERGE output1 with output5 and DIVIDE
			output2=_mm256_add_epi32(m0,output2);
			output2 = division_32(division_case,output2,f);


			//7th col iteration

			//multiply by the mask
			m0=_mm256_maddubs_epi16(r0,c0_sh6);
			m1=_mm256_maddubs_epi16(r1,c1_sh6);
			m2=_mm256_maddubs_epi16(r2,c2_sh6);
			m3=_mm256_maddubs_epi16(r3,c3_sh6);
			m4=_mm256_maddubs_epi16(r4,c4_sh6);
			m5=_mm256_maddubs_epi16(r5,c5_sh6);
			m6=_mm256_maddubs_epi16(r6,c6_sh6);

			//shift m0-m6 by one position because hadd follows
			m0=my_rshift16_by_one(m0,mask3);
			m1=my_rshift16_by_one(m1,mask3);
			m2=my_rshift16_by_one(m2,mask3);
			m3=my_rshift16_by_one(m3,mask3);
			m4=my_rshift16_by_one(m4,mask3);
			m5=my_rshift16_by_one(m5,mask3);
			m6=my_rshift16_by_one(m6,mask3);

			//make 16-bit values 32-bit (horizontal add)
			m0=_mm256_madd_epi16(m0,ones);
			m1=_mm256_madd_epi16(m1,ones);
			m2=_mm256_madd_epi16(m2,ones);
			m3=_mm256_madd_epi16(m3,ones);
			m4=_mm256_madd_epi16(m4,ones);
			m5=_mm256_madd_epi16(m5,ones);
			m6=_mm256_madd_epi16(m6,ones);

			//vertical add
			m0=_mm256_add_epi32(m0,m1);
			m0=_mm256_add_epi32(m0,m2);
			m0=_mm256_add_epi32(m0,m3);
			m0=_mm256_add_epi32(m0,m4);
			m0=_mm256_add_epi32(m0,m5);
			m0=_mm256_add_epi32(m0,m6);

			//last horizontal add
			m3=_mm256_srli_si256(m0,4);
			m4=_mm256_and_si256(m0,mask4);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_slli_si256(m4,12);//shift 3 int positions
			m3=_mm256_add_epi32(m3,m4);
			m0=_mm256_add_epi32(m0,m3);
			m0=_mm256_and_si256(m0,mask_odd_32);//keep only 1,3,5,7 and discard 0,2,4,6

			//MERGE output1 with output5 and DIVIDE
			output3=_mm256_add_epi32(m0,output3);
			output3 = division_32(division_case,output3,f);

			//8th col iteration

			//multiply by the mask
			m0=_mm256_maddubs_epi16(r0,c0_sh7);
			m1=_mm256_maddubs_epi16(r1,c1_sh7);
			m2=_mm256_maddubs_epi16(r2,c2_sh7);
			m3=_mm256_maddubs_epi16(r3,c3_sh7);
			m4=_mm256_maddubs_epi16(r4,c4_sh7);
			m5=_mm256_maddubs_epi16(r5,c5_sh7);
			m6=_mm256_maddubs_epi16(r6,c6_sh7);

			//shift m0-m6 by one position because hadd follows
			m0=my_rshift16_by_one(m0,mask3);
			m1=my_rshift16_by_one(m1,mask3);
			m2=my_rshift16_by_one(m2,mask3);
			m3=my_rshift16_by_one(m3,mask3);
			m4=my_rshift16_by_one(m4,mask3);
			m5=my_rshift16_by_one(m5,mask3);
			m6=my_rshift16_by_one(m6,mask3);

			//make 16-bit values 32-bit (horizontal add)
			m0=_mm256_madd_epi16(m0,ones);
			m1=_mm256_madd_epi16(m1,ones);
			m2=_mm256_madd_epi16(m2,ones);
			m3=_mm256_madd_epi16(m3,ones);
			m4=_mm256_madd_epi16(m4,ones);
			m5=_mm256_madd_epi16(m5,ones);
			m6=_mm256_madd_epi16(m6,ones);

			//vertical add
			m0=_mm256_add_epi32(m0,m1);
			m0=_mm256_add_epi32(m0,m2);
			m0=_mm256_add_epi32(m0,m3);
			m0=_mm256_add_epi32(m0,m4);
			m0=_mm256_add_epi32(m0,m5);
			m0=_mm256_add_epi32(m0,m6);

			//last horizontal add
			m3=_mm256_srli_si256(m0,4);
			m4=_mm256_and_si256(m0,mask4);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_slli_si256(m4,12);//shift 3 int positions
			m3=_mm256_add_epi32(m3,m4);
			m0=_mm256_add_epi32(m0,m3);
			m0=_mm256_and_si256(m0,mask_odd_32);//keep only 1,3,5,7 and discard 0,2,4,6

			//MERGE output1 with output5 and DIVIDE
			output4=_mm256_add_epi32(m0,output4);
			output4 = division_32(division_case,output4,f);



			//blend output1, output2, output3, output4 into one register
			output2 = _mm256_slli_si256(output2,1);
			output1 = _mm256_add_epi8(output1,output2);
			output3 = _mm256_slli_si256(output3,2);
			output1 = _mm256_add_epi8(output1,output3);
			output4 = _mm256_slli_si256(output4,3);
			output1 = _mm256_add_epi8(output1,output4);

			_mm256_storeu_si256( (__m256i *) &filt[row][col],output1 );
		}

		}
}

}//end of parallel


}






int convolution_old_7x7_blocking(unsigned char **frame1,unsigned char **filt,const unsigned int M, const unsigned int N, const unsigned short int divisor, signed char **kernel){

const signed char f00=kernel[0][0];
const signed char f01=kernel[0][1];
const signed char f02=kernel[0][2];
const signed char f03=kernel[0][3];
const signed char f04=kernel[0][4];
const signed char f05=kernel[0][5];
const signed char f06=kernel[0][6];

const signed char f10=kernel[1][0];
const signed char f11=kernel[1][1];
const signed char f12=kernel[1][2];
const signed char f13=kernel[1][3];
const signed char f14=kernel[1][4];
const signed char f15=kernel[1][5];
const signed char f16=kernel[1][6];

const signed char f20=kernel[2][0];
const signed char f21=kernel[2][1];
const signed char f22=kernel[2][2];
const signed char f23=kernel[2][3];
const signed char f24=kernel[2][4];
const signed char f25=kernel[2][5];
const signed char f26=kernel[2][6];

const signed char f30=kernel[3][0];
const signed char f31=kernel[3][1];
const signed char f32=kernel[3][2];
const signed char f33=kernel[3][3];
const signed char f34=kernel[3][4];
const signed char f35=kernel[3][5];
const signed char f36=kernel[3][6];

const signed char f40=kernel[4][0];
const signed char f41=kernel[4][1];
const signed char f42=kernel[4][2];
const signed char f43=kernel[4][3];
const signed char f44=kernel[4][4];
const signed char f45=kernel[4][5];
const signed char f46=kernel[4][6];

const signed char f50=kernel[5][0];
const signed char f51=kernel[5][1];
const signed char f52=kernel[5][2];
const signed char f53=kernel[5][3];
const signed char f54=kernel[5][4];
const signed char f55=kernel[5][5];
const signed char f56=kernel[5][6];

const signed char f60=kernel[6][0];
const signed char f61=kernel[6][1];
const signed char f62=kernel[6][2];
const signed char f63=kernel[6][3];
const signed char f64=kernel[6][4];
const signed char f65=kernel[6][5];
const signed char f66=kernel[6][6];

const signed char mask_vector[56][32] __attribute__((aligned(64))) ={
	{f00,f01,f02,f03,f04,f05,f06,0,f00,f01,f02,f03,f04,f05,f06,0,f00,f01,f02,f03,f04,f05,f06,0,f00,f01,f02,f03,f04,f05,f06,0},
	{f10,f11,f12,f13,f14,f15,f16,0,f10,f11,f12,f13,f14,f15,f16,0,f10,f11,f12,f13,f14,f15,f16,0,f10,f11,f12,f13,f14,f15,f16,0},
	{f20,f21,f22,f23,f24,f25,f26,0,f20,f21,f22,f23,f24,f25,f26,0,f20,f21,f22,f23,f24,f25,f26,0,f20,f21,f22,f23,f24,f25,f26,0},
	{f30,f31,f32,f33,f34,f35,f36,0,f30,f31,f32,f33,f34,f35,f36,0,f30,f31,f32,f33,f34,f35,f36,0,f30,f31,f32,f33,f34,f35,f36,0},
	{f40,f41,f42,f43,f44,f45,f46,0,f40,f41,f42,f43,f44,f45,f46,0,f40,f41,f42,f43,f44,f45,f46,0,f40,f41,f42,f43,f44,f45,f46,0},
	{f50,f51,f52,f53,f54,f55,f56,0,f50,f51,f52,f53,f54,f55,f56,0,f50,f51,f52,f53,f54,f55,f56,0,f50,f51,f52,f53,f54,f55,f56,0},
	{f60,f61,f62,f63,f64,f65,f66,0,f60,f61,f62,f63,f64,f65,f66,0,f60,f61,f62,f63,f64,f65,f66,0,f60,f61,f62,f63,f64,f65,f66,0},

	{0,f00,f01,f02,f03,f04,f05,f06,0,f00,f01,f02,f03,f04,f05,f06,0,f00,f01,f02,f03,f04,f05,f06,0,f00,f01,f02,f03,f04,f05,f06},
	{0,f10,f11,f12,f13,f14,f15,f16,0,f10,f11,f12,f13,f14,f15,f16,0,f10,f11,f12,f13,f14,f15,f16,0,f10,f11,f12,f13,f14,f15,f16},
	{0,f20,f21,f22,f23,f24,f25,f26,0,f20,f21,f22,f23,f24,f25,f26,0,f20,f21,f22,f23,f24,f25,f26,0,f20,f21,f22,f23,f24,f25,f26},
	{0,f30,f31,f32,f33,f34,f35,f36,0,f30,f31,f32,f33,f34,f35,f36,0,f30,f31,f32,f33,f34,f35,f36,0,f30,f31,f32,f33,f34,f35,f36},
	{0,f40,f41,f42,f43,f44,f45,f46,0,f40,f41,f42,f43,f44,f45,f46,0,f40,f41,f42,f43,f44,f45,f46,0,f40,f41,f42,f43,f44,f45,f46},
	{0,f50,f51,f52,f53,f54,f55,f56,0,f50,f51,f52,f53,f54,f55,f56,0,f50,f51,f52,f53,f54,f55,f56,0,f50,f51,f52,f53,f54,f55,f56},
	{0,f60,f61,f62,f63,f64,f65,f66,0,f60,f61,f62,f63,f64,f65,f66,0,f60,f61,f62,f63,f64,f65,f66,0,f60,f61,f62,f63,f64,f65,f66},

	{0,0,f00,f01,f02,f03,f04,f05,f06,0,f00,f01,f02,f03,f04,f05,f06,0,f00,f01,f02,f03,f04,f05,f06,0,0,0,0,0,0,0},
	{0,0,f10,f11,f12,f13,f14,f15,f16,0,f10,f11,f12,f13,f14,f15,f16,0,f10,f11,f12,f13,f14,f15,f16,0,0,0,0,0,0,0},
	{0,0,f20,f21,f22,f23,f24,f25,f26,0,f20,f21,f22,f23,f24,f25,f26,0,f20,f21,f22,f23,f24,f25,f26,0,0,0,0,0,0,0},
	{0,0,f30,f31,f32,f33,f34,f35,f36,0,f30,f31,f32,f33,f34,f35,f36,0,f30,f31,f32,f33,f34,f35,f36,0,0,0,0,0,0,0},
	{0,0,f40,f41,f42,f43,f44,f45,f46,0,f40,f41,f42,f43,f44,f45,f46,0,f40,f41,f42,f43,f44,f45,f46,0,0,0,0,0,0,0},
	{0,0,f50,f51,f52,f53,f54,f55,f56,0,f50,f51,f52,f53,f54,f55,f56,0,f50,f51,f52,f53,f54,f55,f56,0,0,0,0,0,0,0},
	{0,0,f60,f61,f62,f63,f64,f65,f66,0,f60,f61,f62,f63,f64,f65,f66,0,f60,f61,f62,f63,f64,f65,f66,0,0,0,0,0,0,0},

	{0,0,0,f00,f01,f02,f03,f04,f05,f06,0,f00,f01,f02,f03,f04,f05,f06,0,f00,f01,f02,f03,f04,f05,f06,0,0,0,0,0,0},
	{0,0,0,f10,f11,f12,f13,f14,f15,f16,0,f10,f11,f12,f13,f14,f15,f16,0,f10,f11,f12,f13,f14,f15,f16,0,0,0,0,0,0},
	{0,0,0,f20,f21,f22,f23,f24,f25,f26,0,f20,f21,f22,f23,f24,f25,f26,0,f20,f21,f22,f23,f24,f25,f26,0,0,0,0,0,0},
	{0,0,0,f30,f31,f32,f33,f34,f35,f36,0,f30,f31,f32,f33,f34,f35,f36,0,f30,f31,f32,f33,f34,f35,f36,0,0,0,0,0,0},
	{0,0,0,f40,f41,f42,f43,f44,f45,f46,0,f40,f41,f42,f43,f44,f45,f46,0,f40,f41,f42,f43,f44,f45,f46,0,0,0,0,0,0},
	{0,0,0,f50,f51,f52,f53,f54,f55,f56,0,f50,f51,f52,f53,f54,f55,f56,0,f50,f51,f52,f53,f54,f55,f56,0,0,0,0,0,0},
	{0,0,0,f60,f61,f62,f63,f64,f65,f66,0,f60,f61,f62,f63,f64,f65,f66,0,f60,f61,f62,f63,f64,f65,f66,0,0,0,0,0,0},

	{0,0,0,0,f00,f01,f02,f03,f04,f05,f06,0,f00,f01,f02,f03,f04,f05,f06,0,f00,f01,f02,f03,f04,f05,f06,0,0,0,0,0},
	{0,0,0,0,f10,f11,f12,f13,f14,f15,f16,0,f10,f11,f12,f13,f14,f15,f16,0,f10,f11,f12,f13,f14,f15,f16,0,0,0,0,0},
	{0,0,0,0,f20,f21,f22,f23,f24,f25,f26,0,f20,f21,f22,f23,f24,f25,f26,0,f20,f21,f22,f23,f24,f25,f26,0,0,0,0,0},
	{0,0,0,0,f30,f31,f32,f33,f34,f35,f36,0,f30,f31,f32,f33,f34,f35,f36,0,f30,f31,f32,f33,f34,f35,f36,0,0,0,0,0},
	{0,0,0,0,f40,f41,f42,f43,f44,f45,f46,0,f40,f41,f42,f43,f44,f45,f46,0,f40,f41,f42,f43,f44,f45,f46,0,0,0,0,0},
	{0,0,0,0,f50,f51,f52,f53,f54,f55,f56,0,f50,f51,f52,f53,f54,f55,f56,0,f50,f51,f52,f53,f54,f55,f56,0,0,0,0,0},
	{0,0,0,0,f60,f61,f62,f63,f64,f65,f66,0,f60,f61,f62,f63,f64,f65,f66,0,f60,f61,f62,f63,f64,f65,f66,0,0,0,0,0},

	{0,0,0,0,0,f00,f01,f02,f03,f04,f05,f06,0,f00,f01,f02,f03,f04,f05,f06,0,f00,f01,f02,f03,f04,f05,f06,0,0,0,0},
	{0,0,0,0,0,f10,f11,f12,f13,f14,f15,f16,0,f10,f11,f12,f13,f14,f15,f16,0,f10,f11,f12,f13,f14,f15,f16,0,0,0,0},
	{0,0,0,0,0,f20,f21,f22,f23,f24,f25,f26,0,f20,f21,f22,f23,f24,f25,f26,0,f20,f21,f22,f23,f24,f25,f26,0,0,0,0},
	{0,0,0,0,0,f30,f31,f32,f33,f34,f35,f36,0,f30,f31,f32,f33,f34,f35,f36,0,f30,f31,f32,f33,f34,f35,f36,0,0,0,0},
	{0,0,0,0,0,f40,f41,f42,f43,f44,f45,f46,0,f40,f41,f42,f43,f44,f45,f46,0,f40,f41,f42,f43,f44,f45,f46,0,0,0,0},
	{0,0,0,0,0,f50,f51,f52,f53,f54,f55,f56,0,f50,f51,f52,f53,f54,f55,f56,0,f50,f51,f52,f53,f54,f55,f56,0,0,0,0},
	{0,0,0,0,0,f60,f61,f62,f63,f64,f65,f66,0,f60,f61,f62,f63,f64,f65,f66,0,f60,f61,f62,f63,f64,f65,f66,0,0,0,0},

	{0,0,0,0,0,0,f00,f01,f02,f03,f04,f05,f06,0,f00,f01,f02,f03,f04,f05,f06,0,f00,f01,f02,f03,f04,f05,f06,0,0,0},
	{0,0,0,0,0,0,f10,f11,f12,f13,f14,f15,f16,0,f10,f11,f12,f13,f14,f15,f16,0,f10,f11,f12,f13,f14,f15,f16,0,0,0},
	{0,0,0,0,0,0,f20,f21,f22,f23,f24,f25,f26,0,f20,f21,f22,f23,f24,f25,f26,0,f20,f21,f22,f23,f24,f25,f26,0,0,0},
	{0,0,0,0,0,0,f30,f31,f32,f33,f34,f35,f36,0,f30,f31,f32,f33,f34,f35,f36,0,f30,f31,f32,f33,f34,f35,f36,0,0,0},
	{0,0,0,0,0,0,f40,f41,f42,f43,f44,f45,f46,0,f40,f41,f42,f43,f44,f45,f46,0,f40,f41,f42,f43,f44,f45,f46,0,0,0},
	{0,0,0,0,0,0,f50,f51,f52,f53,f54,f55,f56,0,f50,f51,f52,f53,f54,f55,f56,0,f50,f51,f52,f53,f54,f55,f56,0,0,0},
	{0,0,0,0,0,0,f60,f61,f62,f63,f64,f65,f66,0,f60,f61,f62,f63,f64,f65,f66,0,f60,f61,f62,f63,f64,f65,f66,0,0,0},

	{0,0,0,0,0,0,0,f00,f01,f02,f03,f04,f05,f06,0,f00,f01,f02,f03,f04,f05,f06,0,f00,f01,f02,f03,f04,f05,f06,0,0},
	{0,0,0,0,0,0,0,f10,f11,f12,f13,f14,f15,f16,0,f10,f11,f12,f13,f14,f15,f16,0,f10,f11,f12,f13,f14,f15,f16,0,0},
	{0,0,0,0,0,0,0,f20,f21,f22,f23,f24,f25,f26,0,f20,f21,f22,f23,f24,f25,f26,0,f20,f21,f22,f23,f24,f25,f26,0,0},
	{0,0,0,0,0,0,0,f30,f31,f32,f33,f34,f35,f36,0,f30,f31,f32,f33,f34,f35,f36,0,f30,f31,f32,f33,f34,f35,f36,0,0},
	{0,0,0,0,0,0,0,f40,f41,f42,f43,f44,f45,f46,0,f40,f41,f42,f43,f44,f45,f46,0,f40,f41,f42,f43,f44,f45,f46,0,0},
	{0,0,0,0,0,0,0,f50,f51,f52,f53,f54,f55,f56,0,f50,f51,f52,f53,f54,f55,f56,0,f50,f51,f52,f53,f54,f55,f56,0,0},
	{0,0,0,0,0,0,0,f60,f61,f62,f63,f64,f65,f66,0,f60,f61,f62,f63,f64,f65,f66,0,f60,f61,f62,f63,f64,f65,f66,0,0},
};


	const __m256i c0=_mm256_load_si256( (__m256i *) &mask_vector[0][0]);
	const __m256i c1=_mm256_load_si256( (__m256i *) &mask_vector[1][0]);
	const __m256i c2=_mm256_load_si256( (__m256i *) &mask_vector[2][0]);
	const __m256i c3=_mm256_load_si256( (__m256i *) &mask_vector[3][0]);
	const __m256i c4=_mm256_load_si256( (__m256i *) &mask_vector[4][0]);
	const __m256i c5=_mm256_load_si256( (__m256i *) &mask_vector[5][0]);
	const __m256i c6=_mm256_load_si256( (__m256i *) &mask_vector[6][0]);

	const __m256i c0_sh1=_mm256_load_si256( (__m256i *) &mask_vector[7][0]);
	const __m256i c1_sh1=_mm256_load_si256( (__m256i *) &mask_vector[8][0]);
	const __m256i c2_sh1=_mm256_load_si256( (__m256i *) &mask_vector[9][0]);
	const __m256i c3_sh1=_mm256_load_si256( (__m256i *) &mask_vector[10][0]);
	const __m256i c4_sh1=_mm256_load_si256( (__m256i *) &mask_vector[11][0]);
	const __m256i c5_sh1=_mm256_load_si256( (__m256i *) &mask_vector[12][0]);
	const __m256i c6_sh1=_mm256_load_si256( (__m256i *) &mask_vector[13][0]);

	const __m256i c0_sh2=_mm256_load_si256( (__m256i *) &mask_vector[14][0]);
	const __m256i c1_sh2=_mm256_load_si256( (__m256i *) &mask_vector[15][0]);
	const __m256i c2_sh2=_mm256_load_si256( (__m256i *) &mask_vector[16][0]);
	const __m256i c3_sh2=_mm256_load_si256( (__m256i *) &mask_vector[17][0]);
	const __m256i c4_sh2=_mm256_load_si256( (__m256i *) &mask_vector[18][0]);
	const __m256i c5_sh2=_mm256_load_si256( (__m256i *) &mask_vector[19][0]);
	const __m256i c6_sh2=_mm256_load_si256( (__m256i *) &mask_vector[20][0]);


	const __m256i c0_sh3=_mm256_load_si256( (__m256i *) &mask_vector[21][0]);
	const __m256i c1_sh3=_mm256_load_si256( (__m256i *) &mask_vector[22][0]);
	const __m256i c2_sh3=_mm256_load_si256( (__m256i *) &mask_vector[23][0]);
	const __m256i c3_sh3=_mm256_load_si256( (__m256i *) &mask_vector[24][0]);
	const __m256i c4_sh3=_mm256_load_si256( (__m256i *) &mask_vector[25][0]);
	const __m256i c5_sh3=_mm256_load_si256( (__m256i *) &mask_vector[26][0]);
	const __m256i c6_sh3=_mm256_load_si256( (__m256i *) &mask_vector[27][0]);


	const __m256i c0_sh4=_mm256_load_si256( (__m256i *) &mask_vector[28][0]);
	const __m256i c1_sh4=_mm256_load_si256( (__m256i *) &mask_vector[29][0]);
	const __m256i c2_sh4=_mm256_load_si256( (__m256i *) &mask_vector[30][0]);
	const __m256i c3_sh4=_mm256_load_si256( (__m256i *) &mask_vector[31][0]);
	const __m256i c4_sh4=_mm256_load_si256( (__m256i *) &mask_vector[32][0]);
	const __m256i c5_sh4=_mm256_load_si256( (__m256i *) &mask_vector[33][0]);
	const __m256i c6_sh4=_mm256_load_si256( (__m256i *) &mask_vector[34][0]);


	const __m256i c0_sh5=_mm256_load_si256( (__m256i *) &mask_vector[35][0]);
	const __m256i c1_sh5=_mm256_load_si256( (__m256i *) &mask_vector[36][0]);
	const __m256i c2_sh5=_mm256_load_si256( (__m256i *) &mask_vector[37][0]);
	const __m256i c3_sh5=_mm256_load_si256( (__m256i *) &mask_vector[38][0]);
	const __m256i c4_sh5=_mm256_load_si256( (__m256i *) &mask_vector[39][0]);
	const __m256i c5_sh5=_mm256_load_si256( (__m256i *) &mask_vector[40][0]);
	const __m256i c6_sh5=_mm256_load_si256( (__m256i *) &mask_vector[41][0]);

	const __m256i c0_sh6=_mm256_load_si256( (__m256i *) &mask_vector[42][0]);
	const __m256i c1_sh6=_mm256_load_si256( (__m256i *) &mask_vector[43][0]);
	const __m256i c2_sh6=_mm256_load_si256( (__m256i *) &mask_vector[44][0]);
	const __m256i c3_sh6=_mm256_load_si256( (__m256i *) &mask_vector[45][0]);
	const __m256i c4_sh6=_mm256_load_si256( (__m256i *) &mask_vector[46][0]);
	const __m256i c5_sh6=_mm256_load_si256( (__m256i *) &mask_vector[47][0]);
	const __m256i c6_sh6=_mm256_load_si256( (__m256i *) &mask_vector[48][0]);

	const __m256i c0_sh7=_mm256_load_si256( (__m256i *) &mask_vector[49][0]);
	const __m256i c1_sh7=_mm256_load_si256( (__m256i *) &mask_vector[50][0]);
	const __m256i c2_sh7=_mm256_load_si256( (__m256i *) &mask_vector[51][0]);
	const __m256i c3_sh7=_mm256_load_si256( (__m256i *) &mask_vector[52][0]);
	const __m256i c4_sh7=_mm256_load_si256( (__m256i *) &mask_vector[53][0]);
	const __m256i c5_sh7=_mm256_load_si256( (__m256i *) &mask_vector[54][0]);
	const __m256i c6_sh7=_mm256_load_si256( (__m256i *) &mask_vector[55][0]);

	//const __m256i f  = _mm256_set_epi16(52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759);
	//const __m256i mask  = _mm256_set_epi8(0,0,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255);
	//const __m256i mask2  = _mm256_set_epi8(0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0);
	//const __m256i mask6  = _mm256_set_epi8(0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0);

	const __m256i mask3  = _mm256_set_epi16(0,0,0,0,0,0,0,65535,0,0,0,0,0,0,0,0);
	const __m256i mask_prelude  = _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0);


	const __m256i mask_even_32  = _mm256_set_epi32(0,0xffffffff,0,0xffffffff,0,0xffffffff,0,0xffffffff);
	const __m256i mask_odd_32  = _mm256_set_epi32(0xffffffff,0,0xffffffff,0,0xffffffff,0,0xffffffff,0);
	const __m256i mask4  = _mm256_set_epi32(0,0,0,0xffffffff,0,0,0,0);



	//const unsigned int REMINDER_ITERATIONS =  (M-((((M-32)/26)*26)+26)); //M-(last_col_value+26)
	//printf("\n%d",REMINDER_ITERATIONS);


	const unsigned int division_case=prepare_for_division_32(divisor); //determine which is the division case (A, B or C)
	const __m256i f = _mm256_load_si256( (__m256i *) &f_vector[0]);// initialize the division vector

//printf("\n%d %d",division_case,b);

	if ((N-6)%2!=0)
		return -1;

#pragma omp parallel
{

unsigned int row,col;
__m256i r0,r1,r2,r3,r4,r5,r6,r7,m0,m1,m2,m3,m4,m5,m6,m7;
__m256i output1,output2,output3,output4;
const __m256i ones=_mm256_set1_epi16(1);

/*---------------------- Gaussian Blur ---------------------------------*/

    #pragma omp for schedule(static)
	for (row = 3; row < N-3; row+=2) {

		if (row==3){
			;
		}

		else if (row==N-4){
			;
		}

	    else {


	  for (col = 0; col <= M-32; col+=26){

		  if (col==0){

				//load the 5 rows
				r0=_mm256_loadu_si256( (__m256i *) &frame1[row-3][0]);
				r1=_mm256_loadu_si256( (__m256i *) &frame1[row-2][0]);
				r2=_mm256_loadu_si256( (__m256i *) &frame1[row-1][0]);
				r3=_mm256_loadu_si256( (__m256i *) &frame1[row][0]);
				r4=_mm256_loadu_si256( (__m256i *) &frame1[row+1][0]);
				r5=_mm256_loadu_si256( (__m256i *) &frame1[row+2][0]);
				r6=_mm256_loadu_si256( (__m256i *) &frame1[row+3][0]);
				r7=_mm256_loadu_si256( (__m256i *) &frame1[row+4][0]);


		  			//START - extra code needed for prelude
		  					m0=_mm256_slli_si256(r0,3);//shift 3 elements left - equivalent to filling with two zeros inthe beginning
		  					m1=_mm256_slli_si256(r1,3);//shift 3 elements left - equivalent to filling with two zeros inthe beginning
		  					m2=_mm256_slli_si256(r2,3);//shift 3 elements left - equivalent to filling with two zeros inthe beginning
		  					m3=_mm256_slli_si256(r3,3);//shift 3 elements left - equivalent to filling with two zeros inthe beginning
		  					m4=_mm256_slli_si256(r4,3);//shift 3 elements left - equivalent to filling with two zeros inthe beginning
		  					m5=_mm256_slli_si256(r5,3);//shift 3 elements left - equivalent to filling with two zeros inthe beginning
		  					m6=_mm256_slli_si256(r6,3);//shift 3 elements left - equivalent to filling with two zeros inthe beginning
		  					m7=_mm256_slli_si256(r7,3);//shift 3 elements left - equivalent to filling with two zeros inthe beginning

		  					r0=_mm256_and_si256(r0,mask_prelude);
		  					r0=_mm256_permute2f128_si256(r0,r0,1);
		  					r0=_mm256_srli_si256(r0,13);//shift 14 elements
		  					r0=_mm256_add_epi16(m0,r0);

		  					r1=_mm256_and_si256(r1,mask_prelude);
		  					r1=_mm256_permute2f128_si256(r1,r1,1);
		  					r1=_mm256_srli_si256(r1,13);//shift 14 elements
		  					r1=_mm256_add_epi16(m1,r1);

		  					r2=_mm256_and_si256(r2,mask_prelude);
		  					r2=_mm256_permute2f128_si256(r2,r2,1);
		  					r2=_mm256_srli_si256(r2,13);//shift 14 elements
		  					r2=_mm256_add_epi16(m2,r2);

		  					r3=_mm256_and_si256(r3,mask_prelude);
		  					r3=_mm256_permute2f128_si256(r3,r3,1);
		  					r3=_mm256_srli_si256(r3,13);//shift 14 elements
		  					r3=_mm256_add_epi16(m3,r3);

		  					r4=_mm256_and_si256(r4,mask_prelude);
		  					r4=_mm256_permute2f128_si256(r4,r4,1);
		  					r4=_mm256_srli_si256(r4,13);//shift 14 elements
		  					r4=_mm256_add_epi16(m4,r4);

		  					r5=_mm256_and_si256(r5,mask_prelude);
		  					r5=_mm256_permute2f128_si256(r5,r5,1);
		  					r5=_mm256_srli_si256(r5,13);//shift 14 elements
		  					r5=_mm256_add_epi16(m5,r5);

		  					r6=_mm256_and_si256(r6,mask_prelude);
		  					r6=_mm256_permute2f128_si256(r6,r6,1);
		  					r6=_mm256_srli_si256(r6,13);//shift 14 elements
		  					r6=_mm256_add_epi16(m6,r6);

		  					r7=_mm256_and_si256(r7,mask_prelude);
		  					r7=_mm256_permute2f128_si256(r7,r7,1);
		  					r7=_mm256_srli_si256(r7,13);//shift 14 elements
		  					r7=_mm256_add_epi16(m7,r7);
		  			//END - extra code needed for prelude
		  		}
		  else {
				//load the 5 rows
				r0=_mm256_loadu_si256( (__m256i *) &frame1[row-3][col-3]);
				r1=_mm256_loadu_si256( (__m256i *) &frame1[row-2][col-3]);
				r2=_mm256_loadu_si256( (__m256i *) &frame1[row-1][col-3]);
				r3=_mm256_loadu_si256( (__m256i *) &frame1[row][col-3]);
				r4=_mm256_loadu_si256( (__m256i *) &frame1[row+1][col-3]);
				r5=_mm256_loadu_si256( (__m256i *) &frame1[row+2][col-3]);
				r6=_mm256_loadu_si256( (__m256i *) &frame1[row+3][col-3]);
				r7=_mm256_loadu_si256( (__m256i *) &frame1[row+4][col-3]);

		  }

		// col iteration computes output pixels of 0,8,16,24
		// col+1 iteration computes output pixels of 1,9,17,25
		// col+2 iteration computes output pixels of 2,10,18
		// col+3 iteration computes output pixels of 3,11,19
		// col+4 iteration computes output pixels of 4,12,20
		// col+5 iteration computes output pixels of 5,13,19,21
		// col+6 iteration computes output pixels of 6,14,22
		// col+7 iteration computes output pixels of 7,15,19,23
		//afterwards, col becomes 26 and repeat the above process

			//1st col iteration

			//multiply by the mask
			m0=_mm256_maddubs_epi16(r0,c0);
			m1=_mm256_maddubs_epi16(r1,c1);
			m2=_mm256_maddubs_epi16(r2,c2);
			m3=_mm256_maddubs_epi16(r3,c3);
			m4=_mm256_maddubs_epi16(r4,c4);
			m5=_mm256_maddubs_epi16(r5,c5);
			m6=_mm256_maddubs_epi16(r6,c6);


			//make 16-bit values 32-bit (horizontal add)
			m0=_mm256_madd_epi16(m0,ones);
			m1=_mm256_madd_epi16(m1,ones);
			m2=_mm256_madd_epi16(m2,ones);
			m3=_mm256_madd_epi16(m3,ones);
			m4=_mm256_madd_epi16(m4,ones);
			m5=_mm256_madd_epi16(m5,ones);
			m6=_mm256_madd_epi16(m6,ones);


			//vertical add
			m0=_mm256_add_epi32(m0,m1);
			m0=_mm256_add_epi32(m0,m2);
			m0=_mm256_add_epi32(m0,m3);
			m0=_mm256_add_epi32(m0,m4);
			m0=_mm256_add_epi32(m0,m5);
			m0=_mm256_add_epi32(m0,m6);


			//last horizontal add
			m1=_mm256_slli_si256(m0,4);//shift by one integer
			m0=_mm256_add_epi32(m0,m1);//add
			m1=_mm256_srli_si256(m0,4);//shift by one integer
			output1=_mm256_and_si256(m1,mask_even_32);//keep only 0,2,4,6 and discard 1,3,5,7


			//2nd col iteration

			//multiply by the mask
			m0=_mm256_maddubs_epi16(r0,c0_sh1);
			m1=_mm256_maddubs_epi16(r1,c1_sh1);
			m2=_mm256_maddubs_epi16(r2,c2_sh1);
			m3=_mm256_maddubs_epi16(r3,c3_sh1);
			m4=_mm256_maddubs_epi16(r4,c4_sh1);
			m5=_mm256_maddubs_epi16(r5,c5_sh1);
			m6=_mm256_maddubs_epi16(r6,c6_sh1);

			//make 16-bit values 32-bit (horizontal add)
			m0=_mm256_madd_epi16(m0,ones);
			m1=_mm256_madd_epi16(m1,ones);
			m2=_mm256_madd_epi16(m2,ones);
			m3=_mm256_madd_epi16(m3,ones);
			m4=_mm256_madd_epi16(m4,ones);
			m5=_mm256_madd_epi16(m5,ones);
			m6=_mm256_madd_epi16(m6,ones);

			//vertical add
			m0=_mm256_add_epi32(m0,m1);
			m0=_mm256_add_epi32(m0,m2);
			m0=_mm256_add_epi32(m0,m3);
			m0=_mm256_add_epi32(m0,m4);
			m0=_mm256_add_epi32(m0,m5);
			m0=_mm256_add_epi32(m0,m6);

			//last horizontal add
			m1=_mm256_slli_si256(m0,4);//shift by one integer
			m0=_mm256_add_epi32(m0,m1);//add
			m1=_mm256_srli_si256(m0,4);//shift by one integer
			output2=_mm256_and_si256(m1,mask_even_32);//keep only 0,2,4,6 and discard 1,3,5,7


			//3rd col iteration

			//multiply by the mask
			m0=_mm256_maddubs_epi16(r0,c0_sh2);
			m1=_mm256_maddubs_epi16(r1,c1_sh2);
			m2=_mm256_maddubs_epi16(r2,c2_sh2);
			m3=_mm256_maddubs_epi16(r3,c3_sh2);
			m4=_mm256_maddubs_epi16(r4,c4_sh2);
			m5=_mm256_maddubs_epi16(r5,c5_sh2);
			m6=_mm256_maddubs_epi16(r6,c6_sh2);

			//shift m0-m6 by one position because hadd follows
			m0=my_rshift16_by_one(m0,mask3);
			m1=my_rshift16_by_one(m1,mask3);
			m2=my_rshift16_by_one(m2,mask3);
			m3=my_rshift16_by_one(m3,mask3);
			m4=my_rshift16_by_one(m4,mask3);
			m5=my_rshift16_by_one(m5,mask3);
			m6=my_rshift16_by_one(m6,mask3);

			//make 16-bit values 32-bit (horizontal add)
			m0=_mm256_madd_epi16(m0,ones);
			m1=_mm256_madd_epi16(m1,ones);
			m2=_mm256_madd_epi16(m2,ones);
			m3=_mm256_madd_epi16(m3,ones);
			m4=_mm256_madd_epi16(m4,ones);
			m5=_mm256_madd_epi16(m5,ones);
			m6=_mm256_madd_epi16(m6,ones);

			//vertical add
			m0=_mm256_add_epi32(m0,m1);
			m0=_mm256_add_epi32(m0,m2);
			m0=_mm256_add_epi32(m0,m3);
			m0=_mm256_add_epi32(m0,m4);
			m0=_mm256_add_epi32(m0,m5);
			m0=_mm256_add_epi32(m0,m6);

			//last horizontal add
			m1=_mm256_slli_si256(m0,4);//shift by one integer
			m0=_mm256_add_epi32(m0,m1);//add
			m1=_mm256_srli_si256(m0,4);//shift by one integer
			output3=_mm256_and_si256(m1,mask_even_32);//keep only 0,2,4,6 and discard 1,3,5,7


			//4th col iteration

			//multiply by the mask
			m0=_mm256_maddubs_epi16(r0,c0_sh3);
			m1=_mm256_maddubs_epi16(r1,c1_sh3);
			m2=_mm256_maddubs_epi16(r2,c2_sh3);
			m3=_mm256_maddubs_epi16(r3,c3_sh3);
			m4=_mm256_maddubs_epi16(r4,c4_sh3);
			m5=_mm256_maddubs_epi16(r5,c5_sh3);
			m6=_mm256_maddubs_epi16(r6,c6_sh3);

			//shift m0-m6 by one position because hadd follows
			m0=my_rshift16_by_one(m0,mask3);
			m1=my_rshift16_by_one(m1,mask3);
			m2=my_rshift16_by_one(m2,mask3);
			m3=my_rshift16_by_one(m3,mask3);
			m4=my_rshift16_by_one(m4,mask3);
			m5=my_rshift16_by_one(m5,mask3);
			m6=my_rshift16_by_one(m6,mask3);

			//make 16-bit values 32-bit (horizontal add)
			m0=_mm256_madd_epi16(m0,ones);
			m1=_mm256_madd_epi16(m1,ones);
			m2=_mm256_madd_epi16(m2,ones);
			m3=_mm256_madd_epi16(m3,ones);
			m4=_mm256_madd_epi16(m4,ones);
			m5=_mm256_madd_epi16(m5,ones);
			m6=_mm256_madd_epi16(m6,ones);

			//vertical add
			m0=_mm256_add_epi32(m0,m1);
			m0=_mm256_add_epi32(m0,m2);
			m0=_mm256_add_epi32(m0,m3);
			m0=_mm256_add_epi32(m0,m4);
			m0=_mm256_add_epi32(m0,m5);
			m0=_mm256_add_epi32(m0,m6);

			//last horizontal add
			m1=_mm256_slli_si256(m0,4);//shift by one integer
			m0=_mm256_add_epi32(m0,m1);//add
			m1=_mm256_srli_si256(m0,4);//shift by one integer
			output4=_mm256_and_si256(m1,mask_even_32);//keep only 0,2,4,6 and discard 1,3,5,7


			//5th col iteration

			//multiply by the mask
			m0=_mm256_maddubs_epi16(r0,c0_sh4);
			m1=_mm256_maddubs_epi16(r1,c1_sh4);
			m2=_mm256_maddubs_epi16(r2,c2_sh4);
			m3=_mm256_maddubs_epi16(r3,c3_sh4);
			m4=_mm256_maddubs_epi16(r4,c4_sh4);
			m5=_mm256_maddubs_epi16(r5,c5_sh4);
			m6=_mm256_maddubs_epi16(r6,c6_sh4);

			//No need for shift m0-m6 now

			//make 16-bit values 32-bit (horizontal add)
			m0=_mm256_madd_epi16(m0,ones);
			m1=_mm256_madd_epi16(m1,ones);
			m2=_mm256_madd_epi16(m2,ones);
			m3=_mm256_madd_epi16(m3,ones);
			m4=_mm256_madd_epi16(m4,ones);
			m5=_mm256_madd_epi16(m5,ones);
			m6=_mm256_madd_epi16(m6,ones);

			//vertical add
			m0=_mm256_add_epi32(m0,m1);
			m0=_mm256_add_epi32(m0,m2);
			m0=_mm256_add_epi32(m0,m3);
			m0=_mm256_add_epi32(m0,m4);
			m0=_mm256_add_epi32(m0,m5);
			m0=_mm256_add_epi32(m0,m6);


			//last horizontal add
			m3=_mm256_srli_si256(m0,4);
			m4=_mm256_and_si256(m0,mask4);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_slli_si256(m4,12);//shift 3 int positions
			m3=_mm256_add_epi32(m3,m4);
			m0=_mm256_add_epi32(m0,m3);
			m0=_mm256_and_si256(m0,mask_odd_32);//keep only 1,3,5,7 and discard 0,2,4,6

			//MERGE output1 with output5 and DIVIDE
			output1=_mm256_add_epi32(m0,output1);
			output1 = division_32(division_case,output1,f);


			//6th col iteration

			//multiply by the mask
			m0=_mm256_maddubs_epi16(r0,c0_sh5);
			m1=_mm256_maddubs_epi16(r1,c1_sh5);
			m2=_mm256_maddubs_epi16(r2,c2_sh5);
			m3=_mm256_maddubs_epi16(r3,c3_sh5);
			m4=_mm256_maddubs_epi16(r4,c4_sh5);
			m5=_mm256_maddubs_epi16(r5,c5_sh5);
			m6=_mm256_maddubs_epi16(r6,c6_sh5);

			//No need for shift m0-m6 now

			//make 16-bit values 32-bit (horizontal add)
			m0=_mm256_madd_epi16(m0,ones);
			m1=_mm256_madd_epi16(m1,ones);
			m2=_mm256_madd_epi16(m2,ones);
			m3=_mm256_madd_epi16(m3,ones);
			m4=_mm256_madd_epi16(m4,ones);
			m5=_mm256_madd_epi16(m5,ones);
			m6=_mm256_madd_epi16(m6,ones);

			//vertical add
			m0=_mm256_add_epi32(m0,m1);
			m0=_mm256_add_epi32(m0,m2);
			m0=_mm256_add_epi32(m0,m3);
			m0=_mm256_add_epi32(m0,m4);
			m0=_mm256_add_epi32(m0,m5);
			m0=_mm256_add_epi32(m0,m6);

			//last horizontal add
			m3=_mm256_srli_si256(m0,4);
			m4=_mm256_and_si256(m0,mask4);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_slli_si256(m4,12);//shift 3 int positions
			m3=_mm256_add_epi32(m3,m4);
			m0=_mm256_add_epi32(m0,m3);
			m0=_mm256_and_si256(m0,mask_odd_32);//keep only 1,3,5,7 and discard 0,2,4,6

			//MERGE output1 with output5 and DIVIDE
			output2=_mm256_add_epi32(m0,output2);
			output2 = division_32(division_case,output2,f);


			//7th col iteration

			//multiply by the mask
			m0=_mm256_maddubs_epi16(r0,c0_sh6);
			m1=_mm256_maddubs_epi16(r1,c1_sh6);
			m2=_mm256_maddubs_epi16(r2,c2_sh6);
			m3=_mm256_maddubs_epi16(r3,c3_sh6);
			m4=_mm256_maddubs_epi16(r4,c4_sh6);
			m5=_mm256_maddubs_epi16(r5,c5_sh6);
			m6=_mm256_maddubs_epi16(r6,c6_sh6);

			//shift m0-m6 by one position because hadd follows
			m0=my_rshift16_by_one(m0,mask3);
			m1=my_rshift16_by_one(m1,mask3);
			m2=my_rshift16_by_one(m2,mask3);
			m3=my_rshift16_by_one(m3,mask3);
			m4=my_rshift16_by_one(m4,mask3);
			m5=my_rshift16_by_one(m5,mask3);
			m6=my_rshift16_by_one(m6,mask3);

			//make 16-bit values 32-bit (horizontal add)
			m0=_mm256_madd_epi16(m0,ones);
			m1=_mm256_madd_epi16(m1,ones);
			m2=_mm256_madd_epi16(m2,ones);
			m3=_mm256_madd_epi16(m3,ones);
			m4=_mm256_madd_epi16(m4,ones);
			m5=_mm256_madd_epi16(m5,ones);
			m6=_mm256_madd_epi16(m6,ones);

			//vertical add
			m0=_mm256_add_epi32(m0,m1);
			m0=_mm256_add_epi32(m0,m2);
			m0=_mm256_add_epi32(m0,m3);
			m0=_mm256_add_epi32(m0,m4);
			m0=_mm256_add_epi32(m0,m5);
			m0=_mm256_add_epi32(m0,m6);

			//last horizontal add
			m3=_mm256_srli_si256(m0,4);
			m4=_mm256_and_si256(m0,mask4);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_slli_si256(m4,12);//shift 3 int positions
			m3=_mm256_add_epi32(m3,m4);
			m0=_mm256_add_epi32(m0,m3);
			m0=_mm256_and_si256(m0,mask_odd_32);//keep only 1,3,5,7 and discard 0,2,4,6

			//MERGE output1 with output5 and DIVIDE
			output3=_mm256_add_epi32(m0,output3);
			output3 = division_32(division_case,output3,f);

			//8th col iteration

			//multiply by the mask
			m0=_mm256_maddubs_epi16(r0,c0_sh7);
			m1=_mm256_maddubs_epi16(r1,c1_sh7);
			m2=_mm256_maddubs_epi16(r2,c2_sh7);
			m3=_mm256_maddubs_epi16(r3,c3_sh7);
			m4=_mm256_maddubs_epi16(r4,c4_sh7);
			m5=_mm256_maddubs_epi16(r5,c5_sh7);
			m6=_mm256_maddubs_epi16(r6,c6_sh7);

			//shift m0-m6 by one position because hadd follows
			m0=my_rshift16_by_one(m0,mask3);
			m1=my_rshift16_by_one(m1,mask3);
			m2=my_rshift16_by_one(m2,mask3);
			m3=my_rshift16_by_one(m3,mask3);
			m4=my_rshift16_by_one(m4,mask3);
			m5=my_rshift16_by_one(m5,mask3);
			m6=my_rshift16_by_one(m6,mask3);

			//make 16-bit values 32-bit (horizontal add)
			m0=_mm256_madd_epi16(m0,ones);
			m1=_mm256_madd_epi16(m1,ones);
			m2=_mm256_madd_epi16(m2,ones);
			m3=_mm256_madd_epi16(m3,ones);
			m4=_mm256_madd_epi16(m4,ones);
			m5=_mm256_madd_epi16(m5,ones);
			m6=_mm256_madd_epi16(m6,ones);

			//vertical add
			m0=_mm256_add_epi32(m0,m1);
			m0=_mm256_add_epi32(m0,m2);
			m0=_mm256_add_epi32(m0,m3);
			m0=_mm256_add_epi32(m0,m4);
			m0=_mm256_add_epi32(m0,m5);
			m0=_mm256_add_epi32(m0,m6);

			//last horizontal add
			m3=_mm256_srli_si256(m0,4);
			m4=_mm256_and_si256(m0,mask4);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_slli_si256(m4,12);//shift 3 int positions
			m3=_mm256_add_epi32(m3,m4);
			m0=_mm256_add_epi32(m0,m3);
			m0=_mm256_and_si256(m0,mask_odd_32);//keep only 1,3,5,7 and discard 0,2,4,6

			//MERGE output1 with output5 and DIVIDE
			output4=_mm256_add_epi32(m0,output4);
			output4 = division_32(division_case,output4,f);



			//blend output1, output2, output3, output4 into one register
			output2 = _mm256_slli_si256(output2,1);
			output1 = _mm256_add_epi8(output1,output2);
			output3 = _mm256_slli_si256(output3,2);
			output1 = _mm256_add_epi8(output1,output3);
			output4 = _mm256_slli_si256(output4,3);
			output1 = _mm256_add_epi8(output1,output4);

			_mm256_storeu_si256( (__m256i *) &filt[row][col],output1 );



			//1st col iteration

			//multiply by the mask
			m0=_mm256_maddubs_epi16(r1,c0);
			m1=_mm256_maddubs_epi16(r2,c1);
			m2=_mm256_maddubs_epi16(r3,c2);
			m3=_mm256_maddubs_epi16(r4,c3);
			m4=_mm256_maddubs_epi16(r5,c4);
			m5=_mm256_maddubs_epi16(r6,c5);
			m6=_mm256_maddubs_epi16(r7,c6);


			//make 16-bit values 32-bit (horizontal add)
			m0=_mm256_madd_epi16(m0,ones);
			m1=_mm256_madd_epi16(m1,ones);
			m2=_mm256_madd_epi16(m2,ones);
			m3=_mm256_madd_epi16(m3,ones);
			m4=_mm256_madd_epi16(m4,ones);
			m5=_mm256_madd_epi16(m5,ones);
			m6=_mm256_madd_epi16(m6,ones);


			//vertical add
			m0=_mm256_add_epi32(m0,m1);
			m0=_mm256_add_epi32(m0,m2);
			m0=_mm256_add_epi32(m0,m3);
			m0=_mm256_add_epi32(m0,m4);
			m0=_mm256_add_epi32(m0,m5);
			m0=_mm256_add_epi32(m0,m6);


			//last horizontal add
			m1=_mm256_slli_si256(m0,4);//shift by one integer
			m0=_mm256_add_epi32(m0,m1);//add
			m1=_mm256_srli_si256(m0,4);//shift by one integer
			output1=_mm256_and_si256(m1,mask_even_32);//keep only 0,2,4,6 and discard 1,3,5,7


			//2nd col iteration

			//multiply by the mask
			m0=_mm256_maddubs_epi16(r1,c0_sh1);
			m1=_mm256_maddubs_epi16(r2,c1_sh1);
			m2=_mm256_maddubs_epi16(r3,c2_sh1);
			m3=_mm256_maddubs_epi16(r4,c3_sh1);
			m4=_mm256_maddubs_epi16(r5,c4_sh1);
			m5=_mm256_maddubs_epi16(r6,c5_sh1);
			m6=_mm256_maddubs_epi16(r7,c6_sh1);

			//make 16-bit values 32-bit (horizontal add)
			m0=_mm256_madd_epi16(m0,ones);
			m1=_mm256_madd_epi16(m1,ones);
			m2=_mm256_madd_epi16(m2,ones);
			m3=_mm256_madd_epi16(m3,ones);
			m4=_mm256_madd_epi16(m4,ones);
			m5=_mm256_madd_epi16(m5,ones);
			m6=_mm256_madd_epi16(m6,ones);

			//vertical add
			m0=_mm256_add_epi32(m0,m1);
			m0=_mm256_add_epi32(m0,m2);
			m0=_mm256_add_epi32(m0,m3);
			m0=_mm256_add_epi32(m0,m4);
			m0=_mm256_add_epi32(m0,m5);
			m0=_mm256_add_epi32(m0,m6);

			//last horizontal add
			m1=_mm256_slli_si256(m0,4);//shift by one integer
			m0=_mm256_add_epi32(m0,m1);//add
			m1=_mm256_srli_si256(m0,4);//shift by one integer
			output2=_mm256_and_si256(m1,mask_even_32);//keep only 0,2,4,6 and discard 1,3,5,7


			//3rd col iteration

			//multiply by the mask
			m0=_mm256_maddubs_epi16(r1,c0_sh2);
			m1=_mm256_maddubs_epi16(r2,c1_sh2);
			m2=_mm256_maddubs_epi16(r3,c2_sh2);
			m3=_mm256_maddubs_epi16(r4,c3_sh2);
			m4=_mm256_maddubs_epi16(r5,c4_sh2);
			m5=_mm256_maddubs_epi16(r6,c5_sh2);
			m6=_mm256_maddubs_epi16(r7,c6_sh2);

			//shift m0-m6 by one position because hadd follows
			m0=my_rshift16_by_one(m0,mask3);
			m1=my_rshift16_by_one(m1,mask3);
			m2=my_rshift16_by_one(m2,mask3);
			m3=my_rshift16_by_one(m3,mask3);
			m4=my_rshift16_by_one(m4,mask3);
			m5=my_rshift16_by_one(m5,mask3);
			m6=my_rshift16_by_one(m6,mask3);

			//make 16-bit values 32-bit (horizontal add)
			m0=_mm256_madd_epi16(m0,ones);
			m1=_mm256_madd_epi16(m1,ones);
			m2=_mm256_madd_epi16(m2,ones);
			m3=_mm256_madd_epi16(m3,ones);
			m4=_mm256_madd_epi16(m4,ones);
			m5=_mm256_madd_epi16(m5,ones);
			m6=_mm256_madd_epi16(m6,ones);

			//vertical add
			m0=_mm256_add_epi32(m0,m1);
			m0=_mm256_add_epi32(m0,m2);
			m0=_mm256_add_epi32(m0,m3);
			m0=_mm256_add_epi32(m0,m4);
			m0=_mm256_add_epi32(m0,m5);
			m0=_mm256_add_epi32(m0,m6);

			//last horizontal add
			m1=_mm256_slli_si256(m0,4);//shift by one integer
			m0=_mm256_add_epi32(m0,m1);//add
			m1=_mm256_srli_si256(m0,4);//shift by one integer
			output3=_mm256_and_si256(m1,mask_even_32);//keep only 0,2,4,6 and discard 1,3,5,7


			//4th col iteration

			//multiply by the mask
			m0=_mm256_maddubs_epi16(r1,c0_sh3);
			m1=_mm256_maddubs_epi16(r2,c1_sh3);
			m2=_mm256_maddubs_epi16(r3,c2_sh3);
			m3=_mm256_maddubs_epi16(r4,c3_sh3);
			m4=_mm256_maddubs_epi16(r5,c4_sh3);
			m5=_mm256_maddubs_epi16(r6,c5_sh3);
			m6=_mm256_maddubs_epi16(r7,c6_sh3);

			//shift m0-m6 by one position because hadd follows
			m0=my_rshift16_by_one(m0,mask3);
			m1=my_rshift16_by_one(m1,mask3);
			m2=my_rshift16_by_one(m2,mask3);
			m3=my_rshift16_by_one(m3,mask3);
			m4=my_rshift16_by_one(m4,mask3);
			m5=my_rshift16_by_one(m5,mask3);
			m6=my_rshift16_by_one(m6,mask3);

			//make 16-bit values 32-bit (horizontal add)
			m0=_mm256_madd_epi16(m0,ones);
			m1=_mm256_madd_epi16(m1,ones);
			m2=_mm256_madd_epi16(m2,ones);
			m3=_mm256_madd_epi16(m3,ones);
			m4=_mm256_madd_epi16(m4,ones);
			m5=_mm256_madd_epi16(m5,ones);
			m6=_mm256_madd_epi16(m6,ones);

			//vertical add
			m0=_mm256_add_epi32(m0,m1);
			m0=_mm256_add_epi32(m0,m2);
			m0=_mm256_add_epi32(m0,m3);
			m0=_mm256_add_epi32(m0,m4);
			m0=_mm256_add_epi32(m0,m5);
			m0=_mm256_add_epi32(m0,m6);

			//last horizontal add
			m1=_mm256_slli_si256(m0,4);//shift by one integer
			m0=_mm256_add_epi32(m0,m1);//add
			m1=_mm256_srli_si256(m0,4);//shift by one integer
			output4=_mm256_and_si256(m1,mask_even_32);//keep only 0,2,4,6 and discard 1,3,5,7


			//5th col iteration

			//multiply by the mask
			m0=_mm256_maddubs_epi16(r1,c0_sh4);
			m1=_mm256_maddubs_epi16(r2,c1_sh4);
			m2=_mm256_maddubs_epi16(r3,c2_sh4);
			m3=_mm256_maddubs_epi16(r4,c3_sh4);
			m4=_mm256_maddubs_epi16(r5,c4_sh4);
			m5=_mm256_maddubs_epi16(r6,c5_sh4);
			m6=_mm256_maddubs_epi16(r7,c6_sh4);

			//No need for shift m0-m6 now

			//make 16-bit values 32-bit (horizontal add)
			m0=_mm256_madd_epi16(m0,ones);
			m1=_mm256_madd_epi16(m1,ones);
			m2=_mm256_madd_epi16(m2,ones);
			m3=_mm256_madd_epi16(m3,ones);
			m4=_mm256_madd_epi16(m4,ones);
			m5=_mm256_madd_epi16(m5,ones);
			m6=_mm256_madd_epi16(m6,ones);

			//vertical add
			m0=_mm256_add_epi32(m0,m1);
			m0=_mm256_add_epi32(m0,m2);
			m0=_mm256_add_epi32(m0,m3);
			m0=_mm256_add_epi32(m0,m4);
			m0=_mm256_add_epi32(m0,m5);
			m0=_mm256_add_epi32(m0,m6);


			//last horizontal add
			m3=_mm256_srli_si256(m0,4);
			m4=_mm256_and_si256(m0,mask4);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_slli_si256(m4,12);//shift 3 int positions
			m3=_mm256_add_epi32(m3,m4);
			m0=_mm256_add_epi32(m0,m3);
			m0=_mm256_and_si256(m0,mask_odd_32);//keep only 1,3,5,7 and discard 0,2,4,6

			//MERGE output1 with output5 and DIVIDE
			output1=_mm256_add_epi32(m0,output1);
			output1 = division_32(division_case,output1,f);


			//6th col iteration

			//multiply by the mask
			m0=_mm256_maddubs_epi16(r1,c0_sh5);
			m1=_mm256_maddubs_epi16(r2,c1_sh5);
			m2=_mm256_maddubs_epi16(r3,c2_sh5);
			m3=_mm256_maddubs_epi16(r4,c3_sh5);
			m4=_mm256_maddubs_epi16(r5,c4_sh5);
			m5=_mm256_maddubs_epi16(r6,c5_sh5);
			m6=_mm256_maddubs_epi16(r7,c6_sh5);

			//No need for shift m0-m6 now

			//make 16-bit values 32-bit (horizontal add)
			m0=_mm256_madd_epi16(m0,ones);
			m1=_mm256_madd_epi16(m1,ones);
			m2=_mm256_madd_epi16(m2,ones);
			m3=_mm256_madd_epi16(m3,ones);
			m4=_mm256_madd_epi16(m4,ones);
			m5=_mm256_madd_epi16(m5,ones);
			m6=_mm256_madd_epi16(m6,ones);

			//vertical add
			m0=_mm256_add_epi32(m0,m1);
			m0=_mm256_add_epi32(m0,m2);
			m0=_mm256_add_epi32(m0,m3);
			m0=_mm256_add_epi32(m0,m4);
			m0=_mm256_add_epi32(m0,m5);
			m0=_mm256_add_epi32(m0,m6);

			//last horizontal add
			m3=_mm256_srli_si256(m0,4);
			m4=_mm256_and_si256(m0,mask4);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_slli_si256(m4,12);//shift 3 int positions
			m3=_mm256_add_epi32(m3,m4);
			m0=_mm256_add_epi32(m0,m3);
			m0=_mm256_and_si256(m0,mask_odd_32);//keep only 1,3,5,7 and discard 0,2,4,6

			//MERGE output1 with output5 and DIVIDE
			output2=_mm256_add_epi32(m0,output2);
			output2 = division_32(division_case,output2,f);


			//7th col iteration

			//multiply by the mask
			m0=_mm256_maddubs_epi16(r1,c0_sh6);
			m1=_mm256_maddubs_epi16(r2,c1_sh6);
			m2=_mm256_maddubs_epi16(r3,c2_sh6);
			m3=_mm256_maddubs_epi16(r4,c3_sh6);
			m4=_mm256_maddubs_epi16(r5,c4_sh6);
			m5=_mm256_maddubs_epi16(r6,c5_sh6);
			m6=_mm256_maddubs_epi16(r7,c6_sh6);

			//shift m0-m6 by one position because hadd follows
			m0=my_rshift16_by_one(m0,mask3);
			m1=my_rshift16_by_one(m1,mask3);
			m2=my_rshift16_by_one(m2,mask3);
			m3=my_rshift16_by_one(m3,mask3);
			m4=my_rshift16_by_one(m4,mask3);
			m5=my_rshift16_by_one(m5,mask3);
			m6=my_rshift16_by_one(m6,mask3);

			//make 16-bit values 32-bit (horizontal add)
			m0=_mm256_madd_epi16(m0,ones);
			m1=_mm256_madd_epi16(m1,ones);
			m2=_mm256_madd_epi16(m2,ones);
			m3=_mm256_madd_epi16(m3,ones);
			m4=_mm256_madd_epi16(m4,ones);
			m5=_mm256_madd_epi16(m5,ones);
			m6=_mm256_madd_epi16(m6,ones);

			//vertical add
			m0=_mm256_add_epi32(m0,m1);
			m0=_mm256_add_epi32(m0,m2);
			m0=_mm256_add_epi32(m0,m3);
			m0=_mm256_add_epi32(m0,m4);
			m0=_mm256_add_epi32(m0,m5);
			m0=_mm256_add_epi32(m0,m6);

			//last horizontal add
			m3=_mm256_srli_si256(m0,4);
			m4=_mm256_and_si256(m0,mask4);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_slli_si256(m4,12);//shift 3 int positions
			m3=_mm256_add_epi32(m3,m4);
			m0=_mm256_add_epi32(m0,m3);
			m0=_mm256_and_si256(m0,mask_odd_32);//keep only 1,3,5,7 and discard 0,2,4,6

			//MERGE output1 with output5 and DIVIDE
			output3=_mm256_add_epi32(m0,output3);
			output3 = division_32(division_case,output3,f);

			//8th col iteration

			//multiply by the mask
			m0=_mm256_maddubs_epi16(r1,c0_sh7);
			m1=_mm256_maddubs_epi16(r2,c1_sh7);
			m2=_mm256_maddubs_epi16(r3,c2_sh7);
			m3=_mm256_maddubs_epi16(r4,c3_sh7);
			m4=_mm256_maddubs_epi16(r5,c4_sh7);
			m5=_mm256_maddubs_epi16(r6,c5_sh7);
			m6=_mm256_maddubs_epi16(r7,c6_sh7);

			//shift m0-m6 by one position because hadd follows
			m0=my_rshift16_by_one(m0,mask3);
			m1=my_rshift16_by_one(m1,mask3);
			m2=my_rshift16_by_one(m2,mask3);
			m3=my_rshift16_by_one(m3,mask3);
			m4=my_rshift16_by_one(m4,mask3);
			m5=my_rshift16_by_one(m5,mask3);
			m6=my_rshift16_by_one(m6,mask3);

			//make 16-bit values 32-bit (horizontal add)
			m0=_mm256_madd_epi16(m0,ones);
			m1=_mm256_madd_epi16(m1,ones);
			m2=_mm256_madd_epi16(m2,ones);
			m3=_mm256_madd_epi16(m3,ones);
			m4=_mm256_madd_epi16(m4,ones);
			m5=_mm256_madd_epi16(m5,ones);
			m6=_mm256_madd_epi16(m6,ones);

			//vertical add
			m0=_mm256_add_epi32(m0,m1);
			m0=_mm256_add_epi32(m0,m2);
			m0=_mm256_add_epi32(m0,m3);
			m0=_mm256_add_epi32(m0,m4);
			m0=_mm256_add_epi32(m0,m5);
			m0=_mm256_add_epi32(m0,m6);

			//last horizontal add
			m3=_mm256_srli_si256(m0,4);
			m4=_mm256_and_si256(m0,mask4);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_slli_si256(m4,12);//shift 3 int positions
			m3=_mm256_add_epi32(m3,m4);
			m0=_mm256_add_epi32(m0,m3);
			m0=_mm256_and_si256(m0,mask_odd_32);//keep only 1,3,5,7 and discard 0,2,4,6

			//MERGE output1 with output5 and DIVIDE
			output4=_mm256_add_epi32(m0,output4);
			output4 = division_32(division_case,output4,f);



			//blend output1, output2, output3, output4 into one register
			output2 = _mm256_slli_si256(output2,1);
			output1 = _mm256_add_epi8(output1,output2);
			output3 = _mm256_slli_si256(output3,2);
			output1 = _mm256_add_epi8(output1,output3);
			output4 = _mm256_slli_si256(output4,3);
			output1 = _mm256_add_epi8(output1,output4);

			_mm256_storeu_si256( (__m256i *) &filt[row+1][col],output1 );

		}

		}
}

}//end of parallel

return 0;
}


inline __m256i my_rshift16_by_one(__m256i m0,__m256i mask3){

//shift m0 by one position because hadd follows
__m256i m3=_mm256_srli_si256(m0,2);
__m256i m4=_mm256_and_si256(m0,mask3);
m4=_mm256_permute2f128_si256(m4,m4,1);
m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
return _mm256_add_epi16(m3,m4);

}

void debug(int r, int c, unsigned char **frame1, signed char **kernel){
	int temp=0;
	int row=r-3;
	int col=c-3;

	for (int i=row;i<row+7;i++){
		printf("\n");
		for (int j=col;j<col+7;j++){
			temp+=frame1[i][j]*kernel[i-row][j-col];
			//printf(" %d x %d + ",frame1[i][j],kernel[i-row][j-col]);
			printf(" %d + ",frame1[i][j]*kernel[i-row][j-col]);
		}
	}

	printf("\n debug function gives %d",temp);
}



void convolution_optimized_7x7_32(unsigned char **frame1,unsigned char **filt,const unsigned int M, const unsigned int N, const unsigned short int divisor, signed char **kernel){

const signed char f00=kernel[0][0];
const signed char f01=kernel[0][1];
const signed char f02=kernel[0][2];
const signed char f03=kernel[0][3];
const signed char f04=kernel[0][4];
const signed char f05=kernel[0][5];
const signed char f06=kernel[0][6];

const signed char f10=kernel[1][0];
const signed char f11=kernel[1][1];
const signed char f12=kernel[1][2];
const signed char f13=kernel[1][3];
const signed char f14=kernel[1][4];
const signed char f15=kernel[1][5];
const signed char f16=kernel[1][6];

const signed char f20=kernel[2][0];
const signed char f21=kernel[2][1];
const signed char f22=kernel[2][2];
const signed char f23=kernel[2][3];
const signed char f24=kernel[2][4];
const signed char f25=kernel[2][5];
const signed char f26=kernel[2][6];

const signed char f30=kernel[3][0];
const signed char f31=kernel[3][1];
const signed char f32=kernel[3][2];
const signed char f33=kernel[3][3];
const signed char f34=kernel[3][4];
const signed char f35=kernel[3][5];
const signed char f36=kernel[3][6];

const signed char f40=kernel[4][0];
const signed char f41=kernel[4][1];
const signed char f42=kernel[4][2];
const signed char f43=kernel[4][3];
const signed char f44=kernel[4][4];
const signed char f45=kernel[4][5];
const signed char f46=kernel[4][6];

const signed char f50=kernel[5][0];
const signed char f51=kernel[5][1];
const signed char f52=kernel[5][2];
const signed char f53=kernel[5][3];
const signed char f54=kernel[5][4];
const signed char f55=kernel[5][5];
const signed char f56=kernel[5][6];

const signed char f60=kernel[6][0];
const signed char f61=kernel[6][1];
const signed char f62=kernel[6][2];
const signed char f63=kernel[6][3];
const signed char f64=kernel[6][4];
const signed char f65=kernel[6][5];
const signed char f66=kernel[6][6];

const signed char mask_vector[7][32] __attribute__((aligned(64))) ={
	{f00,f01,f02,f03,f04,f05,f06,0,f00,f01,f02,f03,f04,f05,f06,0,f00,f01,f02,f03,f04,f05,f06,0,f00,f01,f02,f03,f04,f05,f06,0},
	{f10,f11,f12,f13,f14,f15,f16,0,f10,f11,f12,f13,f14,f15,f16,0,f10,f11,f12,f13,f14,f15,f16,0,f10,f11,f12,f13,f14,f15,f16,0},
	{f20,f21,f22,f23,f24,f25,f26,0,f20,f21,f22,f23,f24,f25,f26,0,f20,f21,f22,f23,f24,f25,f26,0,f20,f21,f22,f23,f24,f25,f26,0},
	{f30,f31,f32,f33,f34,f35,f36,0,f30,f31,f32,f33,f34,f35,f36,0,f30,f31,f32,f33,f34,f35,f36,0,f30,f31,f32,f33,f34,f35,f36,0},
	{f40,f41,f42,f43,f44,f45,f46,0,f40,f41,f42,f43,f44,f45,f46,0,f40,f41,f42,f43,f44,f45,f46,0,f40,f41,f42,f43,f44,f45,f46,0},
	{f50,f51,f52,f53,f54,f55,f56,0,f50,f51,f52,f53,f54,f55,f56,0,f50,f51,f52,f53,f54,f55,f56,0,f50,f51,f52,f53,f54,f55,f56,0},
	{f60,f61,f62,f63,f64,f65,f66,0,f60,f61,f62,f63,f64,f65,f66,0,f60,f61,f62,f63,f64,f65,f66,0,f60,f61,f62,f63,f64,f65,f66,0},

};


	const __m256i c0=_mm256_load_si256( (__m256i *) &mask_vector[0][0]);
	const __m256i c1=_mm256_load_si256( (__m256i *) &mask_vector[1][0]);
	const __m256i c2=_mm256_load_si256( (__m256i *) &mask_vector[2][0]);
	const __m256i c3=_mm256_load_si256( (__m256i *) &mask_vector[3][0]);
	const __m256i c4=_mm256_load_si256( (__m256i *) &mask_vector[4][0]);
	const __m256i c5=_mm256_load_si256( (__m256i *) &mask_vector[5][0]);
	const __m256i c6=_mm256_load_si256( (__m256i *) &mask_vector[6][0]);



	//const __m256i f  = _mm256_set_epi16(52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759);
	//const __m256i mask  = _mm256_set_epi8(0,0,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255);
	//const __m256i mask2  = _mm256_set_epi8(0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0);
	//const __m256i mask6  = _mm256_set_epi8(0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0);

	const __m256i mask_prelude  = _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0);
	const __m256i mask_prelude2  = _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0);
	const __m256i mask_prelude3  = _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0);


	const __m256i mask_even_32  = _mm256_set_epi32(0,0xffffffff,0,0xffffffff,0,0xffffffff,0,0xffffffff);




	const unsigned int REMINDER_ITERATIONS =  (M-((((M-36)/32)*32)+32));
	//printf("\nreminder iterations %d",REMINDER_ITERATIONS);


	const unsigned int division_case=prepare_for_division_32(divisor); //determine which is the division case (A, B or C)
	const __m256i f = _mm256_load_si256( (__m256i *) &f_vector[0]);// initialize the division vector

//printf("\n%d",division_case);


#pragma omp parallel
{

unsigned int row,col;
__m256i r0,r1,r2,r3,r4,r5,r6,m0,m1,m2,m3,m4,m5,m6,rr0,rr1,rr2,rr3,rr4,rr5,rr6;
__m256i output1,output2,output3,output4;
const __m256i ones=_mm256_set1_epi16(1);

/*---------------------- Gaussian Blur ---------------------------------*/

    #pragma omp for schedule(static)
	for (row = 2; row <= N-3; row++) {

		if (row==2){
			//row=0
				for (col = 0; col <= M-32-4; col+=32){

						  if (col==0){
							  //1st col
									//load the 5 rows
									rr0=_mm256_loadu_si256( (__m256i *) &frame1[0][0]);
									rr1=_mm256_loadu_si256( (__m256i *) &frame1[1][0]);
									rr2=_mm256_loadu_si256( (__m256i *) &frame1[2][0]);
									rr3=_mm256_loadu_si256( (__m256i *) &frame1[3][0]);

									//START - extra code needed for prelude
								 r0=insert_three_zeros_front(rr0,mask_prelude);
								 r1=insert_three_zeros_front(rr1,mask_prelude);
								 r2=insert_three_zeros_front(rr2,mask_prelude);
								 r3=insert_three_zeros_front(rr3,mask_prelude);

								output1=prelude_block_1_7x7_no_load(r0,r1,r2,r3,c3,c4,c5,c6,ones,mask_even_32);

									  //2nd col

										//START - extra code needed for prelude
									  r0=insert_two_zeros_front(rr0,mask_prelude2);
									  r1=insert_two_zeros_front(rr1,mask_prelude2);
									  r2=insert_two_zeros_front(rr2,mask_prelude2);
									  r3=insert_two_zeros_front(rr3,mask_prelude2);

											output2=prelude_block_1_7x7_no_load(r0,r1,r2,r3,c3,c4,c5,c6,ones,mask_even_32);

									  //3rd col
										//START - extra code needed for prelude
									  r0=insert_one_zeros_front(rr0,mask_prelude3);
									  r1=insert_one_zeros_front(rr1,mask_prelude3);
									  r2=insert_one_zeros_front(rr2,mask_prelude3);
									  r3=insert_one_zeros_front(rr3,mask_prelude3);

											output3=prelude_block_1_7x7_no_load(r0,r1,r2,r3,c3,c4,c5,c6,ones,mask_even_32);

											  //4th col
										  output4 = prelude_block_1_7x7(frame1,0, 3, c3,c4,c5,c6,ones, mask_even_32);

															//5th col iteration

										  m0 = prelude_block_1_7x7(frame1,0, 4, c3,c4,c5,c6,ones, mask_even_32);

											//MERGE output1 with output5 and DIVIDE
											m0=_mm256_slli_si256(m0,4);//shift by one integer
											output1=_mm256_add_epi32(m0,output1);
											output1 = division_32(division_case,output1,f);


											//6th col iteration
										   m0 = prelude_block_1_7x7(frame1,0, 5, c3,c4,c5,c6,ones, mask_even_32);

											//MERGE output1 with output5 and DIVIDE
											m0=_mm256_slli_si256(m0,4);//shift by one integer
											output2=_mm256_add_epi32(m0,output2);
											output2 = division_32(division_case,output2,f);

											//7th col iteration
										   m0 = prelude_block_1_7x7(frame1,0, 6, c3,c4,c5,c6,ones, mask_even_32);

											//MERGE output1 with output5 and DIVIDE
											m0=_mm256_slli_si256(m0,4);//shift by one integer
											output3=_mm256_add_epi32(m0,output3);
											output3 = division_32(division_case,output3,f);

											//8th col iteration

										   m0 = prelude_block_1_7x7(frame1,0, 7, c3,c4,c5,c6,ones, mask_even_32);

											//MERGE output1 with output5 and DIVIDE
											m0=_mm256_slli_si256(m0,4);//shift by one integer
											output4=_mm256_add_epi32(m0,output4);
											output4 = division_32(division_case,output4,f);


											//blend output1, output2, output3, output4 into one register
											output2 = _mm256_slli_si256(output2,1);
											output1 = _mm256_add_epi8(output1,output2);
											output3 = _mm256_slli_si256(output3,2);
											output1 = _mm256_add_epi8(output1,output3);
											output4 = _mm256_slli_si256(output4,3);
											output1 = _mm256_add_epi8(output1,output4);

											_mm256_store_si256( (__m256i *) &filt[0][0],output1 );
						  }
						  else {

				  output1=prelude_block_1_7x7(frame1,0,col,c3,c4,c5,c6,ones,mask_even_32);
				  output2=prelude_block_1_7x7(frame1,0,col+1,c3,c4,c5,c6,ones,mask_even_32);
				  output3=prelude_block_1_7x7(frame1,0,col+2,c3,c4,c5,c6,ones,mask_even_32);
				  output4=prelude_block_1_7x7(frame1,0,col+3,c3,c4,c5,c6,ones,mask_even_32);

				    m0=prelude_block_1_7x7(frame1,0,col+4,c3,c4,c5,c6,ones,mask_even_32);
					//MERGE output1 with output5 and DIVIDE
					m0=_mm256_slli_si256(m0,4);//shift by one integer
					output1=_mm256_add_epi32(m0,output1);
					output1 = division_32(division_case,output1,f);

					m0=prelude_block_1_7x7(frame1,0,col+5,c3,c4,c5,c6,ones,mask_even_32);
					//MERGE output1 with output5 and DIVIDE
					m0=_mm256_slli_si256(m0,4);//shift by one integer
					output2=_mm256_add_epi32(m0,output2);
					output2 = division_32(division_case,output2,f);

					m0=prelude_block_1_7x7(frame1,0,col+6,c3,c4,c5,c6,ones,mask_even_32);
					//MERGE output1 with output5 and DIVIDE
				    m0=_mm256_slli_si256(m0,4);//shift by one integer
					output3=_mm256_add_epi32(m0,output3);
					output3 = division_32(division_case,output3,f);

					m0=prelude_block_1_7x7(frame1,0,col+7,c3,c4,c5,c6,ones,mask_even_32);
					//MERGE output1 with output5 and DIVIDE
					m0=_mm256_slli_si256(m0,4);//shift by one integer
					output4=_mm256_add_epi32(m0,output4);
					output4 = division_32(division_case,output4,f);

					//blend output1, output2, output3, output4 into one register
					output2 = _mm256_slli_si256(output2,1);
					output1 = _mm256_add_epi8(output1,output2);
					output3 = _mm256_slli_si256(output3,2);
					output1 = _mm256_add_epi8(output1,output3);
					output4 = _mm256_slli_si256(output4,3);
					output1 = _mm256_add_epi8(output1,output4);

					//_mm256_stream_si256( (__m256i *) &filt[row][col],output1 );
					_mm256_store_si256( (__m256i *) &filt[0][col],output1 );
						  }
				}
				loop_reminder_7x7_row_boundaries(frame1,filt,N,0, col,REMINDER_ITERATIONS,division_case, mask_vector, f, divisor, kernel);


				//row=1
				for (col = 0; col <= M-32-4; col+=32){

						  if (col==0){
							  //1st col
								//load the 5 rows
								rr0=_mm256_loadu_si256( (__m256i *) &frame1[0][0]);
								rr1=_mm256_loadu_si256( (__m256i *) &frame1[1][0]);
								rr2=_mm256_loadu_si256( (__m256i *) &frame1[2][0]);
								rr3=_mm256_loadu_si256( (__m256i *) &frame1[3][0]);
								rr4=_mm256_loadu_si256( (__m256i *) &frame1[4][0]);

								//START - extra code needed for prelude
							 r0=insert_three_zeros_front(rr0,mask_prelude);
							 r1=insert_three_zeros_front(rr1,mask_prelude);
							 r2=insert_three_zeros_front(rr2,mask_prelude);
							 r3=insert_three_zeros_front(rr3,mask_prelude);
							 r4=insert_three_zeros_front(rr4,mask_prelude);

							output1=prelude_block_2_7x7_no_load(r0,r1,r2,r3,r4,c2,c3,c4,c5,c6,ones,mask_even_32);

								  //2nd col

									//START - extra code needed for prelude
								  r0=insert_two_zeros_front(rr0,mask_prelude2);
								  r1=insert_two_zeros_front(rr1,mask_prelude2);
								  r2=insert_two_zeros_front(rr2,mask_prelude2);
								  r3=insert_two_zeros_front(rr3,mask_prelude2);
								  r4=insert_two_zeros_front(rr4,mask_prelude2);

										output2=prelude_block_2_7x7_no_load(r0,r1,r2,r3,r4,c2,c3,c4,c5,c6,ones,mask_even_32);

								  //3rd col
									//START - extra code needed for prelude
								  r0=insert_one_zeros_front(rr0,mask_prelude3);
								  r1=insert_one_zeros_front(rr1,mask_prelude3);
								  r2=insert_one_zeros_front(rr2,mask_prelude3);
								  r3=insert_one_zeros_front(rr3,mask_prelude3);
								  r4=insert_one_zeros_front(rr4,mask_prelude3);

										output3=prelude_block_2_7x7_no_load(r0,r1,r2,r3,r4,c2,c3,c4,c5,c6,ones,mask_even_32);

										  //4th col
									  output4 = prelude_block_2_7x7(frame1,0, 3, c2,c3,c4,c5,c6,ones, mask_even_32);

														//5th col iteration

									  m0 = prelude_block_2_7x7(frame1,0, 4, c2,c3,c4,c5,c6,ones, mask_even_32);

										//MERGE output1 with output5 and DIVIDE
										m0=_mm256_slli_si256(m0,4);//shift by one integer
										output1=_mm256_add_epi32(m0,output1);
										output1 = division_32(division_case,output1,f);


										//6th col iteration
									   m0 = prelude_block_2_7x7(frame1,0, 5, c2,c3,c4,c5,c6,ones, mask_even_32);

										//MERGE output1 with output5 and DIVIDE
										m0=_mm256_slli_si256(m0,4);//shift by one integer
										output2=_mm256_add_epi32(m0,output2);
										output2 = division_32(division_case,output2,f);

										//7th col iteration
									   m0 = prelude_block_2_7x7(frame1,0, 6, c2,c3,c4,c5,c6,ones, mask_even_32);

										//MERGE output1 with output5 and DIVIDE
										m0=_mm256_slli_si256(m0,4);//shift by one integer
										output3=_mm256_add_epi32(m0,output3);
										output3 = division_32(division_case,output3,f);

										//8th col iteration

									   m0 = prelude_block_2_7x7(frame1,0, 7, c2,c3,c4,c5,c6,ones, mask_even_32);

										//MERGE output1 with output5 and DIVIDE
										m0=_mm256_slli_si256(m0,4);//shift by one integer
										output4=_mm256_add_epi32(m0,output4);
										output4 = division_32(division_case,output4,f);


										//blend output1, output2, output3, output4 into one register
										output2 = _mm256_slli_si256(output2,1);
										output1 = _mm256_add_epi8(output1,output2);
										output3 = _mm256_slli_si256(output3,2);
										output1 = _mm256_add_epi8(output1,output3);
										output4 = _mm256_slli_si256(output4,3);
										output1 = _mm256_add_epi8(output1,output4);

										_mm256_store_si256( (__m256i *) &filt[1][0],output1 );
						  }
						  else {

				  output1=prelude_block_2_7x7(frame1,0,col,c2,c3,c4,c5,c6,ones,mask_even_32);
				  output2=prelude_block_2_7x7(frame1,0,col+1,c2,c3,c4,c5,c6,ones,mask_even_32);
				  output3=prelude_block_2_7x7(frame1,0,col+2,c2,c3,c4,c5,c6,ones,mask_even_32);
				  output4=prelude_block_2_7x7(frame1,0,col+3,c2,c3,c4,c5,c6,ones,mask_even_32);

				    m0=prelude_block_2_7x7(frame1,0,col+4,c2,c3,c4,c5,c6,ones,mask_even_32);
					//MERGE output1 with output5 and DIVIDE
					m0=_mm256_slli_si256(m0,4);//shift by one integer
					output1=_mm256_add_epi32(m0,output1);
					output1 = division_32(division_case,output1,f);

					m0=prelude_block_2_7x7(frame1,0,col+5,c2,c3,c4,c5,c6,ones,mask_even_32);
					//MERGE output1 with output5 and DIVIDE
					m0=_mm256_slli_si256(m0,4);//shift by one integer
					output2=_mm256_add_epi32(m0,output2);
					output2 = division_32(division_case,output2,f);

					m0=prelude_block_2_7x7(frame1,0,col+6,c2,c3,c4,c5,c6,ones,mask_even_32);
					//MERGE output1 with output5 and DIVIDE
				    m0=_mm256_slli_si256(m0,4);//shift by one integer
					output3=_mm256_add_epi32(m0,output3);
					output3 = division_32(division_case,output3,f);

					m0=prelude_block_2_7x7(frame1,0,col+7,c2,c3,c4,c5,c6,ones,mask_even_32);
					//MERGE output1 with output5 and DIVIDE
					m0=_mm256_slli_si256(m0,4);//shift by one integer
					output4=_mm256_add_epi32(m0,output4);
					output4 = division_32(division_case,output4,f);

					//blend output1, output2, output3, output4 into one register
					output2 = _mm256_slli_si256(output2,1);
					output1 = _mm256_add_epi8(output1,output2);
					output3 = _mm256_slli_si256(output3,2);
					output1 = _mm256_add_epi8(output1,output3);
					output4 = _mm256_slli_si256(output4,3);
					output1 = _mm256_add_epi8(output1,output4);

					//_mm256_stream_si256( (__m256i *) &filt[row][col],output1 );
					_mm256_store_si256( (__m256i *) &filt[1][col],output1 );
						  }
				}
				loop_reminder_7x7_row_boundaries(frame1,filt,N,1, col,REMINDER_ITERATIONS,division_case, mask_vector, f, divisor, kernel);

				//row=2
				for (col = 0; col <= M-32-4; col+=32){

						  if (col==0){
						  //1st col
							//load the 5 rows
							rr0=_mm256_loadu_si256( (__m256i *) &frame1[0][0]);
							rr1=_mm256_loadu_si256( (__m256i *) &frame1[1][0]);
							rr2=_mm256_loadu_si256( (__m256i *) &frame1[2][0]);
							rr3=_mm256_loadu_si256( (__m256i *) &frame1[3][0]);
							rr4=_mm256_loadu_si256( (__m256i *) &frame1[4][0]);
							rr5=_mm256_loadu_si256( (__m256i *) &frame1[5][0]);


							//START - extra code needed for prelude
						 r0=insert_three_zeros_front(rr0,mask_prelude);
						 r1=insert_three_zeros_front(rr1,mask_prelude);
						 r2=insert_three_zeros_front(rr2,mask_prelude);
						 r3=insert_three_zeros_front(rr3,mask_prelude);
						 r4=insert_three_zeros_front(rr4,mask_prelude);
						 r5=insert_three_zeros_front(rr5,mask_prelude);

						output1=prelude_block_3_7x7_no_load(r0,r1,r2,r3,r4,r5,c1,c2,c3,c4,c5,c6,ones,mask_even_32);

							  //2nd col

								//START - extra code needed for prelude
							  r0=insert_two_zeros_front(rr0,mask_prelude2);
							  r1=insert_two_zeros_front(rr1,mask_prelude2);
							  r2=insert_two_zeros_front(rr2,mask_prelude2);
							  r3=insert_two_zeros_front(rr3,mask_prelude2);
							  r4=insert_two_zeros_front(rr4,mask_prelude2);
							  r5=insert_two_zeros_front(rr5,mask_prelude2);

							output2=prelude_block_3_7x7_no_load(r0,r1,r2,r3,r4,r5,c1,c2,c3,c4,c5,c6,ones,mask_even_32);

							  //3rd col
								//START - extra code needed for prelude
							  r0=insert_one_zeros_front(rr0,mask_prelude3);
							  r1=insert_one_zeros_front(rr1,mask_prelude3);
							  r2=insert_one_zeros_front(rr2,mask_prelude3);
							  r3=insert_one_zeros_front(rr3,mask_prelude3);
							  r4=insert_one_zeros_front(rr4,mask_prelude3);
							  r5=insert_one_zeros_front(rr5,mask_prelude3);


									output3=prelude_block_3_7x7_no_load(r0,r1,r2,r3,r4,r5,c1,c2,c3,c4,c5,c6,ones,mask_even_32);

									  //4th col
								  output4 = prelude_block_3_7x7(frame1,0, 3, c1,c2,c3,c4,c5,c6,ones, mask_even_32);

													//5th col iteration

								  m0 = prelude_block_3_7x7(frame1,0, 4, c1,c2,c3,c4,c5,c6,ones, mask_even_32);

									//MERGE output1 with output5 and DIVIDE
									m0=_mm256_slli_si256(m0,4);//shift by one integer
									output1=_mm256_add_epi32(m0,output1);
									output1 = division_32(division_case,output1,f);


									//6th col iteration
								   m0 = prelude_block_3_7x7(frame1,0, 5, c1,c2,c3,c4,c5,c6,ones, mask_even_32);

									//MERGE output1 with output5 and DIVIDE
									m0=_mm256_slli_si256(m0,4);//shift by one integer
									output2=_mm256_add_epi32(m0,output2);
									output2 = division_32(division_case,output2,f);

									//7th col iteration
								   m0 = prelude_block_3_7x7(frame1,0, 6, c1,c2,c3,c4,c5,c6,ones, mask_even_32);

									//MERGE output1 with output5 and DIVIDE
									m0=_mm256_slli_si256(m0,4);//shift by one integer
									output3=_mm256_add_epi32(m0,output3);
									output3 = division_32(division_case,output3,f);

									//8th col iteration

								   m0 = prelude_block_3_7x7(frame1,0, 7, c1,c2,c3,c4,c5,c6,ones, mask_even_32);

									//MERGE output1 with output5 and DIVIDE
									m0=_mm256_slli_si256(m0,4);//shift by one integer
									output4=_mm256_add_epi32(m0,output4);
									output4 = division_32(division_case,output4,f);


									//blend output1, output2, output3, output4 into one register
									output2 = _mm256_slli_si256(output2,1);
									output1 = _mm256_add_epi8(output1,output2);
									output3 = _mm256_slli_si256(output3,2);
									output1 = _mm256_add_epi8(output1,output3);
									output4 = _mm256_slli_si256(output4,3);
									output1 = _mm256_add_epi8(output1,output4);

									_mm256_store_si256( (__m256i *) &filt[2][0],output1 );
						  }
						  else {
							  output1=prelude_block_3_7x7(frame1,0,col,c1,c2,c3,c4,c5,c6,ones,mask_even_32);
							  output2=prelude_block_3_7x7(frame1,0,col+1,c1,c2,c3,c4,c5,c6,ones,mask_even_32);
							  output3=prelude_block_3_7x7(frame1,0,col+2,c1,c2,c3,c4,c5,c6,ones,mask_even_32);
							  output4=prelude_block_3_7x7(frame1,0,col+3,c1,c2,c3,c4,c5,c6,ones,mask_even_32);

							    m0=prelude_block_3_7x7(frame1,0,col+4,c1,c2,c3,c4,c5,c6,ones,mask_even_32);
								//MERGE output1 with output5 and DIVIDE
								m0=_mm256_slli_si256(m0,4);//shift by one integer
								output1=_mm256_add_epi32(m0,output1);
								output1 = division_32(division_case,output1,f);

							    m0=prelude_block_3_7x7(frame1,0,col+5,c1,c2,c3,c4,c5,c6,ones,mask_even_32);
								//MERGE output1 with output5 and DIVIDE
								m0=_mm256_slli_si256(m0,4);//shift by one integer
								output2=_mm256_add_epi32(m0,output2);
								output2 = division_32(division_case,output2,f);

								m0=prelude_block_3_7x7(frame1,0,col+6,c1,c2,c3,c4,c5,c6,ones,mask_even_32);
								//MERGE output1 with output5 and DIVIDE
							    m0=_mm256_slli_si256(m0,4);//shift by one integer
								output3=_mm256_add_epi32(m0,output3);
								output3 = division_32(division_case,output3,f);

								m0=prelude_block_3_7x7(frame1,0,col+7,c1,c2,c3,c4,c5,c6,ones,mask_even_32);
								//MERGE output1 with output5 and DIVIDE
								m0=_mm256_slli_si256(m0,4);//shift by one integer
								output4=_mm256_add_epi32(m0,output4);
								output4 = division_32(division_case,output4,f);

								//blend output1, output2, output3, output4 into one register
								output2 = _mm256_slli_si256(output2,1);
								output1 = _mm256_add_epi8(output1,output2);
								output3 = _mm256_slli_si256(output3,2);
								output1 = _mm256_add_epi8(output1,output3);
								output4 = _mm256_slli_si256(output4,3);
								output1 = _mm256_add_epi8(output1,output4);

								//_mm256_stream_si256( (__m256i *) &filt[row][col],output1 );
								_mm256_store_si256( (__m256i *) &filt[2][col],output1 );
						  }
				}
				loop_reminder_7x7_row_boundaries(frame1,filt,N,2, col,REMINDER_ITERATIONS,division_case, mask_vector, f, divisor, kernel);


		}

		else if (row==N-3){

			//row=N-1
				for (col = 0; col <= M-32-4; col+=32){
				 if (col==0){
					  //1st col
							rr0=_mm256_loadu_si256( (__m256i *) &frame1[N-4][0]);
							rr1=_mm256_loadu_si256( (__m256i *) &frame1[N-3][0]);
							rr2=_mm256_loadu_si256( (__m256i *) &frame1[N-2][0]);
							rr3=_mm256_loadu_si256( (__m256i *) &frame1[N-1][0]);


							//START - extra code needed for prelude
						 r0=insert_three_zeros_front(rr0,mask_prelude);
						 r1=insert_three_zeros_front(rr1,mask_prelude);
						 r2=insert_three_zeros_front(rr2,mask_prelude);
						 r3=insert_three_zeros_front(rr3,mask_prelude);

						output1=prelude_block_1_7x7_no_load(r0,r1,r2,r3,c0,c1,c2,c3,ones,mask_even_32);

							  //2nd col

								//START - extra code needed for prelude
							  r0=insert_two_zeros_front(rr0,mask_prelude2);
							  r1=insert_two_zeros_front(rr1,mask_prelude2);
							  r2=insert_two_zeros_front(rr2,mask_prelude2);
							  r3=insert_two_zeros_front(rr3,mask_prelude2);

							output2=prelude_block_1_7x7_no_load(r0,r1,r2,r3,c0,c1,c2,c3,ones,mask_even_32);

							  //3rd col
								//START - extra code needed for prelude
							  r0=insert_one_zeros_front(rr0,mask_prelude3);
							  r1=insert_one_zeros_front(rr1,mask_prelude3);
							  r2=insert_one_zeros_front(rr2,mask_prelude3);
							  r3=insert_one_zeros_front(rr3,mask_prelude3);


									output3=prelude_block_1_7x7_no_load(r0,r1,r2,r3,c0,c1,c2,c3,ones,mask_even_32);

									  //4th col
								  output4 = prelude_block_1_7x7(frame1,N-4,3,c0,c1,c2,c3,ones,mask_even_32);

													//5th col iteration

								  m0 = prelude_block_1_7x7(frame1,N-4,4,c0,c1,c2,c3,ones,mask_even_32);

									//MERGE output1 with output5 and DIVIDE
									m0=_mm256_slli_si256(m0,4);//shift by one integer
									output1=_mm256_add_epi32(m0,output1);
									output1 = division_32(division_case,output1,f);


									//6th col iteration
								   m0 = prelude_block_1_7x7(frame1,N-4,5,c0,c1,c2,c3,ones,mask_even_32);

									//MERGE output1 with output5 and DIVIDE
									m0=_mm256_slli_si256(m0,4);//shift by one integer
									output2=_mm256_add_epi32(m0,output2);
									output2 = division_32(division_case,output2,f);

									//7th col iteration
								   m0 = prelude_block_1_7x7(frame1,N-4,6,c0,c1,c2,c3,ones,mask_even_32);

									//MERGE output1 with output5 and DIVIDE
									m0=_mm256_slli_si256(m0,4);//shift by one integer
									output3=_mm256_add_epi32(m0,output3);
									output3 = division_32(division_case,output3,f);

									//8th col iteration

								   m0 = prelude_block_1_7x7(frame1,N-4,7,c0,c1,c2,c3,ones,mask_even_32);

									//MERGE output1 with output5 and DIVIDE
									m0=_mm256_slli_si256(m0,4);//shift by one integer
									output4=_mm256_add_epi32(m0,output4);
									output4 = division_32(division_case,output4,f);


									//blend output1, output2, output3, output4 into one register
									output2 = _mm256_slli_si256(output2,1);
									output1 = _mm256_add_epi8(output1,output2);
									output3 = _mm256_slli_si256(output3,2);
									output1 = _mm256_add_epi8(output1,output3);
									output4 = _mm256_slli_si256(output4,3);
									output1 = _mm256_add_epi8(output1,output4);

									_mm256_store_si256( (__m256i *) &filt[N-1][0],output1 );
					  }
				 else {

						  output1=prelude_block_1_7x7(frame1,N-4,col,c0,c1,c2,c3,ones,mask_even_32);
						  output2=prelude_block_1_7x7(frame1,N-4,col+1,c0,c1,c2,c3,ones,mask_even_32);
						  output3=prelude_block_1_7x7(frame1,N-4,col+2,c0,c1,c2,c3,ones,mask_even_32);
						  output4=prelude_block_1_7x7(frame1,N-4,col+3,c0,c1,c2,c3,ones,mask_even_32);

						    m0=prelude_block_1_7x7(frame1,N-4,col+4,c0,c1,c2,c3,ones,mask_even_32);
							//MERGE output1 with output5 and DIVIDE
							m0=_mm256_slli_si256(m0,4);//shift by one integer
							output1=_mm256_add_epi32(m0,output1);
							output1 = division_32(division_case,output1,f);

							m0=prelude_block_1_7x7(frame1,N-4,col+5,c0,c1,c2,c3,ones,mask_even_32);
						    //MERGE output1 with output5 and DIVIDE
							m0=_mm256_slli_si256(m0,4);//shift by one integer
							output2=_mm256_add_epi32(m0,output2);
							output2 = division_32(division_case,output2,f);

							m0=prelude_block_1_7x7(frame1,N-4,col+6,c0,c1,c2,c3,ones,mask_even_32);
							//MERGE output1 with output5 and DIVIDE
						    m0=_mm256_slli_si256(m0,4);//shift by one integer
							output3=_mm256_add_epi32(m0,output3);
							output3 = division_32(division_case,output3,f);

							m0=prelude_block_1_7x7(frame1,N-4,col+7,c0,c1,c2,c3,ones,mask_even_32);
							//MERGE output1 with output5 and DIVIDE
							m0=_mm256_slli_si256(m0,4);//shift by one integer
							output4=_mm256_add_epi32(m0,output4);
							output4 = division_32(division_case,output4,f);

							//blend output1, output2, output3, output4 into one register
							output2 = _mm256_slli_si256(output2,1);
							output1 = _mm256_add_epi8(output1,output2);
							output3 = _mm256_slli_si256(output3,2);
							output1 = _mm256_add_epi8(output1,output3);
							output4 = _mm256_slli_si256(output4,3);
							output1 = _mm256_add_epi8(output1,output4);

							//_mm256_stream_si256( (__m256i *) &filt[row][col],output1 );
							_mm256_store_si256( (__m256i *) &filt[N-1][col],output1 );
					  }
				}
				loop_reminder_7x7_row_boundaries(frame1,filt,N,N-1, col,REMINDER_ITERATIONS,division_case, mask_vector, f, divisor, kernel);

				//row=N-2

				for (col = 0; col <= M-32-4; col+=32){

					if (col==0){
						  //1st col
							rr0=_mm256_loadu_si256( (__m256i *) &frame1[N-5][0]);
							rr1=_mm256_loadu_si256( (__m256i *) &frame1[N-4][0]);
							rr2=_mm256_loadu_si256( (__m256i *) &frame1[N-3][0]);
							rr3=_mm256_loadu_si256( (__m256i *) &frame1[N-2][0]);
							rr4=_mm256_loadu_si256( (__m256i *) &frame1[N-1][0]);


							//START - extra code needed for prelude
						 r0=insert_three_zeros_front(rr0,mask_prelude);
						 r1=insert_three_zeros_front(rr1,mask_prelude);
						 r2=insert_three_zeros_front(rr2,mask_prelude);
						 r3=insert_three_zeros_front(rr3,mask_prelude);
						 r4=insert_three_zeros_front(rr4,mask_prelude);

						output1=prelude_block_2_7x7_no_load(r0,r1,r2,r3,r4,c0,c1,c2,c3,c4,ones,mask_even_32);

							  //2nd col

								//START - extra code needed for prelude
							  r0=insert_two_zeros_front(rr0,mask_prelude2);
							  r1=insert_two_zeros_front(rr1,mask_prelude2);
							  r2=insert_two_zeros_front(rr2,mask_prelude2);
							  r3=insert_two_zeros_front(rr3,mask_prelude2);
							  r4=insert_two_zeros_front(rr4,mask_prelude2);

							output2=prelude_block_2_7x7_no_load(r0,r1,r2,r3,r4,c0,c1,c2,c3,c4,ones,mask_even_32);

							  //3rd col
								//START - extra code needed for prelude
							  r0=insert_one_zeros_front(rr0,mask_prelude3);
							  r1=insert_one_zeros_front(rr1,mask_prelude3);
							  r2=insert_one_zeros_front(rr2,mask_prelude3);
							  r3=insert_one_zeros_front(rr3,mask_prelude3);
							  r4=insert_one_zeros_front(rr4,mask_prelude3);


									output3=prelude_block_2_7x7_no_load(r0,r1,r2,r3,r4,c0,c1,c2,c3,c4,ones,mask_even_32);

									  //4th col
								  output4 = prelude_block_2_7x7(frame1,N-5,3,c0,c1,c2,c3,c4,ones,mask_even_32);

													//5th col iteration

								  m0 = prelude_block_2_7x7(frame1,N-5,4,c0,c1,c2,c3,c4,ones,mask_even_32);

									//MERGE output1 with output5 and DIVIDE
									m0=_mm256_slli_si256(m0,4);//shift by one integer
									output1=_mm256_add_epi32(m0,output1);
									output1 = division_32(division_case,output1,f);


									//6th col iteration
								   m0 = prelude_block_2_7x7(frame1,N-5,5,c0,c1,c2,c3,c4,ones,mask_even_32);

									//MERGE output1 with output5 and DIVIDE
									m0=_mm256_slli_si256(m0,4);//shift by one integer
									output2=_mm256_add_epi32(m0,output2);
									output2 = division_32(division_case,output2,f);

									//7th col iteration
								   m0 = prelude_block_2_7x7(frame1,N-5,6,c0,c1,c2,c3,c4,ones,mask_even_32);

									//MERGE output1 with output5 and DIVIDE
									m0=_mm256_slli_si256(m0,4);//shift by one integer
									output3=_mm256_add_epi32(m0,output3);
									output3 = division_32(division_case,output3,f);

									//8th col iteration

								   m0 = prelude_block_2_7x7(frame1,N-5,7,c0,c1,c2,c3,c4,ones,mask_even_32);

									//MERGE output1 with output5 and DIVIDE
									m0=_mm256_slli_si256(m0,4);//shift by one integer
									output4=_mm256_add_epi32(m0,output4);
									output4 = division_32(division_case,output4,f);


									//blend output1, output2, output3, output4 into one register
									output2 = _mm256_slli_si256(output2,1);
									output1 = _mm256_add_epi8(output1,output2);
									output3 = _mm256_slli_si256(output3,2);
									output1 = _mm256_add_epi8(output1,output3);
									output4 = _mm256_slli_si256(output4,3);
									output1 = _mm256_add_epi8(output1,output4);

									_mm256_store_si256( (__m256i *) &filt[N-2][0],output1 );
						  }
					else {
							  output1=prelude_block_2_7x7(frame1,N-5,col,c0,c1,c2,c3,c4,ones,mask_even_32);
							  output2=prelude_block_2_7x7(frame1,N-5,col+1,c0,c1,c2,c3,c4,ones,mask_even_32);
							  output3=prelude_block_2_7x7(frame1,N-5,col+2,c0,c1,c2,c3,c4,ones,mask_even_32);
							  output4=prelude_block_2_7x7(frame1,N-5,col+3,c0,c1,c2,c3,c4,ones,mask_even_32);

							    m0=prelude_block_2_7x7(frame1,N-5,col+4,c0,c1,c2,c3,c4,ones,mask_even_32);
								//MERGE output1 with output5 and DIVIDE
								m0=_mm256_slli_si256(m0,4);//shift by one integer
								output1=_mm256_add_epi32(m0,output1);
								output1 = division_32(division_case,output1,f);

								m0=prelude_block_2_7x7(frame1,N-5,col+5,c0,c1,c2,c3,c4,ones,mask_even_32);
							    //MERGE output1 with output5 and DIVIDE
								m0=_mm256_slli_si256(m0,4);//shift by one integer
								output2=_mm256_add_epi32(m0,output2);
								output2 = division_32(division_case,output2,f);

								m0=prelude_block_2_7x7(frame1,N-5,col+6,c0,c1,c2,c3,c4,ones,mask_even_32);
								//MERGE output1 with output5 and DIVIDE
							    m0=_mm256_slli_si256(m0,4);//shift by one integer
								output3=_mm256_add_epi32(m0,output3);
								output3 = division_32(division_case,output3,f);

								m0=prelude_block_2_7x7(frame1,N-5,col+7,c0,c1,c2,c3,c4,ones,mask_even_32);
								//MERGE output1 with output5 and DIVIDE
								m0=_mm256_slli_si256(m0,4);//shift by one integer
								output4=_mm256_add_epi32(m0,output4);
								output4 = division_32(division_case,output4,f);

								//blend output1, output2, output3, output4 into one register
								output2 = _mm256_slli_si256(output2,1);
								output1 = _mm256_add_epi8(output1,output2);
								output3 = _mm256_slli_si256(output3,2);
								output1 = _mm256_add_epi8(output1,output3);
								output4 = _mm256_slli_si256(output4,3);
								output1 = _mm256_add_epi8(output1,output4);

								//_mm256_stream_si256( (__m256i *) &filt[row][col],output1 );
								_mm256_store_si256( (__m256i *) &filt[N-2][col],output1 );
						  }
				}
				loop_reminder_7x7_row_boundaries(frame1,filt,N,N-2, col,REMINDER_ITERATIONS,division_case, mask_vector, f, divisor, kernel);

				//row=N-3
				for (col = 0; col <= M-32-4; col+=32){

						  if (col==0){
							  //1st col
								//load the 5 rows
								rr0=_mm256_loadu_si256( (__m256i *) &frame1[N-6][0]);
								rr1=_mm256_loadu_si256( (__m256i *) &frame1[N-5][0]);
								rr2=_mm256_loadu_si256( (__m256i *) &frame1[N-4][0]);
								rr3=_mm256_loadu_si256( (__m256i *) &frame1[N-3][0]);
								rr4=_mm256_loadu_si256( (__m256i *) &frame1[N-2][0]);
								rr5=_mm256_loadu_si256( (__m256i *) &frame1[N-1][0]);


								//START - extra code needed for prelude
							 r0=insert_three_zeros_front(rr0,mask_prelude);
							 r1=insert_three_zeros_front(rr1,mask_prelude);
							 r2=insert_three_zeros_front(rr2,mask_prelude);
							 r3=insert_three_zeros_front(rr3,mask_prelude);
							 r4=insert_three_zeros_front(rr4,mask_prelude);
							 r5=insert_three_zeros_front(rr5,mask_prelude);

							output1=prelude_block_3_7x7_no_load(r0,r1,r2,r3,r4,r5,c0,c1,c2,c3,c4,c5,ones,mask_even_32);

								  //2nd col

									//START - extra code needed for prelude
								  r0=insert_two_zeros_front(rr0,mask_prelude2);
								  r1=insert_two_zeros_front(rr1,mask_prelude2);
								  r2=insert_two_zeros_front(rr2,mask_prelude2);
								  r3=insert_two_zeros_front(rr3,mask_prelude2);
								  r4=insert_two_zeros_front(rr4,mask_prelude2);
								  r5=insert_two_zeros_front(rr5,mask_prelude2);

								output2=prelude_block_3_7x7_no_load(r0,r1,r2,r3,r4,r5,c0,c1,c2,c3,c4,c5,ones,mask_even_32);

								  //3rd col
									//START - extra code needed for prelude
								  r0=insert_one_zeros_front(rr0,mask_prelude3);
								  r1=insert_one_zeros_front(rr1,mask_prelude3);
								  r2=insert_one_zeros_front(rr2,mask_prelude3);
								  r3=insert_one_zeros_front(rr3,mask_prelude3);
								  r4=insert_one_zeros_front(rr4,mask_prelude3);
								  r5=insert_one_zeros_front(rr5,mask_prelude3);


										output3=prelude_block_3_7x7_no_load(r0,r1,r2,r3,r4,r5,c0,c1,c2,c3,c4,c5,ones,mask_even_32);

										  //4th col
									  output4 = prelude_block_3_7x7(frame1,N-6,3,c0,c1,c2,c3,c4,c5,ones,mask_even_32);

														//5th col iteration

									  m0 = prelude_block_3_7x7(frame1,N-6,4,c0,c1,c2,c3,c4,c5,ones,mask_even_32);

										//MERGE output1 with output5 and DIVIDE
										m0=_mm256_slli_si256(m0,4);//shift by one integer
										output1=_mm256_add_epi32(m0,output1);
										output1 = division_32(division_case,output1,f);


										//6th col iteration
									   m0 = prelude_block_3_7x7(frame1,N-6,5,c0,c1,c2,c3,c4,c5,ones,mask_even_32);

										//MERGE output1 with output5 and DIVIDE
										m0=_mm256_slli_si256(m0,4);//shift by one integer
										output2=_mm256_add_epi32(m0,output2);
										output2 = division_32(division_case,output2,f);

										//7th col iteration
									   m0 = prelude_block_3_7x7(frame1,N-6,6,c0,c1,c2,c3,c4,c5,ones,mask_even_32);

										//MERGE output1 with output5 and DIVIDE
										m0=_mm256_slli_si256(m0,4);//shift by one integer
										output3=_mm256_add_epi32(m0,output3);
										output3 = division_32(division_case,output3,f);

										//8th col iteration

									   m0 = prelude_block_3_7x7(frame1,N-6,7,c0,c1,c2,c3,c4,c5,ones,mask_even_32);

										//MERGE output1 with output5 and DIVIDE
										m0=_mm256_slli_si256(m0,4);//shift by one integer
										output4=_mm256_add_epi32(m0,output4);
										output4 = division_32(division_case,output4,f);


										//blend output1, output2, output3, output4 into one register
										output2 = _mm256_slli_si256(output2,1);
										output1 = _mm256_add_epi8(output1,output2);
										output3 = _mm256_slli_si256(output3,2);
										output1 = _mm256_add_epi8(output1,output3);
										output4 = _mm256_slli_si256(output4,3);
										output1 = _mm256_add_epi8(output1,output4);

										_mm256_store_si256( (__m256i *) &filt[N-3][0],output1 );
						  }
						  else {
							  output1=prelude_block_3_7x7(frame1,N-6,col,c0,c1,c2,c3,c4,c5,ones,mask_even_32);
							  output2=prelude_block_3_7x7(frame1,N-6,col+1,c0,c1,c2,c3,c4,c5,ones,mask_even_32);
							  output3=prelude_block_3_7x7(frame1,N-6,col+2,c0,c1,c2,c3,c4,c5,ones,mask_even_32);
							  output4=prelude_block_3_7x7(frame1,N-6,col+3,c0,c1,c2,c3,c4,c5,ones,mask_even_32);

							    m0=prelude_block_3_7x7(frame1,N-6,col+4,c0,c1,c2,c3,c4,c5,ones,mask_even_32);
								//MERGE output1 with output5 and DIVIDE
								m0=_mm256_slli_si256(m0,4);//shift by one integer
								output1=_mm256_add_epi32(m0,output1);
								output1 = division_32(division_case,output1,f);

							    m0=prelude_block_3_7x7(frame1,N-6,col+5,c0,c1,c2,c3,c4,c5,ones,mask_even_32);
							    //MERGE output1 with output5 and DIVIDE
								m0=_mm256_slli_si256(m0,4);//shift by one integer
								output2=_mm256_add_epi32(m0,output2);
								output2 = division_32(division_case,output2,f);

							    m0=prelude_block_3_7x7(frame1,N-6,col+6,c0,c1,c2,c3,c4,c5,ones,mask_even_32);
								//MERGE output1 with output5 and DIVIDE
							    m0=_mm256_slli_si256(m0,4);//shift by one integer
								output3=_mm256_add_epi32(m0,output3);
								output3 = division_32(division_case,output3,f);

							    m0=prelude_block_3_7x7(frame1,N-6,col+7,c0,c1,c2,c3,c4,c5,ones,mask_even_32);
								//MERGE output1 with output5 and DIVIDE
								m0=_mm256_slli_si256(m0,4);//shift by one integer
								output4=_mm256_add_epi32(m0,output4);
								output4 = division_32(division_case,output4,f);

								//blend output1, output2, output3, output4 into one register
								output2 = _mm256_slli_si256(output2,1);
								output1 = _mm256_add_epi8(output1,output2);
								output3 = _mm256_slli_si256(output3,2);
								output1 = _mm256_add_epi8(output1,output3);
								output4 = _mm256_slli_si256(output4,3);
								output1 = _mm256_add_epi8(output1,output4);

								//_mm256_stream_si256( (__m256i *) &filt[row][col],output1 );
								_mm256_store_si256( (__m256i *) &filt[N-3][col],output1 );
						  }
				}
				loop_reminder_7x7_row_boundaries(frame1,filt,N,N-3, col,REMINDER_ITERATIONS,division_case, mask_vector, f, divisor, kernel);


		}

	  else { //main loop


	  for (col = 0; col <= M-32-4; col+=32){

		  if (col==0){
			  //1st col
				//load the 5 rows
				rr0=_mm256_loadu_si256( (__m256i *) &frame1[row-3][0]);
				rr1=_mm256_loadu_si256( (__m256i *) &frame1[row-2][0]);
				rr2=_mm256_loadu_si256( (__m256i *) &frame1[row-1][0]);
				rr3=_mm256_loadu_si256( (__m256i *) &frame1[row][0]);
				rr4=_mm256_loadu_si256( (__m256i *) &frame1[row+1][0]);
				rr5=_mm256_loadu_si256( (__m256i *) &frame1[row+2][0]);
				rr6=_mm256_loadu_si256( (__m256i *) &frame1[row+3][0]);

				//START - extra code needed for prelude
			 r0=insert_three_zeros_front(rr0,mask_prelude);
			 r1=insert_three_zeros_front(rr1,mask_prelude);
			 r2=insert_three_zeros_front(rr2,mask_prelude);
			 r3=insert_three_zeros_front(rr3,mask_prelude);
			 r4=insert_three_zeros_front(rr4,mask_prelude);
			 r5=insert_three_zeros_front(rr5,mask_prelude);
			 r6=insert_three_zeros_front(rr6,mask_prelude);

			output1=main_block_pre_7x7(r0,r1,r2,r3,r4,r5,r6,c0,c1,c2,c3,c4,c5,c6,ones,mask_even_32);

		  				  //2nd col

				  			//START - extra code needed for prelude
		  				  r0=insert_two_zeros_front(rr0,mask_prelude2);
		  				  r1=insert_two_zeros_front(rr1,mask_prelude2);
		  				  r2=insert_two_zeros_front(rr2,mask_prelude2);
		  				  r3=insert_two_zeros_front(rr3,mask_prelude2);
		  				  r4=insert_two_zeros_front(rr4,mask_prelude2);
		  				  r5=insert_two_zeros_front(rr5,mask_prelude2);
		  				  r6=insert_two_zeros_front(rr6,mask_prelude2);

		  				output2=main_block_pre_7x7(r0,r1,r2,r3,r4,r5,r6,c0,c1,c2,c3,c4,c5,c6,ones,mask_even_32);

		  			  				  //3rd col
		  					  			//START - extra code needed for prelude
		  			  				  r0=insert_one_zeros_front(rr0,mask_prelude3);
		  			  				  r1=insert_one_zeros_front(rr1,mask_prelude3);
		  			  				  r2=insert_one_zeros_front(rr2,mask_prelude3);
		  			  				  r3=insert_one_zeros_front(rr3,mask_prelude3);
		  			  				  r4=insert_one_zeros_front(rr4,mask_prelude3);
		  			  				  r5=insert_one_zeros_front(rr5,mask_prelude3);
		  			  				  r6=insert_one_zeros_front(rr6,mask_prelude3);


		  			  	output3=main_block_pre_7x7(r0,r1,r2,r3,r4,r5,r6,c0,c1,c2,c3,c4,c5,c6,ones,mask_even_32);

		  				  //4th col
		  			  output4 = main_block_7x7(frame1,row, 3, c0,c1,c2,c3,c4,c5,c6,ones, mask_even_32);

		  	 			  				//5th col iteration

		  			  m0 = main_block_7x7(frame1,row, 4, c0,c1,c2,c3,c4,c5,c6,ones, mask_even_32);

		  				  			  				//MERGE output1 with output5 and DIVIDE
		  				  			  				m0=_mm256_slli_si256(m0,4);//shift by one integer
		  				  			  				output1=_mm256_add_epi32(m0,output1);
		  				  			  				output1 = division_32(division_case,output1,f);


		  				  			  				//6th col iteration
		  				  			  			m0 = main_block_7x7(frame1,row, 5, c0,c1,c2,c3,c4,c5,c6,ones, mask_even_32);

		  				  			  				//MERGE output1 with output5 and DIVIDE
		  				  			  				m0=_mm256_slli_si256(m0,4);//shift by one integer
		  				  			  				output2=_mm256_add_epi32(m0,output2);
		  				  			  				output2 = division_32(division_case,output2,f);

		  				  			  				//7th col iteration
		  				  			  			m0 = main_block_7x7(frame1,row, 6, c0,c1,c2,c3,c4,c5,c6,ones, mask_even_32);

		  				  			  				//MERGE output1 with output5 and DIVIDE
		  				  			  				m0=_mm256_slli_si256(m0,4);//shift by one integer
		  				  			  				output3=_mm256_add_epi32(m0,output3);
		  				  			  				output3 = division_32(division_case,output3,f);

		  				  			  				//8th col iteration

		  				  			  			m0 = main_block_7x7(frame1,row, 7, c0,c1,c2,c3,c4,c5,c6,ones, mask_even_32);

		  				  			  				//MERGE output1 with output5 and DIVIDE
		  				  			  				m0=_mm256_slli_si256(m0,4);//shift by one integer
		  				  			  				output4=_mm256_add_epi32(m0,output4);
		  				  			  				output4 = division_32(division_case,output4,f);


		  				  			  				//blend output1, output2, output3, output4 into one register
		  				  			  				output2 = _mm256_slli_si256(output2,1);
		  				  			  				output1 = _mm256_add_epi8(output1,output2);
		  				  			  				output3 = _mm256_slli_si256(output3,2);
		  				  			  				output1 = _mm256_add_epi8(output1,output3);
		  				  			  				output4 = _mm256_slli_si256(output4,3);
		  				  			  				output1 = _mm256_add_epi8(output1,output4);

		  				  			  				_mm256_store_si256( (__m256i *) &filt[row][0],output1 );

		  			//END - extra code needed for prelude
		  		}
		  else {
				//load the 7 rows
				r0=_mm256_loadu_si256( (__m256i *) &frame1[row-3][col-3]);
				r1=_mm256_loadu_si256( (__m256i *) &frame1[row-2][col-3]);
				r2=_mm256_loadu_si256( (__m256i *) &frame1[row-1][col-3]);
				r3=_mm256_loadu_si256( (__m256i *) &frame1[row][col-3]);
				r4=_mm256_loadu_si256( (__m256i *) &frame1[row+1][col-3]);
				r5=_mm256_loadu_si256( (__m256i *) &frame1[row+2][col-3]);
				r6=_mm256_loadu_si256( (__m256i *) &frame1[row+3][col-3]);

		// col iteration computes output pixels of   0,8,16,24
		// col+1 iteration computes output pixels of 1,9,17,25
		// col+2 iteration computes output pixels of 2,10,18,26
		// col+3 iteration computes output pixels of 3,11,19,27
		// col+4 iteration computes output pixels of 4,12,20,28
		// col+5 iteration computes output pixels of 5,13,21,29
		// col+6 iteration computes output pixels of 6,14,22,30
		// col+7 iteration computes output pixels of 7,15,23,31
		//afterwards, col becomes 32 and repeat the above process

			//1st col iteration

			//multiply by the mask
			m0=_mm256_maddubs_epi16(r0,c0);
			m1=_mm256_maddubs_epi16(r1,c1);
			m2=_mm256_maddubs_epi16(r2,c2);
			m3=_mm256_maddubs_epi16(r3,c3);
			m4=_mm256_maddubs_epi16(r4,c4);
			m5=_mm256_maddubs_epi16(r5,c5);
			m6=_mm256_maddubs_epi16(r6,c6);


			//make 16-bit values 32-bit (horizontal add)
			m0=_mm256_madd_epi16(m0,ones);
			m1=_mm256_madd_epi16(m1,ones);
			m2=_mm256_madd_epi16(m2,ones);
			m3=_mm256_madd_epi16(m3,ones);
			m4=_mm256_madd_epi16(m4,ones);
			m5=_mm256_madd_epi16(m5,ones);
			m6=_mm256_madd_epi16(m6,ones);


			//vertical add
			m0=_mm256_add_epi32(m0,m1);
			m0=_mm256_add_epi32(m0,m2);
			m0=_mm256_add_epi32(m0,m3);
			m0=_mm256_add_epi32(m0,m4);
			m0=_mm256_add_epi32(m0,m5);
			m0=_mm256_add_epi32(m0,m6);


			//last horizontal add
			m1=_mm256_slli_si256(m0,4);//shift by one integer
			m0=_mm256_add_epi32(m0,m1);//add
			m1=_mm256_srli_si256(m0,4);//shift by one integer
			output1=_mm256_and_si256(m1,mask_even_32);//keep only 0,2,4,6 and discard 1,3,5,7



			//2nd col iteration
			//load the 5 rows
			r0=_mm256_loadu_si256( (__m256i *) &frame1[row-3][col-3+1]);
			r1=_mm256_loadu_si256( (__m256i *) &frame1[row-2][col-3+1]);
			r2=_mm256_loadu_si256( (__m256i *) &frame1[row-1][col-3+1]);
			r3=_mm256_loadu_si256( (__m256i *) &frame1[row][col-3+1]);
			r4=_mm256_loadu_si256( (__m256i *) &frame1[row+1][col-3+1]);
			r5=_mm256_loadu_si256( (__m256i *) &frame1[row+2][col-3+1]);
			r6=_mm256_loadu_si256( (__m256i *) &frame1[row+3][col-3+1]);

			//multiply by the mask
			m0=_mm256_maddubs_epi16(r0,c0);
			m1=_mm256_maddubs_epi16(r1,c1);
			m2=_mm256_maddubs_epi16(r2,c2);
			m3=_mm256_maddubs_epi16(r3,c3);
			m4=_mm256_maddubs_epi16(r4,c4);
			m5=_mm256_maddubs_epi16(r5,c5);
			m6=_mm256_maddubs_epi16(r6,c6);


			//make 16-bit values 32-bit (horizontal add)
			m0=_mm256_madd_epi16(m0,ones);
			m1=_mm256_madd_epi16(m1,ones);
			m2=_mm256_madd_epi16(m2,ones);
			m3=_mm256_madd_epi16(m3,ones);
			m4=_mm256_madd_epi16(m4,ones);
			m5=_mm256_madd_epi16(m5,ones);
			m6=_mm256_madd_epi16(m6,ones);


			//vertical add
			m0=_mm256_add_epi32(m0,m1);
			m0=_mm256_add_epi32(m0,m2);
			m0=_mm256_add_epi32(m0,m3);
			m0=_mm256_add_epi32(m0,m4);
			m0=_mm256_add_epi32(m0,m5);
			m0=_mm256_add_epi32(m0,m6);


			//last horizontal add
			m1=_mm256_slli_si256(m0,4);//shift by one integer
			m0=_mm256_add_epi32(m0,m1);//add
			m1=_mm256_srli_si256(m0,4);//shift by one integer
			output2=_mm256_and_si256(m1,mask_even_32);//keep only 0,2,4,6 and discard 1,3,5,7


			//3rd col iteration
			//load the 5 rows
			r0=_mm256_loadu_si256( (__m256i *) &frame1[row-3][col-3+2]);
			r1=_mm256_loadu_si256( (__m256i *) &frame1[row-2][col-3+2]);
			r2=_mm256_loadu_si256( (__m256i *) &frame1[row-1][col-3+2]);
			r3=_mm256_loadu_si256( (__m256i *) &frame1[row][col-3+2]);
			r4=_mm256_loadu_si256( (__m256i *) &frame1[row+1][col-3+2]);
			r5=_mm256_loadu_si256( (__m256i *) &frame1[row+2][col-3+2]);
			r6=_mm256_loadu_si256( (__m256i *) &frame1[row+3][col-3+2]);

			//multiply by the mask
			m0=_mm256_maddubs_epi16(r0,c0);
			m1=_mm256_maddubs_epi16(r1,c1);
			m2=_mm256_maddubs_epi16(r2,c2);
			m3=_mm256_maddubs_epi16(r3,c3);
			m4=_mm256_maddubs_epi16(r4,c4);
			m5=_mm256_maddubs_epi16(r5,c5);
			m6=_mm256_maddubs_epi16(r6,c6);


			//make 16-bit values 32-bit (horizontal add)
			m0=_mm256_madd_epi16(m0,ones);
			m1=_mm256_madd_epi16(m1,ones);
			m2=_mm256_madd_epi16(m2,ones);
			m3=_mm256_madd_epi16(m3,ones);
			m4=_mm256_madd_epi16(m4,ones);
			m5=_mm256_madd_epi16(m5,ones);
			m6=_mm256_madd_epi16(m6,ones);


			//vertical add
			m0=_mm256_add_epi32(m0,m1);
			m0=_mm256_add_epi32(m0,m2);
			m0=_mm256_add_epi32(m0,m3);
			m0=_mm256_add_epi32(m0,m4);
			m0=_mm256_add_epi32(m0,m5);
			m0=_mm256_add_epi32(m0,m6);


			//last horizontal add
			m1=_mm256_slli_si256(m0,4);//shift by one integer
			m0=_mm256_add_epi32(m0,m1);//add
			m1=_mm256_srli_si256(m0,4);//shift by one integer
			output3=_mm256_and_si256(m1,mask_even_32);//keep only 0,2,4,6 and discard 1,3,5,7


			//4th col iteration
			//load the 5 rows
			r0=_mm256_loadu_si256( (__m256i *) &frame1[row-3][col-3+3]);
			r1=_mm256_loadu_si256( (__m256i *) &frame1[row-2][col-3+3]);
			r2=_mm256_loadu_si256( (__m256i *) &frame1[row-1][col-3+3]);
			r3=_mm256_loadu_si256( (__m256i *) &frame1[row][col-3+3]);
			r4=_mm256_loadu_si256( (__m256i *) &frame1[row+1][col-3+3]);
			r5=_mm256_loadu_si256( (__m256i *) &frame1[row+2][col-3+3]);
			r6=_mm256_loadu_si256( (__m256i *) &frame1[row+3][col-3+3]);

			//multiply by the mask
			m0=_mm256_maddubs_epi16(r0,c0);
			m1=_mm256_maddubs_epi16(r1,c1);
			m2=_mm256_maddubs_epi16(r2,c2);
			m3=_mm256_maddubs_epi16(r3,c3);
			m4=_mm256_maddubs_epi16(r4,c4);
			m5=_mm256_maddubs_epi16(r5,c5);
			m6=_mm256_maddubs_epi16(r6,c6);


			//make 16-bit values 32-bit (horizontal add)
			m0=_mm256_madd_epi16(m0,ones);
			m1=_mm256_madd_epi16(m1,ones);
			m2=_mm256_madd_epi16(m2,ones);
			m3=_mm256_madd_epi16(m3,ones);
			m4=_mm256_madd_epi16(m4,ones);
			m5=_mm256_madd_epi16(m5,ones);
			m6=_mm256_madd_epi16(m6,ones);


			//vertical add
			m0=_mm256_add_epi32(m0,m1);
			m0=_mm256_add_epi32(m0,m2);
			m0=_mm256_add_epi32(m0,m3);
			m0=_mm256_add_epi32(m0,m4);
			m0=_mm256_add_epi32(m0,m5);
			m0=_mm256_add_epi32(m0,m6);


			//last horizontal add
			m1=_mm256_slli_si256(m0,4);//shift by one integer
			m0=_mm256_add_epi32(m0,m1);//add
			m1=_mm256_srli_si256(m0,4);//shift by one integer
			output4=_mm256_and_si256(m1,mask_even_32);//keep only 0,2,4,6 and discard 1,3,5,7


			//5th col iteration

			//load the 5 rows
			r0=_mm256_loadu_si256( (__m256i *) &frame1[row-3][col-3+4]);
			r1=_mm256_loadu_si256( (__m256i *) &frame1[row-2][col-3+4]);
			r2=_mm256_loadu_si256( (__m256i *) &frame1[row-1][col-3+4]);
			r3=_mm256_loadu_si256( (__m256i *) &frame1[row][col-3+4]);
			r4=_mm256_loadu_si256( (__m256i *) &frame1[row+1][col-3+4]);
			r5=_mm256_loadu_si256( (__m256i *) &frame1[row+2][col-3+4]);
			r6=_mm256_loadu_si256( (__m256i *) &frame1[row+3][col-3+4]);

			//multiply by the mask
			m0=_mm256_maddubs_epi16(r0,c0);
			m1=_mm256_maddubs_epi16(r1,c1);
			m2=_mm256_maddubs_epi16(r2,c2);
			m3=_mm256_maddubs_epi16(r3,c3);
			m4=_mm256_maddubs_epi16(r4,c4);
			m5=_mm256_maddubs_epi16(r5,c5);
			m6=_mm256_maddubs_epi16(r6,c6);


			//make 16-bit values 32-bit (horizontal add)
			m0=_mm256_madd_epi16(m0,ones);
			m1=_mm256_madd_epi16(m1,ones);
			m2=_mm256_madd_epi16(m2,ones);
			m3=_mm256_madd_epi16(m3,ones);
			m4=_mm256_madd_epi16(m4,ones);
			m5=_mm256_madd_epi16(m5,ones);
			m6=_mm256_madd_epi16(m6,ones);


			//vertical add
			m0=_mm256_add_epi32(m0,m1);
			m0=_mm256_add_epi32(m0,m2);
			m0=_mm256_add_epi32(m0,m3);
			m0=_mm256_add_epi32(m0,m4);
			m0=_mm256_add_epi32(m0,m5);
			m0=_mm256_add_epi32(m0,m6);


			//last horizontal add
			m1=_mm256_slli_si256(m0,4);//shift by one integer
			m0=_mm256_add_epi32(m0,m1);//add
			m1=_mm256_srli_si256(m0,4);//shift by one integer
			m0=_mm256_and_si256(m1,mask_even_32);//keep only 0,2,4,6 and discard 1,3,5,7


			//MERGE output1 with output5 and DIVIDE
			m0=_mm256_slli_si256(m0,4);//shift by one integer
			output1=_mm256_add_epi32(m0,output1);
			output1 = division_32(division_case,output1,f);


			//6th col iteration
			//load the 5 rows
			r0=_mm256_loadu_si256( (__m256i *) &frame1[row-3][col-3+5]);
			r1=_mm256_loadu_si256( (__m256i *) &frame1[row-2][col-3+5]);
			r2=_mm256_loadu_si256( (__m256i *) &frame1[row-1][col-3+5]);
			r3=_mm256_loadu_si256( (__m256i *) &frame1[row][col-3+5]);
			r4=_mm256_loadu_si256( (__m256i *) &frame1[row+1][col-3+5]);
			r5=_mm256_loadu_si256( (__m256i *) &frame1[row+2][col-3+5]);
			r6=_mm256_loadu_si256( (__m256i *) &frame1[row+3][col-3+5]);

			//multiply by the mask
			m0=_mm256_maddubs_epi16(r0,c0);
			m1=_mm256_maddubs_epi16(r1,c1);
			m2=_mm256_maddubs_epi16(r2,c2);
			m3=_mm256_maddubs_epi16(r3,c3);
			m4=_mm256_maddubs_epi16(r4,c4);
			m5=_mm256_maddubs_epi16(r5,c5);
			m6=_mm256_maddubs_epi16(r6,c6);


			//make 16-bit values 32-bit (horizontal add)
			m0=_mm256_madd_epi16(m0,ones);
			m1=_mm256_madd_epi16(m1,ones);
			m2=_mm256_madd_epi16(m2,ones);
			m3=_mm256_madd_epi16(m3,ones);
			m4=_mm256_madd_epi16(m4,ones);
			m5=_mm256_madd_epi16(m5,ones);
			m6=_mm256_madd_epi16(m6,ones);


			//vertical add
			m0=_mm256_add_epi32(m0,m1);
			m0=_mm256_add_epi32(m0,m2);
			m0=_mm256_add_epi32(m0,m3);
			m0=_mm256_add_epi32(m0,m4);
			m0=_mm256_add_epi32(m0,m5);
			m0=_mm256_add_epi32(m0,m6);


			//last horizontal add
			m1=_mm256_slli_si256(m0,4);//shift by one integer
			m0=_mm256_add_epi32(m0,m1);//add
			m1=_mm256_srli_si256(m0,4);//shift by one integer
			m0=_mm256_and_si256(m1,mask_even_32);//keep only 0,2,4,6 and discard 1,3,5,7

			//MERGE output1 with output5 and DIVIDE
			m0=_mm256_slli_si256(m0,4);//shift by one integer
			output2=_mm256_add_epi32(m0,output2);
			output2 = division_32(division_case,output2,f);



			//7th col iteration
			//load the 5 rows
			r0=_mm256_loadu_si256( (__m256i *) &frame1[row-3][col-3+6]);
			r1=_mm256_loadu_si256( (__m256i *) &frame1[row-2][col-3+6]);
			r2=_mm256_loadu_si256( (__m256i *) &frame1[row-1][col-3+6]);
			r3=_mm256_loadu_si256( (__m256i *) &frame1[row][col-3+6]);
			r4=_mm256_loadu_si256( (__m256i *) &frame1[row+1][col-3+6]);
			r5=_mm256_loadu_si256( (__m256i *) &frame1[row+2][col-3+6]);
			r6=_mm256_loadu_si256( (__m256i *) &frame1[row+3][col-3+6]);

			//multiply by the mask
			m0=_mm256_maddubs_epi16(r0,c0);
			m1=_mm256_maddubs_epi16(r1,c1);
			m2=_mm256_maddubs_epi16(r2,c2);
			m3=_mm256_maddubs_epi16(r3,c3);
			m4=_mm256_maddubs_epi16(r4,c4);
			m5=_mm256_maddubs_epi16(r5,c5);
			m6=_mm256_maddubs_epi16(r6,c6);


			//make 16-bit values 32-bit (horizontal add)
			m0=_mm256_madd_epi16(m0,ones);
			m1=_mm256_madd_epi16(m1,ones);
			m2=_mm256_madd_epi16(m2,ones);
			m3=_mm256_madd_epi16(m3,ones);
			m4=_mm256_madd_epi16(m4,ones);
			m5=_mm256_madd_epi16(m5,ones);
			m6=_mm256_madd_epi16(m6,ones);


			//vertical add
			m0=_mm256_add_epi32(m0,m1);
			m0=_mm256_add_epi32(m0,m2);
			m0=_mm256_add_epi32(m0,m3);
			m0=_mm256_add_epi32(m0,m4);
			m0=_mm256_add_epi32(m0,m5);
			m0=_mm256_add_epi32(m0,m6);


			//last horizontal add
			m1=_mm256_slli_si256(m0,4);//shift by one integer
			m0=_mm256_add_epi32(m0,m1);//add
			m1=_mm256_srli_si256(m0,4);//shift by one integer
			m0=_mm256_and_si256(m1,mask_even_32);//keep only 0,2,4,6 and discard 1,3,5,7

			//MERGE output1 with output5 and DIVIDE
			m0=_mm256_slli_si256(m0,4);//shift by one integer
			output3=_mm256_add_epi32(m0,output3);
			output3 = division_32(division_case,output3,f);



			//8th col iteration

			//load the 5 rows
			r0=_mm256_loadu_si256( (__m256i *) &frame1[row-3][col-3+7]);
			r1=_mm256_loadu_si256( (__m256i *) &frame1[row-2][col-3+7]);
			r2=_mm256_loadu_si256( (__m256i *) &frame1[row-1][col-3+7]);
			r3=_mm256_loadu_si256( (__m256i *) &frame1[row][col-3+7]);
			r4=_mm256_loadu_si256( (__m256i *) &frame1[row+1][col-3+7]);
			r5=_mm256_loadu_si256( (__m256i *) &frame1[row+2][col-3+7]);
			r6=_mm256_loadu_si256( (__m256i *) &frame1[row+3][col-3+7]);

			//multiply by the mask
			m0=_mm256_maddubs_epi16(r0,c0);
			m1=_mm256_maddubs_epi16(r1,c1);
			m2=_mm256_maddubs_epi16(r2,c2);
			m3=_mm256_maddubs_epi16(r3,c3);
			m4=_mm256_maddubs_epi16(r4,c4);
			m5=_mm256_maddubs_epi16(r5,c5);
			m6=_mm256_maddubs_epi16(r6,c6);


			//make 16-bit values 32-bit (horizontal add)
			m0=_mm256_madd_epi16(m0,ones);
			m1=_mm256_madd_epi16(m1,ones);
			m2=_mm256_madd_epi16(m2,ones);
			m3=_mm256_madd_epi16(m3,ones);
			m4=_mm256_madd_epi16(m4,ones);
			m5=_mm256_madd_epi16(m5,ones);
			m6=_mm256_madd_epi16(m6,ones);


			//vertical add
			m0=_mm256_add_epi32(m0,m1);
			m0=_mm256_add_epi32(m0,m2);
			m0=_mm256_add_epi32(m0,m3);
			m0=_mm256_add_epi32(m0,m4);
			m0=_mm256_add_epi32(m0,m5);
			m0=_mm256_add_epi32(m0,m6);


			//last horizontal add
			m1=_mm256_slli_si256(m0,4);//shift by one integer
			m0=_mm256_add_epi32(m0,m1);//add
			m1=_mm256_srli_si256(m0,4);//shift by one integer
			m0=_mm256_and_si256(m1,mask_even_32);//keep only 0,2,4,6 and discard 1,3,5,7


			//MERGE output1 with output5 and DIVIDE
			m0=_mm256_slli_si256(m0,4);//shift by one integer
			output4=_mm256_add_epi32(m0,output4);
			output4 = division_32(division_case,output4,f);


			//blend output1, output2, output3, output4 into one register
			output2 = _mm256_slli_si256(output2,1);
			output1 = _mm256_add_epi8(output1,output2);
			output3 = _mm256_slli_si256(output3,2);
			output1 = _mm256_add_epi8(output1,output3);
			output4 = _mm256_slli_si256(output4,3);
			output1 = _mm256_add_epi8(output1,output4);

			//_mm256_stream_si256( (__m256i *) &filt[row][col],output1 );
			_mm256_store_si256( (__m256i *) &filt[row][col],output1 );
		  }
		}
	  loop_reminder_7x7(frame1,filt,row, col,REMINDER_ITERATIONS,division_case, mask_vector, f, divisor, kernel);

		}
}

}//end of parallel


}


inline __m256i insert_four_zeros_front(__m256i input, __m256i mask_prelude){

			__m256i m0=_mm256_slli_si256(input,4);//shift 4 elements left - equivalent to filling with two zeros inthe beginning

			__m256i r0=_mm256_and_si256(input,mask_prelude);
			r0=_mm256_permute2f128_si256(r0,r0,1);
			r0=_mm256_srli_si256(r0,12);
			return _mm256_add_epi16(m0,r0);

}


inline __m256i insert_three_zeros_front(__m256i input, __m256i mask_prelude){

			__m256i m0=_mm256_slli_si256(input,3);//shift 3 elements left - equivalent to filling with two zeros inthe beginning

			__m256i r0=_mm256_and_si256(input,mask_prelude);
			r0=_mm256_permute2f128_si256(r0,r0,1);
			r0=_mm256_srli_si256(r0,13);
			return _mm256_add_epi16(m0,r0);

}

inline __m256i insert_two_zeros_front(__m256i input, __m256i mask_prelude){

			__m256i m0=_mm256_slli_si256(input,2);//shift 3 elements left - equivalent to filling with two zeros inthe beginning

			__m256i r0=_mm256_and_si256(input,mask_prelude);
			r0=_mm256_permute2f128_si256(r0,r0,1);
			r0=_mm256_srli_si256(r0,14);
			return _mm256_add_epi16(m0,r0);

}

inline __m256i insert_one_zeros_front(__m256i input, __m256i mask_prelude){

			__m256i m0=_mm256_slli_si256(input,1);//shift 3 elements left - equivalent to filling with two zeros inthe beginning

			__m256i r0=_mm256_and_si256(input,mask_prelude);
			r0=_mm256_permute2f128_si256(r0,r0,1);
			r0=_mm256_srli_si256(r0,15);
			return _mm256_add_epi16(m0,r0);

}


inline __m256i main_block_7x7(unsigned char **frame1,const int row, const int col, const __m256i c0,const __m256i c1,const __m256i c2,const __m256i c3,const __m256i c4,const __m256i c5,const __m256i c6,const __m256i ones,const __m256i mask_even_32){

	__m256i	r0=_mm256_loadu_si256( (__m256i *) &frame1[row-3][col-3]);
	__m256i	r1=_mm256_loadu_si256( (__m256i *) &frame1[row-2][col-3]);
	__m256i	r2=_mm256_loadu_si256( (__m256i *) &frame1[row-1][col-3]);
	__m256i	r3=_mm256_loadu_si256( (__m256i *) &frame1[row][col-3]);
	__m256i	r4=_mm256_loadu_si256( (__m256i *) &frame1[row+1][col-3]);
	__m256i	r5=_mm256_loadu_si256( (__m256i *) &frame1[row+2][col-3]);
	__m256i	r6=_mm256_loadu_si256( (__m256i *) &frame1[row+3][col-3]);

// col iteration computes output pixels of   0,8,16,24
// col+1 iteration computes output pixels of 1,9,17,25
// col+2 iteration computes output pixels of 2,10,18,26
// col+3 iteration computes output pixels of 3,11,19,27
// col+4 iteration computes output pixels of 4,12,20,28
// col+5 iteration computes output pixels of 5,13,21,29
// col+6 iteration computes output pixels of 6,14,22,30
// col+7 iteration computes output pixels of 7,15,23,31
//afterwards, col becomes 26 and repeat the above process

//1st col iteration

//multiply by the mask
__m256i m0=_mm256_maddubs_epi16(r0,c0);
__m256i m1=_mm256_maddubs_epi16(r1,c1);
__m256i m2=_mm256_maddubs_epi16(r2,c2);
__m256i m3=_mm256_maddubs_epi16(r3,c3);
__m256i m4=_mm256_maddubs_epi16(r4,c4);
__m256i m5=_mm256_maddubs_epi16(r5,c5);
__m256i m6=_mm256_maddubs_epi16(r6,c6);


//make 16-bit values 32-bit (horizontal add)
m0=_mm256_madd_epi16(m0,ones);
m1=_mm256_madd_epi16(m1,ones);
m2=_mm256_madd_epi16(m2,ones);
m3=_mm256_madd_epi16(m3,ones);
m4=_mm256_madd_epi16(m4,ones);
m5=_mm256_madd_epi16(m5,ones);
m6=_mm256_madd_epi16(m6,ones);


//vertical add
m0=_mm256_add_epi32(m0,m1);
m0=_mm256_add_epi32(m0,m2);
m0=_mm256_add_epi32(m0,m3);
m0=_mm256_add_epi32(m0,m4);
m0=_mm256_add_epi32(m0,m5);
m0=_mm256_add_epi32(m0,m6);


//last horizontal add
m1=_mm256_slli_si256(m0,4);//shift by one integer
m0=_mm256_add_epi32(m0,m1);//add
m1=_mm256_srli_si256(m0,4);//shift by one integer
return _mm256_and_si256(m1,mask_even_32);//keep only 0,2,4,6 and discard 1,3,5,7
}


//this function is computing the row=2 and row=N-3 only
//for row=2 the input arguments are prelude_block_3_7x7(frame,0,col,c1,c2,c3,c4,c5,c6,ones,mask)
//row=N-3 the input arguments are prelude_block_3_7x7(frame,N-6,col,c0,c1,c2,c3,c4,c5,ones,mask)
__m256i prelude_block_3_7x7(unsigned char **frame1,const int row, const int col, const __m256i c0,const __m256i c1,const __m256i c2,const __m256i c3,const __m256i c4,const __m256i c5,const __m256i ones,const __m256i mask_even_32){

	__m256i	r0=_mm256_loadu_si256( (__m256i *) &frame1[row][col-3]);
	__m256i	r1=_mm256_loadu_si256( (__m256i *) &frame1[row+1][col-3]);
	__m256i	r2=_mm256_loadu_si256( (__m256i *) &frame1[row+2][col-3]);
	__m256i	r3=_mm256_loadu_si256( (__m256i *) &frame1[row+3][col-3]);
	__m256i	r4=_mm256_loadu_si256( (__m256i *) &frame1[row+4][col-3]);
	__m256i	r5=_mm256_loadu_si256( (__m256i *) &frame1[row+5][col-3]);

//multiply by the mask
__m256i m0=_mm256_maddubs_epi16(r0,c0);
__m256i m1=_mm256_maddubs_epi16(r1,c1);
__m256i m2=_mm256_maddubs_epi16(r2,c2);
__m256i m3=_mm256_maddubs_epi16(r3,c3);
__m256i m4=_mm256_maddubs_epi16(r4,c4);
__m256i m5=_mm256_maddubs_epi16(r5,c5);


//make 16-bit values 32-bit (horizontal add)
m0=_mm256_madd_epi16(m0,ones);
m1=_mm256_madd_epi16(m1,ones);
m2=_mm256_madd_epi16(m2,ones);
m3=_mm256_madd_epi16(m3,ones);
m4=_mm256_madd_epi16(m4,ones);
m5=_mm256_madd_epi16(m5,ones);


//vertical add
m0=_mm256_add_epi32(m0,m1);
m0=_mm256_add_epi32(m0,m2);
m0=_mm256_add_epi32(m0,m3);
m0=_mm256_add_epi32(m0,m4);
m0=_mm256_add_epi32(m0,m5);


//last horizontal add
m1=_mm256_slli_si256(m0,4);//shift by one integer
m0=_mm256_add_epi32(m0,m1);//add
m1=_mm256_srli_si256(m0,4);//shift by one integer
return _mm256_and_si256(m1,mask_even_32);//keep only 0,2,4,6 and discard 1,3,5,7
}

__m256i prelude_block_3_7x7_no_load( const __m256i r0,const __m256i r1,const __m256i r2,const __m256i r3,const __m256i r4, const __m256i r5,const __m256i c0,const __m256i c1,const __m256i c2,const __m256i c3,const __m256i c4,const __m256i c5,const __m256i ones,const __m256i mask_even_32){


//multiply by the mask
__m256i m0=_mm256_maddubs_epi16(r0,c0);
__m256i m1=_mm256_maddubs_epi16(r1,c1);
__m256i m2=_mm256_maddubs_epi16(r2,c2);
__m256i m3=_mm256_maddubs_epi16(r3,c3);
__m256i m4=_mm256_maddubs_epi16(r4,c4);
__m256i m5=_mm256_maddubs_epi16(r5,c5);


//make 16-bit values 32-bit (horizontal add)
m0=_mm256_madd_epi16(m0,ones);
m1=_mm256_madd_epi16(m1,ones);
m2=_mm256_madd_epi16(m2,ones);
m3=_mm256_madd_epi16(m3,ones);
m4=_mm256_madd_epi16(m4,ones);
m5=_mm256_madd_epi16(m5,ones);


//vertical add
m0=_mm256_add_epi32(m0,m1);
m0=_mm256_add_epi32(m0,m2);
m0=_mm256_add_epi32(m0,m3);
m0=_mm256_add_epi32(m0,m4);
m0=_mm256_add_epi32(m0,m5);


//last horizontal add
m1=_mm256_slli_si256(m0,4);//shift by one integer
m0=_mm256_add_epi32(m0,m1);//add
m1=_mm256_srli_si256(m0,4);//shift by one integer
return _mm256_and_si256(m1,mask_even_32);//keep only 0,2,4,6 and discard 1,3,5,7
}


//this function is computing the row=1 and row=N-2 only
//for row=1 the input arguments are prelude_block_3_7x7(frame,0,col,c2,c3,c4,c5,c6,ones,mask)
//row=N-2 the input arguments are prelude_block_3_7x7(frame,N-5,col,c0,c1,c2,c3,c4,ones,mask)
__m256i prelude_block_2_7x7(unsigned char **frame1,const int row, const int col, const __m256i c0,const __m256i c1,const __m256i c2,const __m256i c3,const __m256i c4,const __m256i ones,const __m256i mask_even_32){

	__m256i	r0=_mm256_loadu_si256( (__m256i *) &frame1[row][col-3]);
	__m256i	r1=_mm256_loadu_si256( (__m256i *) &frame1[row+1][col-3]);
	__m256i	r2=_mm256_loadu_si256( (__m256i *) &frame1[row+2][col-3]);
	__m256i	r3=_mm256_loadu_si256( (__m256i *) &frame1[row+3][col-3]);
	__m256i	r4=_mm256_loadu_si256( (__m256i *) &frame1[row+4][col-3]);


//multiply by the mask
__m256i m0=_mm256_maddubs_epi16(r0,c0);
__m256i m1=_mm256_maddubs_epi16(r1,c1);
__m256i m2=_mm256_maddubs_epi16(r2,c2);
__m256i m3=_mm256_maddubs_epi16(r3,c3);
__m256i m4=_mm256_maddubs_epi16(r4,c4);


//make 16-bit values 32-bit (horizontal add)
m0=_mm256_madd_epi16(m0,ones);
m1=_mm256_madd_epi16(m1,ones);
m2=_mm256_madd_epi16(m2,ones);
m3=_mm256_madd_epi16(m3,ones);
m4=_mm256_madd_epi16(m4,ones);


//vertical add
m0=_mm256_add_epi32(m0,m1);
m0=_mm256_add_epi32(m0,m2);
m0=_mm256_add_epi32(m0,m3);
m0=_mm256_add_epi32(m0,m4);


//last horizontal add
m1=_mm256_slli_si256(m0,4);//shift by one integer
m0=_mm256_add_epi32(m0,m1);//add
m1=_mm256_srli_si256(m0,4);//shift by one integer
return _mm256_and_si256(m1,mask_even_32);//keep only 0,2,4,6 and discard 1,3,5,7
}

__m256i prelude_block_2_7x7_no_load( const __m256i r0,const __m256i r1,const __m256i r2,const __m256i r3,const __m256i r4, const __m256i c0,const __m256i c1,const __m256i c2,const __m256i c3,const __m256i c4,const __m256i ones,const __m256i mask_even_32){

	//multiply by the mask
	__m256i m0=_mm256_maddubs_epi16(r0,c0);
	__m256i m1=_mm256_maddubs_epi16(r1,c1);
	__m256i m2=_mm256_maddubs_epi16(r2,c2);
	__m256i m3=_mm256_maddubs_epi16(r3,c3);
	__m256i m4=_mm256_maddubs_epi16(r4,c4);


	//make 16-bit values 32-bit (horizontal add)
	m0=_mm256_madd_epi16(m0,ones);
	m1=_mm256_madd_epi16(m1,ones);
	m2=_mm256_madd_epi16(m2,ones);
	m3=_mm256_madd_epi16(m3,ones);
	m4=_mm256_madd_epi16(m4,ones);


	//vertical add
	m0=_mm256_add_epi32(m0,m1);
	m0=_mm256_add_epi32(m0,m2);
	m0=_mm256_add_epi32(m0,m3);
	m0=_mm256_add_epi32(m0,m4);


	//last horizontal add
	m1=_mm256_slli_si256(m0,4);//shift by one integer
	m0=_mm256_add_epi32(m0,m1);//add
	m1=_mm256_srli_si256(m0,4);//shift by one integer
	return _mm256_and_si256(m1,mask_even_32);//keep only 0,2,4,6 and discard 1,3,5,7
}


//this function is computing the row=0 and row=N-1 only
//for row=0 the input arguments are prelude_block_3_7x7(frame,0,col,c3,c4,c5,c6,ones,mask)
//row=N-1 the input arguments are prelude_block_3_7x7(frame,N-4,col,c0,c1,c2,c3,ones,mask)
__m256i prelude_block_1_7x7(unsigned char **frame1,const int row, const int col, const __m256i c0,const __m256i c1,const __m256i c2,const __m256i c3,const __m256i ones,const __m256i mask_even_32){

	__m256i	r0=_mm256_loadu_si256( (__m256i *) &frame1[row][col-3]);
	__m256i	r1=_mm256_loadu_si256( (__m256i *) &frame1[row+1][col-3]);
	__m256i	r2=_mm256_loadu_si256( (__m256i *) &frame1[row+2][col-3]);
	__m256i	r3=_mm256_loadu_si256( (__m256i *) &frame1[row+3][col-3]);


//multiply by the mask
__m256i m0=_mm256_maddubs_epi16(r0,c0);
__m256i m1=_mm256_maddubs_epi16(r1,c1);
__m256i m2=_mm256_maddubs_epi16(r2,c2);
__m256i m3=_mm256_maddubs_epi16(r3,c3);


//make 16-bit values 32-bit (horizontal add)
m0=_mm256_madd_epi16(m0,ones);
m1=_mm256_madd_epi16(m1,ones);
m2=_mm256_madd_epi16(m2,ones);
m3=_mm256_madd_epi16(m3,ones);


//vertical add
m0=_mm256_add_epi32(m0,m1);
m0=_mm256_add_epi32(m0,m2);
m0=_mm256_add_epi32(m0,m3);


//last horizontal add
m1=_mm256_slli_si256(m0,4);//shift by one integer
m0=_mm256_add_epi32(m0,m1);//add
m1=_mm256_srli_si256(m0,4);//shift by one integer
return _mm256_and_si256(m1,mask_even_32);//keep only 0,2,4,6 and discard 1,3,5,7
}

__m256i prelude_block_1_7x7_no_load( const __m256i r0,const __m256i r1,const __m256i r2,const __m256i r3, const __m256i c0,const __m256i c1,const __m256i c2,const __m256i c3,const __m256i ones,const __m256i mask_even_32){


	//multiply by the mask
	__m256i m0=_mm256_maddubs_epi16(r0,c0);
	__m256i m1=_mm256_maddubs_epi16(r1,c1);
	__m256i m2=_mm256_maddubs_epi16(r2,c2);
	__m256i m3=_mm256_maddubs_epi16(r3,c3);


	//make 16-bit values 32-bit (horizontal add)
	m0=_mm256_madd_epi16(m0,ones);
	m1=_mm256_madd_epi16(m1,ones);
	m2=_mm256_madd_epi16(m2,ones);
	m3=_mm256_madd_epi16(m3,ones);


	//vertical add
	m0=_mm256_add_epi32(m0,m1);
	m0=_mm256_add_epi32(m0,m2);
	m0=_mm256_add_epi32(m0,m3);


	//last horizontal add
	m1=_mm256_slli_si256(m0,4);//shift by one integer
	m0=_mm256_add_epi32(m0,m1);//add
	m1=_mm256_srli_si256(m0,4);//shift by one integer
	return _mm256_and_si256(m1,mask_even_32);//keep only 0,2,4,6 and discard 1,3,5,7
}


__m256i main_block_pre_7x7(const __m256i r0,const __m256i r1,const __m256i r2,const __m256i r3,const __m256i r4,const __m256i r5,const __m256i r6, const __m256i c0,const __m256i c1,const __m256i c2,const __m256i c3,const __m256i c4,const __m256i c5,const __m256i c6,const __m256i ones,const __m256i mask_even_32){

				//multiply by the mask
__m256i				m0=_mm256_maddubs_epi16(r0,c0);
__m256i				m1=_mm256_maddubs_epi16(r1,c1);
__m256i				m2=_mm256_maddubs_epi16(r2,c2);
__m256i				m3=_mm256_maddubs_epi16(r3,c3);
__m256i				m4=_mm256_maddubs_epi16(r4,c4);
__m256i				m5=_mm256_maddubs_epi16(r5,c5);
__m256i				m6=_mm256_maddubs_epi16(r6,c6);


				//make 16-bit values 32-bit (horizontal add)
				m0=_mm256_madd_epi16(m0,ones);
				m1=_mm256_madd_epi16(m1,ones);
				m2=_mm256_madd_epi16(m2,ones);
				m3=_mm256_madd_epi16(m3,ones);
				m4=_mm256_madd_epi16(m4,ones);
				m5=_mm256_madd_epi16(m5,ones);
				m6=_mm256_madd_epi16(m6,ones);


				//vertical add
				m0=_mm256_add_epi32(m0,m1);
				m0=_mm256_add_epi32(m0,m2);
				m0=_mm256_add_epi32(m0,m3);
				m0=_mm256_add_epi32(m0,m4);
				m0=_mm256_add_epi32(m0,m5);
				m0=_mm256_add_epi32(m0,m6);


				//last horizontal add
				m1=_mm256_slli_si256(m0,4);//shift by one integer
				m0=_mm256_add_epi32(m0,m1);//add
				m1=_mm256_srli_si256(m0,4);//shift by one integer
				return _mm256_and_si256(m1,mask_even_32);//keep only 0,2,4,6 and discard 1,3,5,7
}


void loop_reminder_7x7(unsigned char **frame1,unsigned char **filt,const unsigned int row, const unsigned int col,const unsigned int REMINDER_ITERATIONS,const unsigned int division_case,const signed char mask_vector[][32], const __m256i f,const unsigned short int divisor,signed char **filter7x7){

	const __m256i c0=_mm256_load_si256( (__m256i *) &mask_vector[0][0]);
	const __m256i c1=_mm256_load_si256( (__m256i *) &mask_vector[1][0]);
	const __m256i c2=_mm256_load_si256( (__m256i *) &mask_vector[2][0]);
	const __m256i c3=_mm256_load_si256( (__m256i *) &mask_vector[3][0]);
	const __m256i c4=_mm256_load_si256( (__m256i *) &mask_vector[4][0]);
	const __m256i c5=_mm256_load_si256( (__m256i *) &mask_vector[5][0]);
	const __m256i c6=_mm256_load_si256( (__m256i *) &mask_vector[6][0]);

	const __m256i mask_even_32  = _mm256_set_epi32(0,0xffffffff,0,0xffffffff,0,0xffffffff,0,0xffffffff);

	__m256i r0,r1,r2,r3,r4,r5,r6,m0,m1,m2,m3,m4,m5,m6;
	__m256i output1,output2,output3,output4,reminder_mask1;
	__m256i ones=_mm256_set1_epi16(1);


//important: REMINDER_ITERATIONS_GLOBAL ranges from [4,35]. If >32, two iterations are needed, one for computing the first 32 output pixels and another for computing the rest


		//1st col iteration

		r0=_mm256_loadu_si256( (__m256i *) &frame1[row-3][col-3]);
		r1=_mm256_loadu_si256( (__m256i *) &frame1[row-2][col-3]);
		r2=_mm256_loadu_si256( (__m256i *) &frame1[row-1][col-3]);
		r3=_mm256_loadu_si256( (__m256i *) &frame1[row][col-3]);
		r4=_mm256_loadu_si256( (__m256i *) &frame1[row+1][col-3]);
		r5=_mm256_loadu_si256( (__m256i *) &frame1[row+2][col-3]);
		r6=_mm256_loadu_si256( (__m256i *) &frame1[row+3][col-3]);

		reminder_mask1=_mm256_load_si256( (__m256i *) &reminder_msk_7x7[REMINDER_ITERATIONS-1][0]);

		//zero the values that exceed the array bounds
		r0=_mm256_and_si256(r0,reminder_mask1);
		r1=_mm256_and_si256(r1,reminder_mask1);
		r2=_mm256_and_si256(r2,reminder_mask1);
		r3=_mm256_and_si256(r3,reminder_mask1);
		r4=_mm256_and_si256(r4,reminder_mask1);
		r5=_mm256_and_si256(r5,reminder_mask1);
		r6=_mm256_and_si256(r6,reminder_mask1);

// col iteration computes output pixels of   0,8,16,24
// col+1 iteration computes output pixels of 1,9,17,25
// col+2 iteration computes output pixels of 2,10,18,26
// col+3 iteration computes output pixels of 3,11,19,27
// col+4 iteration computes output pixels of 4,12,20,28
// col+5 iteration computes output pixels of 5,13,21,29
// col+6 iteration computes output pixels of 6,14,22,30
// col+7 iteration computes output pixels of 7,15,23,31
//afterwards, col becomes 32 and repeat the above process



	//multiply by the mask
	m0=_mm256_maddubs_epi16(r0,c0);
	m1=_mm256_maddubs_epi16(r1,c1);
	m2=_mm256_maddubs_epi16(r2,c2);
	m3=_mm256_maddubs_epi16(r3,c3);
	m4=_mm256_maddubs_epi16(r4,c4);
	m5=_mm256_maddubs_epi16(r5,c5);
	m6=_mm256_maddubs_epi16(r6,c6);


	//make 16-bit values 32-bit (horizontal add)
	m0=_mm256_madd_epi16(m0,ones);
	m1=_mm256_madd_epi16(m1,ones);
	m2=_mm256_madd_epi16(m2,ones);
	m3=_mm256_madd_epi16(m3,ones);
	m4=_mm256_madd_epi16(m4,ones);
	m5=_mm256_madd_epi16(m5,ones);
	m6=_mm256_madd_epi16(m6,ones);


	//vertical add
	m0=_mm256_add_epi32(m0,m1);
	m0=_mm256_add_epi32(m0,m2);
	m0=_mm256_add_epi32(m0,m3);
	m0=_mm256_add_epi32(m0,m4);
	m0=_mm256_add_epi32(m0,m5);
	m0=_mm256_add_epi32(m0,m6);


	//last horizontal add
	m1=_mm256_slli_si256(m0,4);//shift by one integer
	m0=_mm256_add_epi32(m0,m1);//add
	m1=_mm256_srli_si256(m0,4);//shift by one integer
	output1=_mm256_and_si256(m1,mask_even_32);//keep only 0,2,4,6 and discard 1,3,5,7



	//2nd col iteration
	//load the 5 rows
	r0=_mm256_loadu_si256( (__m256i *) &frame1[row-3][col-3+1]);
	r1=_mm256_loadu_si256( (__m256i *) &frame1[row-2][col-3+1]);
	r2=_mm256_loadu_si256( (__m256i *) &frame1[row-1][col-3+1]);
	r3=_mm256_loadu_si256( (__m256i *) &frame1[row][col-3+1]);
	r4=_mm256_loadu_si256( (__m256i *) &frame1[row+1][col-3+1]);
	r5=_mm256_loadu_si256( (__m256i *) &frame1[row+2][col-3+1]);
	r6=_mm256_loadu_si256( (__m256i *) &frame1[row+3][col-3+1]);

	reminder_mask1=_mm256_load_si256( (__m256i *) &reminder_msk_7x7[REMINDER_ITERATIONS-2][0]);

	//zero the values that exceed the array bounds
	r0=_mm256_and_si256(r0,reminder_mask1);
	r1=_mm256_and_si256(r1,reminder_mask1);
	r2=_mm256_and_si256(r2,reminder_mask1);
	r3=_mm256_and_si256(r3,reminder_mask1);
	r4=_mm256_and_si256(r4,reminder_mask1);
	r5=_mm256_and_si256(r5,reminder_mask1);
	r6=_mm256_and_si256(r6,reminder_mask1);

	//multiply by the mask
	m0=_mm256_maddubs_epi16(r0,c0);
	m1=_mm256_maddubs_epi16(r1,c1);
	m2=_mm256_maddubs_epi16(r2,c2);
	m3=_mm256_maddubs_epi16(r3,c3);
	m4=_mm256_maddubs_epi16(r4,c4);
	m5=_mm256_maddubs_epi16(r5,c5);
	m6=_mm256_maddubs_epi16(r6,c6);


	//make 16-bit values 32-bit (horizontal add)
	m0=_mm256_madd_epi16(m0,ones);
	m1=_mm256_madd_epi16(m1,ones);
	m2=_mm256_madd_epi16(m2,ones);
	m3=_mm256_madd_epi16(m3,ones);
	m4=_mm256_madd_epi16(m4,ones);
	m5=_mm256_madd_epi16(m5,ones);
	m6=_mm256_madd_epi16(m6,ones);


	//vertical add
	m0=_mm256_add_epi32(m0,m1);
	m0=_mm256_add_epi32(m0,m2);
	m0=_mm256_add_epi32(m0,m3);
	m0=_mm256_add_epi32(m0,m4);
	m0=_mm256_add_epi32(m0,m5);
	m0=_mm256_add_epi32(m0,m6);


	//last horizontal add
	m1=_mm256_slli_si256(m0,4);//shift by one integer
	m0=_mm256_add_epi32(m0,m1);//add
	m1=_mm256_srli_si256(m0,4);//shift by one integer
	output2=_mm256_and_si256(m1,mask_even_32);//keep only 0,2,4,6 and discard 1,3,5,7


	//3rd col iteration
	//load the 5 rows
	r0=_mm256_loadu_si256( (__m256i *) &frame1[row-3][col-3+2]);
	r1=_mm256_loadu_si256( (__m256i *) &frame1[row-2][col-3+2]);
	r2=_mm256_loadu_si256( (__m256i *) &frame1[row-1][col-3+2]);
	r3=_mm256_loadu_si256( (__m256i *) &frame1[row][col-3+2]);
	r4=_mm256_loadu_si256( (__m256i *) &frame1[row+1][col-3+2]);
	r5=_mm256_loadu_si256( (__m256i *) &frame1[row+2][col-3+2]);
	r6=_mm256_loadu_si256( (__m256i *) &frame1[row+3][col-3+2]);

	reminder_mask1=_mm256_load_si256( (__m256i *) &reminder_msk_7x7[REMINDER_ITERATIONS-3][0]);

	//zero the values that exceed the array bounds
	r0=_mm256_and_si256(r0,reminder_mask1);
	r1=_mm256_and_si256(r1,reminder_mask1);
	r2=_mm256_and_si256(r2,reminder_mask1);
	r3=_mm256_and_si256(r3,reminder_mask1);
	r4=_mm256_and_si256(r4,reminder_mask1);
	r5=_mm256_and_si256(r5,reminder_mask1);
	r6=_mm256_and_si256(r6,reminder_mask1);

	//multiply by the mask
	m0=_mm256_maddubs_epi16(r0,c0);
	m1=_mm256_maddubs_epi16(r1,c1);
	m2=_mm256_maddubs_epi16(r2,c2);
	m3=_mm256_maddubs_epi16(r3,c3);
	m4=_mm256_maddubs_epi16(r4,c4);
	m5=_mm256_maddubs_epi16(r5,c5);
	m6=_mm256_maddubs_epi16(r6,c6);


	//make 16-bit values 32-bit (horizontal add)
	m0=_mm256_madd_epi16(m0,ones);
	m1=_mm256_madd_epi16(m1,ones);
	m2=_mm256_madd_epi16(m2,ones);
	m3=_mm256_madd_epi16(m3,ones);
	m4=_mm256_madd_epi16(m4,ones);
	m5=_mm256_madd_epi16(m5,ones);
	m6=_mm256_madd_epi16(m6,ones);


	//vertical add
	m0=_mm256_add_epi32(m0,m1);
	m0=_mm256_add_epi32(m0,m2);
	m0=_mm256_add_epi32(m0,m3);
	m0=_mm256_add_epi32(m0,m4);
	m0=_mm256_add_epi32(m0,m5);
	m0=_mm256_add_epi32(m0,m6);


	//last horizontal add
	m1=_mm256_slli_si256(m0,4);//shift by one integer
	m0=_mm256_add_epi32(m0,m1);//add
	m1=_mm256_srli_si256(m0,4);//shift by one integer
	output3=_mm256_and_si256(m1,mask_even_32);//keep only 0,2,4,6 and discard 1,3,5,7


	//4th col iteration
	//load the 5 rows
	r0=_mm256_loadu_si256( (__m256i *) &frame1[row-3][col-3+3]);
	r1=_mm256_loadu_si256( (__m256i *) &frame1[row-2][col-3+3]);
	r2=_mm256_loadu_si256( (__m256i *) &frame1[row-1][col-3+3]);
	r3=_mm256_loadu_si256( (__m256i *) &frame1[row][col-3+3]);
	r4=_mm256_loadu_si256( (__m256i *) &frame1[row+1][col-3+3]);
	r5=_mm256_loadu_si256( (__m256i *) &frame1[row+2][col-3+3]);
	r6=_mm256_loadu_si256( (__m256i *) &frame1[row+3][col-3+3]);

	reminder_mask1=_mm256_load_si256( (__m256i *) &reminder_msk_7x7[REMINDER_ITERATIONS-4][0]);

	//zero the values that exceed the array bounds
	r0=_mm256_and_si256(r0,reminder_mask1);
	r1=_mm256_and_si256(r1,reminder_mask1);
	r2=_mm256_and_si256(r2,reminder_mask1);
	r3=_mm256_and_si256(r3,reminder_mask1);
	r4=_mm256_and_si256(r4,reminder_mask1);
	r5=_mm256_and_si256(r5,reminder_mask1);
	r6=_mm256_and_si256(r6,reminder_mask1);

	//multiply by the mask
	m0=_mm256_maddubs_epi16(r0,c0);
	m1=_mm256_maddubs_epi16(r1,c1);
	m2=_mm256_maddubs_epi16(r2,c2);
	m3=_mm256_maddubs_epi16(r3,c3);
	m4=_mm256_maddubs_epi16(r4,c4);
	m5=_mm256_maddubs_epi16(r5,c5);
	m6=_mm256_maddubs_epi16(r6,c6);


	//make 16-bit values 32-bit (horizontal add)
	m0=_mm256_madd_epi16(m0,ones);
	m1=_mm256_madd_epi16(m1,ones);
	m2=_mm256_madd_epi16(m2,ones);
	m3=_mm256_madd_epi16(m3,ones);
	m4=_mm256_madd_epi16(m4,ones);
	m5=_mm256_madd_epi16(m5,ones);
	m6=_mm256_madd_epi16(m6,ones);


	//vertical add
	m0=_mm256_add_epi32(m0,m1);
	m0=_mm256_add_epi32(m0,m2);
	m0=_mm256_add_epi32(m0,m3);
	m0=_mm256_add_epi32(m0,m4);
	m0=_mm256_add_epi32(m0,m5);
	m0=_mm256_add_epi32(m0,m6);


	//last horizontal add
	m1=_mm256_slli_si256(m0,4);//shift by one integer
	m0=_mm256_add_epi32(m0,m1);//add
	m1=_mm256_srli_si256(m0,4);//shift by one integer
	output4=_mm256_and_si256(m1,mask_even_32);//keep only 0,2,4,6 and discard 1,3,5,7

if (REMINDER_ITERATIONS>=5){
	//5th col iteration

	//load the 5 rows
	r0=_mm256_loadu_si256( (__m256i *) &frame1[row-3][col-3+4]);
	r1=_mm256_loadu_si256( (__m256i *) &frame1[row-2][col-3+4]);
	r2=_mm256_loadu_si256( (__m256i *) &frame1[row-1][col-3+4]);
	r3=_mm256_loadu_si256( (__m256i *) &frame1[row][col-3+4]);
	r4=_mm256_loadu_si256( (__m256i *) &frame1[row+1][col-3+4]);
	r5=_mm256_loadu_si256( (__m256i *) &frame1[row+2][col-3+4]);
	r6=_mm256_loadu_si256( (__m256i *) &frame1[row+3][col-3+4]);

	reminder_mask1=_mm256_load_si256( (__m256i *) &reminder_msk_7x7[REMINDER_ITERATIONS-5][0]);

	//zero the values that exceed the array bounds
	r0=_mm256_and_si256(r0,reminder_mask1);
	r1=_mm256_and_si256(r1,reminder_mask1);
	r2=_mm256_and_si256(r2,reminder_mask1);
	r3=_mm256_and_si256(r3,reminder_mask1);
	r4=_mm256_and_si256(r4,reminder_mask1);
	r5=_mm256_and_si256(r5,reminder_mask1);
	r6=_mm256_and_si256(r6,reminder_mask1);

	//multiply by the mask
	m0=_mm256_maddubs_epi16(r0,c0);
	m1=_mm256_maddubs_epi16(r1,c1);
	m2=_mm256_maddubs_epi16(r2,c2);
	m3=_mm256_maddubs_epi16(r3,c3);
	m4=_mm256_maddubs_epi16(r4,c4);
	m5=_mm256_maddubs_epi16(r5,c5);
	m6=_mm256_maddubs_epi16(r6,c6);


	//make 16-bit values 32-bit (horizontal add)
	m0=_mm256_madd_epi16(m0,ones);
	m1=_mm256_madd_epi16(m1,ones);
	m2=_mm256_madd_epi16(m2,ones);
	m3=_mm256_madd_epi16(m3,ones);
	m4=_mm256_madd_epi16(m4,ones);
	m5=_mm256_madd_epi16(m5,ones);
	m6=_mm256_madd_epi16(m6,ones);


	//vertical add
	m0=_mm256_add_epi32(m0,m1);
	m0=_mm256_add_epi32(m0,m2);
	m0=_mm256_add_epi32(m0,m3);
	m0=_mm256_add_epi32(m0,m4);
	m0=_mm256_add_epi32(m0,m5);
	m0=_mm256_add_epi32(m0,m6);


	//last horizontal add
	m1=_mm256_slli_si256(m0,4);//shift by one integer
	m0=_mm256_add_epi32(m0,m1);//add
	m1=_mm256_srli_si256(m0,4);//shift by one integer
	m0=_mm256_and_si256(m1,mask_even_32);//keep only 0,2,4,6 and discard 1,3,5,7


	//MERGE output1 with output5 and DIVIDE
	m0=_mm256_slli_si256(m0,4);//shift by one integer
	output1=_mm256_add_epi32(m0,output1);
}
output1 = division_32(division_case,output1,f);

if (REMINDER_ITERATIONS>=6){
	//6th col iteration
	//load the 5 rows
	r0=_mm256_loadu_si256( (__m256i *) &frame1[row-3][col-3+5]);
	r1=_mm256_loadu_si256( (__m256i *) &frame1[row-2][col-3+5]);
	r2=_mm256_loadu_si256( (__m256i *) &frame1[row-1][col-3+5]);
	r3=_mm256_loadu_si256( (__m256i *) &frame1[row][col-3+5]);
	r4=_mm256_loadu_si256( (__m256i *) &frame1[row+1][col-3+5]);
	r5=_mm256_loadu_si256( (__m256i *) &frame1[row+2][col-3+5]);
	r6=_mm256_loadu_si256( (__m256i *) &frame1[row+3][col-3+5]);

	reminder_mask1=_mm256_load_si256( (__m256i *) &reminder_msk_7x7[REMINDER_ITERATIONS-6][0]);

	//zero the values that exceed the array bounds
	r0=_mm256_and_si256(r0,reminder_mask1);
	r1=_mm256_and_si256(r1,reminder_mask1);
	r2=_mm256_and_si256(r2,reminder_mask1);
	r3=_mm256_and_si256(r3,reminder_mask1);
	r4=_mm256_and_si256(r4,reminder_mask1);
	r5=_mm256_and_si256(r5,reminder_mask1);
	r6=_mm256_and_si256(r6,reminder_mask1);

	//multiply by the mask
	m0=_mm256_maddubs_epi16(r0,c0);
	m1=_mm256_maddubs_epi16(r1,c1);
	m2=_mm256_maddubs_epi16(r2,c2);
	m3=_mm256_maddubs_epi16(r3,c3);
	m4=_mm256_maddubs_epi16(r4,c4);
	m5=_mm256_maddubs_epi16(r5,c5);
	m6=_mm256_maddubs_epi16(r6,c6);


	//make 16-bit values 32-bit (horizontal add)
	m0=_mm256_madd_epi16(m0,ones);
	m1=_mm256_madd_epi16(m1,ones);
	m2=_mm256_madd_epi16(m2,ones);
	m3=_mm256_madd_epi16(m3,ones);
	m4=_mm256_madd_epi16(m4,ones);
	m5=_mm256_madd_epi16(m5,ones);
	m6=_mm256_madd_epi16(m6,ones);


	//vertical add
	m0=_mm256_add_epi32(m0,m1);
	m0=_mm256_add_epi32(m0,m2);
	m0=_mm256_add_epi32(m0,m3);
	m0=_mm256_add_epi32(m0,m4);
	m0=_mm256_add_epi32(m0,m5);
	m0=_mm256_add_epi32(m0,m6);


	//last horizontal add
	m1=_mm256_slli_si256(m0,4);//shift by one integer
	m0=_mm256_add_epi32(m0,m1);//add
	m1=_mm256_srli_si256(m0,4);//shift by one integer
	m0=_mm256_and_si256(m1,mask_even_32);//keep only 0,2,4,6 and discard 1,3,5,7

	//MERGE output1 with output5 and DIVIDE
	m0=_mm256_slli_si256(m0,4);//shift by one integer
	output2=_mm256_add_epi32(m0,output2);
}
output2 = division_32(division_case,output2,f);


if (REMINDER_ITERATIONS>=7){
	//7th col iteration
	//load the 5 rows
	r0=_mm256_loadu_si256( (__m256i *) &frame1[row-3][col-3+6]);
	r1=_mm256_loadu_si256( (__m256i *) &frame1[row-2][col-3+6]);
	r2=_mm256_loadu_si256( (__m256i *) &frame1[row-1][col-3+6]);
	r3=_mm256_loadu_si256( (__m256i *) &frame1[row][col-3+6]);
	r4=_mm256_loadu_si256( (__m256i *) &frame1[row+1][col-3+6]);
	r5=_mm256_loadu_si256( (__m256i *) &frame1[row+2][col-3+6]);
	r6=_mm256_loadu_si256( (__m256i *) &frame1[row+3][col-3+6]);

	reminder_mask1=_mm256_load_si256( (__m256i *) &reminder_msk_7x7[REMINDER_ITERATIONS-7][0]);

	//zero the values that exceed the array bounds
	r0=_mm256_and_si256(r0,reminder_mask1);
	r1=_mm256_and_si256(r1,reminder_mask1);
	r2=_mm256_and_si256(r2,reminder_mask1);
	r3=_mm256_and_si256(r3,reminder_mask1);
	r4=_mm256_and_si256(r4,reminder_mask1);
	r5=_mm256_and_si256(r5,reminder_mask1);
	r6=_mm256_and_si256(r6,reminder_mask1);

	//multiply by the mask
	m0=_mm256_maddubs_epi16(r0,c0);
	m1=_mm256_maddubs_epi16(r1,c1);
	m2=_mm256_maddubs_epi16(r2,c2);
	m3=_mm256_maddubs_epi16(r3,c3);
	m4=_mm256_maddubs_epi16(r4,c4);
	m5=_mm256_maddubs_epi16(r5,c5);
	m6=_mm256_maddubs_epi16(r6,c6);


	//make 16-bit values 32-bit (horizontal add)
	m0=_mm256_madd_epi16(m0,ones);
	m1=_mm256_madd_epi16(m1,ones);
	m2=_mm256_madd_epi16(m2,ones);
	m3=_mm256_madd_epi16(m3,ones);
	m4=_mm256_madd_epi16(m4,ones);
	m5=_mm256_madd_epi16(m5,ones);
	m6=_mm256_madd_epi16(m6,ones);


	//vertical add
	m0=_mm256_add_epi32(m0,m1);
	m0=_mm256_add_epi32(m0,m2);
	m0=_mm256_add_epi32(m0,m3);
	m0=_mm256_add_epi32(m0,m4);
	m0=_mm256_add_epi32(m0,m5);
	m0=_mm256_add_epi32(m0,m6);


	//last horizontal add
	m1=_mm256_slli_si256(m0,4);//shift by one integer
	m0=_mm256_add_epi32(m0,m1);//add
	m1=_mm256_srli_si256(m0,4);//shift by one integer
	m0=_mm256_and_si256(m1,mask_even_32);//keep only 0,2,4,6 and discard 1,3,5,7

	//MERGE output1 with output5 and DIVIDE
	m0=_mm256_slli_si256(m0,4);//shift by one integer
	output3=_mm256_add_epi32(m0,output3);
}

output3 = division_32(division_case,output3,f);


if (REMINDER_ITERATIONS>=8){
	//8th col iteration

	//load the 5 rows
	r0=_mm256_loadu_si256( (__m256i *) &frame1[row-3][col-3+7]);
	r1=_mm256_loadu_si256( (__m256i *) &frame1[row-2][col-3+7]);
	r2=_mm256_loadu_si256( (__m256i *) &frame1[row-1][col-3+7]);
	r3=_mm256_loadu_si256( (__m256i *) &frame1[row][col-3+7]);
	r4=_mm256_loadu_si256( (__m256i *) &frame1[row+1][col-3+7]);
	r5=_mm256_loadu_si256( (__m256i *) &frame1[row+2][col-3+7]);
	r6=_mm256_loadu_si256( (__m256i *) &frame1[row+3][col-3+7]);

	reminder_mask1=_mm256_load_si256( (__m256i *) &reminder_msk_7x7[REMINDER_ITERATIONS-8][0]);

	//zero the values that exceed the array bounds
	r0=_mm256_and_si256(r0,reminder_mask1);
	r1=_mm256_and_si256(r1,reminder_mask1);
	r2=_mm256_and_si256(r2,reminder_mask1);
	r3=_mm256_and_si256(r3,reminder_mask1);
	r4=_mm256_and_si256(r4,reminder_mask1);
	r5=_mm256_and_si256(r5,reminder_mask1);
	r6=_mm256_and_si256(r6,reminder_mask1);

	//multiply by the mask
	m0=_mm256_maddubs_epi16(r0,c0);
	m1=_mm256_maddubs_epi16(r1,c1);
	m2=_mm256_maddubs_epi16(r2,c2);
	m3=_mm256_maddubs_epi16(r3,c3);
	m4=_mm256_maddubs_epi16(r4,c4);
	m5=_mm256_maddubs_epi16(r5,c5);
	m6=_mm256_maddubs_epi16(r6,c6);


	//make 16-bit values 32-bit (horizontal add)
	m0=_mm256_madd_epi16(m0,ones);
	m1=_mm256_madd_epi16(m1,ones);
	m2=_mm256_madd_epi16(m2,ones);
	m3=_mm256_madd_epi16(m3,ones);
	m4=_mm256_madd_epi16(m4,ones);
	m5=_mm256_madd_epi16(m5,ones);
	m6=_mm256_madd_epi16(m6,ones);


	//vertical add
	m0=_mm256_add_epi32(m0,m1);
	m0=_mm256_add_epi32(m0,m2);
	m0=_mm256_add_epi32(m0,m3);
	m0=_mm256_add_epi32(m0,m4);
	m0=_mm256_add_epi32(m0,m5);
	m0=_mm256_add_epi32(m0,m6);


	//last horizontal add
	m1=_mm256_slli_si256(m0,4);//shift by one integer
	m0=_mm256_add_epi32(m0,m1);//add
	m1=_mm256_srli_si256(m0,4);//shift by one integer
	m0=_mm256_and_si256(m1,mask_even_32);//keep only 0,2,4,6 and discard 1,3,5,7


	//MERGE output1 with output5 and DIVIDE
	m0=_mm256_slli_si256(m0,4);//shift by one integer
	output4=_mm256_add_epi32(m0,output4);
}
output4 = division_32(division_case,output4,f);

	//blend output1, output2, output3, output4 into one register
	output2 = _mm256_slli_si256(output2,1);
	output1 = _mm256_add_epi8(output1,output2);
	output3 = _mm256_slli_si256(output3,2);
	output1 = _mm256_add_epi8(output1,output3);
	output4 = _mm256_slli_si256(output4,3);
	output1 = _mm256_add_epi8(output1,output4);

	//_mm256_stream_si256( (__m256i *) &filt[row][col],output1 );
	//_mm256_store_si256( (__m256i *) &filt[row][col],output1 );

	switch (REMINDER_ITERATIONS){

	case 4:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output1,3);
		  break;
	case 5:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output1,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output1,4);
		  break;
	case 6:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output1,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output1,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output1,5);
		  break;
	case 7:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output1,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output1,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output1,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output1,6);
		  break;
	case 8:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output1,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output1,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output1,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output1,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output1,7);
		  break;
	case 9:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output1,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output1,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output1,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output1,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output1,7);
		filt[row][col+8] = (unsigned char) _mm256_extract_epi8(output1,8);
		  break;
	case 10:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output1,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output1,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output1,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output1,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output1,7);
		filt[row][col+8] = (unsigned char) _mm256_extract_epi8(output1,8);
		filt[row][col+9] = (unsigned char) _mm256_extract_epi8(output1,9);
		  break;
	case 11:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output1,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output1,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output1,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output1,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output1,7);
		filt[row][col+8] = (unsigned char) _mm256_extract_epi8(output1,8);
		filt[row][col+9] = (unsigned char) _mm256_extract_epi8(output1,9);
		filt[row][col+10] = (unsigned char) _mm256_extract_epi8(output1,10);
		  break;
	case 12:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output1,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output1,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output1,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output1,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output1,7);
		filt[row][col+8] = (unsigned char) _mm256_extract_epi8(output1,8);
		filt[row][col+9] = (unsigned char) _mm256_extract_epi8(output1,9);
		filt[row][col+10] = (unsigned char) _mm256_extract_epi8(output1,10);
		filt[row][col+11] = (unsigned char) _mm256_extract_epi8(output1,11);
		  break;
	case 13:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output1,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output1,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output1,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output1,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output1,7);
		filt[row][col+8] = (unsigned char) _mm256_extract_epi8(output1,8);
		filt[row][col+9] = (unsigned char) _mm256_extract_epi8(output1,9);
		filt[row][col+10] = (unsigned char) _mm256_extract_epi8(output1,10);
		filt[row][col+11] = (unsigned char) _mm256_extract_epi8(output1,11);
		filt[row][col+12] = (unsigned char) _mm256_extract_epi8(output1,12);
		  break;
	case 14:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output1,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output1,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output1,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output1,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output1,7);
		filt[row][col+8] = (unsigned char) _mm256_extract_epi8(output1,8);
		filt[row][col+9] = (unsigned char) _mm256_extract_epi8(output1,9);
		filt[row][col+10] = (unsigned char) _mm256_extract_epi8(output1,10);
		filt[row][col+11] = (unsigned char) _mm256_extract_epi8(output1,11);
		filt[row][col+12] = (unsigned char) _mm256_extract_epi8(output1,12);
		filt[row][col+13] = (unsigned char) _mm256_extract_epi8(output1,13);
		  break;
	case 15:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output1,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output1,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output1,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output1,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output1,7);
		filt[row][col+8] = (unsigned char) _mm256_extract_epi8(output1,8);
		filt[row][col+9] = (unsigned char) _mm256_extract_epi8(output1,9);
		filt[row][col+10] = (unsigned char) _mm256_extract_epi8(output1,10);
		filt[row][col+11] = (unsigned char) _mm256_extract_epi8(output1,11);
		filt[row][col+12] = (unsigned char) _mm256_extract_epi8(output1,12);
		filt[row][col+13] = (unsigned char) _mm256_extract_epi8(output1,13);
		filt[row][col+14] = (unsigned char) _mm256_extract_epi8(output1,14);
		  break;
	case 16:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
		  break;
	case 17:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
		  break;
	case 18:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
		  break;
	case 19:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
		  break;
	case 20:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output1,19);
		  break;
	case 21:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output1,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output1,20);
		  break;
	case 22:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output1,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output1,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output1,21);
		break;
	case 23:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output1,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output1,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output1,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output1,22);
		  break;
	case 24:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output1,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output1,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output1,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output1,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output1,23);
		  break;
	case 25:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output1,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output1,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output1,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output1,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output1,23);
	filt[row][col+24] = (unsigned char) _mm256_extract_epi8(output1,24);
		  break;
	case 26:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output1,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output1,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output1,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output1,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output1,23);
	filt[row][col+24] = (unsigned char) _mm256_extract_epi8(output1,24);
	filt[row][col+25] = (unsigned char) _mm256_extract_epi8(output1,25);
		  break;
	case 27:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output1,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output1,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output1,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output1,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output1,23);
	filt[row][col+24] = (unsigned char) _mm256_extract_epi8(output1,24);
	filt[row][col+25] = (unsigned char) _mm256_extract_epi8(output1,25);
	filt[row][col+26] = (unsigned char) _mm256_extract_epi8(output1,26);
		  break;
	case 28:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output1,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output1,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output1,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output1,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output1,23);
	filt[row][col+24] = (unsigned char) _mm256_extract_epi8(output1,24);
	filt[row][col+25] = (unsigned char) _mm256_extract_epi8(output1,25);
	filt[row][col+26] = (unsigned char) _mm256_extract_epi8(output1,26);
	filt[row][col+27] = (unsigned char) _mm256_extract_epi8(output1,27);
		  break;
	case 29:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output1,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output1,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output1,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output1,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output1,23);
	filt[row][col+24] = (unsigned char) _mm256_extract_epi8(output1,24);
	filt[row][col+25] = (unsigned char) _mm256_extract_epi8(output1,25);
	filt[row][col+26] = (unsigned char) _mm256_extract_epi8(output1,26);
	filt[row][col+27] = (unsigned char) _mm256_extract_epi8(output1,27);
	filt[row][col+28] = (unsigned char) _mm256_extract_epi8(output1,28);
		  break;
	case 30:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output1,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output1,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output1,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output1,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output1,23);
	filt[row][col+24] = (unsigned char) _mm256_extract_epi8(output1,24);
	filt[row][col+25] = (unsigned char) _mm256_extract_epi8(output1,25);
	filt[row][col+26] = (unsigned char) _mm256_extract_epi8(output1,26);
	filt[row][col+27] = (unsigned char) _mm256_extract_epi8(output1,27);
	filt[row][col+28] = (unsigned char) _mm256_extract_epi8(output1,28);
	filt[row][col+29] = (unsigned char) _mm256_extract_epi8(output1,29);
		  break;
	case 31:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output1,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output1,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output1,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output1,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output1,23);
	filt[row][col+24] = (unsigned char) _mm256_extract_epi8(output1,24);
	filt[row][col+25] = (unsigned char) _mm256_extract_epi8(output1,25);
	filt[row][col+26] = (unsigned char) _mm256_extract_epi8(output1,26);
	filt[row][col+27] = (unsigned char) _mm256_extract_epi8(output1,27);
	filt[row][col+28] = (unsigned char) _mm256_extract_epi8(output1,28);
	filt[row][col+29] = (unsigned char) _mm256_extract_epi8(output1,29);
	filt[row][col+30] = (unsigned char) _mm256_extract_epi8(output1,30);
		  break;
	case 32:
		_mm256_store_si256( (__m256i *) &filt[row][col],output1 );
		  break;
	case 33:
		_mm256_store_si256( (__m256i *) &filt[row][col],output1 );

	    int newPixel = 0;

        newPixel += frame1[row-3][col-3+32] * filter7x7[0][0];
        newPixel += frame1[row-3][col-2+32] * filter7x7[0][1];
        newPixel += frame1[row-3][col-1+32] * filter7x7[0][2];
        newPixel += frame1[row-3][col-0+32] * filter7x7[0][3];

        newPixel += frame1[row-2][col-3+32] * filter7x7[1][0];
        newPixel += frame1[row-2][col-2+32] * filter7x7[1][1];
        newPixel += frame1[row-2][col-1+32] * filter7x7[1][2];
        newPixel += frame1[row-2][col-0+32] * filter7x7[1][3];

        newPixel += frame1[row-1][col-3+32] * filter7x7[2][0];
        newPixel += frame1[row-1][col-2+32] * filter7x7[2][1];
        newPixel += frame1[row-1][col-1+32] * filter7x7[2][2];
        newPixel += frame1[row-1][col-0+32] * filter7x7[2][3];

        newPixel += frame1[row-0][col-3+32] * filter7x7[3][0];
        newPixel += frame1[row-0][col-2+32] * filter7x7[3][1];
        newPixel += frame1[row-0][col-1+32] * filter7x7[3][2];
        newPixel += frame1[row-0][col-0+32] * filter7x7[3][3];

        newPixel += frame1[row+1][col-3+32] * filter7x7[4][0];
        newPixel += frame1[row+1][col-2+32] * filter7x7[4][1];
        newPixel += frame1[row+1][col-1+32] * filter7x7[4][2];
        newPixel += frame1[row+1][col-0+32] * filter7x7[4][3];

        newPixel += frame1[row+2][col-3+32] * filter7x7[5][0];
        newPixel += frame1[row+2][col-2+32] * filter7x7[5][1];
        newPixel += frame1[row+2][col-1+32] * filter7x7[5][2];
        newPixel += frame1[row+2][col-0+32] * filter7x7[5][3];

        newPixel += frame1[row+3][col-3+32] * filter7x7[6][0];
        newPixel += frame1[row+3][col-2+32] * filter7x7[6][1];
        newPixel += frame1[row+3][col-1+32] * filter7x7[6][2];
        newPixel += frame1[row+3][col-0+32] * filter7x7[6][3];

		filt[row][col+32] = (unsigned char) (newPixel / divisor);
		  break;
	case 34:
		_mm256_store_si256( (__m256i *) &filt[row][col],output1 );

	    newPixel = 0;

        newPixel += frame1[row-3][col-3+32] * filter7x7[0][0];
        newPixel += frame1[row-3][col-2+32] * filter7x7[0][1];
        newPixel += frame1[row-3][col-1+32] * filter7x7[0][2];
        newPixel += frame1[row-3][col-0+32] * filter7x7[0][3];
        newPixel += frame1[row-3][col+1+32] * filter7x7[0][4];

        newPixel += frame1[row-2][col-3+32] * filter7x7[1][0];
        newPixel += frame1[row-2][col-2+32] * filter7x7[1][1];
        newPixel += frame1[row-2][col-1+32] * filter7x7[1][2];
        newPixel += frame1[row-2][col-0+32] * filter7x7[1][3];
        newPixel += frame1[row-2][col+1+32] * filter7x7[1][4];

        newPixel += frame1[row-1][col-3+32] * filter7x7[2][0];
        newPixel += frame1[row-1][col-2+32] * filter7x7[2][1];
        newPixel += frame1[row-1][col-1+32] * filter7x7[2][2];
        newPixel += frame1[row-1][col-0+32] * filter7x7[2][3];
        newPixel += frame1[row-1][col+1+32] * filter7x7[2][4];

        newPixel += frame1[row-0][col-3+32] * filter7x7[3][0];
        newPixel += frame1[row-0][col-2+32] * filter7x7[3][1];
        newPixel += frame1[row-0][col-1+32] * filter7x7[3][2];
        newPixel += frame1[row-0][col-0+32] * filter7x7[3][3];
        newPixel += frame1[row-0][col+1+32] * filter7x7[3][4];

        newPixel += frame1[row+1][col-3+32] * filter7x7[4][0];
        newPixel += frame1[row+1][col-2+32] * filter7x7[4][1];
        newPixel += frame1[row+1][col-1+32] * filter7x7[4][2];
        newPixel += frame1[row+1][col-0+32] * filter7x7[4][3];
        newPixel += frame1[row+1][col+1+32] * filter7x7[4][4];

        newPixel += frame1[row+2][col-3+32] * filter7x7[5][0];
        newPixel += frame1[row+2][col-2+32] * filter7x7[5][1];
        newPixel += frame1[row+2][col-1+32] * filter7x7[5][2];
        newPixel += frame1[row+2][col-0+32] * filter7x7[5][3];
        newPixel += frame1[row+2][col+1+32] * filter7x7[5][4];

        newPixel += frame1[row+3][col-3+32] * filter7x7[6][0];
        newPixel += frame1[row+3][col-2+32] * filter7x7[6][1];
        newPixel += frame1[row+3][col-1+32] * filter7x7[6][2];
        newPixel += frame1[row+3][col-0+32] * filter7x7[6][3];
        newPixel += frame1[row+3][col+1+32] * filter7x7[6][4];

		filt[row][col+32] = (unsigned char) (newPixel / divisor);

	    newPixel = 0;

        newPixel += frame1[row-3][col-3+32+1] * filter7x7[0][0];
        newPixel += frame1[row-3][col-2+32+1] * filter7x7[0][1];
        newPixel += frame1[row-3][col-1+32+1] * filter7x7[0][2];
        newPixel += frame1[row-3][col-0+32+1] * filter7x7[0][3];

        newPixel += frame1[row-2][col-3+32+1] * filter7x7[1][0];
        newPixel += frame1[row-2][col-2+32+1] * filter7x7[1][1];
        newPixel += frame1[row-2][col-1+32+1] * filter7x7[1][2];
        newPixel += frame1[row-2][col-0+32+1] * filter7x7[1][3];

        newPixel += frame1[row-1][col-3+32+1] * filter7x7[2][0];
        newPixel += frame1[row-1][col-2+32+1] * filter7x7[2][1];
        newPixel += frame1[row-1][col-1+32+1] * filter7x7[2][2];
        newPixel += frame1[row-1][col-0+32+1] * filter7x7[2][3];

        newPixel += frame1[row-0][col-3+32+1] * filter7x7[3][0];
        newPixel += frame1[row-0][col-2+32+1] * filter7x7[3][1];
        newPixel += frame1[row-0][col-1+32+1] * filter7x7[3][2];
        newPixel += frame1[row-0][col-0+32+1] * filter7x7[3][3];

        newPixel += frame1[row+1][col-3+32+1] * filter7x7[4][0];
        newPixel += frame1[row+1][col-2+32+1] * filter7x7[4][1];
        newPixel += frame1[row+1][col-1+32+1] * filter7x7[4][2];
        newPixel += frame1[row+1][col-0+32+1] * filter7x7[4][3];

        newPixel += frame1[row+2][col-3+32+1] * filter7x7[5][0];
        newPixel += frame1[row+2][col-2+32+1] * filter7x7[5][1];
        newPixel += frame1[row+2][col-1+32+1] * filter7x7[5][2];
        newPixel += frame1[row+2][col-0+32+1] * filter7x7[5][3];

        newPixel += frame1[row+3][col-3+32+1] * filter7x7[6][0];
        newPixel += frame1[row+3][col-2+32+1] * filter7x7[6][1];
        newPixel += frame1[row+3][col-1+32+1] * filter7x7[6][2];
        newPixel += frame1[row+3][col-0+32+1] * filter7x7[6][3];

		filt[row][col+33] = (unsigned char) (newPixel / divisor);

		  break;
	case 35:
		_mm256_store_si256( (__m256i *) &filt[row][col],output1 );

	    newPixel = 0;

        newPixel += frame1[row-3][col-3+32] * filter7x7[0][0];
        newPixel += frame1[row-3][col-2+32] * filter7x7[0][1];
        newPixel += frame1[row-3][col-1+32] * filter7x7[0][2];
        newPixel += frame1[row-3][col-0+32] * filter7x7[0][3];
        newPixel += frame1[row-3][col+1+32] * filter7x7[0][4];
        newPixel += frame1[row-3][col+2+32] * filter7x7[0][5];

        newPixel += frame1[row-2][col-3+32] * filter7x7[1][0];
        newPixel += frame1[row-2][col-2+32] * filter7x7[1][1];
        newPixel += frame1[row-2][col-1+32] * filter7x7[1][2];
        newPixel += frame1[row-2][col-0+32] * filter7x7[1][3];
        newPixel += frame1[row-2][col+1+32] * filter7x7[1][4];
        newPixel += frame1[row-2][col+2+32] * filter7x7[1][5];

        newPixel += frame1[row-1][col-3+32] * filter7x7[2][0];
        newPixel += frame1[row-1][col-2+32] * filter7x7[2][1];
        newPixel += frame1[row-1][col-1+32] * filter7x7[2][2];
        newPixel += frame1[row-1][col-0+32] * filter7x7[2][3];
        newPixel += frame1[row-1][col+1+32] * filter7x7[2][4];
        newPixel += frame1[row-1][col+2+32] * filter7x7[2][5];

        newPixel += frame1[row-0][col-3+32] * filter7x7[3][0];
        newPixel += frame1[row-0][col-2+32] * filter7x7[3][1];
        newPixel += frame1[row-0][col-1+32] * filter7x7[3][2];
        newPixel += frame1[row-0][col-0+32] * filter7x7[3][3];
        newPixel += frame1[row-0][col+1+32] * filter7x7[3][4];
        newPixel += frame1[row-0][col+2+32] * filter7x7[3][5];

        newPixel += frame1[row+1][col-3+32] * filter7x7[4][0];
        newPixel += frame1[row+1][col-2+32] * filter7x7[4][1];
        newPixel += frame1[row+1][col-1+32] * filter7x7[4][2];
        newPixel += frame1[row+1][col-0+32] * filter7x7[4][3];
        newPixel += frame1[row+1][col+1+32] * filter7x7[4][4];
        newPixel += frame1[row+1][col+2+32] * filter7x7[4][5];

        newPixel += frame1[row+2][col-3+32] * filter7x7[5][0];
        newPixel += frame1[row+2][col-2+32] * filter7x7[5][1];
        newPixel += frame1[row+2][col-1+32] * filter7x7[5][2];
        newPixel += frame1[row+2][col-0+32] * filter7x7[5][3];
        newPixel += frame1[row+2][col+1+32] * filter7x7[5][4];
        newPixel += frame1[row+2][col+2+32] * filter7x7[5][5];

        newPixel += frame1[row+3][col-3+32] * filter7x7[6][0];
        newPixel += frame1[row+3][col-2+32] * filter7x7[6][1];
        newPixel += frame1[row+3][col-1+32] * filter7x7[6][2];
        newPixel += frame1[row+3][col-0+32] * filter7x7[6][3];
        newPixel += frame1[row+3][col+1+32] * filter7x7[6][4];
        newPixel += frame1[row+3][col+2+32] * filter7x7[6][5];

		filt[row][col+32] = (unsigned char) (newPixel / divisor);

	    newPixel = 0;

        newPixel += frame1[row-3][col-3+32+1] * filter7x7[0][0];
        newPixel += frame1[row-3][col-2+32+1] * filter7x7[0][1];
        newPixel += frame1[row-3][col-1+32+1] * filter7x7[0][2];
        newPixel += frame1[row-3][col-0+32+1] * filter7x7[0][3];
        newPixel += frame1[row-3][col+1+32+1] * filter7x7[0][4];

        newPixel += frame1[row-2][col-3+32+1] * filter7x7[1][0];
        newPixel += frame1[row-2][col-2+32+1] * filter7x7[1][1];
        newPixel += frame1[row-2][col-1+32+1] * filter7x7[1][2];
        newPixel += frame1[row-2][col-0+32+1] * filter7x7[1][3];
        newPixel += frame1[row-2][col+1+32+1] * filter7x7[1][4];

        newPixel += frame1[row-1][col-3+32+1] * filter7x7[2][0];
        newPixel += frame1[row-1][col-2+32+1] * filter7x7[2][1];
        newPixel += frame1[row-1][col-1+32+1] * filter7x7[2][2];
        newPixel += frame1[row-1][col-0+32+1] * filter7x7[2][3];
        newPixel += frame1[row-1][col+1+32+1] * filter7x7[2][4];

        newPixel += frame1[row-0][col-3+32+1] * filter7x7[3][0];
        newPixel += frame1[row-0][col-2+32+1] * filter7x7[3][1];
        newPixel += frame1[row-0][col-1+32+1] * filter7x7[3][2];
        newPixel += frame1[row-0][col-0+32+1] * filter7x7[3][3];
        newPixel += frame1[row-0][col+1+32+1] * filter7x7[3][4];

        newPixel += frame1[row+1][col-3+32+1] * filter7x7[4][0];
        newPixel += frame1[row+1][col-2+32+1] * filter7x7[4][1];
        newPixel += frame1[row+1][col-1+32+1] * filter7x7[4][2];
        newPixel += frame1[row+1][col-0+32+1] * filter7x7[4][3];
        newPixel += frame1[row+1][col+1+32+1] * filter7x7[4][4];

        newPixel += frame1[row+2][col-3+32+1] * filter7x7[5][0];
        newPixel += frame1[row+2][col-2+32+1] * filter7x7[5][1];
        newPixel += frame1[row+2][col-1+32+1] * filter7x7[5][2];
        newPixel += frame1[row+2][col-0+32+1] * filter7x7[5][3];
        newPixel += frame1[row+2][col+1+32+1] * filter7x7[5][4];

        newPixel += frame1[row+3][col-3+32+1] * filter7x7[6][0];
        newPixel += frame1[row+3][col-2+32+1] * filter7x7[6][1];
        newPixel += frame1[row+3][col-1+32+1] * filter7x7[6][2];
        newPixel += frame1[row+3][col-0+32+1] * filter7x7[6][3];
        newPixel += frame1[row+3][col+1+32+1] * filter7x7[6][4];

		filt[row][col+33] = (unsigned char) (newPixel / divisor);

	    newPixel = 0;

        newPixel += frame1[row-3][col-3+32+2] * filter7x7[0][0];
        newPixel += frame1[row-3][col-2+32+2] * filter7x7[0][1];
        newPixel += frame1[row-3][col-1+32+2] * filter7x7[0][2];
        newPixel += frame1[row-3][col+0+32+2] * filter7x7[0][3];

        newPixel += frame1[row-2][col-3+32+2] * filter7x7[1][0];
        newPixel += frame1[row-2][col-2+32+2] * filter7x7[1][1];
        newPixel += frame1[row-2][col-1+32+2] * filter7x7[1][2];
        newPixel += frame1[row-2][col+0+32+2] * filter7x7[1][3];

        newPixel += frame1[row-1][col-3+32+2] * filter7x7[2][0];
        newPixel += frame1[row-1][col-2+32+2] * filter7x7[2][1];
        newPixel += frame1[row-1][col-1+32+2] * filter7x7[2][2];
        newPixel += frame1[row-1][col+0+32+2] * filter7x7[2][3];

        newPixel += frame1[row-0][col-3+32+2] * filter7x7[3][0];
        newPixel += frame1[row-0][col-2+32+2] * filter7x7[3][1];
        newPixel += frame1[row-0][col-1+32+2] * filter7x7[3][2];
        newPixel += frame1[row-0][col+0+32+2] * filter7x7[3][3];

        newPixel += frame1[row+1][col-3+32+2] * filter7x7[4][0];
        newPixel += frame1[row+1][col-2+32+2] * filter7x7[4][1];
        newPixel += frame1[row+1][col-1+32+2] * filter7x7[4][2];
        newPixel += frame1[row+1][col+0+32+2] * filter7x7[4][3];

        newPixel += frame1[row+2][col-3+32+2] * filter7x7[5][0];
        newPixel += frame1[row+2][col-2+32+2] * filter7x7[5][1];
        newPixel += frame1[row+2][col-1+32+2] * filter7x7[5][2];
        newPixel += frame1[row+2][col+0+32+2] * filter7x7[5][3];

        newPixel += frame1[row+3][col-3+32+2] * filter7x7[6][0];
        newPixel += frame1[row+3][col-2+32+2] * filter7x7[6][1];
        newPixel += frame1[row+3][col-1+32+2] * filter7x7[6][2];
        newPixel += frame1[row+3][col+0+32+2] * filter7x7[6][3];

		filt[row][col+34] = (unsigned char) (newPixel / divisor);
		  break;
	default:
		printf("\nsomething went wrong");
		exit(EXIT_FAILURE);
 	}




}



void loop_reminder_7x7_row_boundaries(unsigned char **frame1,unsigned char **filt,const unsigned int N,const unsigned int row, const unsigned int col,const unsigned int REMINDER_ITERATIONS,const unsigned int division_case,const signed char mask_vector[][32], const __m256i f,const unsigned short int divisor,signed char **filter7x7){

	__m256i c0=_mm256_load_si256( (__m256i *) &mask_vector[0][0]);
	__m256i c1=_mm256_load_si256( (__m256i *) &mask_vector[1][0]);
	__m256i c2=_mm256_load_si256( (__m256i *) &mask_vector[2][0]);
	__m256i c3=_mm256_load_si256( (__m256i *) &mask_vector[3][0]);
	__m256i c4=_mm256_load_si256( (__m256i *) &mask_vector[4][0]);
	__m256i c5=_mm256_load_si256( (__m256i *) &mask_vector[5][0]);
	__m256i c6=_mm256_load_si256( (__m256i *) &mask_vector[6][0]);

	const __m256i mask_even_32  = _mm256_set_epi32(0,0xffffffff,0,0xffffffff,0,0xffffffff,0,0xffffffff);

	__m256i r0,r1,r2,r3,r4,r5,r6,m0,m1,m2,m3,m4,m5,m6;
	__m256i output1,output2,output3,output4,reminder_mask1;
	__m256i ones=_mm256_set1_epi16(1);




//important: REMINDER_ITERATIONS_GLOBAL ranges from [4,35]. If >32, two iterations are needed, one for computing the first 32 output pixels and another for computing the rest


if (row==0){
	r0=_mm256_loadu_si256( (__m256i *) &frame1[0][col-3]);
	r1=_mm256_loadu_si256( (__m256i *) &frame1[1][col-3]);
	r2=_mm256_loadu_si256( (__m256i *) &frame1[2][col-3]);
	r3=_mm256_loadu_si256( (__m256i *) &frame1[3][col-3]);
    r4=_mm256_setzero_si256();
    r5=_mm256_setzero_si256();
    r6=_mm256_setzero_si256();
    c0=c3; c1=c4; c2=c5;c3=c6;
}
else if (row==1){
	r0=_mm256_loadu_si256( (__m256i *) &frame1[0][col-3]);
	r1=_mm256_loadu_si256( (__m256i *) &frame1[1][col-3]);
	r2=_mm256_loadu_si256( (__m256i *) &frame1[2][col-3]);
	r3=_mm256_loadu_si256( (__m256i *) &frame1[3][col-3]);
	r4=_mm256_loadu_si256( (__m256i *) &frame1[4][col-3]);
    r5=_mm256_setzero_si256();
    r6=_mm256_setzero_si256();
    c0=c2; c1=c3; c2=c4;c3=c5; c4=c6;
}
else if (row==2){
	r0=_mm256_loadu_si256( (__m256i *) &frame1[0][col-3]);
	r1=_mm256_loadu_si256( (__m256i *) &frame1[1][col-3]);
	r2=_mm256_loadu_si256( (__m256i *) &frame1[2][col-3]);
	r3=_mm256_loadu_si256( (__m256i *) &frame1[3][col-3]);
	r4=_mm256_loadu_si256( (__m256i *) &frame1[4][col-3]);
	r5=_mm256_loadu_si256( (__m256i *) &frame1[5][col-3]);
    r6=_mm256_setzero_si256();
    c0=c1; c1=c2; c2=c3;c3=c4; c4=c5; c5=c6;
}
else if (row==N-3){
    r0=_mm256_setzero_si256();
    r1=_mm256_loadu_si256( (__m256i *) &frame1[N-6][col-3]);
	r2=_mm256_loadu_si256( (__m256i *) &frame1[N-5][col-3]);
	r3=_mm256_loadu_si256( (__m256i *) &frame1[N-4][col-3]);
	r4=_mm256_loadu_si256( (__m256i *) &frame1[N-3][col-3]);
	r5=_mm256_loadu_si256( (__m256i *) &frame1[N-2][col-3]);
	r6=_mm256_loadu_si256( (__m256i *) &frame1[N-1][col-3]);
	c6=c5; c5=c4; c4=c3; c3=c2; c2=c1; c1=c0;
}
else if (row==N-2){
    r0=_mm256_setzero_si256();
    r1=_mm256_setzero_si256();
	r2=_mm256_loadu_si256( (__m256i *) &frame1[N-5][col-3]);
	r3=_mm256_loadu_si256( (__m256i *) &frame1[N-4][col-3]);
	r4=_mm256_loadu_si256( (__m256i *) &frame1[N-3][col-3]);
	r5=_mm256_loadu_si256( (__m256i *) &frame1[N-2][col-3]);
	r6=_mm256_loadu_si256( (__m256i *) &frame1[N-1][col-3]);
	c6=c4; c5=c3; c4=c2; c3=c1; c2=c0;
}
else if (row==N-1){
    r0=_mm256_setzero_si256();
    r1=_mm256_setzero_si256();
    r2=_mm256_setzero_si256();
	r3=_mm256_loadu_si256( (__m256i *) &frame1[N-4][col-3]);
	r4=_mm256_loadu_si256( (__m256i *) &frame1[N-3][col-3]);
	r5=_mm256_loadu_si256( (__m256i *) &frame1[N-2][col-3]);
	r6=_mm256_loadu_si256( (__m256i *) &frame1[N-1][col-3]);
	c6=c3; c5=c2; c4=c1; c3=c0;
}
else {
	printf("\nsomething went wrong");
	exit(EXIT_FAILURE);
}
		reminder_mask1=_mm256_load_si256( (__m256i *) &reminder_msk_7x7[REMINDER_ITERATIONS-1][0]);

		//zero the values that exceed the array bounds
		r0=_mm256_and_si256(r0,reminder_mask1);
		r1=_mm256_and_si256(r1,reminder_mask1);
		r2=_mm256_and_si256(r2,reminder_mask1);
		r3=_mm256_and_si256(r3,reminder_mask1);
		r4=_mm256_and_si256(r4,reminder_mask1);
		r5=_mm256_and_si256(r5,reminder_mask1);
		r6=_mm256_and_si256(r6,reminder_mask1);

// col iteration computes output pixels of   0,8,16,24
// col+1 iteration computes output pixels of 1,9,17,25
// col+2 iteration computes output pixels of 2,10,18,26
// col+3 iteration computes output pixels of 3,11,19,27
// col+4 iteration computes output pixels of 4,12,20,28
// col+5 iteration computes output pixels of 5,13,21,29
// col+6 iteration computes output pixels of 6,14,22,30
// col+7 iteration computes output pixels of 7,15,23,31
//afterwards, col becomes 32 and repeat the above process



	//multiply by the mask
	m0=_mm256_maddubs_epi16(r0,c0);
	m1=_mm256_maddubs_epi16(r1,c1);
	m2=_mm256_maddubs_epi16(r2,c2);
	m3=_mm256_maddubs_epi16(r3,c3);
	m4=_mm256_maddubs_epi16(r4,c4);
	m5=_mm256_maddubs_epi16(r5,c5);
	m6=_mm256_maddubs_epi16(r6,c6);


	//make 16-bit values 32-bit (horizontal add)
	m0=_mm256_madd_epi16(m0,ones);
	m1=_mm256_madd_epi16(m1,ones);
	m2=_mm256_madd_epi16(m2,ones);
	m3=_mm256_madd_epi16(m3,ones);
	m4=_mm256_madd_epi16(m4,ones);
	m5=_mm256_madd_epi16(m5,ones);
	m6=_mm256_madd_epi16(m6,ones);


	//vertical add
	m0=_mm256_add_epi32(m0,m1);
	m0=_mm256_add_epi32(m0,m2);
	m0=_mm256_add_epi32(m0,m3);
	m0=_mm256_add_epi32(m0,m4);
	m0=_mm256_add_epi32(m0,m5);
	m0=_mm256_add_epi32(m0,m6);


	//last horizontal add
	m1=_mm256_slli_si256(m0,4);//shift by one integer
	m0=_mm256_add_epi32(m0,m1);//add
	m1=_mm256_srli_si256(m0,4);//shift by one integer
	output1=_mm256_and_si256(m1,mask_even_32);//keep only 0,2,4,6 and discard 1,3,5,7



	//2nd col iteration
	if (row==0){
		r0=_mm256_loadu_si256( (__m256i *) &frame1[0][col-3+1]);
		r1=_mm256_loadu_si256( (__m256i *) &frame1[1][col-3+1]);
		r2=_mm256_loadu_si256( (__m256i *) &frame1[2][col-3+1]);
		r3=_mm256_loadu_si256( (__m256i *) &frame1[3][col-3+1]);
	    r4=_mm256_setzero_si256();
	    r5=_mm256_setzero_si256();
	    r6=_mm256_setzero_si256();
	}
	else if (row==1){
		r0=_mm256_loadu_si256( (__m256i *) &frame1[0][col-3+1]);
		r1=_mm256_loadu_si256( (__m256i *) &frame1[1][col-3+1]);
		r2=_mm256_loadu_si256( (__m256i *) &frame1[2][col-3+1]);
		r3=_mm256_loadu_si256( (__m256i *) &frame1[3][col-3+1]);
		r4=_mm256_loadu_si256( (__m256i *) &frame1[4][col-3+1]);
	    r5=_mm256_setzero_si256();
	    r6=_mm256_setzero_si256();
	}
	else if (row==2){
		r0=_mm256_loadu_si256( (__m256i *) &frame1[0][col-3+1]);
		r1=_mm256_loadu_si256( (__m256i *) &frame1[1][col-3+1]);
		r2=_mm256_loadu_si256( (__m256i *) &frame1[2][col-3+1]);
		r3=_mm256_loadu_si256( (__m256i *) &frame1[3][col-3+1]);
		r4=_mm256_loadu_si256( (__m256i *) &frame1[4][col-3+1]);
		r5=_mm256_loadu_si256( (__m256i *) &frame1[5][col-3+1]);
	    r6=_mm256_setzero_si256();
	}
	else if (row==N-3){
	    r0=_mm256_setzero_si256();
	    r1=_mm256_loadu_si256( (__m256i *) &frame1[N-6][col-3+1]);
		r2=_mm256_loadu_si256( (__m256i *) &frame1[N-5][col-3+1]);
		r3=_mm256_loadu_si256( (__m256i *) &frame1[N-4][col-3+1]);
		r4=_mm256_loadu_si256( (__m256i *) &frame1[N-3][col-3+1]);
		r5=_mm256_loadu_si256( (__m256i *) &frame1[N-2][col-3+1]);
		r6=_mm256_loadu_si256( (__m256i *) &frame1[N-1][col-3+1]);
	}
	else if (row==N-2){
	    r0=_mm256_setzero_si256();
	    r1=_mm256_setzero_si256();
		r2=_mm256_loadu_si256( (__m256i *) &frame1[N-5][col-3+1]);
		r3=_mm256_loadu_si256( (__m256i *) &frame1[N-4][col-3+1]);
		r4=_mm256_loadu_si256( (__m256i *) &frame1[N-3][col-3+1]);
		r5=_mm256_loadu_si256( (__m256i *) &frame1[N-2][col-3+1]);
		r6=_mm256_loadu_si256( (__m256i *) &frame1[N-1][col-3+1]);
	}
	else if (row==N-1){
	    r0=_mm256_setzero_si256();
	    r1=_mm256_setzero_si256();
	    r2=_mm256_setzero_si256();
		r3=_mm256_loadu_si256( (__m256i *) &frame1[N-4][col-3+1]);
		r4=_mm256_loadu_si256( (__m256i *) &frame1[N-3][col-3+1]);
		r5=_mm256_loadu_si256( (__m256i *) &frame1[N-2][col-3+1]);
		r6=_mm256_loadu_si256( (__m256i *) &frame1[N-1][col-3+1]);
	}

	reminder_mask1=_mm256_load_si256( (__m256i *) &reminder_msk_7x7[REMINDER_ITERATIONS-2][0]);

	//zero the values that exceed the array bounds
	r0=_mm256_and_si256(r0,reminder_mask1);
	r1=_mm256_and_si256(r1,reminder_mask1);
	r2=_mm256_and_si256(r2,reminder_mask1);
	r3=_mm256_and_si256(r3,reminder_mask1);
	r4=_mm256_and_si256(r4,reminder_mask1);
	r5=_mm256_and_si256(r5,reminder_mask1);
	r6=_mm256_and_si256(r6,reminder_mask1);

	//multiply by the mask
	m0=_mm256_maddubs_epi16(r0,c0);
	m1=_mm256_maddubs_epi16(r1,c1);
	m2=_mm256_maddubs_epi16(r2,c2);
	m3=_mm256_maddubs_epi16(r3,c3);
	m4=_mm256_maddubs_epi16(r4,c4);
	m5=_mm256_maddubs_epi16(r5,c5);
	m6=_mm256_maddubs_epi16(r6,c6);


	//make 16-bit values 32-bit (horizontal add)
	m0=_mm256_madd_epi16(m0,ones);
	m1=_mm256_madd_epi16(m1,ones);
	m2=_mm256_madd_epi16(m2,ones);
	m3=_mm256_madd_epi16(m3,ones);
	m4=_mm256_madd_epi16(m4,ones);
	m5=_mm256_madd_epi16(m5,ones);
	m6=_mm256_madd_epi16(m6,ones);


	//vertical add
	m0=_mm256_add_epi32(m0,m1);
	m0=_mm256_add_epi32(m0,m2);
	m0=_mm256_add_epi32(m0,m3);
	m0=_mm256_add_epi32(m0,m4);
	m0=_mm256_add_epi32(m0,m5);
	m0=_mm256_add_epi32(m0,m6);


	//last horizontal add
	m1=_mm256_slli_si256(m0,4);//shift by one integer
	m0=_mm256_add_epi32(m0,m1);//add
	m1=_mm256_srli_si256(m0,4);//shift by one integer
	output2=_mm256_and_si256(m1,mask_even_32);//keep only 0,2,4,6 and discard 1,3,5,7


	//3rd col iteration
	if (row==0){
		r0=_mm256_loadu_si256( (__m256i *) &frame1[0][col-3+2]);
		r1=_mm256_loadu_si256( (__m256i *) &frame1[1][col-3+2]);
		r2=_mm256_loadu_si256( (__m256i *) &frame1[2][col-3+2]);
		r3=_mm256_loadu_si256( (__m256i *) &frame1[3][col-3+2]);
	    r4=_mm256_setzero_si256();
	    r5=_mm256_setzero_si256();
	    r6=_mm256_setzero_si256();
	}
	else if (row==1){
		r0=_mm256_loadu_si256( (__m256i *) &frame1[0][col-3+2]);
		r1=_mm256_loadu_si256( (__m256i *) &frame1[1][col-3+2]);
		r2=_mm256_loadu_si256( (__m256i *) &frame1[2][col-3+2]);
		r3=_mm256_loadu_si256( (__m256i *) &frame1[3][col-3+2]);
		r4=_mm256_loadu_si256( (__m256i *) &frame1[4][col-3+2]);
	    r5=_mm256_setzero_si256();
	    r6=_mm256_setzero_si256();
	}
	else if (row==2){
		r0=_mm256_loadu_si256( (__m256i *) &frame1[0][col-3+2]);
		r1=_mm256_loadu_si256( (__m256i *) &frame1[1][col-3+2]);
		r2=_mm256_loadu_si256( (__m256i *) &frame1[2][col-3+2]);
		r3=_mm256_loadu_si256( (__m256i *) &frame1[3][col-3+2]);
		r4=_mm256_loadu_si256( (__m256i *) &frame1[4][col-3+2]);
		r5=_mm256_loadu_si256( (__m256i *) &frame1[5][col-3+2]);
	    r6=_mm256_setzero_si256();
	}
	else if (row==N-3){
	    r0=_mm256_setzero_si256();
	    r1=_mm256_loadu_si256( (__m256i *) &frame1[N-6][col-3+2]);
		r2=_mm256_loadu_si256( (__m256i *) &frame1[N-5][col-3+2]);
		r3=_mm256_loadu_si256( (__m256i *) &frame1[N-4][col-3+2]);
		r4=_mm256_loadu_si256( (__m256i *) &frame1[N-3][col-3+2]);
		r5=_mm256_loadu_si256( (__m256i *) &frame1[N-2][col-3+2]);
		r6=_mm256_loadu_si256( (__m256i *) &frame1[N-1][col-3+2]);
	}
	else if (row==N-2){
	    r0=_mm256_setzero_si256();
	    r1=_mm256_setzero_si256();
		r2=_mm256_loadu_si256( (__m256i *) &frame1[N-5][col-3+2]);
		r3=_mm256_loadu_si256( (__m256i *) &frame1[N-4][col-3+2]);
		r4=_mm256_loadu_si256( (__m256i *) &frame1[N-3][col-3+2]);
		r5=_mm256_loadu_si256( (__m256i *) &frame1[N-2][col-3+2]);
		r6=_mm256_loadu_si256( (__m256i *) &frame1[N-1][col-3+2]);
	}
	else if (row==N-1){
	    r0=_mm256_setzero_si256();
	    r1=_mm256_setzero_si256();
	    r2=_mm256_setzero_si256();
		r3=_mm256_loadu_si256( (__m256i *) &frame1[N-4][col-3+2]);
		r4=_mm256_loadu_si256( (__m256i *) &frame1[N-3][col-3+2]);
		r5=_mm256_loadu_si256( (__m256i *) &frame1[N-2][col-3+2]);
		r6=_mm256_loadu_si256( (__m256i *) &frame1[N-1][col-3+2]);
	}

	reminder_mask1=_mm256_load_si256( (__m256i *) &reminder_msk_7x7[REMINDER_ITERATIONS-3][0]);

	//zero the values that exceed the array bounds
	r0=_mm256_and_si256(r0,reminder_mask1);
	r1=_mm256_and_si256(r1,reminder_mask1);
	r2=_mm256_and_si256(r2,reminder_mask1);
	r3=_mm256_and_si256(r3,reminder_mask1);
	r4=_mm256_and_si256(r4,reminder_mask1);
	r5=_mm256_and_si256(r5,reminder_mask1);
	r6=_mm256_and_si256(r6,reminder_mask1);

	//multiply by the mask
	m0=_mm256_maddubs_epi16(r0,c0);
	m1=_mm256_maddubs_epi16(r1,c1);
	m2=_mm256_maddubs_epi16(r2,c2);
	m3=_mm256_maddubs_epi16(r3,c3);
	m4=_mm256_maddubs_epi16(r4,c4);
	m5=_mm256_maddubs_epi16(r5,c5);
	m6=_mm256_maddubs_epi16(r6,c6);


	//make 16-bit values 32-bit (horizontal add)
	m0=_mm256_madd_epi16(m0,ones);
	m1=_mm256_madd_epi16(m1,ones);
	m2=_mm256_madd_epi16(m2,ones);
	m3=_mm256_madd_epi16(m3,ones);
	m4=_mm256_madd_epi16(m4,ones);
	m5=_mm256_madd_epi16(m5,ones);
	m6=_mm256_madd_epi16(m6,ones);


	//vertical add
	m0=_mm256_add_epi32(m0,m1);
	m0=_mm256_add_epi32(m0,m2);
	m0=_mm256_add_epi32(m0,m3);
	m0=_mm256_add_epi32(m0,m4);
	m0=_mm256_add_epi32(m0,m5);
	m0=_mm256_add_epi32(m0,m6);


	//last horizontal add
	m1=_mm256_slli_si256(m0,4);//shift by one integer
	m0=_mm256_add_epi32(m0,m1);//add
	m1=_mm256_srli_si256(m0,4);//shift by one integer
	output3=_mm256_and_si256(m1,mask_even_32);//keep only 0,2,4,6 and discard 1,3,5,7


	//4th col iteration
	if (row==0){
		r0=_mm256_loadu_si256( (__m256i *) &frame1[0][col-3+3]);
		r1=_mm256_loadu_si256( (__m256i *) &frame1[1][col-3+3]);
		r2=_mm256_loadu_si256( (__m256i *) &frame1[2][col-3+3]);
		r3=_mm256_loadu_si256( (__m256i *) &frame1[3][col-3+3]);
	    r4=_mm256_setzero_si256();
	    r5=_mm256_setzero_si256();
	    r6=_mm256_setzero_si256();
	}
	else if (row==1){
		r0=_mm256_loadu_si256( (__m256i *) &frame1[0][col-3+3]);
		r1=_mm256_loadu_si256( (__m256i *) &frame1[1][col-3+3]);
		r2=_mm256_loadu_si256( (__m256i *) &frame1[2][col-3+3]);
		r3=_mm256_loadu_si256( (__m256i *) &frame1[3][col-3+3]);
		r4=_mm256_loadu_si256( (__m256i *) &frame1[4][col-3+3]);
	    r5=_mm256_setzero_si256();
	    r6=_mm256_setzero_si256();
	}
	else if (row==2){
		r0=_mm256_loadu_si256( (__m256i *) &frame1[0][col-3+3]);
		r1=_mm256_loadu_si256( (__m256i *) &frame1[1][col-3+3]);
		r2=_mm256_loadu_si256( (__m256i *) &frame1[2][col-3+3]);
		r3=_mm256_loadu_si256( (__m256i *) &frame1[3][col-3+3]);
		r4=_mm256_loadu_si256( (__m256i *) &frame1[4][col-3+3]);
		r5=_mm256_loadu_si256( (__m256i *) &frame1[5][col-3+3]);
	    r6=_mm256_setzero_si256();
	}
	else if (row==N-3){
	    r0=_mm256_setzero_si256();
	    r1=_mm256_loadu_si256( (__m256i *) &frame1[N-6][col-3+3]);
		r2=_mm256_loadu_si256( (__m256i *) &frame1[N-5][col-3+3]);
		r3=_mm256_loadu_si256( (__m256i *) &frame1[N-4][col-3+3]);
		r4=_mm256_loadu_si256( (__m256i *) &frame1[N-3][col-3+3]);
		r5=_mm256_loadu_si256( (__m256i *) &frame1[N-2][col-3+3]);
		r6=_mm256_loadu_si256( (__m256i *) &frame1[N-1][col-3+3]);
	}
	else if (row==N-2){
	    r0=_mm256_setzero_si256();
	    r1=_mm256_setzero_si256();
		r2=_mm256_loadu_si256( (__m256i *) &frame1[N-5][col-3+3]);
		r3=_mm256_loadu_si256( (__m256i *) &frame1[N-4][col-3+3]);
		r4=_mm256_loadu_si256( (__m256i *) &frame1[N-3][col-3+3]);
		r5=_mm256_loadu_si256( (__m256i *) &frame1[N-2][col-3+3]);
		r6=_mm256_loadu_si256( (__m256i *) &frame1[N-1][col-3+3]);
	}
	else if (row==N-1){
	    r0=_mm256_setzero_si256();
	    r1=_mm256_setzero_si256();
	    r2=_mm256_setzero_si256();
		r3=_mm256_loadu_si256( (__m256i *) &frame1[N-4][col-3+3]);
		r4=_mm256_loadu_si256( (__m256i *) &frame1[N-3][col-3+3]);
		r5=_mm256_loadu_si256( (__m256i *) &frame1[N-2][col-3+3]);
		r6=_mm256_loadu_si256( (__m256i *) &frame1[N-1][col-3+3]);
	}

	reminder_mask1=_mm256_load_si256( (__m256i *) &reminder_msk_7x7[REMINDER_ITERATIONS-4][0]);

	//zero the values that exceed the array bounds
	r0=_mm256_and_si256(r0,reminder_mask1);
	r1=_mm256_and_si256(r1,reminder_mask1);
	r2=_mm256_and_si256(r2,reminder_mask1);
	r3=_mm256_and_si256(r3,reminder_mask1);
	r4=_mm256_and_si256(r4,reminder_mask1);
	r5=_mm256_and_si256(r5,reminder_mask1);
	r6=_mm256_and_si256(r6,reminder_mask1);

	//multiply by the mask
	m0=_mm256_maddubs_epi16(r0,c0);
	m1=_mm256_maddubs_epi16(r1,c1);
	m2=_mm256_maddubs_epi16(r2,c2);
	m3=_mm256_maddubs_epi16(r3,c3);
	m4=_mm256_maddubs_epi16(r4,c4);
	m5=_mm256_maddubs_epi16(r5,c5);
	m6=_mm256_maddubs_epi16(r6,c6);


	//make 16-bit values 32-bit (horizontal add)
	m0=_mm256_madd_epi16(m0,ones);
	m1=_mm256_madd_epi16(m1,ones);
	m2=_mm256_madd_epi16(m2,ones);
	m3=_mm256_madd_epi16(m3,ones);
	m4=_mm256_madd_epi16(m4,ones);
	m5=_mm256_madd_epi16(m5,ones);
	m6=_mm256_madd_epi16(m6,ones);


	//vertical add
	m0=_mm256_add_epi32(m0,m1);
	m0=_mm256_add_epi32(m0,m2);
	m0=_mm256_add_epi32(m0,m3);
	m0=_mm256_add_epi32(m0,m4);
	m0=_mm256_add_epi32(m0,m5);
	m0=_mm256_add_epi32(m0,m6);


	//last horizontal add
	m1=_mm256_slli_si256(m0,4);//shift by one integer
	m0=_mm256_add_epi32(m0,m1);//add
	m1=_mm256_srli_si256(m0,4);//shift by one integer
	output4=_mm256_and_si256(m1,mask_even_32);//keep only 0,2,4,6 and discard 1,3,5,7

if (REMINDER_ITERATIONS>=5){
	//5th col iteration

	if (row==0){
		r0=_mm256_loadu_si256( (__m256i *) &frame1[0][col-3+4]);
		r1=_mm256_loadu_si256( (__m256i *) &frame1[1][col-3+4]);
		r2=_mm256_loadu_si256( (__m256i *) &frame1[2][col-3+4]);
		r3=_mm256_loadu_si256( (__m256i *) &frame1[3][col-3+4]);
	    r4=_mm256_setzero_si256();
	    r5=_mm256_setzero_si256();
	    r6=_mm256_setzero_si256();
	}
	else if (row==1){
		r0=_mm256_loadu_si256( (__m256i *) &frame1[0][col-3+4]);
		r1=_mm256_loadu_si256( (__m256i *) &frame1[1][col-3+4]);
		r2=_mm256_loadu_si256( (__m256i *) &frame1[2][col-3+4]);
		r3=_mm256_loadu_si256( (__m256i *) &frame1[3][col-3+4]);
		r4=_mm256_loadu_si256( (__m256i *) &frame1[4][col-3+4]);
	    r5=_mm256_setzero_si256();
	    r6=_mm256_setzero_si256();
	}
	else if (row==2){
		r0=_mm256_loadu_si256( (__m256i *) &frame1[0][col-3+4]);
		r1=_mm256_loadu_si256( (__m256i *) &frame1[1][col-3+4]);
		r2=_mm256_loadu_si256( (__m256i *) &frame1[2][col-3+4]);
		r3=_mm256_loadu_si256( (__m256i *) &frame1[3][col-3+4]);
		r4=_mm256_loadu_si256( (__m256i *) &frame1[4][col-3+4]);
		r5=_mm256_loadu_si256( (__m256i *) &frame1[5][col-3+4]);
	    r6=_mm256_setzero_si256();
	}
	else if (row==N-3){
	    r0=_mm256_setzero_si256();
	    r1=_mm256_loadu_si256( (__m256i *) &frame1[N-6][col-3+4]);
		r2=_mm256_loadu_si256( (__m256i *) &frame1[N-5][col-3+4]);
		r3=_mm256_loadu_si256( (__m256i *) &frame1[N-4][col-3+4]);
		r4=_mm256_loadu_si256( (__m256i *) &frame1[N-3][col-3+4]);
		r5=_mm256_loadu_si256( (__m256i *) &frame1[N-2][col-3+4]);
		r6=_mm256_loadu_si256( (__m256i *) &frame1[N-1][col-3+4]);
	}
	else if (row==N-2){
	    r0=_mm256_setzero_si256();
	    r1=_mm256_setzero_si256();
		r2=_mm256_loadu_si256( (__m256i *) &frame1[N-5][col-3+4]);
		r3=_mm256_loadu_si256( (__m256i *) &frame1[N-4][col-3+4]);
		r4=_mm256_loadu_si256( (__m256i *) &frame1[N-3][col-3+4]);
		r5=_mm256_loadu_si256( (__m256i *) &frame1[N-2][col-3+4]);
		r6=_mm256_loadu_si256( (__m256i *) &frame1[N-1][col-3+4]);
	}
	else if (row==N-1){
	    r0=_mm256_setzero_si256();
	    r1=_mm256_setzero_si256();
	    r2=_mm256_setzero_si256();
		r3=_mm256_loadu_si256( (__m256i *) &frame1[N-4][col-3+4]);
		r4=_mm256_loadu_si256( (__m256i *) &frame1[N-3][col-3+4]);
		r5=_mm256_loadu_si256( (__m256i *) &frame1[N-2][col-3+4]);
		r6=_mm256_loadu_si256( (__m256i *) &frame1[N-1][col-3+4]);
	}

	reminder_mask1=_mm256_load_si256( (__m256i *) &reminder_msk_7x7[REMINDER_ITERATIONS-5][0]);

	//zero the values that exceed the array bounds
	r0=_mm256_and_si256(r0,reminder_mask1);
	r1=_mm256_and_si256(r1,reminder_mask1);
	r2=_mm256_and_si256(r2,reminder_mask1);
	r3=_mm256_and_si256(r3,reminder_mask1);
	r4=_mm256_and_si256(r4,reminder_mask1);
	r5=_mm256_and_si256(r5,reminder_mask1);
	r6=_mm256_and_si256(r6,reminder_mask1);

	//multiply by the mask
	m0=_mm256_maddubs_epi16(r0,c0);
	m1=_mm256_maddubs_epi16(r1,c1);
	m2=_mm256_maddubs_epi16(r2,c2);
	m3=_mm256_maddubs_epi16(r3,c3);
	m4=_mm256_maddubs_epi16(r4,c4);
	m5=_mm256_maddubs_epi16(r5,c5);
	m6=_mm256_maddubs_epi16(r6,c6);


	//make 16-bit values 32-bit (horizontal add)
	m0=_mm256_madd_epi16(m0,ones);
	m1=_mm256_madd_epi16(m1,ones);
	m2=_mm256_madd_epi16(m2,ones);
	m3=_mm256_madd_epi16(m3,ones);
	m4=_mm256_madd_epi16(m4,ones);
	m5=_mm256_madd_epi16(m5,ones);
	m6=_mm256_madd_epi16(m6,ones);


	//vertical add
	m0=_mm256_add_epi32(m0,m1);
	m0=_mm256_add_epi32(m0,m2);
	m0=_mm256_add_epi32(m0,m3);
	m0=_mm256_add_epi32(m0,m4);
	m0=_mm256_add_epi32(m0,m5);
	m0=_mm256_add_epi32(m0,m6);


	//last horizontal add
	m1=_mm256_slli_si256(m0,4);//shift by one integer
	m0=_mm256_add_epi32(m0,m1);//add
	m1=_mm256_srli_si256(m0,4);//shift by one integer
	m0=_mm256_and_si256(m1,mask_even_32);//keep only 0,2,4,6 and discard 1,3,5,7


	//MERGE output1 with output5 and DIVIDE
	m0=_mm256_slli_si256(m0,4);//shift by one integer
	output1=_mm256_add_epi32(m0,output1);
}
output1 = division_32(division_case,output1,f);

if (REMINDER_ITERATIONS>=6){
	//6th col iteration
	if (row==0){
		r0=_mm256_loadu_si256( (__m256i *) &frame1[0][col-3+5]);
		r1=_mm256_loadu_si256( (__m256i *) &frame1[1][col-3+5]);
		r2=_mm256_loadu_si256( (__m256i *) &frame1[2][col-3+5]);
		r3=_mm256_loadu_si256( (__m256i *) &frame1[3][col-3+5]);
	    r4=_mm256_setzero_si256();
	    r5=_mm256_setzero_si256();
	    r6=_mm256_setzero_si256();
	}
	else if (row==1){
		r0=_mm256_loadu_si256( (__m256i *) &frame1[0][col-3+5]);
		r1=_mm256_loadu_si256( (__m256i *) &frame1[1][col-3+5]);
		r2=_mm256_loadu_si256( (__m256i *) &frame1[2][col-3+5]);
		r3=_mm256_loadu_si256( (__m256i *) &frame1[3][col-3+5]);
		r4=_mm256_loadu_si256( (__m256i *) &frame1[4][col-3+5]);
	    r5=_mm256_setzero_si256();
	    r6=_mm256_setzero_si256();
	}
	else if (row==2){
		r0=_mm256_loadu_si256( (__m256i *) &frame1[0][col-3+5]);
		r1=_mm256_loadu_si256( (__m256i *) &frame1[1][col-3+5]);
		r2=_mm256_loadu_si256( (__m256i *) &frame1[2][col-3+5]);
		r3=_mm256_loadu_si256( (__m256i *) &frame1[3][col-3+5]);
		r4=_mm256_loadu_si256( (__m256i *) &frame1[4][col-3+5]);
		r5=_mm256_loadu_si256( (__m256i *) &frame1[5][col-3+5]);
	    r6=_mm256_setzero_si256();
	}
	else if (row==N-3){
	    r0=_mm256_setzero_si256();
	    r1=_mm256_loadu_si256( (__m256i *) &frame1[N-6][col-3+5]);
		r2=_mm256_loadu_si256( (__m256i *) &frame1[N-5][col-3+5]);
		r3=_mm256_loadu_si256( (__m256i *) &frame1[N-4][col-3+5]);
		r4=_mm256_loadu_si256( (__m256i *) &frame1[N-3][col-3+5]);
		r5=_mm256_loadu_si256( (__m256i *) &frame1[N-2][col-3+5]);
		r6=_mm256_loadu_si256( (__m256i *) &frame1[N-1][col-3+5]);
	}
	else if (row==N-2){
	    r0=_mm256_setzero_si256();
	    r1=_mm256_setzero_si256();
		r2=_mm256_loadu_si256( (__m256i *) &frame1[N-5][col-3+5]);
		r3=_mm256_loadu_si256( (__m256i *) &frame1[N-4][col-3+5]);
		r4=_mm256_loadu_si256( (__m256i *) &frame1[N-3][col-3+5]);
		r5=_mm256_loadu_si256( (__m256i *) &frame1[N-2][col-3+5]);
		r6=_mm256_loadu_si256( (__m256i *) &frame1[N-1][col-3+5]);
	}
	else if (row==N-1){
	    r0=_mm256_setzero_si256();
	    r1=_mm256_setzero_si256();
	    r2=_mm256_setzero_si256();
		r3=_mm256_loadu_si256( (__m256i *) &frame1[N-4][col-3+5]);
		r4=_mm256_loadu_si256( (__m256i *) &frame1[N-3][col-3+5]);
		r5=_mm256_loadu_si256( (__m256i *) &frame1[N-2][col-3+5]);
		r6=_mm256_loadu_si256( (__m256i *) &frame1[N-1][col-3+5]);
	}

	reminder_mask1=_mm256_load_si256( (__m256i *) &reminder_msk_7x7[REMINDER_ITERATIONS-6][0]);

	//zero the values that exceed the array bounds
	r0=_mm256_and_si256(r0,reminder_mask1);
	r1=_mm256_and_si256(r1,reminder_mask1);
	r2=_mm256_and_si256(r2,reminder_mask1);
	r3=_mm256_and_si256(r3,reminder_mask1);
	r4=_mm256_and_si256(r4,reminder_mask1);
	r5=_mm256_and_si256(r5,reminder_mask1);
	r6=_mm256_and_si256(r6,reminder_mask1);

	//multiply by the mask
	m0=_mm256_maddubs_epi16(r0,c0);
	m1=_mm256_maddubs_epi16(r1,c1);
	m2=_mm256_maddubs_epi16(r2,c2);
	m3=_mm256_maddubs_epi16(r3,c3);
	m4=_mm256_maddubs_epi16(r4,c4);
	m5=_mm256_maddubs_epi16(r5,c5);
	m6=_mm256_maddubs_epi16(r6,c6);


	//make 16-bit values 32-bit (horizontal add)
	m0=_mm256_madd_epi16(m0,ones);
	m1=_mm256_madd_epi16(m1,ones);
	m2=_mm256_madd_epi16(m2,ones);
	m3=_mm256_madd_epi16(m3,ones);
	m4=_mm256_madd_epi16(m4,ones);
	m5=_mm256_madd_epi16(m5,ones);
	m6=_mm256_madd_epi16(m6,ones);


	//vertical add
	m0=_mm256_add_epi32(m0,m1);
	m0=_mm256_add_epi32(m0,m2);
	m0=_mm256_add_epi32(m0,m3);
	m0=_mm256_add_epi32(m0,m4);
	m0=_mm256_add_epi32(m0,m5);
	m0=_mm256_add_epi32(m0,m6);


	//last horizontal add
	m1=_mm256_slli_si256(m0,4);//shift by one integer
	m0=_mm256_add_epi32(m0,m1);//add
	m1=_mm256_srli_si256(m0,4);//shift by one integer
	m0=_mm256_and_si256(m1,mask_even_32);//keep only 0,2,4,6 and discard 1,3,5,7

	//MERGE output1 with output5 and DIVIDE
	m0=_mm256_slli_si256(m0,4);//shift by one integer
	output2=_mm256_add_epi32(m0,output2);
}
output2 = division_32(division_case,output2,f);


if (REMINDER_ITERATIONS>=7){
	//7th col iteration
	if (row==0){
		r0=_mm256_loadu_si256( (__m256i *) &frame1[0][col-3+6]);
		r1=_mm256_loadu_si256( (__m256i *) &frame1[1][col-3+6]);
		r2=_mm256_loadu_si256( (__m256i *) &frame1[2][col-3+6]);
		r3=_mm256_loadu_si256( (__m256i *) &frame1[3][col-3+6]);
	    r4=_mm256_setzero_si256();
	    r5=_mm256_setzero_si256();
	    r6=_mm256_setzero_si256();
	}
	else if (row==1){
		r0=_mm256_loadu_si256( (__m256i *) &frame1[0][col-3+6]);
		r1=_mm256_loadu_si256( (__m256i *) &frame1[1][col-3+6]);
		r2=_mm256_loadu_si256( (__m256i *) &frame1[2][col-3+6]);
		r3=_mm256_loadu_si256( (__m256i *) &frame1[3][col-3+6]);
		r4=_mm256_loadu_si256( (__m256i *) &frame1[4][col-3+6]);
	    r5=_mm256_setzero_si256();
	    r6=_mm256_setzero_si256();
	}
	else if (row==2){
		r0=_mm256_loadu_si256( (__m256i *) &frame1[0][col-3+6]);
		r1=_mm256_loadu_si256( (__m256i *) &frame1[1][col-3+6]);
		r2=_mm256_loadu_si256( (__m256i *) &frame1[2][col-3+6]);
		r3=_mm256_loadu_si256( (__m256i *) &frame1[3][col-3+6]);
		r4=_mm256_loadu_si256( (__m256i *) &frame1[4][col-3+6]);
		r5=_mm256_loadu_si256( (__m256i *) &frame1[5][col-3+6]);
	    r6=_mm256_setzero_si256();
	}
	else if (row==N-3){
	    r0=_mm256_setzero_si256();
	    r1=_mm256_loadu_si256( (__m256i *) &frame1[N-6][col-3+6]);
		r2=_mm256_loadu_si256( (__m256i *) &frame1[N-5][col-3+6]);
		r3=_mm256_loadu_si256( (__m256i *) &frame1[N-4][col-3+6]);
		r4=_mm256_loadu_si256( (__m256i *) &frame1[N-3][col-3+6]);
		r5=_mm256_loadu_si256( (__m256i *) &frame1[N-2][col-3+6]);
		r6=_mm256_loadu_si256( (__m256i *) &frame1[N-1][col-3+6]);
	}
	else if (row==N-2){
	    r0=_mm256_setzero_si256();
	    r1=_mm256_setzero_si256();
		r2=_mm256_loadu_si256( (__m256i *) &frame1[N-5][col-3+6]);
		r3=_mm256_loadu_si256( (__m256i *) &frame1[N-4][col-3+6]);
		r4=_mm256_loadu_si256( (__m256i *) &frame1[N-3][col-3+6]);
		r5=_mm256_loadu_si256( (__m256i *) &frame1[N-2][col-3+6]);
		r6=_mm256_loadu_si256( (__m256i *) &frame1[N-1][col-3+6]);
	}
	else if (row==N-1){
	    r0=_mm256_setzero_si256();
	    r1=_mm256_setzero_si256();
	    r2=_mm256_setzero_si256();
		r3=_mm256_loadu_si256( (__m256i *) &frame1[N-4][col-3+6]);
		r4=_mm256_loadu_si256( (__m256i *) &frame1[N-3][col-3+6]);
		r5=_mm256_loadu_si256( (__m256i *) &frame1[N-2][col-3+6]);
		r6=_mm256_loadu_si256( (__m256i *) &frame1[N-1][col-3+6]);
	}

	reminder_mask1=_mm256_load_si256( (__m256i *) &reminder_msk_7x7[REMINDER_ITERATIONS-7][0]);

	//zero the values that exceed the array bounds
	r0=_mm256_and_si256(r0,reminder_mask1);
	r1=_mm256_and_si256(r1,reminder_mask1);
	r2=_mm256_and_si256(r2,reminder_mask1);
	r3=_mm256_and_si256(r3,reminder_mask1);
	r4=_mm256_and_si256(r4,reminder_mask1);
	r5=_mm256_and_si256(r5,reminder_mask1);
	r6=_mm256_and_si256(r6,reminder_mask1);

	//multiply by the mask
	m0=_mm256_maddubs_epi16(r0,c0);
	m1=_mm256_maddubs_epi16(r1,c1);
	m2=_mm256_maddubs_epi16(r2,c2);
	m3=_mm256_maddubs_epi16(r3,c3);
	m4=_mm256_maddubs_epi16(r4,c4);
	m5=_mm256_maddubs_epi16(r5,c5);
	m6=_mm256_maddubs_epi16(r6,c6);


	//make 16-bit values 32-bit (horizontal add)
	m0=_mm256_madd_epi16(m0,ones);
	m1=_mm256_madd_epi16(m1,ones);
	m2=_mm256_madd_epi16(m2,ones);
	m3=_mm256_madd_epi16(m3,ones);
	m4=_mm256_madd_epi16(m4,ones);
	m5=_mm256_madd_epi16(m5,ones);
	m6=_mm256_madd_epi16(m6,ones);


	//vertical add
	m0=_mm256_add_epi32(m0,m1);
	m0=_mm256_add_epi32(m0,m2);
	m0=_mm256_add_epi32(m0,m3);
	m0=_mm256_add_epi32(m0,m4);
	m0=_mm256_add_epi32(m0,m5);
	m0=_mm256_add_epi32(m0,m6);


	//last horizontal add
	m1=_mm256_slli_si256(m0,4);//shift by one integer
	m0=_mm256_add_epi32(m0,m1);//add
	m1=_mm256_srli_si256(m0,4);//shift by one integer
	m0=_mm256_and_si256(m1,mask_even_32);//keep only 0,2,4,6 and discard 1,3,5,7

	//MERGE output1 with output5 and DIVIDE
	m0=_mm256_slli_si256(m0,4);//shift by one integer
	output3=_mm256_add_epi32(m0,output3);
}

output3 = division_32(division_case,output3,f);


if (REMINDER_ITERATIONS>=8){
	//8th col iteration

	if (row==0){
		r0=_mm256_loadu_si256( (__m256i *) &frame1[0][col-3+7]);
		r1=_mm256_loadu_si256( (__m256i *) &frame1[1][col-3+7]);
		r2=_mm256_loadu_si256( (__m256i *) &frame1[2][col-3+7]);
		r3=_mm256_loadu_si256( (__m256i *) &frame1[3][col-3+7]);
	    r4=_mm256_setzero_si256();
	    r5=_mm256_setzero_si256();
	    r6=_mm256_setzero_si256();
	}
	else if (row==1){
		r0=_mm256_loadu_si256( (__m256i *) &frame1[0][col-3+7]);
		r1=_mm256_loadu_si256( (__m256i *) &frame1[1][col-3+7]);
		r2=_mm256_loadu_si256( (__m256i *) &frame1[2][col-3+7]);
		r3=_mm256_loadu_si256( (__m256i *) &frame1[3][col-3+7]);
		r4=_mm256_loadu_si256( (__m256i *) &frame1[4][col-3+7]);
	    r5=_mm256_setzero_si256();
	    r6=_mm256_setzero_si256();
	}
	else if (row==2){
		r0=_mm256_loadu_si256( (__m256i *) &frame1[0][col-3+7]);
		r1=_mm256_loadu_si256( (__m256i *) &frame1[1][col-3+7]);
		r2=_mm256_loadu_si256( (__m256i *) &frame1[2][col-3+7]);
		r3=_mm256_loadu_si256( (__m256i *) &frame1[3][col-3+7]);
		r4=_mm256_loadu_si256( (__m256i *) &frame1[4][col-3+7]);
		r5=_mm256_loadu_si256( (__m256i *) &frame1[5][col-3+7]);
	    r6=_mm256_setzero_si256();
	}
	else if (row==N-3){
	    r0=_mm256_setzero_si256();
	    r1=_mm256_loadu_si256( (__m256i *) &frame1[N-6][col-3+7]);
		r2=_mm256_loadu_si256( (__m256i *) &frame1[N-5][col-3+7]);
		r3=_mm256_loadu_si256( (__m256i *) &frame1[N-4][col-3+7]);
		r4=_mm256_loadu_si256( (__m256i *) &frame1[N-3][col-3+7]);
		r5=_mm256_loadu_si256( (__m256i *) &frame1[N-2][col-3+7]);
		r6=_mm256_loadu_si256( (__m256i *) &frame1[N-1][col-3+7]);
	}
	else if (row==N-2){
	    r0=_mm256_setzero_si256();
	    r1=_mm256_setzero_si256();
		r2=_mm256_loadu_si256( (__m256i *) &frame1[N-5][col-3+7]);
		r3=_mm256_loadu_si256( (__m256i *) &frame1[N-4][col-3+7]);
		r4=_mm256_loadu_si256( (__m256i *) &frame1[N-3][col-3+7]);
		r5=_mm256_loadu_si256( (__m256i *) &frame1[N-2][col-3+7]);
		r6=_mm256_loadu_si256( (__m256i *) &frame1[N-1][col-3+7]);
	}
	else if (row==N-1){
	    r0=_mm256_setzero_si256();
	    r1=_mm256_setzero_si256();
	    r2=_mm256_setzero_si256();
		r3=_mm256_loadu_si256( (__m256i *) &frame1[N-4][col-3+7]);
		r4=_mm256_loadu_si256( (__m256i *) &frame1[N-3][col-3+7]);
		r5=_mm256_loadu_si256( (__m256i *) &frame1[N-2][col-3+7]);
		r6=_mm256_loadu_si256( (__m256i *) &frame1[N-1][col-3+7]);
	}

	reminder_mask1=_mm256_load_si256( (__m256i *) &reminder_msk_7x7[REMINDER_ITERATIONS-8][0]);

	//zero the values that exceed the array bounds
	r0=_mm256_and_si256(r0,reminder_mask1);
	r1=_mm256_and_si256(r1,reminder_mask1);
	r2=_mm256_and_si256(r2,reminder_mask1);
	r3=_mm256_and_si256(r3,reminder_mask1);
	r4=_mm256_and_si256(r4,reminder_mask1);
	r5=_mm256_and_si256(r5,reminder_mask1);
	r6=_mm256_and_si256(r6,reminder_mask1);

	//multiply by the mask
	m0=_mm256_maddubs_epi16(r0,c0);
	m1=_mm256_maddubs_epi16(r1,c1);
	m2=_mm256_maddubs_epi16(r2,c2);
	m3=_mm256_maddubs_epi16(r3,c3);
	m4=_mm256_maddubs_epi16(r4,c4);
	m5=_mm256_maddubs_epi16(r5,c5);
	m6=_mm256_maddubs_epi16(r6,c6);


	//make 16-bit values 32-bit (horizontal add)
	m0=_mm256_madd_epi16(m0,ones);
	m1=_mm256_madd_epi16(m1,ones);
	m2=_mm256_madd_epi16(m2,ones);
	m3=_mm256_madd_epi16(m3,ones);
	m4=_mm256_madd_epi16(m4,ones);
	m5=_mm256_madd_epi16(m5,ones);
	m6=_mm256_madd_epi16(m6,ones);


	//vertical add
	m0=_mm256_add_epi32(m0,m1);
	m0=_mm256_add_epi32(m0,m2);
	m0=_mm256_add_epi32(m0,m3);
	m0=_mm256_add_epi32(m0,m4);
	m0=_mm256_add_epi32(m0,m5);
	m0=_mm256_add_epi32(m0,m6);


	//last horizontal add
	m1=_mm256_slli_si256(m0,4);//shift by one integer
	m0=_mm256_add_epi32(m0,m1);//add
	m1=_mm256_srli_si256(m0,4);//shift by one integer
	m0=_mm256_and_si256(m1,mask_even_32);//keep only 0,2,4,6 and discard 1,3,5,7


	//MERGE output1 with output5 and DIVIDE
	m0=_mm256_slli_si256(m0,4);//shift by one integer
	output4=_mm256_add_epi32(m0,output4);
}
output4 = division_32(division_case,output4,f);

	//blend output1, output2, output3, output4 into one register
	output2 = _mm256_slli_si256(output2,1);
	output1 = _mm256_add_epi8(output1,output2);
	output3 = _mm256_slli_si256(output3,2);
	output1 = _mm256_add_epi8(output1,output3);
	output4 = _mm256_slli_si256(output4,3);
	output1 = _mm256_add_epi8(output1,output4);

	//_mm256_stream_si256( (__m256i *) &filt[row][col],output1 );
	//_mm256_store_si256( (__m256i *) &filt[row][col],output1 );


	switch (REMINDER_ITERATIONS){

	case 4:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output1,3);
		  break;
	case 5:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output1,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output1,4);
		  break;
	case 6:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output1,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output1,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output1,5);
		  break;
	case 7:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output1,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output1,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output1,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output1,6);
		  break;
	case 8:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output1,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output1,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output1,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output1,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output1,7);
		  break;
	case 9:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output1,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output1,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output1,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output1,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output1,7);
		filt[row][col+8] = (unsigned char) _mm256_extract_epi8(output1,8);
		  break;
	case 10:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output1,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output1,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output1,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output1,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output1,7);
		filt[row][col+8] = (unsigned char) _mm256_extract_epi8(output1,8);
		filt[row][col+9] = (unsigned char) _mm256_extract_epi8(output1,9);
		  break;
	case 11:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output1,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output1,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output1,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output1,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output1,7);
		filt[row][col+8] = (unsigned char) _mm256_extract_epi8(output1,8);
		filt[row][col+9] = (unsigned char) _mm256_extract_epi8(output1,9);
		filt[row][col+10] = (unsigned char) _mm256_extract_epi8(output1,10);
		  break;
	case 12:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output1,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output1,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output1,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output1,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output1,7);
		filt[row][col+8] = (unsigned char) _mm256_extract_epi8(output1,8);
		filt[row][col+9] = (unsigned char) _mm256_extract_epi8(output1,9);
		filt[row][col+10] = (unsigned char) _mm256_extract_epi8(output1,10);
		filt[row][col+11] = (unsigned char) _mm256_extract_epi8(output1,11);
		  break;
	case 13:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output1,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output1,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output1,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output1,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output1,7);
		filt[row][col+8] = (unsigned char) _mm256_extract_epi8(output1,8);
		filt[row][col+9] = (unsigned char) _mm256_extract_epi8(output1,9);
		filt[row][col+10] = (unsigned char) _mm256_extract_epi8(output1,10);
		filt[row][col+11] = (unsigned char) _mm256_extract_epi8(output1,11);
		filt[row][col+12] = (unsigned char) _mm256_extract_epi8(output1,12);
		  break;
	case 14:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output1,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output1,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output1,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output1,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output1,7);
		filt[row][col+8] = (unsigned char) _mm256_extract_epi8(output1,8);
		filt[row][col+9] = (unsigned char) _mm256_extract_epi8(output1,9);
		filt[row][col+10] = (unsigned char) _mm256_extract_epi8(output1,10);
		filt[row][col+11] = (unsigned char) _mm256_extract_epi8(output1,11);
		filt[row][col+12] = (unsigned char) _mm256_extract_epi8(output1,12);
		filt[row][col+13] = (unsigned char) _mm256_extract_epi8(output1,13);
		  break;
	case 15:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output1,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output1,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output1,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output1,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output1,7);
		filt[row][col+8] = (unsigned char) _mm256_extract_epi8(output1,8);
		filt[row][col+9] = (unsigned char) _mm256_extract_epi8(output1,9);
		filt[row][col+10] = (unsigned char) _mm256_extract_epi8(output1,10);
		filt[row][col+11] = (unsigned char) _mm256_extract_epi8(output1,11);
		filt[row][col+12] = (unsigned char) _mm256_extract_epi8(output1,12);
		filt[row][col+13] = (unsigned char) _mm256_extract_epi8(output1,13);
		filt[row][col+14] = (unsigned char) _mm256_extract_epi8(output1,14);
		  break;
	case 16:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
		  break;
	case 17:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
		  break;
	case 18:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
		  break;
	case 19:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
		  break;
	case 20:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output1,19);
		  break;
	case 21:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output1,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output1,20);
		  break;
	case 22:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output1,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output1,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output1,21);
		break;
	case 23:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output1,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output1,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output1,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output1,22);
		  break;
	case 24:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output1,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output1,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output1,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output1,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output1,23);
		  break;
	case 25:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output1,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output1,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output1,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output1,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output1,23);
	filt[row][col+24] = (unsigned char) _mm256_extract_epi8(output1,24);
		  break;
	case 26:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output1,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output1,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output1,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output1,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output1,23);
	filt[row][col+24] = (unsigned char) _mm256_extract_epi8(output1,24);
	filt[row][col+25] = (unsigned char) _mm256_extract_epi8(output1,25);
		  break;
	case 27:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output1,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output1,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output1,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output1,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output1,23);
	filt[row][col+24] = (unsigned char) _mm256_extract_epi8(output1,24);
	filt[row][col+25] = (unsigned char) _mm256_extract_epi8(output1,25);
	filt[row][col+26] = (unsigned char) _mm256_extract_epi8(output1,26);
		  break;
	case 28:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output1,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output1,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output1,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output1,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output1,23);
	filt[row][col+24] = (unsigned char) _mm256_extract_epi8(output1,24);
	filt[row][col+25] = (unsigned char) _mm256_extract_epi8(output1,25);
	filt[row][col+26] = (unsigned char) _mm256_extract_epi8(output1,26);
	filt[row][col+27] = (unsigned char) _mm256_extract_epi8(output1,27);
		  break;
	case 29:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output1,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output1,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output1,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output1,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output1,23);
	filt[row][col+24] = (unsigned char) _mm256_extract_epi8(output1,24);
	filt[row][col+25] = (unsigned char) _mm256_extract_epi8(output1,25);
	filt[row][col+26] = (unsigned char) _mm256_extract_epi8(output1,26);
	filt[row][col+27] = (unsigned char) _mm256_extract_epi8(output1,27);
	filt[row][col+28] = (unsigned char) _mm256_extract_epi8(output1,28);
		  break;
	case 30:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output1,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output1,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output1,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output1,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output1,23);
	filt[row][col+24] = (unsigned char) _mm256_extract_epi8(output1,24);
	filt[row][col+25] = (unsigned char) _mm256_extract_epi8(output1,25);
	filt[row][col+26] = (unsigned char) _mm256_extract_epi8(output1,26);
	filt[row][col+27] = (unsigned char) _mm256_extract_epi8(output1,27);
	filt[row][col+28] = (unsigned char) _mm256_extract_epi8(output1,28);
	filt[row][col+29] = (unsigned char) _mm256_extract_epi8(output1,29);
		  break;
	case 31:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output1,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output1,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output1,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output1,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output1,23);
	filt[row][col+24] = (unsigned char) _mm256_extract_epi8(output1,24);
	filt[row][col+25] = (unsigned char) _mm256_extract_epi8(output1,25);
	filt[row][col+26] = (unsigned char) _mm256_extract_epi8(output1,26);
	filt[row][col+27] = (unsigned char) _mm256_extract_epi8(output1,27);
	filt[row][col+28] = (unsigned char) _mm256_extract_epi8(output1,28);
	filt[row][col+29] = (unsigned char) _mm256_extract_epi8(output1,29);
	filt[row][col+30] = (unsigned char) _mm256_extract_epi8(output1,30);
		  break;
	case 32:
		_mm256_store_si256( (__m256i *) &filt[row][col],output1 );
		  break;
	case 33:
		_mm256_store_si256( (__m256i *) &filt[row][col],output1 );

		filt[row][col+32] = (unsigned char) (scalar_iterations_4(frame1,  N, row,  col, filter7x7) / divisor);

		  break;
	case 34:
		_mm256_store_si256( (__m256i *) &filt[row][col],output1 );

		filt[row][col+32] = (unsigned char) (scalar_iterations_5(frame1,  N, row,  col, filter7x7) / divisor );
		filt[row][col+33] = (unsigned char) (scalar_iterations_4(frame1,  N, row,  col+1, filter7x7) / divisor);

		  break;
	case 35:
		_mm256_store_si256( (__m256i *) &filt[row][col],output1 );

		filt[row][col+32] = (unsigned char) (scalar_iterations_6(frame1,  N, row,  col, filter7x7) / divisor );
		filt[row][col+33] = (unsigned char) (scalar_iterations_5(frame1,  N, row,  col+1, filter7x7) / divisor );
		filt[row][col+34] = (unsigned char) (scalar_iterations_4(frame1,  N, row,  col+2, filter7x7) / divisor);

		  break;
	default:
		printf("\nsomething went wrong");
		exit(EXIT_FAILURE);
 	}




}


int scalar_iterations_4(unsigned char **frame1, const unsigned int N,const unsigned int row_g, const unsigned int col, signed char **filter7x7){

    int newPixel = 0;
    int row=row_g;

    if ((row-3)>=0){
    newPixel += frame1[row-3][col-3+32] * filter7x7[0][0];
    newPixel += frame1[row-3][col-2+32] * filter7x7[0][1];
    newPixel += frame1[row-3][col-1+32] * filter7x7[0][2];
    newPixel += frame1[row-3][col-0+32] * filter7x7[0][3];
    }

    if ((row-2)>=0){
    newPixel += frame1[row-2][col-3+32] * filter7x7[1][0];
    newPixel += frame1[row-2][col-2+32] * filter7x7[1][1];
    newPixel += frame1[row-2][col-1+32] * filter7x7[1][2];
    newPixel += frame1[row-2][col-0+32] * filter7x7[1][3];
    }
    if ((row-1)>=0){
    newPixel += frame1[row-1][col-3+32] * filter7x7[2][0];
    newPixel += frame1[row-1][col-2+32] * filter7x7[2][1];
    newPixel += frame1[row-1][col-1+32] * filter7x7[2][2];
    newPixel += frame1[row-1][col-0+32] * filter7x7[2][3];
    }

    newPixel += frame1[row-0][col-3+32] * filter7x7[3][0];
    newPixel += frame1[row-0][col-2+32] * filter7x7[3][1];
    newPixel += frame1[row-0][col-1+32] * filter7x7[3][2];
    newPixel += frame1[row-0][col-0+32] * filter7x7[3][3];

    if ((row+1)<N){
    newPixel += frame1[row+1][col-3+32] * filter7x7[4][0];
    newPixel += frame1[row+1][col-2+32] * filter7x7[4][1];
    newPixel += frame1[row+1][col-1+32] * filter7x7[4][2];
    newPixel += frame1[row+1][col-0+32] * filter7x7[4][3];
    }
    if ((row+2)<N){
    newPixel += frame1[row+2][col-3+32] * filter7x7[5][0];
    newPixel += frame1[row+2][col-2+32] * filter7x7[5][1];
    newPixel += frame1[row+2][col-1+32] * filter7x7[5][2];
    newPixel += frame1[row+2][col-0+32] * filter7x7[5][3];
    }
    if ((row+3)<N){
    newPixel += frame1[row+3][col-3+32] * filter7x7[6][0];
    newPixel += frame1[row+3][col-2+32] * filter7x7[6][1];
    newPixel += frame1[row+3][col-1+32] * filter7x7[6][2];
    newPixel += frame1[row+3][col-0+32] * filter7x7[6][3];
    }

    return newPixel;
}

int scalar_iterations_5(unsigned char **frame1, const unsigned int N,const unsigned int row_g, const unsigned int col, signed char **filter7x7){

int newPixel = 0;
int row=row_g;

if (row-3>=0){
newPixel += frame1[row-3][col-3+32] * filter7x7[0][0];
newPixel += frame1[row-3][col-2+32] * filter7x7[0][1];
newPixel += frame1[row-3][col-1+32] * filter7x7[0][2];
newPixel += frame1[row-3][col-0+32] * filter7x7[0][3];
newPixel += frame1[row-3][col+1+32] * filter7x7[0][4];
}
if (row-2>=0){
newPixel += frame1[row-2][col-3+32] * filter7x7[1][0];
newPixel += frame1[row-2][col-2+32] * filter7x7[1][1];
newPixel += frame1[row-2][col-1+32] * filter7x7[1][2];
newPixel += frame1[row-2][col-0+32] * filter7x7[1][3];
newPixel += frame1[row-2][col+1+32] * filter7x7[1][4];
}
if (row-1>=0){
newPixel += frame1[row-1][col-3+32] * filter7x7[2][0];
newPixel += frame1[row-1][col-2+32] * filter7x7[2][1];
newPixel += frame1[row-1][col-1+32] * filter7x7[2][2];
newPixel += frame1[row-1][col-0+32] * filter7x7[2][3];
newPixel += frame1[row-1][col+1+32] * filter7x7[2][4];
}

newPixel += frame1[row-0][col-3+32] * filter7x7[3][0];
newPixel += frame1[row-0][col-2+32] * filter7x7[3][1];
newPixel += frame1[row-0][col-1+32] * filter7x7[3][2];
newPixel += frame1[row-0][col-0+32] * filter7x7[3][3];
newPixel += frame1[row-0][col+1+32] * filter7x7[3][4];

if (row+1<N){
newPixel += frame1[row+1][col-3+32] * filter7x7[4][0];
newPixel += frame1[row+1][col-2+32] * filter7x7[4][1];
newPixel += frame1[row+1][col-1+32] * filter7x7[4][2];
newPixel += frame1[row+1][col-0+32] * filter7x7[4][3];
newPixel += frame1[row+1][col+1+32] * filter7x7[4][4];
}
if (row+2<N){
newPixel += frame1[row+2][col-3+32] * filter7x7[5][0];
newPixel += frame1[row+2][col-2+32] * filter7x7[5][1];
newPixel += frame1[row+2][col-1+32] * filter7x7[5][2];
newPixel += frame1[row+2][col-0+32] * filter7x7[5][3];
newPixel += frame1[row+2][col+1+32] * filter7x7[5][4];
}
if (row+3<N){
newPixel += frame1[row+3][col-3+32] * filter7x7[6][0];
newPixel += frame1[row+3][col-2+32] * filter7x7[6][1];
newPixel += frame1[row+3][col-1+32] * filter7x7[6][2];
newPixel += frame1[row+3][col-0+32] * filter7x7[6][3];
newPixel += frame1[row+3][col+1+32] * filter7x7[6][4];
}

return newPixel;
}

int scalar_iterations_6(unsigned char **frame1, const unsigned int N,const unsigned int row_g, const unsigned int col, signed char **filter7x7){

int newPixel = 0;
int row=row_g;

newPixel = 0;
if (row-3>=0){
newPixel += frame1[row-3][col-3+32] * filter7x7[0][0];
newPixel += frame1[row-3][col-2+32] * filter7x7[0][1];
newPixel += frame1[row-3][col-1+32] * filter7x7[0][2];
newPixel += frame1[row-3][col-0+32] * filter7x7[0][3];
newPixel += frame1[row-3][col+1+32] * filter7x7[0][4];
newPixel += frame1[row-3][col+2+32] * filter7x7[0][5];
}
if (row-2>=0){
newPixel += frame1[row-2][col-3+32] * filter7x7[1][0];
newPixel += frame1[row-2][col-2+32] * filter7x7[1][1];
newPixel += frame1[row-2][col-1+32] * filter7x7[1][2];
newPixel += frame1[row-2][col-0+32] * filter7x7[1][3];
newPixel += frame1[row-2][col+1+32] * filter7x7[1][4];
newPixel += frame1[row-2][col+2+32] * filter7x7[1][5];
}
if (row-1>=0){
newPixel += frame1[row-1][col-3+32] * filter7x7[2][0];
newPixel += frame1[row-1][col-2+32] * filter7x7[2][1];
newPixel += frame1[row-1][col-1+32] * filter7x7[2][2];
newPixel += frame1[row-1][col-0+32] * filter7x7[2][3];
newPixel += frame1[row-1][col+1+32] * filter7x7[2][4];
newPixel += frame1[row-1][col+2+32] * filter7x7[2][5];
}

newPixel += frame1[row-0][col-3+32] * filter7x7[3][0];
newPixel += frame1[row-0][col-2+32] * filter7x7[3][1];
newPixel += frame1[row-0][col-1+32] * filter7x7[3][2];
newPixel += frame1[row-0][col-0+32] * filter7x7[3][3];
newPixel += frame1[row-0][col+1+32] * filter7x7[3][4];
newPixel += frame1[row-0][col+2+32] * filter7x7[3][5];

if (row+1<N){
newPixel += frame1[row+1][col-3+32] * filter7x7[4][0];
newPixel += frame1[row+1][col-2+32] * filter7x7[4][1];
newPixel += frame1[row+1][col-1+32] * filter7x7[4][2];
newPixel += frame1[row+1][col-0+32] * filter7x7[4][3];
newPixel += frame1[row+1][col+1+32] * filter7x7[4][4];
newPixel += frame1[row+1][col+2+32] * filter7x7[4][5];
}

if (row+2<N){
newPixel += frame1[row+2][col-3+32] * filter7x7[5][0];
newPixel += frame1[row+2][col-2+32] * filter7x7[5][1];
newPixel += frame1[row+2][col-1+32] * filter7x7[5][2];
newPixel += frame1[row+2][col-0+32] * filter7x7[5][3];
newPixel += frame1[row+2][col+1+32] * filter7x7[5][4];
newPixel += frame1[row+2][col+2+32] * filter7x7[5][5];
}
if (row+3<N){
newPixel += frame1[row+3][col-3+32] * filter7x7[6][0];
newPixel += frame1[row+3][col-2+32] * filter7x7[6][1];
newPixel += frame1[row+3][col-1+32] * filter7x7[6][2];
newPixel += frame1[row+3][col-0+32] * filter7x7[6][3];
newPixel += frame1[row+3][col+1+32] * filter7x7[6][4];
newPixel += frame1[row+3][col+2+32] * filter7x7[6][5];
}

return newPixel;
}




void convolution_optimized_9x9_32(unsigned char **frame1,unsigned char **filt,const unsigned int M, const unsigned int N, const unsigned short int divisor, signed char **kernel){

const signed char f00=kernel[0][0];
const signed char f01=kernel[0][1];
const signed char f02=kernel[0][2];
const signed char f03=kernel[0][3];
const signed char f04=kernel[0][4];
const signed char f05=kernel[0][5];
const signed char f06=kernel[0][6];
const signed char f07=kernel[0][7];
const signed char f08=kernel[0][8];

const signed char f10=kernel[1][0];
const signed char f11=kernel[1][1];
const signed char f12=kernel[1][2];
const signed char f13=kernel[1][3];
const signed char f14=kernel[1][4];
const signed char f15=kernel[1][5];
const signed char f16=kernel[1][6];
const signed char f17=kernel[1][7];
const signed char f18=kernel[1][8];

const signed char f20=kernel[2][0];
const signed char f21=kernel[2][1];
const signed char f22=kernel[2][2];
const signed char f23=kernel[2][3];
const signed char f24=kernel[2][4];
const signed char f25=kernel[2][5];
const signed char f26=kernel[2][6];
const signed char f27=kernel[2][7];
const signed char f28=kernel[2][8];

const signed char f30=kernel[3][0];
const signed char f31=kernel[3][1];
const signed char f32=kernel[3][2];
const signed char f33=kernel[3][3];
const signed char f34=kernel[3][4];
const signed char f35=kernel[3][5];
const signed char f36=kernel[3][6];
const signed char f37=kernel[3][7];
const signed char f38=kernel[3][8];

const signed char f40=kernel[4][0];
const signed char f41=kernel[4][1];
const signed char f42=kernel[4][2];
const signed char f43=kernel[4][3];
const signed char f44=kernel[4][4];
const signed char f45=kernel[4][5];
const signed char f46=kernel[4][6];
const signed char f47=kernel[4][7];
const signed char f48=kernel[4][8];

const signed char f50=kernel[5][0];
const signed char f51=kernel[5][1];
const signed char f52=kernel[5][2];
const signed char f53=kernel[5][3];
const signed char f54=kernel[5][4];
const signed char f55=kernel[5][5];
const signed char f56=kernel[5][6];
const signed char f57=kernel[5][7];
const signed char f58=kernel[5][8];

const signed char f60=kernel[6][0];
const signed char f61=kernel[6][1];
const signed char f62=kernel[6][2];
const signed char f63=kernel[6][3];
const signed char f64=kernel[6][4];
const signed char f65=kernel[6][5];
const signed char f66=kernel[6][6];
const signed char f67=kernel[6][7];
const signed char f68=kernel[6][8];

const signed char f70=kernel[7][0];
const signed char f71=kernel[7][1];
const signed char f72=kernel[7][2];
const signed char f73=kernel[7][3];
const signed char f74=kernel[7][4];
const signed char f75=kernel[7][5];
const signed char f76=kernel[7][6];
const signed char f77=kernel[7][7];
const signed char f78=kernel[7][8];

const signed char f80=kernel[8][0];
const signed char f81=kernel[8][1];
const signed char f82=kernel[8][2];
const signed char f83=kernel[8][3];
const signed char f84=kernel[8][4];
const signed char f85=kernel[8][5];
const signed char f86=kernel[8][6];
const signed char f87=kernel[8][7];
const signed char f88=kernel[8][8];

const signed char mask_vector[9][32] __attribute__((aligned(64))) ={
	{f00,f01,f02,f03,f04,f05,f06,f07,f08,0,f00,f01,f02,f03,f04,f05,f06,f07,f08,0,f00,f01,f02,f03,f04,f05,f06,f07,f08,0,0,0},
	{f10,f11,f12,f13,f14,f15,f16,f17,f18,0,f10,f11,f12,f13,f14,f15,f16,f17,f18,0,f10,f11,f12,f13,f14,f15,f16,f17,f18,0,0,0},
	{f20,f21,f22,f23,f24,f25,f26,f27,f28,0,f20,f21,f22,f23,f24,f25,f26,f27,f28,0,f20,f21,f22,f23,f24,f25,f26,f27,f28,0,0,0},
	{f30,f31,f32,f33,f34,f35,f36,f37,f38,0,f30,f31,f32,f33,f34,f35,f36,f37,f38,0,f30,f31,f32,f33,f34,f35,f36,f37,f38,0,0,0},
	{f40,f41,f42,f43,f44,f45,f46,f47,f48,0,f40,f41,f42,f43,f44,f45,f46,f47,f48,0,f40,f41,f42,f43,f44,f45,f46,f47,f48,0,0,0},
	{f50,f51,f52,f53,f54,f55,f56,f57,f58,0,f50,f51,f52,f53,f54,f55,f56,f57,f58,0,f50,f51,f52,f53,f54,f55,f56,f57,f58,0,0,0},
	{f60,f61,f62,f63,f64,f65,f66,f67,f68,0,f60,f61,f62,f63,f64,f65,f66,f67,f68,0,f60,f61,f62,f63,f64,f65,f66,f67,f68,0,0,0},
	{f70,f71,f72,f73,f74,f75,f76,f77,f78,0,f70,f71,f72,f73,f74,f75,f76,f77,f78,0,f70,f71,f72,f73,f74,f75,f76,f77,f78,0,0,0},
	{f80,f81,f82,f83,f84,f85,f86,f87,f88,0,f80,f81,f82,f83,f84,f85,f86,f87,f88,0,f80,f81,f82,f83,f84,f85,f86,f87,f88,0,0,0},

};


	const __m256i c0=_mm256_load_si256( (__m256i *) &mask_vector[0][0]);
	const __m256i c1=_mm256_load_si256( (__m256i *) &mask_vector[1][0]);
	const __m256i c2=_mm256_load_si256( (__m256i *) &mask_vector[2][0]);
	const __m256i c3=_mm256_load_si256( (__m256i *) &mask_vector[3][0]);
	const __m256i c4=_mm256_load_si256( (__m256i *) &mask_vector[4][0]);
	const __m256i c5=_mm256_load_si256( (__m256i *) &mask_vector[5][0]);
	const __m256i c6=_mm256_load_si256( (__m256i *) &mask_vector[6][0]);
	const __m256i c7=_mm256_load_si256( (__m256i *) &mask_vector[7][0]);
	const __m256i c8=_mm256_load_si256( (__m256i *) &mask_vector[8][0]);


	const __m256i mask_z    = _mm256_set_epi16(0,0,0,0,0,0,0,0,0,0,0,65535,0,0,0,0);
	const __m256i mask_unz    = _mm256_set_epi16(65535,65535,65535,65535,65535,65535,65535,65535,65535,65535,65535,0,65535,65535,65535,65535);
	const __m256i mask_32_1    = _mm256_set_epi32(0xffffffff,0,0,0xffffffff,0,0,0,0);
	const __m256i mask_32_2    = _mm256_set_epi32(0,0,0,0xffffffff,0,0,0,0);
	const __m256i mask_32_a    = _mm256_set_epi32(0,0,0xffffffff,0,0,0xffffffff,0,0xffffffff);
	const __m256i out_mask_1    = _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255);//0,20
	const __m256i out_mask_2    = _mm256_set_epi8(0,0,0,0,0,0,0,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255,0,0,0,0);//4,24
	const __m256i out_mask_3    = _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, 0,0,0,0,0,0,0,255,0,0,0,0,0,0,0,0);//8
	const __m256i out_mask_4    = _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, 0,0,0,255,0,0,0,0,0,0,0,0,0,0,0,0);//12
	const __m256i out_mask_5    = _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, 0,0,0,0,255,255,0,0,0,0,0,0,0,0,0,0);//10,11


	const __m256i mask_prelude   = _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0);
	const __m256i mask_prelude1  = _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0);
	const __m256i mask_prelude2  = _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0);
	const __m256i mask_prelude3  = _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0);
	const __m256i ones=_mm256_set1_epi16(1);


	//const __m256i mask_even_32  = _mm256_set_epi32(0,0xffffffff,0,0xffffffff,0,0xffffffff,0,0xffffffff);




	const unsigned int REMINDER_ITERATIONS =  (M-((((M-37)/30)*30)+30));
	//printf("\nreminder iterations %d",REMINDER_ITERATIONS);


	const unsigned int division_case=prepare_for_division_32(divisor); //determine which is the division case (A, B or C)
	const __m256i f = _mm256_load_si256( (__m256i *) &f_vector[0]);// initialize the division vector

//printf("\n%d ",division_case);


#pragma omp parallel
{

unsigned int row,col;
__m256i r0,r1,r2,r3,r4,r5,r6,r7,r8,m0,m1,m2,m3,m4,m5,m6,m7,m8,m0_copy,m1_copy,m2_copy,m3_copy,m4_copy,m5_copy,m6_copy,m7_copy,m8_copy;
__m256i rr0,rr1,rr2,rr3,rr4,rr5,rr6,rr7,rr8,output1,output2,output3,output4,output5,out1,out2;

/*---------------------- Gaussian Blur ---------------------------------*/

    #pragma omp for schedule(static)
	for (row = 3; row < N-3; row++) {

		if (row==3){
			prelude_block_0_9x9(frame1,filt,0,0,M, division_case, f, c4,c5,c6,c7,c8);
			prelude_block_1_9x9(frame1,filt,1,0,M, division_case, f, c3,c4,c5,c6,c7,c8);
			prelude_block_2_9x9(frame1,filt,2,0,M, division_case, f, c2,c3,c4,c5,c6,c7,c8);
			prelude_block_3_9x9(frame1,filt,3,0,M, division_case, f, c1,c2,c3,c4,c5,c6,c7,c8);

			loop_reminder_9x9_row_boundaries(frame1,filt,0, N, M, REMINDER_ITERATIONS, division_case,mask_vector, f,divisor,kernel);
			loop_reminder_9x9_row_boundaries(frame1,filt,1, N, M, REMINDER_ITERATIONS, division_case,mask_vector, f,divisor,kernel);
			loop_reminder_9x9_row_boundaries(frame1,filt,2, N, M, REMINDER_ITERATIONS, division_case,mask_vector, f,divisor,kernel);
			loop_reminder_9x9_row_boundaries(frame1,filt,3, N, M, REMINDER_ITERATIONS, division_case,mask_vector, f,divisor,kernel);

			}


		else if (row==N-4){
			prelude_block_3_9x9(frame1,filt,N-4,N-8,M, division_case, f, c0,c1,c2,c3,c4,c5,c6,c7);
			prelude_block_2_9x9(frame1,filt,N-3,N-7,M, division_case, f, c0,c1,c2,c3,c4,c5,c6);
			prelude_block_1_9x9(frame1,filt,N-2,N-6,M, division_case, f, c0,c1,c2,c3,c4,c5);
			prelude_block_0_9x9(frame1,filt,N-1,N-5,M, division_case, f, c0,c1,c2,c3,c4);

			loop_reminder_9x9_row_boundaries(frame1,filt,N-4, N, M, REMINDER_ITERATIONS, division_case,mask_vector, f,divisor,kernel);
			loop_reminder_9x9_row_boundaries(frame1,filt,N-3, N, M, REMINDER_ITERATIONS, division_case,mask_vector, f,divisor,kernel);
			loop_reminder_9x9_row_boundaries(frame1,filt,N-2, N, M, REMINDER_ITERATIONS, division_case,mask_vector, f,divisor,kernel);
			loop_reminder_9x9_row_boundaries(frame1,filt,N-1, N, M, REMINDER_ITERATIONS, division_case,mask_vector, f,divisor,kernel);

			}



	  else { //main loop


	  for (col = 0; col <= M-32-5; col+=30){

		  if (col==0){
			  //1st col
			rr0=_mm256_load_si256( (__m256i *) &frame1[row-4][0]);
			rr1=_mm256_load_si256( (__m256i *) &frame1[row-3][0]);
			rr2=_mm256_load_si256( (__m256i *) &frame1[row-2][0]);
			rr3=_mm256_load_si256( (__m256i *) &frame1[row-1][0]);
			rr4=_mm256_load_si256( (__m256i *) &frame1[row][0]);
			rr5=_mm256_load_si256( (__m256i *) &frame1[row+1][0]);
			rr6=_mm256_load_si256( (__m256i *) &frame1[row+2][0]);
			rr7=_mm256_load_si256( (__m256i *) &frame1[row+3][0]);
			rr8=_mm256_load_si256( (__m256i *) &frame1[row+4][0]);

			//START - extra code needed for prelude
		 r0=insert_four_zeros_front(rr0,mask_prelude);
		 r1=insert_four_zeros_front(rr1,mask_prelude);
		 r2=insert_four_zeros_front(rr2,mask_prelude);
		 r3=insert_four_zeros_front(rr3,mask_prelude);
		 r4=insert_four_zeros_front(rr4,mask_prelude);
		 r5=insert_four_zeros_front(rr5,mask_prelude);
		 r6=insert_four_zeros_front(rr6,mask_prelude);
		 r7=insert_four_zeros_front(rr7,mask_prelude);
		 r8=insert_four_zeros_front(rr8,mask_prelude);

		out1=main_block_pre_9x9(r0,r1,r2,r3,r4,r5,r6,r7,r8,c0,c1,c2,c3,c4,c5,c6,c7,c8);

		  //2nd col

			//START - extra code needed for prelude
		  r0=insert_three_zeros_front(rr0,mask_prelude1);
		  r1=insert_three_zeros_front(rr1,mask_prelude1);
		  r2=insert_three_zeros_front(rr2,mask_prelude1);
		  r3=insert_three_zeros_front(rr3,mask_prelude1);
		  r4=insert_three_zeros_front(rr4,mask_prelude1);
		  r5=insert_three_zeros_front(rr5,mask_prelude1);
		  r6=insert_three_zeros_front(rr6,mask_prelude1);
		  r7=insert_three_zeros_front(rr7,mask_prelude1);
		  r8=insert_three_zeros_front(rr8,mask_prelude1);

		out2=main_block_pre_9x9(r0,r1,r2,r3,r4,r5,r6,r7,r8,c0,c1,c2,c3,c4,c5,c6,c7,c8);

			//merge out1 and out2 and then divide
			out2=_mm256_slli_si256(out2,4);
			out1=_mm256_add_epi32(out1,out2);

			out1 = division_32(division_case,out1,f);

			//elements must be re-arranged before store
			m0=_mm256_and_si256(out1,out_mask_1);
			m1=_mm256_and_si256(out1,out_mask_2);
			m1=_mm256_srli_si256(m1,3);
			m2=_mm256_and_si256(out1,out_mask_3);
			m2=_mm256_slli_si256(m2,2);
			m3=_mm256_and_si256(out1,out_mask_4);
			m3=_mm256_srli_si256(m3,1);
			m0=_mm256_add_epi8(m0,m1);
			m0=_mm256_add_epi8(m0,m2);
			output1=_mm256_add_epi8(m0,m3);

		//3rd col
		//START - extra code needed for prelude
		r0=insert_two_zeros_front(rr0,mask_prelude2);
		r1=insert_two_zeros_front(rr1,mask_prelude2);
		r2=insert_two_zeros_front(rr2,mask_prelude2);
		r3=insert_two_zeros_front(rr3,mask_prelude2);
		r4=insert_two_zeros_front(rr4,mask_prelude2);
		r5=insert_two_zeros_front(rr5,mask_prelude2);
		r6=insert_two_zeros_front(rr6,mask_prelude2);
		r7=insert_two_zeros_front(rr7,mask_prelude2);
		r8=insert_two_zeros_front(rr8,mask_prelude2);

		out1=main_block_pre_9x9(r0,r1,r2,r3,r4,r5,r6,r7,r8,c0,c1,c2,c3,c4,c5,c6,c7,c8);

		//4th col
		r0=insert_one_zeros_front(rr0,mask_prelude3);
		r1=insert_one_zeros_front(rr1,mask_prelude3);
		r2=insert_one_zeros_front(rr2,mask_prelude3);
		r3=insert_one_zeros_front(rr3,mask_prelude3);
		r4=insert_one_zeros_front(rr4,mask_prelude3);
		r5=insert_one_zeros_front(rr5,mask_prelude3);
		r6=insert_one_zeros_front(rr6,mask_prelude3);
		r7=insert_one_zeros_front(rr7,mask_prelude3);
		r8=insert_one_zeros_front(rr8,mask_prelude3);
		out2 = main_block_pre_9x9(r0,r1,r2,r3,r4,r5,r6,r7,r8,c0,c1,c2,c3,c4,c5,c6,c7,c8);

		//merge out1 and out2 and then divide
		out2=_mm256_slli_si256(out2,4);
		out1=_mm256_add_epi32(out1,out2);

		out1 = division_32(division_case,out1,f);

		//elements must be re-arranged before store
		m0=_mm256_and_si256(out1,out_mask_1);
		m1=_mm256_and_si256(out1,out_mask_2);
		m1=_mm256_srli_si256(m1,3);
		m2=_mm256_and_si256(out1,out_mask_3);
		m2=_mm256_slli_si256(m2,2);
		m3=_mm256_and_si256(out1,out_mask_4);
		m3=_mm256_srli_si256(m3,1);
		m0=_mm256_add_epi8(m0,m1);
		m0=_mm256_add_epi8(m0,m2);
		output2=_mm256_add_epi8(m0,m3);

		//5th col iteration
		out1 = main_block_pre_9x9(rr0,rr1,rr2,rr3,rr4,rr5,rr6,rr7,rr8,c0,c1,c2,c3,c4,c5,c6,c7,c8);

		//6th col iteration
		rr0=_mm256_loadu_si256( (__m256i *) &frame1[row-4][1]);
		rr1=_mm256_loadu_si256( (__m256i *) &frame1[row-3][1]);
		rr2=_mm256_loadu_si256( (__m256i *) &frame1[row-2][1]);
		rr3=_mm256_loadu_si256( (__m256i *) &frame1[row-1][1]);
		rr4=_mm256_loadu_si256( (__m256i *) &frame1[row][1]);
		rr5=_mm256_loadu_si256( (__m256i *) &frame1[row+1][1]);
		rr6=_mm256_loadu_si256( (__m256i *) &frame1[row+2][1]);
		rr7=_mm256_loadu_si256( (__m256i *) &frame1[row+3][1]);
		rr8=_mm256_loadu_si256( (__m256i *) &frame1[row+4][1]);

		out2 = main_block_pre_9x9(rr0,rr1,rr2,rr3,rr4,rr5,rr6,rr7,rr8,c0,c1,c2,c3,c4,c5,c6,c7,c8);

		//merge out1 and out2 and then divide
		out2=_mm256_slli_si256(out2,4);
		out1=_mm256_add_epi32(out1,out2);

		out1 = division_32(division_case,out1,f);

		//elements must be re-arranged before store
		m0=_mm256_and_si256(out1,out_mask_1);
		m1=_mm256_and_si256(out1,out_mask_2);
		m1=_mm256_srli_si256(m1,3);
		m2=_mm256_and_si256(out1,out_mask_3);
		m2=_mm256_slli_si256(m2,2);
		m3=_mm256_and_si256(out1,out_mask_4);
		m3=_mm256_srli_si256(m3,1);
		m0=_mm256_add_epi8(m0,m1);
		m0=_mm256_add_epi8(m0,m2);
		output3=_mm256_add_epi8(m0,m3);

		//7th col iteration
		rr0=_mm256_loadu_si256( (__m256i *) &frame1[row-4][2]);
		rr1=_mm256_loadu_si256( (__m256i *) &frame1[row-3][2]);
		rr2=_mm256_loadu_si256( (__m256i *) &frame1[row-2][2]);
		rr3=_mm256_loadu_si256( (__m256i *) &frame1[row-1][2]);
		rr4=_mm256_loadu_si256( (__m256i *) &frame1[row][2]);
		rr5=_mm256_loadu_si256( (__m256i *) &frame1[row+1][2]);
		rr6=_mm256_loadu_si256( (__m256i *) &frame1[row+2][2]);
		rr7=_mm256_loadu_si256( (__m256i *) &frame1[row+3][2]);
		rr8=_mm256_loadu_si256( (__m256i *) &frame1[row+4][2]);

		out1 = main_block_pre_9x9(rr0,rr1,rr2,rr3,rr4,rr5,rr6,rr7,rr8,c0,c1,c2,c3,c4,c5,c6,c7,c8);

		//8th col iteration
		rr0=_mm256_loadu_si256( (__m256i *) &frame1[row-4][3]);
		rr1=_mm256_loadu_si256( (__m256i *) &frame1[row-3][3]);
		rr2=_mm256_loadu_si256( (__m256i *) &frame1[row-2][3]);
		rr3=_mm256_loadu_si256( (__m256i *) &frame1[row-1][3]);
		rr4=_mm256_loadu_si256( (__m256i *) &frame1[row][3]);
		rr5=_mm256_loadu_si256( (__m256i *) &frame1[row+1][3]);
		rr6=_mm256_loadu_si256( (__m256i *) &frame1[row+2][3]);
		rr7=_mm256_loadu_si256( (__m256i *) &frame1[row+3][3]);
		rr8=_mm256_loadu_si256( (__m256i *) &frame1[row+4][3]);

		out2 = main_block_pre_9x9(rr0,rr1,rr2,rr3,rr4,rr5,rr6,rr7,rr8,c0,c1,c2,c3,c4,c5,c6,c7,c8);

		//merge out1 and out2 and then divide
		out2=_mm256_slli_si256(out2,4);
		out1=_mm256_add_epi32(out1,out2);

		out1 = division_32(division_case,out1,f);

		//elements must be re-arranged before store
		m0=_mm256_and_si256(out1,out_mask_1);
		m1=_mm256_and_si256(out1,out_mask_2);
		m1=_mm256_srli_si256(m1,3);
		m2=_mm256_and_si256(out1,out_mask_3);
		m2=_mm256_slli_si256(m2,2);
		m3=_mm256_and_si256(out1,out_mask_4);
		m3=_mm256_srli_si256(m3,1);
		m0=_mm256_add_epi8(m0,m1);
		m0=_mm256_add_epi8(m0,m2);
		output4=_mm256_add_epi8(m0,m3);

		//9th col iteration
		rr0=_mm256_loadu_si256( (__m256i *) &frame1[row-4][4]);
		rr1=_mm256_loadu_si256( (__m256i *) &frame1[row-3][4]);
		rr2=_mm256_loadu_si256( (__m256i *) &frame1[row-2][4]);
		rr3=_mm256_loadu_si256( (__m256i *) &frame1[row-1][4]);
		rr4=_mm256_loadu_si256( (__m256i *) &frame1[row][4]);
		rr5=_mm256_loadu_si256( (__m256i *) &frame1[row+1][4]);
		rr6=_mm256_loadu_si256( (__m256i *) &frame1[row+2][4]);
		rr7=_mm256_loadu_si256( (__m256i *) &frame1[row+3][4]);
		rr8=_mm256_loadu_si256( (__m256i *) &frame1[row+4][4]);

		out1 = main_block_pre_9x9(rr0,rr1,rr2,rr3,rr4,rr5,rr6,rr7,rr8,c0,c1,c2,c3,c4,c5,c6,c7,c8);

		//10th col iteration
		rr0=_mm256_loadu_si256( (__m256i *) &frame1[row-4][5]);
		rr1=_mm256_loadu_si256( (__m256i *) &frame1[row-3][5]);
		rr2=_mm256_loadu_si256( (__m256i *) &frame1[row-2][5]);
		rr3=_mm256_loadu_si256( (__m256i *) &frame1[row-1][5]);
		rr4=_mm256_loadu_si256( (__m256i *) &frame1[row][5]);
		rr5=_mm256_loadu_si256( (__m256i *) &frame1[row+1][5]);
		rr6=_mm256_loadu_si256( (__m256i *) &frame1[row+2][5]);
		rr7=_mm256_loadu_si256( (__m256i *) &frame1[row+3][5]);
		rr8=_mm256_loadu_si256( (__m256i *) &frame1[row+4][5]);

		out2 = main_block_pre_9x9(rr0,rr1,rr2,rr3,rr4,rr5,rr6,rr7,rr8,c0,c1,c2,c3,c4,c5,c6,c7,c8);

		//merge out1 and out2 and then divide
		out2=_mm256_slli_si256(out2,4);
		out1=_mm256_add_epi32(out1,out2);

		out1 = division_32(division_case,out1,f);

		//elements must be re-arranged before store
		m0=_mm256_and_si256(out1,out_mask_1);
		m1=_mm256_and_si256(out1,out_mask_2);
		m1=_mm256_srli_si256(m1,3);
		m2=_mm256_and_si256(out1,out_mask_3);
		m2=_mm256_slli_si256(m2,2);
		m3=_mm256_and_si256(out1,out_mask_4);
		m3=_mm256_srli_si256(m3,1);
		m0=_mm256_add_epi8(m0,m1);
		m0=_mm256_add_epi8(m0,m2);
		output5=_mm256_add_epi8(m0,m3);

			//blend the output1:output5
  			output2=_mm256_slli_si256(output2,2);
  			output3=_mm256_slli_si256(output3,4);

  			m3=_mm256_slli_si256(output4,6);
  			m4=_mm256_and_si256(output4,out_mask_5);
  			m4=_mm256_permute2f128_si256(m4,m4,1);
  			m4=_mm256_srli_si256(m4,10);
  			output4=_mm256_add_epi32(m3,m4);

  			m3=_mm256_slli_si256(output5,8);
  			m4=_mm256_and_si256(output5,out_mask_5);
  			m4=_mm256_permute2f128_si256(m4,m4,1);
  			m4=_mm256_srli_si256(m4,8);
  			output5=_mm256_add_epi32(m3,m4);

  			output1=_mm256_add_epi8(output1,output2);
  			output1=_mm256_add_epi8(output1,output3);
  			output1=_mm256_add_epi8(output1,output4);
  			output1=_mm256_add_epi8(output1,output5);

				_mm256_store_si256( (__m256i *) &filt[row][0],output1 );
		  		}
		  else {

		  		// col iteration computes output pixels of   0,10,20
		  		// col+1 iteration computes output pixels of 1,11,21
		  		// col+2 iteration computes output pixels of 2,12,22
		  		// col+3 iteration computes output pixels of 3,13,23
		  		// col+4 iteration computes output pixels of 4,14,24
		  		// col+5 iteration computes output pixels of 5,15,25
		  		// col+6 iteration computes output pixels of 6,16,26
		  		// col+7 iteration computes output pixels of 7,17,27
		  		// col+8 iteration computes output pixels of 8,18,28
		  		// col+9 iteration computes output pixels of 9,19,29
		  		//afterwards, col becomes 30 and repeat the above process

			//1st col iteration
				//load the 9 rows
				r0=_mm256_loadu_si256( (__m256i *) &frame1[row-4][col-4]);
				r1=_mm256_loadu_si256( (__m256i *) &frame1[row-3][col-4]);
				r2=_mm256_loadu_si256( (__m256i *) &frame1[row-2][col-4]);
				r3=_mm256_loadu_si256( (__m256i *) &frame1[row-1][col-4]);
				r4=_mm256_loadu_si256( (__m256i *) &frame1[row][col-4]);
				r5=_mm256_loadu_si256( (__m256i *) &frame1[row+1][col-4]);
				r6=_mm256_loadu_si256( (__m256i *) &frame1[row+2][col-4]);
				r7=_mm256_loadu_si256( (__m256i *) &frame1[row+3][col-4]);
				r8=_mm256_loadu_si256( (__m256i *) &frame1[row+4][col-4]);

			//multiply by the mask
			m0=_mm256_maddubs_epi16(r0,c0);
			m1=_mm256_maddubs_epi16(r1,c1);
			m2=_mm256_maddubs_epi16(r2,c2);
			m3=_mm256_maddubs_epi16(r3,c3);
			m4=_mm256_maddubs_epi16(r4,c4);
			m5=_mm256_maddubs_epi16(r5,c5);
			m6=_mm256_maddubs_epi16(r6,c6);
			m7=_mm256_maddubs_epi16(r7,c7);
			m8=_mm256_maddubs_epi16(r8,c8);

  			//keep a copy of the 4th element
  			m0_copy=_mm256_and_si256(m0,mask_z);
  			m1_copy=_mm256_and_si256(m1,mask_z);
  			m2_copy=_mm256_and_si256(m2,mask_z);
  			m3_copy=_mm256_and_si256(m3,mask_z);
  			m4_copy=_mm256_and_si256(m4,mask_z);
  			m5_copy=_mm256_and_si256(m5,mask_z);
  			m6_copy=_mm256_and_si256(m6,mask_z);
  			m7_copy=_mm256_and_si256(m7,mask_z);
  			m8_copy=_mm256_and_si256(m8,mask_z);

  			//add all the 4th elements above
  			//16-bit addition here
  			m0_copy=_mm256_add_epi16(m0_copy,m1_copy);
  			m0_copy=_mm256_add_epi16(m0_copy,m2_copy);
  			m0_copy=_mm256_add_epi16(m0_copy,m3_copy);
  			m0_copy=_mm256_add_epi16(m0_copy,m4_copy);
  			m0_copy=_mm256_add_epi16(m0_copy,m5_copy);
  			m0_copy=_mm256_add_epi16(m0_copy,m6_copy);
  			m0_copy=_mm256_add_epi16(m0_copy,m7_copy);
  			m0_copy=_mm256_add_epi16(m0_copy,m8_copy);

  			//zero the 4th out of 16 elements
  			m0=_mm256_and_si256(m0,mask_unz);
  			m1=_mm256_and_si256(m1,mask_unz);
  			m2=_mm256_and_si256(m2,mask_unz);
  			m3=_mm256_and_si256(m3,mask_unz);
  			m4=_mm256_and_si256(m4,mask_unz);
  			m5=_mm256_and_si256(m5,mask_unz);
  			m6=_mm256_and_si256(m6,mask_unz);
  			m7=_mm256_and_si256(m7,mask_unz);
  			m8=_mm256_and_si256(m8,mask_unz);


			//make 16-bit values 32-bit (horizontal add)
			m0=_mm256_madd_epi16(m0,ones);
			m1=_mm256_madd_epi16(m1,ones);
			m2=_mm256_madd_epi16(m2,ones);
			m3=_mm256_madd_epi16(m3,ones);
			m4=_mm256_madd_epi16(m4,ones);
			m5=_mm256_madd_epi16(m5,ones);
			m6=_mm256_madd_epi16(m6,ones);
			m7=_mm256_madd_epi16(m7,ones);
			m8=_mm256_madd_epi16(m8,ones);

			//vertical add
			m0=_mm256_add_epi32(m0,m1);
			m0=_mm256_add_epi32(m0,m2);
			m0=_mm256_add_epi32(m0,m3);
			m0=_mm256_add_epi32(m0,m4);
			m0=_mm256_add_epi32(m0,m5);
			m0=_mm256_add_epi32(m0,m6);
			m0=_mm256_add_epi32(m0,m7);
			m0=_mm256_add_epi32(m0,m8);

			//horizontal add
			m2=_mm256_srli_si256(m0,4);
  			m2=_mm256_add_epi32(m0,m2);

  			//keep only 4th and 7th out of 8 elements
  			m1=_mm256_and_si256(m0,mask_32_1);
  			m1=_mm256_add_epi32(m1,m0_copy);

  			//shift m1 and add with m2 (complex shift is needed)
  			m3=_mm256_srli_si256(m1,8);
  			m4=_mm256_and_si256(m1,mask_32_2);
  			m4=_mm256_permute2f128_si256(m4,m4,1);
  			m4=_mm256_slli_si256(m4,8);
  			m3=_mm256_add_epi32(m3,m4);
  			m2=_mm256_add_epi32(m2,m3);
  			out1=_mm256_and_si256(m2,mask_32_a);//in 32-bit format keep only 0,2,5 and discard the others


			//2nd col iteration
			//load the 9 rows
			r0=_mm256_loadu_si256( (__m256i *) &frame1[row-4][col-3]);
			r1=_mm256_loadu_si256( (__m256i *) &frame1[row-3][col-3]);
			r2=_mm256_loadu_si256( (__m256i *) &frame1[row-2][col-3]);
			r3=_mm256_loadu_si256( (__m256i *) &frame1[row-1][col-3]);
			r4=_mm256_loadu_si256( (__m256i *) &frame1[row][col-3]);
			r5=_mm256_loadu_si256( (__m256i *) &frame1[row+1][col-3]);
			r6=_mm256_loadu_si256( (__m256i *) &frame1[row+2][col-3]);
			r7=_mm256_loadu_si256( (__m256i *) &frame1[row+3][col-3]);
			r8=_mm256_loadu_si256( (__m256i *) &frame1[row+4][col-3]);

			//multiply by the mask
			m0=_mm256_maddubs_epi16(r0,c0);
			m1=_mm256_maddubs_epi16(r1,c1);
			m2=_mm256_maddubs_epi16(r2,c2);
			m3=_mm256_maddubs_epi16(r3,c3);
			m4=_mm256_maddubs_epi16(r4,c4);
			m5=_mm256_maddubs_epi16(r5,c5);
			m6=_mm256_maddubs_epi16(r6,c6);
			m7=_mm256_maddubs_epi16(r7,c7);
			m8=_mm256_maddubs_epi16(r8,c8);

  			//keep a copy of the 4th element
  			m0_copy=_mm256_and_si256(m0,mask_z);
  			m1_copy=_mm256_and_si256(m1,mask_z);
  			m2_copy=_mm256_and_si256(m2,mask_z);
  			m3_copy=_mm256_and_si256(m3,mask_z);
  			m4_copy=_mm256_and_si256(m4,mask_z);
  			m5_copy=_mm256_and_si256(m5,mask_z);
  			m6_copy=_mm256_and_si256(m6,mask_z);
  			m7_copy=_mm256_and_si256(m7,mask_z);
  			m8_copy=_mm256_and_si256(m8,mask_z);

  			//add all the 4th elements above
  			//16-bit addition here
  			m0_copy=_mm256_add_epi16(m0_copy,m1_copy);
  			m0_copy=_mm256_add_epi16(m0_copy,m2_copy);
  			m0_copy=_mm256_add_epi16(m0_copy,m3_copy);
  			m0_copy=_mm256_add_epi16(m0_copy,m4_copy);
  			m0_copy=_mm256_add_epi16(m0_copy,m5_copy);
  			m0_copy=_mm256_add_epi16(m0_copy,m6_copy);
  			m0_copy=_mm256_add_epi16(m0_copy,m7_copy);
  			m0_copy=_mm256_add_epi16(m0_copy,m8_copy);

  			//zero the 4th out of 16 elements
  			m0=_mm256_and_si256(m0,mask_unz);
  			m1=_mm256_and_si256(m1,mask_unz);
  			m2=_mm256_and_si256(m2,mask_unz);
  			m3=_mm256_and_si256(m3,mask_unz);
  			m4=_mm256_and_si256(m4,mask_unz);
  			m5=_mm256_and_si256(m5,mask_unz);
  			m6=_mm256_and_si256(m6,mask_unz);
  			m7=_mm256_and_si256(m7,mask_unz);
  			m8=_mm256_and_si256(m8,mask_unz);


			//make 16-bit values 32-bit (horizontal add)
			m0=_mm256_madd_epi16(m0,ones);
			m1=_mm256_madd_epi16(m1,ones);
			m2=_mm256_madd_epi16(m2,ones);
			m3=_mm256_madd_epi16(m3,ones);
			m4=_mm256_madd_epi16(m4,ones);
			m5=_mm256_madd_epi16(m5,ones);
			m6=_mm256_madd_epi16(m6,ones);
			m7=_mm256_madd_epi16(m7,ones);
			m8=_mm256_madd_epi16(m8,ones);

			//vertical add
			m0=_mm256_add_epi32(m0,m1);
			m0=_mm256_add_epi32(m0,m2);
			m0=_mm256_add_epi32(m0,m3);
			m0=_mm256_add_epi32(m0,m4);
			m0=_mm256_add_epi32(m0,m5);
			m0=_mm256_add_epi32(m0,m6);
			m0=_mm256_add_epi32(m0,m7);
			m0=_mm256_add_epi32(m0,m8);

			//horizontal add
			m2=_mm256_srli_si256(m0,4);
  			m2=_mm256_add_epi32(m0,m2);

  			//keep only 4th and 7th out of 8 elements
  			m1=_mm256_and_si256(m0,mask_32_1);
  			m1=_mm256_add_epi32(m1,m0_copy);

  			//shift m1 and add with m2 (complex shift is needed)
  			m3=_mm256_srli_si256(m1,8);
  			m4=_mm256_and_si256(m1,mask_32_2);
  			m4=_mm256_permute2f128_si256(m4,m4,1);
  			m4=_mm256_slli_si256(m4,8);
  			m3=_mm256_add_epi32(m3,m4);
  			m2=_mm256_add_epi32(m2,m3);
  			out2=_mm256_and_si256(m2,mask_32_a);//in 32-bit format keep only 0,2,5 and discard the others


  			//merge out1 and out2 and then divide
  			out2=_mm256_slli_si256(out2,4);
  			out1=_mm256_add_epi32(out1,out2);

  			out1 = division_32(division_case,out1,f);

  			//elements must be re-arranged before store
  			m0=_mm256_and_si256(out1,out_mask_1);
  			m1=_mm256_and_si256(out1,out_mask_2);
  			m1=_mm256_srli_si256(m1,3);
  			m2=_mm256_and_si256(out1,out_mask_3);
  			m2=_mm256_slli_si256(m2,2);
  			m3=_mm256_and_si256(out1,out_mask_4);
  			m3=_mm256_srli_si256(m3,1);
  			m0=_mm256_add_epi8(m0,m1);
  			m0=_mm256_add_epi8(m0,m2);
  			output1=_mm256_add_epi8(m0,m3);


			//3rd col iteration
				//load the 9 rows
				r0=_mm256_loadu_si256( (__m256i *) &frame1[row-4][col-2]);
				r1=_mm256_loadu_si256( (__m256i *) &frame1[row-3][col-2]);
				r2=_mm256_loadu_si256( (__m256i *) &frame1[row-2][col-2]);
				r3=_mm256_loadu_si256( (__m256i *) &frame1[row-1][col-2]);
				r4=_mm256_loadu_si256( (__m256i *) &frame1[row][col-2]);
				r5=_mm256_loadu_si256( (__m256i *) &frame1[row+1][col-2]);
				r6=_mm256_loadu_si256( (__m256i *) &frame1[row+2][col-2]);
				r7=_mm256_loadu_si256( (__m256i *) &frame1[row+3][col-2]);
				r8=_mm256_loadu_si256( (__m256i *) &frame1[row+4][col-2]);

			//multiply by the mask
			m0=_mm256_maddubs_epi16(r0,c0);
			m1=_mm256_maddubs_epi16(r1,c1);
			m2=_mm256_maddubs_epi16(r2,c2);
			m3=_mm256_maddubs_epi16(r3,c3);
			m4=_mm256_maddubs_epi16(r4,c4);
			m5=_mm256_maddubs_epi16(r5,c5);
			m6=_mm256_maddubs_epi16(r6,c6);
			m7=_mm256_maddubs_epi16(r7,c7);
			m8=_mm256_maddubs_epi16(r8,c8);

  			//keep a copy of the 4th element
  			m0_copy=_mm256_and_si256(m0,mask_z);
  			m1_copy=_mm256_and_si256(m1,mask_z);
  			m2_copy=_mm256_and_si256(m2,mask_z);
  			m3_copy=_mm256_and_si256(m3,mask_z);
  			m4_copy=_mm256_and_si256(m4,mask_z);
  			m5_copy=_mm256_and_si256(m5,mask_z);
  			m6_copy=_mm256_and_si256(m6,mask_z);
  			m7_copy=_mm256_and_si256(m7,mask_z);
  			m8_copy=_mm256_and_si256(m8,mask_z);

  			//add all the 4th elements above
  			//16-bit addition here
  			m0_copy=_mm256_add_epi16(m0_copy,m1_copy);
  			m0_copy=_mm256_add_epi16(m0_copy,m2_copy);
  			m0_copy=_mm256_add_epi16(m0_copy,m3_copy);
  			m0_copy=_mm256_add_epi16(m0_copy,m4_copy);
  			m0_copy=_mm256_add_epi16(m0_copy,m5_copy);
  			m0_copy=_mm256_add_epi16(m0_copy,m6_copy);
  			m0_copy=_mm256_add_epi16(m0_copy,m7_copy);
  			m0_copy=_mm256_add_epi16(m0_copy,m8_copy);

  			//zero the 4th out of 16 elements
  			m0=_mm256_and_si256(m0,mask_unz);
  			m1=_mm256_and_si256(m1,mask_unz);
  			m2=_mm256_and_si256(m2,mask_unz);
  			m3=_mm256_and_si256(m3,mask_unz);
  			m4=_mm256_and_si256(m4,mask_unz);
  			m5=_mm256_and_si256(m5,mask_unz);
  			m6=_mm256_and_si256(m6,mask_unz);
  			m7=_mm256_and_si256(m7,mask_unz);
  			m8=_mm256_and_si256(m8,mask_unz);


			//make 16-bit values 32-bit (horizontal add)
			m0=_mm256_madd_epi16(m0,ones);
			m1=_mm256_madd_epi16(m1,ones);
			m2=_mm256_madd_epi16(m2,ones);
			m3=_mm256_madd_epi16(m3,ones);
			m4=_mm256_madd_epi16(m4,ones);
			m5=_mm256_madd_epi16(m5,ones);
			m6=_mm256_madd_epi16(m6,ones);
			m7=_mm256_madd_epi16(m7,ones);
			m8=_mm256_madd_epi16(m8,ones);

			//vertical add
			m0=_mm256_add_epi32(m0,m1);
			m0=_mm256_add_epi32(m0,m2);
			m0=_mm256_add_epi32(m0,m3);
			m0=_mm256_add_epi32(m0,m4);
			m0=_mm256_add_epi32(m0,m5);
			m0=_mm256_add_epi32(m0,m6);
			m0=_mm256_add_epi32(m0,m7);
			m0=_mm256_add_epi32(m0,m8);

			//horizontal add
			m2=_mm256_srli_si256(m0,4);
  			m2=_mm256_add_epi32(m0,m2);

  			//keep only 4th and 7th out of 8 elements
  			m1=_mm256_and_si256(m0,mask_32_1);
  			m1=_mm256_add_epi32(m1,m0_copy);

  			//shift m1 and add with m2 (complex shift is needed)
  			m3=_mm256_srli_si256(m1,8);
  			m4=_mm256_and_si256(m1,mask_32_2);
  			m4=_mm256_permute2f128_si256(m4,m4,1);
  			m4=_mm256_slli_si256(m4,8);
  			m3=_mm256_add_epi32(m3,m4);
  			m2=_mm256_add_epi32(m2,m3);
  			out1=_mm256_and_si256(m2,mask_32_a);//in 32-bit format keep only 0,2,5 and discard the others

			//4th col iteration
			//load the 9 rows
			r0=_mm256_loadu_si256( (__m256i *) &frame1[row-4][col-1]);
			r1=_mm256_loadu_si256( (__m256i *) &frame1[row-3][col-1]);
			r2=_mm256_loadu_si256( (__m256i *) &frame1[row-2][col-1]);
			r3=_mm256_loadu_si256( (__m256i *) &frame1[row-1][col-1]);
			r4=_mm256_loadu_si256( (__m256i *) &frame1[row][col-1]);
			r5=_mm256_loadu_si256( (__m256i *) &frame1[row+1][col-1]);
			r6=_mm256_loadu_si256( (__m256i *) &frame1[row+2][col-1]);
			r7=_mm256_loadu_si256( (__m256i *) &frame1[row+3][col-1]);
			r8=_mm256_loadu_si256( (__m256i *) &frame1[row+4][col-1]);

			//multiply by the mask
			m0=_mm256_maddubs_epi16(r0,c0);
			m1=_mm256_maddubs_epi16(r1,c1);
			m2=_mm256_maddubs_epi16(r2,c2);
			m3=_mm256_maddubs_epi16(r3,c3);
			m4=_mm256_maddubs_epi16(r4,c4);
			m5=_mm256_maddubs_epi16(r5,c5);
			m6=_mm256_maddubs_epi16(r6,c6);
			m7=_mm256_maddubs_epi16(r7,c7);
			m8=_mm256_maddubs_epi16(r8,c8);

  			//keep a copy of the 4th element
  			m0_copy=_mm256_and_si256(m0,mask_z);
  			m1_copy=_mm256_and_si256(m1,mask_z);
  			m2_copy=_mm256_and_si256(m2,mask_z);
  			m3_copy=_mm256_and_si256(m3,mask_z);
  			m4_copy=_mm256_and_si256(m4,mask_z);
  			m5_copy=_mm256_and_si256(m5,mask_z);
  			m6_copy=_mm256_and_si256(m6,mask_z);
  			m7_copy=_mm256_and_si256(m7,mask_z);
  			m8_copy=_mm256_and_si256(m8,mask_z);

  			//add all the 4th elements above
  			//16-bit addition here
  			m0_copy=_mm256_add_epi16(m0_copy,m1_copy);
  			m0_copy=_mm256_add_epi16(m0_copy,m2_copy);
  			m0_copy=_mm256_add_epi16(m0_copy,m3_copy);
  			m0_copy=_mm256_add_epi16(m0_copy,m4_copy);
  			m0_copy=_mm256_add_epi16(m0_copy,m5_copy);
  			m0_copy=_mm256_add_epi16(m0_copy,m6_copy);
  			m0_copy=_mm256_add_epi16(m0_copy,m7_copy);
  			m0_copy=_mm256_add_epi16(m0_copy,m8_copy);

  			//zero the 4th out of 16 elements
  			m0=_mm256_and_si256(m0,mask_unz);
  			m1=_mm256_and_si256(m1,mask_unz);
  			m2=_mm256_and_si256(m2,mask_unz);
  			m3=_mm256_and_si256(m3,mask_unz);
  			m4=_mm256_and_si256(m4,mask_unz);
  			m5=_mm256_and_si256(m5,mask_unz);
  			m6=_mm256_and_si256(m6,mask_unz);
  			m7=_mm256_and_si256(m7,mask_unz);
  			m8=_mm256_and_si256(m8,mask_unz);


			//make 16-bit values 32-bit (horizontal add)
			m0=_mm256_madd_epi16(m0,ones);
			m1=_mm256_madd_epi16(m1,ones);
			m2=_mm256_madd_epi16(m2,ones);
			m3=_mm256_madd_epi16(m3,ones);
			m4=_mm256_madd_epi16(m4,ones);
			m5=_mm256_madd_epi16(m5,ones);
			m6=_mm256_madd_epi16(m6,ones);
			m7=_mm256_madd_epi16(m7,ones);
			m8=_mm256_madd_epi16(m8,ones);

			//vertical add
			m0=_mm256_add_epi32(m0,m1);
			m0=_mm256_add_epi32(m0,m2);
			m0=_mm256_add_epi32(m0,m3);
			m0=_mm256_add_epi32(m0,m4);
			m0=_mm256_add_epi32(m0,m5);
			m0=_mm256_add_epi32(m0,m6);
			m0=_mm256_add_epi32(m0,m7);
			m0=_mm256_add_epi32(m0,m8);

			//horizontal add
			m2=_mm256_srli_si256(m0,4);
  			m2=_mm256_add_epi32(m0,m2);

  			//keep only 4th and 7th out of 8 elements
  			m1=_mm256_and_si256(m0,mask_32_1);
  			m1=_mm256_add_epi32(m1,m0_copy);

  			//shift m1 and add with m2 (complex shift is needed)
  			m3=_mm256_srli_si256(m1,8);
  			m4=_mm256_and_si256(m1,mask_32_2);
  			m4=_mm256_permute2f128_si256(m4,m4,1);
  			m4=_mm256_slli_si256(m4,8);
  			m3=_mm256_add_epi32(m3,m4);
  			m2=_mm256_add_epi32(m2,m3);
  			out2=_mm256_and_si256(m2,mask_32_a);//in 32-bit format keep only 0,2,5 and discard the others

  			//merge out1 and out2 and then divide
  			out2=_mm256_slli_si256(out2,4);
  			out1=_mm256_add_epi32(out1,out2);
  			out1 = division_32(division_case,out1,f);

  			//elements must be re-arranged before store
  			m0=_mm256_and_si256(out1,out_mask_1);
  			m1=_mm256_and_si256(out1,out_mask_2);
  			m1=_mm256_srli_si256(m1,3);
  			m2=_mm256_and_si256(out1,out_mask_3);
  			m2=_mm256_slli_si256(m2,2);
  			m3=_mm256_and_si256(out1,out_mask_4);
  			m3=_mm256_srli_si256(m3,1);
  			m0=_mm256_add_epi8(m0,m1);
  			m0=_mm256_add_epi8(m0,m2);
  			output2=_mm256_add_epi8(m0,m3);

			//5th col iteration
				//load the 9 rows
				r0=_mm256_loadu_si256( (__m256i *) &frame1[row-4][col]);
				r1=_mm256_loadu_si256( (__m256i *) &frame1[row-3][col]);
				r2=_mm256_loadu_si256( (__m256i *) &frame1[row-2][col]);
				r3=_mm256_loadu_si256( (__m256i *) &frame1[row-1][col]);
				r4=_mm256_loadu_si256( (__m256i *) &frame1[row][col]);
				r5=_mm256_loadu_si256( (__m256i *) &frame1[row+1][col]);
				r6=_mm256_loadu_si256( (__m256i *) &frame1[row+2][col]);
				r7=_mm256_loadu_si256( (__m256i *) &frame1[row+3][col]);
				r8=_mm256_loadu_si256( (__m256i *) &frame1[row+4][col]);

			//multiply by the mask
			m0=_mm256_maddubs_epi16(r0,c0);
			m1=_mm256_maddubs_epi16(r1,c1);
			m2=_mm256_maddubs_epi16(r2,c2);
			m3=_mm256_maddubs_epi16(r3,c3);
			m4=_mm256_maddubs_epi16(r4,c4);
			m5=_mm256_maddubs_epi16(r5,c5);
			m6=_mm256_maddubs_epi16(r6,c6);
			m7=_mm256_maddubs_epi16(r7,c7);
			m8=_mm256_maddubs_epi16(r8,c8);

  			//keep a copy of the 4th element
  			m0_copy=_mm256_and_si256(m0,mask_z);
  			m1_copy=_mm256_and_si256(m1,mask_z);
  			m2_copy=_mm256_and_si256(m2,mask_z);
  			m3_copy=_mm256_and_si256(m3,mask_z);
  			m4_copy=_mm256_and_si256(m4,mask_z);
  			m5_copy=_mm256_and_si256(m5,mask_z);
  			m6_copy=_mm256_and_si256(m6,mask_z);
  			m7_copy=_mm256_and_si256(m7,mask_z);
  			m8_copy=_mm256_and_si256(m8,mask_z);

  			//add all the 4th elements above
  			//16-bit addition here
  			m0_copy=_mm256_add_epi16(m0_copy,m1_copy);
  			m0_copy=_mm256_add_epi16(m0_copy,m2_copy);
  			m0_copy=_mm256_add_epi16(m0_copy,m3_copy);
  			m0_copy=_mm256_add_epi16(m0_copy,m4_copy);
  			m0_copy=_mm256_add_epi16(m0_copy,m5_copy);
  			m0_copy=_mm256_add_epi16(m0_copy,m6_copy);
  			m0_copy=_mm256_add_epi16(m0_copy,m7_copy);
  			m0_copy=_mm256_add_epi16(m0_copy,m8_copy);

  			//zero the 4th out of 16 elements
  			m0=_mm256_and_si256(m0,mask_unz);
  			m1=_mm256_and_si256(m1,mask_unz);
  			m2=_mm256_and_si256(m2,mask_unz);
  			m3=_mm256_and_si256(m3,mask_unz);
  			m4=_mm256_and_si256(m4,mask_unz);
  			m5=_mm256_and_si256(m5,mask_unz);
  			m6=_mm256_and_si256(m6,mask_unz);
  			m7=_mm256_and_si256(m7,mask_unz);
  			m8=_mm256_and_si256(m8,mask_unz);


			//make 16-bit values 32-bit (horizontal add)
			m0=_mm256_madd_epi16(m0,ones);
			m1=_mm256_madd_epi16(m1,ones);
			m2=_mm256_madd_epi16(m2,ones);
			m3=_mm256_madd_epi16(m3,ones);
			m4=_mm256_madd_epi16(m4,ones);
			m5=_mm256_madd_epi16(m5,ones);
			m6=_mm256_madd_epi16(m6,ones);
			m7=_mm256_madd_epi16(m7,ones);
			m8=_mm256_madd_epi16(m8,ones);

			//vertical add
			m0=_mm256_add_epi32(m0,m1);
			m0=_mm256_add_epi32(m0,m2);
			m0=_mm256_add_epi32(m0,m3);
			m0=_mm256_add_epi32(m0,m4);
			m0=_mm256_add_epi32(m0,m5);
			m0=_mm256_add_epi32(m0,m6);
			m0=_mm256_add_epi32(m0,m7);
			m0=_mm256_add_epi32(m0,m8);

			//horizontal add
			m2=_mm256_srli_si256(m0,4);
  			m2=_mm256_add_epi32(m0,m2);

  			//keep only 4th and 7th out of 8 elements
  			m1=_mm256_and_si256(m0,mask_32_1);
  			m1=_mm256_add_epi32(m1,m0_copy);

  			//shift m1 and add with m2 (complex shift is needed)
  			m3=_mm256_srli_si256(m1,8);
  			m4=_mm256_and_si256(m1,mask_32_2);
  			m4=_mm256_permute2f128_si256(m4,m4,1);
  			m4=_mm256_slli_si256(m4,8);
  			m3=_mm256_add_epi32(m3,m4);
  			m2=_mm256_add_epi32(m2,m3);
  			out1=_mm256_and_si256(m2,mask_32_a);//in 32-bit format keep only 0,2,5 and discard the others

			//6th col iteration
			//load the 9 rows
			r0=_mm256_loadu_si256( (__m256i *) &frame1[row-4][col+1]);
			r1=_mm256_loadu_si256( (__m256i *) &frame1[row-3][col+1]);
			r2=_mm256_loadu_si256( (__m256i *) &frame1[row-2][col+1]);
			r3=_mm256_loadu_si256( (__m256i *) &frame1[row-1][col+1]);
			r4=_mm256_loadu_si256( (__m256i *) &frame1[row][col+1]);
			r5=_mm256_loadu_si256( (__m256i *) &frame1[row+1][col+1]);
			r6=_mm256_loadu_si256( (__m256i *) &frame1[row+2][col+1]);
			r7=_mm256_loadu_si256( (__m256i *) &frame1[row+3][col+1]);
			r8=_mm256_loadu_si256( (__m256i *) &frame1[row+4][col+1]);

			//multiply by the mask
			m0=_mm256_maddubs_epi16(r0,c0);
			m1=_mm256_maddubs_epi16(r1,c1);
			m2=_mm256_maddubs_epi16(r2,c2);
			m3=_mm256_maddubs_epi16(r3,c3);
			m4=_mm256_maddubs_epi16(r4,c4);
			m5=_mm256_maddubs_epi16(r5,c5);
			m6=_mm256_maddubs_epi16(r6,c6);
			m7=_mm256_maddubs_epi16(r7,c7);
			m8=_mm256_maddubs_epi16(r8,c8);

  			//keep a copy of the 4th element
  			m0_copy=_mm256_and_si256(m0,mask_z);
  			m1_copy=_mm256_and_si256(m1,mask_z);
  			m2_copy=_mm256_and_si256(m2,mask_z);
  			m3_copy=_mm256_and_si256(m3,mask_z);
  			m4_copy=_mm256_and_si256(m4,mask_z);
  			m5_copy=_mm256_and_si256(m5,mask_z);
  			m6_copy=_mm256_and_si256(m6,mask_z);
  			m7_copy=_mm256_and_si256(m7,mask_z);
  			m8_copy=_mm256_and_si256(m8,mask_z);

  			//add all the 4th elements above
  			//16-bit addition here
  			m0_copy=_mm256_add_epi16(m0_copy,m1_copy);
  			m0_copy=_mm256_add_epi16(m0_copy,m2_copy);
  			m0_copy=_mm256_add_epi16(m0_copy,m3_copy);
  			m0_copy=_mm256_add_epi16(m0_copy,m4_copy);
  			m0_copy=_mm256_add_epi16(m0_copy,m5_copy);
  			m0_copy=_mm256_add_epi16(m0_copy,m6_copy);
  			m0_copy=_mm256_add_epi16(m0_copy,m7_copy);
  			m0_copy=_mm256_add_epi16(m0_copy,m8_copy);

  			//zero the 4th out of 16 elements
  			m0=_mm256_and_si256(m0,mask_unz);
  			m1=_mm256_and_si256(m1,mask_unz);
  			m2=_mm256_and_si256(m2,mask_unz);
  			m3=_mm256_and_si256(m3,mask_unz);
  			m4=_mm256_and_si256(m4,mask_unz);
  			m5=_mm256_and_si256(m5,mask_unz);
  			m6=_mm256_and_si256(m6,mask_unz);
  			m7=_mm256_and_si256(m7,mask_unz);
  			m8=_mm256_and_si256(m8,mask_unz);


			//make 16-bit values 32-bit (horizontal add)
			m0=_mm256_madd_epi16(m0,ones);
			m1=_mm256_madd_epi16(m1,ones);
			m2=_mm256_madd_epi16(m2,ones);
			m3=_mm256_madd_epi16(m3,ones);
			m4=_mm256_madd_epi16(m4,ones);
			m5=_mm256_madd_epi16(m5,ones);
			m6=_mm256_madd_epi16(m6,ones);
			m7=_mm256_madd_epi16(m7,ones);
			m8=_mm256_madd_epi16(m8,ones);

			//vertical add
			m0=_mm256_add_epi32(m0,m1);
			m0=_mm256_add_epi32(m0,m2);
			m0=_mm256_add_epi32(m0,m3);
			m0=_mm256_add_epi32(m0,m4);
			m0=_mm256_add_epi32(m0,m5);
			m0=_mm256_add_epi32(m0,m6);
			m0=_mm256_add_epi32(m0,m7);
			m0=_mm256_add_epi32(m0,m8);

			//horizontal add
			m2=_mm256_srli_si256(m0,4);
  			m2=_mm256_add_epi32(m0,m2);

  			//keep only 4th and 7th out of 8 elements
  			m1=_mm256_and_si256(m0,mask_32_1);
  			m1=_mm256_add_epi32(m1,m0_copy);

  			//shift m1 and add with m2 (complex shift is needed)
  			m3=_mm256_srli_si256(m1,8);
  			m4=_mm256_and_si256(m1,mask_32_2);
  			m4=_mm256_permute2f128_si256(m4,m4,1);
  			m4=_mm256_slli_si256(m4,8);
  			m3=_mm256_add_epi32(m3,m4);
  			m2=_mm256_add_epi32(m2,m3);
  			out2=_mm256_and_si256(m2,mask_32_a);//in 32-bit format keep only 0,2,5 and discard the others

  			//merge out1 and out2 and then divide
  			out2=_mm256_slli_si256(out2,4);
  			out1=_mm256_add_epi32(out1,out2);
  			out1 = division_32(division_case,out1,f);

  			//elements must be re-arranged before store
  			m0=_mm256_and_si256(out1,out_mask_1);
  			m1=_mm256_and_si256(out1,out_mask_2);
  			m1=_mm256_srli_si256(m1,3);
  			m2=_mm256_and_si256(out1,out_mask_3);
  			m2=_mm256_slli_si256(m2,2);
  			m3=_mm256_and_si256(out1,out_mask_4);
  			m3=_mm256_srli_si256(m3,1);
  			m0=_mm256_add_epi8(m0,m1);
  			m0=_mm256_add_epi8(m0,m2);
  			output3=_mm256_add_epi8(m0,m3);

			//7th col iteration
				//load the 9 rows
				r0=_mm256_loadu_si256( (__m256i *) &frame1[row-4][col+2]);
				r1=_mm256_loadu_si256( (__m256i *) &frame1[row-3][col+2]);
				r2=_mm256_loadu_si256( (__m256i *) &frame1[row-2][col+2]);
				r3=_mm256_loadu_si256( (__m256i *) &frame1[row-1][col+2]);
				r4=_mm256_loadu_si256( (__m256i *) &frame1[row][col+2]);
				r5=_mm256_loadu_si256( (__m256i *) &frame1[row+1][col+2]);
				r6=_mm256_loadu_si256( (__m256i *) &frame1[row+2][col+2]);
				r7=_mm256_loadu_si256( (__m256i *) &frame1[row+3][col+2]);
				r8=_mm256_loadu_si256( (__m256i *) &frame1[row+4][col+2]);

			//multiply by the mask
			m0=_mm256_maddubs_epi16(r0,c0);
			m1=_mm256_maddubs_epi16(r1,c1);
			m2=_mm256_maddubs_epi16(r2,c2);
			m3=_mm256_maddubs_epi16(r3,c3);
			m4=_mm256_maddubs_epi16(r4,c4);
			m5=_mm256_maddubs_epi16(r5,c5);
			m6=_mm256_maddubs_epi16(r6,c6);
			m7=_mm256_maddubs_epi16(r7,c7);
			m8=_mm256_maddubs_epi16(r8,c8);

  			//keep a copy of the 4th element
  			m0_copy=_mm256_and_si256(m0,mask_z);
  			m1_copy=_mm256_and_si256(m1,mask_z);
  			m2_copy=_mm256_and_si256(m2,mask_z);
  			m3_copy=_mm256_and_si256(m3,mask_z);
  			m4_copy=_mm256_and_si256(m4,mask_z);
  			m5_copy=_mm256_and_si256(m5,mask_z);
  			m6_copy=_mm256_and_si256(m6,mask_z);
  			m7_copy=_mm256_and_si256(m7,mask_z);
  			m8_copy=_mm256_and_si256(m8,mask_z);

  			//add all the 4th elements above
  			//16-bit addition here
  			m0_copy=_mm256_add_epi16(m0_copy,m1_copy);
  			m0_copy=_mm256_add_epi16(m0_copy,m2_copy);
  			m0_copy=_mm256_add_epi16(m0_copy,m3_copy);
  			m0_copy=_mm256_add_epi16(m0_copy,m4_copy);
  			m0_copy=_mm256_add_epi16(m0_copy,m5_copy);
  			m0_copy=_mm256_add_epi16(m0_copy,m6_copy);
  			m0_copy=_mm256_add_epi16(m0_copy,m7_copy);
  			m0_copy=_mm256_add_epi16(m0_copy,m8_copy);

  			//zero the 4th out of 16 elements
  			m0=_mm256_and_si256(m0,mask_unz);
  			m1=_mm256_and_si256(m1,mask_unz);
  			m2=_mm256_and_si256(m2,mask_unz);
  			m3=_mm256_and_si256(m3,mask_unz);
  			m4=_mm256_and_si256(m4,mask_unz);
  			m5=_mm256_and_si256(m5,mask_unz);
  			m6=_mm256_and_si256(m6,mask_unz);
  			m7=_mm256_and_si256(m7,mask_unz);
  			m8=_mm256_and_si256(m8,mask_unz);


			//make 16-bit values 32-bit (horizontal add)
			m0=_mm256_madd_epi16(m0,ones);
			m1=_mm256_madd_epi16(m1,ones);
			m2=_mm256_madd_epi16(m2,ones);
			m3=_mm256_madd_epi16(m3,ones);
			m4=_mm256_madd_epi16(m4,ones);
			m5=_mm256_madd_epi16(m5,ones);
			m6=_mm256_madd_epi16(m6,ones);
			m7=_mm256_madd_epi16(m7,ones);
			m8=_mm256_madd_epi16(m8,ones);

			//vertical add
			m0=_mm256_add_epi32(m0,m1);
			m0=_mm256_add_epi32(m0,m2);
			m0=_mm256_add_epi32(m0,m3);
			m0=_mm256_add_epi32(m0,m4);
			m0=_mm256_add_epi32(m0,m5);
			m0=_mm256_add_epi32(m0,m6);
			m0=_mm256_add_epi32(m0,m7);
			m0=_mm256_add_epi32(m0,m8);

			//horizontal add
			m2=_mm256_srli_si256(m0,4);
  			m2=_mm256_add_epi32(m0,m2);

  			//keep only 4th and 7th out of 8 elements
  			m1=_mm256_and_si256(m0,mask_32_1);
  			m1=_mm256_add_epi32(m1,m0_copy);

  			//shift m1 and add with m2 (complex shift is needed)
  			m3=_mm256_srli_si256(m1,8);
  			m4=_mm256_and_si256(m1,mask_32_2);
  			m4=_mm256_permute2f128_si256(m4,m4,1);
  			m4=_mm256_slli_si256(m4,8);
  			m3=_mm256_add_epi32(m3,m4);
  			m2=_mm256_add_epi32(m2,m3);
  			out1=_mm256_and_si256(m2,mask_32_a);//in 32-bit format keep only 0,2,5 and discard the others

			//8th col iteration
			//load the 9 rows
			r0=_mm256_loadu_si256( (__m256i *) &frame1[row-4][col+3]);
			r1=_mm256_loadu_si256( (__m256i *) &frame1[row-3][col+3]);
			r2=_mm256_loadu_si256( (__m256i *) &frame1[row-2][col+3]);
			r3=_mm256_loadu_si256( (__m256i *) &frame1[row-1][col+3]);
			r4=_mm256_loadu_si256( (__m256i *) &frame1[row][col+3]);
			r5=_mm256_loadu_si256( (__m256i *) &frame1[row+1][col+3]);
			r6=_mm256_loadu_si256( (__m256i *) &frame1[row+2][col+3]);
			r7=_mm256_loadu_si256( (__m256i *) &frame1[row+3][col+3]);
			r8=_mm256_loadu_si256( (__m256i *) &frame1[row+4][col+3]);

			//multiply by the mask
			m0=_mm256_maddubs_epi16(r0,c0);
			m1=_mm256_maddubs_epi16(r1,c1);
			m2=_mm256_maddubs_epi16(r2,c2);
			m3=_mm256_maddubs_epi16(r3,c3);
			m4=_mm256_maddubs_epi16(r4,c4);
			m5=_mm256_maddubs_epi16(r5,c5);
			m6=_mm256_maddubs_epi16(r6,c6);
			m7=_mm256_maddubs_epi16(r7,c7);
			m8=_mm256_maddubs_epi16(r8,c8);

  			//keep a copy of the 4th element
  			m0_copy=_mm256_and_si256(m0,mask_z);
  			m1_copy=_mm256_and_si256(m1,mask_z);
  			m2_copy=_mm256_and_si256(m2,mask_z);
  			m3_copy=_mm256_and_si256(m3,mask_z);
  			m4_copy=_mm256_and_si256(m4,mask_z);
  			m5_copy=_mm256_and_si256(m5,mask_z);
  			m6_copy=_mm256_and_si256(m6,mask_z);
  			m7_copy=_mm256_and_si256(m7,mask_z);
  			m8_copy=_mm256_and_si256(m8,mask_z);

  			//add all the 4th elements above
  			//16-bit addition here
  			m0_copy=_mm256_add_epi16(m0_copy,m1_copy);
  			m0_copy=_mm256_add_epi16(m0_copy,m2_copy);
  			m0_copy=_mm256_add_epi16(m0_copy,m3_copy);
  			m0_copy=_mm256_add_epi16(m0_copy,m4_copy);
  			m0_copy=_mm256_add_epi16(m0_copy,m5_copy);
  			m0_copy=_mm256_add_epi16(m0_copy,m6_copy);
  			m0_copy=_mm256_add_epi16(m0_copy,m7_copy);
  			m0_copy=_mm256_add_epi16(m0_copy,m8_copy);

  			//zero the 4th out of 16 elements
  			m0=_mm256_and_si256(m0,mask_unz);
  			m1=_mm256_and_si256(m1,mask_unz);
  			m2=_mm256_and_si256(m2,mask_unz);
  			m3=_mm256_and_si256(m3,mask_unz);
  			m4=_mm256_and_si256(m4,mask_unz);
  			m5=_mm256_and_si256(m5,mask_unz);
  			m6=_mm256_and_si256(m6,mask_unz);
  			m7=_mm256_and_si256(m7,mask_unz);
  			m8=_mm256_and_si256(m8,mask_unz);


			//make 16-bit values 32-bit (horizontal add)
			m0=_mm256_madd_epi16(m0,ones);
			m1=_mm256_madd_epi16(m1,ones);
			m2=_mm256_madd_epi16(m2,ones);
			m3=_mm256_madd_epi16(m3,ones);
			m4=_mm256_madd_epi16(m4,ones);
			m5=_mm256_madd_epi16(m5,ones);
			m6=_mm256_madd_epi16(m6,ones);
			m7=_mm256_madd_epi16(m7,ones);
			m8=_mm256_madd_epi16(m8,ones);

			//vertical add
			m0=_mm256_add_epi32(m0,m1);
			m0=_mm256_add_epi32(m0,m2);
			m0=_mm256_add_epi32(m0,m3);
			m0=_mm256_add_epi32(m0,m4);
			m0=_mm256_add_epi32(m0,m5);
			m0=_mm256_add_epi32(m0,m6);
			m0=_mm256_add_epi32(m0,m7);
			m0=_mm256_add_epi32(m0,m8);

			//horizontal add
			m2=_mm256_srli_si256(m0,4);
  			m2=_mm256_add_epi32(m0,m2);

  			//keep only 4th and 7th out of 8 elements
  			m1=_mm256_and_si256(m0,mask_32_1);
  			m1=_mm256_add_epi32(m1,m0_copy);

  			//shift m1 and add with m2 (complex shift is needed)
  			m3=_mm256_srli_si256(m1,8);
  			m4=_mm256_and_si256(m1,mask_32_2);
  			m4=_mm256_permute2f128_si256(m4,m4,1);
  			m4=_mm256_slli_si256(m4,8);
  			m3=_mm256_add_epi32(m3,m4);
  			m2=_mm256_add_epi32(m2,m3);
  			out2=_mm256_and_si256(m2,mask_32_a);//in 32-bit format keep only 0,2,5 and discard the others

  			//merge out1 and out2 and then divide
  			out2=_mm256_slli_si256(out2,4);
  			out1=_mm256_add_epi32(out1,out2);
  			out1 = division_32(division_case,out1,f);

  			//elements must be re-arranged before store
  			m0=_mm256_and_si256(out1,out_mask_1);
  			m1=_mm256_and_si256(out1,out_mask_2);
  			m1=_mm256_srli_si256(m1,3);
  			m2=_mm256_and_si256(out1,out_mask_3);
  			m2=_mm256_slli_si256(m2,2);
  			m3=_mm256_and_si256(out1,out_mask_4);
  			m3=_mm256_srli_si256(m3,1);
  			m0=_mm256_add_epi8(m0,m1);
  			m0=_mm256_add_epi8(m0,m2);
  			output4=_mm256_add_epi8(m0,m3);

			//9th col iteration
				//load the 9 rows
				r0=_mm256_loadu_si256( (__m256i *) &frame1[row-4][col+4]);
				r1=_mm256_loadu_si256( (__m256i *) &frame1[row-3][col+4]);
				r2=_mm256_loadu_si256( (__m256i *) &frame1[row-2][col+4]);
				r3=_mm256_loadu_si256( (__m256i *) &frame1[row-1][col+4]);
				r4=_mm256_loadu_si256( (__m256i *) &frame1[row][col+4]);
				r5=_mm256_loadu_si256( (__m256i *) &frame1[row+1][col+4]);
				r6=_mm256_loadu_si256( (__m256i *) &frame1[row+2][col+4]);
				r7=_mm256_loadu_si256( (__m256i *) &frame1[row+3][col+4]);
				r8=_mm256_loadu_si256( (__m256i *) &frame1[row+4][col+4]);

			//multiply by the mask
			m0=_mm256_maddubs_epi16(r0,c0);
			m1=_mm256_maddubs_epi16(r1,c1);
			m2=_mm256_maddubs_epi16(r2,c2);
			m3=_mm256_maddubs_epi16(r3,c3);
			m4=_mm256_maddubs_epi16(r4,c4);
			m5=_mm256_maddubs_epi16(r5,c5);
			m6=_mm256_maddubs_epi16(r6,c6);
			m7=_mm256_maddubs_epi16(r7,c7);
			m8=_mm256_maddubs_epi16(r8,c8);

  			//keep a copy of the 4th element
  			m0_copy=_mm256_and_si256(m0,mask_z);
  			m1_copy=_mm256_and_si256(m1,mask_z);
  			m2_copy=_mm256_and_si256(m2,mask_z);
  			m3_copy=_mm256_and_si256(m3,mask_z);
  			m4_copy=_mm256_and_si256(m4,mask_z);
  			m5_copy=_mm256_and_si256(m5,mask_z);
  			m6_copy=_mm256_and_si256(m6,mask_z);
  			m7_copy=_mm256_and_si256(m7,mask_z);
  			m8_copy=_mm256_and_si256(m8,mask_z);

  			//add all the 4th elements above
  			//16-bit addition here
  			m0_copy=_mm256_add_epi16(m0_copy,m1_copy);
  			m0_copy=_mm256_add_epi16(m0_copy,m2_copy);
  			m0_copy=_mm256_add_epi16(m0_copy,m3_copy);
  			m0_copy=_mm256_add_epi16(m0_copy,m4_copy);
  			m0_copy=_mm256_add_epi16(m0_copy,m5_copy);
  			m0_copy=_mm256_add_epi16(m0_copy,m6_copy);
  			m0_copy=_mm256_add_epi16(m0_copy,m7_copy);
  			m0_copy=_mm256_add_epi16(m0_copy,m8_copy);

  			//zero the 4th out of 16 elements
  			m0=_mm256_and_si256(m0,mask_unz);
  			m1=_mm256_and_si256(m1,mask_unz);
  			m2=_mm256_and_si256(m2,mask_unz);
  			m3=_mm256_and_si256(m3,mask_unz);
  			m4=_mm256_and_si256(m4,mask_unz);
  			m5=_mm256_and_si256(m5,mask_unz);
  			m6=_mm256_and_si256(m6,mask_unz);
  			m7=_mm256_and_si256(m7,mask_unz);
  			m8=_mm256_and_si256(m8,mask_unz);


			//make 16-bit values 32-bit (horizontal add)
			m0=_mm256_madd_epi16(m0,ones);
			m1=_mm256_madd_epi16(m1,ones);
			m2=_mm256_madd_epi16(m2,ones);
			m3=_mm256_madd_epi16(m3,ones);
			m4=_mm256_madd_epi16(m4,ones);
			m5=_mm256_madd_epi16(m5,ones);
			m6=_mm256_madd_epi16(m6,ones);
			m7=_mm256_madd_epi16(m7,ones);
			m8=_mm256_madd_epi16(m8,ones);

			//vertical add
			m0=_mm256_add_epi32(m0,m1);
			m0=_mm256_add_epi32(m0,m2);
			m0=_mm256_add_epi32(m0,m3);
			m0=_mm256_add_epi32(m0,m4);
			m0=_mm256_add_epi32(m0,m5);
			m0=_mm256_add_epi32(m0,m6);
			m0=_mm256_add_epi32(m0,m7);
			m0=_mm256_add_epi32(m0,m8);

			//horizontal add
			m2=_mm256_srli_si256(m0,4);
  			m2=_mm256_add_epi32(m0,m2);

  			//keep only 4th and 7th out of 8 elements
  			m1=_mm256_and_si256(m0,mask_32_1);
  			m1=_mm256_add_epi32(m1,m0_copy);

  			//shift m1 and add with m2 (complex shift is needed)
  			m3=_mm256_srli_si256(m1,8);
  			m4=_mm256_and_si256(m1,mask_32_2);
  			m4=_mm256_permute2f128_si256(m4,m4,1);
  			m4=_mm256_slli_si256(m4,8);
  			m3=_mm256_add_epi32(m3,m4);
  			m2=_mm256_add_epi32(m2,m3);
  			out1=_mm256_and_si256(m2,mask_32_a);//in 32-bit format keep only 0,2,5 and discard the others

			//10th col iteration
			//load the 9 rows
			r0=_mm256_loadu_si256( (__m256i *) &frame1[row-4][col+5]);
			r1=_mm256_loadu_si256( (__m256i *) &frame1[row-3][col+5]);
			r2=_mm256_loadu_si256( (__m256i *) &frame1[row-2][col+5]);
			r3=_mm256_loadu_si256( (__m256i *) &frame1[row-1][col+5]);
			r4=_mm256_loadu_si256( (__m256i *) &frame1[row][col+5]);
			r5=_mm256_loadu_si256( (__m256i *) &frame1[row+1][col+5]);
			r6=_mm256_loadu_si256( (__m256i *) &frame1[row+2][col+5]);
			r7=_mm256_loadu_si256( (__m256i *) &frame1[row+3][col+5]);
			r8=_mm256_loadu_si256( (__m256i *) &frame1[row+4][col+5]);

			//multiply by the mask
			m0=_mm256_maddubs_epi16(r0,c0);
			m1=_mm256_maddubs_epi16(r1,c1);
			m2=_mm256_maddubs_epi16(r2,c2);
			m3=_mm256_maddubs_epi16(r3,c3);
			m4=_mm256_maddubs_epi16(r4,c4);
			m5=_mm256_maddubs_epi16(r5,c5);
			m6=_mm256_maddubs_epi16(r6,c6);
			m7=_mm256_maddubs_epi16(r7,c7);
			m8=_mm256_maddubs_epi16(r8,c8);

  			//keep a copy of the 4th element
  			m0_copy=_mm256_and_si256(m0,mask_z);
  			m1_copy=_mm256_and_si256(m1,mask_z);
  			m2_copy=_mm256_and_si256(m2,mask_z);
  			m3_copy=_mm256_and_si256(m3,mask_z);
  			m4_copy=_mm256_and_si256(m4,mask_z);
  			m5_copy=_mm256_and_si256(m5,mask_z);
  			m6_copy=_mm256_and_si256(m6,mask_z);
  			m7_copy=_mm256_and_si256(m7,mask_z);
  			m8_copy=_mm256_and_si256(m8,mask_z);

  			//add all the 4th elements above
  			//16-bit addition here
  			m0_copy=_mm256_add_epi16(m0_copy,m1_copy);
  			m0_copy=_mm256_add_epi16(m0_copy,m2_copy);
  			m0_copy=_mm256_add_epi16(m0_copy,m3_copy);
  			m0_copy=_mm256_add_epi16(m0_copy,m4_copy);
  			m0_copy=_mm256_add_epi16(m0_copy,m5_copy);
  			m0_copy=_mm256_add_epi16(m0_copy,m6_copy);
  			m0_copy=_mm256_add_epi16(m0_copy,m7_copy);
  			m0_copy=_mm256_add_epi16(m0_copy,m8_copy);

  			//zero the 4th out of 16 elements
  			m0=_mm256_and_si256(m0,mask_unz);
  			m1=_mm256_and_si256(m1,mask_unz);
  			m2=_mm256_and_si256(m2,mask_unz);
  			m3=_mm256_and_si256(m3,mask_unz);
  			m4=_mm256_and_si256(m4,mask_unz);
  			m5=_mm256_and_si256(m5,mask_unz);
  			m6=_mm256_and_si256(m6,mask_unz);
  			m7=_mm256_and_si256(m7,mask_unz);
  			m8=_mm256_and_si256(m8,mask_unz);


			//make 16-bit values 32-bit (horizontal add)
			m0=_mm256_madd_epi16(m0,ones);
			m1=_mm256_madd_epi16(m1,ones);
			m2=_mm256_madd_epi16(m2,ones);
			m3=_mm256_madd_epi16(m3,ones);
			m4=_mm256_madd_epi16(m4,ones);
			m5=_mm256_madd_epi16(m5,ones);
			m6=_mm256_madd_epi16(m6,ones);
			m7=_mm256_madd_epi16(m7,ones);
			m8=_mm256_madd_epi16(m8,ones);

			//vertical add
			m0=_mm256_add_epi32(m0,m1);
			m0=_mm256_add_epi32(m0,m2);
			m0=_mm256_add_epi32(m0,m3);
			m0=_mm256_add_epi32(m0,m4);
			m0=_mm256_add_epi32(m0,m5);
			m0=_mm256_add_epi32(m0,m6);
			m0=_mm256_add_epi32(m0,m7);
			m0=_mm256_add_epi32(m0,m8);

			//horizontal add
			m2=_mm256_srli_si256(m0,4);
  			m2=_mm256_add_epi32(m0,m2);

  			//keep only 4th and 7th out of 8 elements
  			m1=_mm256_and_si256(m0,mask_32_1);
  			m1=_mm256_add_epi32(m1,m0_copy);

  			//shift m1 and add with m2 (complex shift is needed)
  			m3=_mm256_srli_si256(m1,8);
  			m4=_mm256_and_si256(m1,mask_32_2);
  			m4=_mm256_permute2f128_si256(m4,m4,1);
  			m4=_mm256_slli_si256(m4,8);
  			m3=_mm256_add_epi32(m3,m4);
  			m2=_mm256_add_epi32(m2,m3);
  			out2=_mm256_and_si256(m2,mask_32_a);//in 32-bit format keep only 0,2,5 and discard the others

  			//merge out1 and out2 and then divide
  			out2=_mm256_slli_si256(out2,4);
  			out1=_mm256_add_epi32(out1,out2);
  			out1 = division_32(division_case,out1,f);

  			//elements must be re-arranged before store
  			m0=_mm256_and_si256(out1,out_mask_1);
  			m1=_mm256_and_si256(out1,out_mask_2);
  			m1=_mm256_srli_si256(m1,3);
  			m2=_mm256_and_si256(out1,out_mask_3);
  			m2=_mm256_slli_si256(m2,2);
  			m3=_mm256_and_si256(out1,out_mask_4);
  			m3=_mm256_srli_si256(m3,1);
  			m0=_mm256_add_epi8(m0,m1);
  			m0=_mm256_add_epi8(m0,m2);
  			output5=_mm256_add_epi8(m0,m3);

  			//blend the output1:output5
	  			output2=_mm256_slli_si256(output2,2);
	  			output3=_mm256_slli_si256(output3,4);

	  			m3=_mm256_slli_si256(output4,6);
	  			m4=_mm256_and_si256(output4,out_mask_5);
	  			m4=_mm256_permute2f128_si256(m4,m4,1);
	  			m4=_mm256_srli_si256(m4,10);
	  			output4=_mm256_add_epi32(m3,m4);

	  			m3=_mm256_slli_si256(output5,8);
	  			m4=_mm256_and_si256(output5,out_mask_5);
	  			m4=_mm256_permute2f128_si256(m4,m4,1);
	  			m4=_mm256_srli_si256(m4,8);
	  			output5=_mm256_add_epi32(m3,m4);

	  			output1=_mm256_add_epi8(output1,output2);
	  			output1=_mm256_add_epi8(output1,output3);
	  			output1=_mm256_add_epi8(output1,output4);
	  			output1=_mm256_add_epi8(output1,output5);


			_mm256_storeu_si256( (__m256i *) &filt[row][col],output1 );

		  }
		}
	  loop_reminder_9x9(frame1,filt,row, col,N,M,REMINDER_ITERATIONS,division_case, mask_vector, f, divisor, kernel);

		}
}

}//end of parallel


}


__m256i main_block_pre_9x9(const __m256i r0,const __m256i r1,const __m256i r2,const __m256i r3,const __m256i r4,const __m256i r5,const __m256i r6,const __m256i r7,const __m256i r8, const __m256i c0,const __m256i c1,const __m256i c2,const __m256i c3,const __m256i c4,const __m256i c5,const __m256i c6,const __m256i c7,const __m256i c8){

	__m256i m0,m1,m2,m3,m4,m5,m6,m7,m8;
	const __m256i mask_z    = _mm256_set_epi16(0,0,0,0,0,0,0,0,0,0,0,65535,0,0,0,0);
	const __m256i mask_unz    = _mm256_set_epi16(65535,65535,65535,65535,65535,65535,65535,65535,65535,65535,65535,0,65535,65535,65535,65535);
	const __m256i mask_32_1    = _mm256_set_epi32(0xffffffff,0,0,0xffffffff,0,0,0,0);
	const __m256i mask_32_2    = _mm256_set_epi32(0,0,0,0xffffffff,0,0,0,0);
	const __m256i mask_32_a    = _mm256_set_epi32(0,0,0xffffffff,0,0,0xffffffff,0,0xffffffff);
	const __m256i ones=_mm256_set1_epi16(1);


//multiply by the mask
m0=_mm256_maddubs_epi16(r0,c0);
m1=_mm256_maddubs_epi16(r1,c1);
m2=_mm256_maddubs_epi16(r2,c2);
m3=_mm256_maddubs_epi16(r3,c3);
m4=_mm256_maddubs_epi16(r4,c4);
m5=_mm256_maddubs_epi16(r5,c5);
m6=_mm256_maddubs_epi16(r6,c6);
m7=_mm256_maddubs_epi16(r7,c7);
m8=_mm256_maddubs_epi16(r8,c8);

	//keep a copy of the 4th element
__m256i	m0_copy=_mm256_and_si256(m0,mask_z);
__m256i	m1_copy=_mm256_and_si256(m1,mask_z);
__m256i	m2_copy=_mm256_and_si256(m2,mask_z);
__m256i	m3_copy=_mm256_and_si256(m3,mask_z);
__m256i	m4_copy=_mm256_and_si256(m4,mask_z);
__m256i	m5_copy=_mm256_and_si256(m5,mask_z);
__m256i	m6_copy=_mm256_and_si256(m6,mask_z);
__m256i	m7_copy=_mm256_and_si256(m7,mask_z);
__m256i	m8_copy=_mm256_and_si256(m8,mask_z);

	//add all the 4th elements above
	//16-bit addition here
	m0_copy=_mm256_add_epi16(m0_copy,m1_copy);
	m0_copy=_mm256_add_epi16(m0_copy,m2_copy);
	m0_copy=_mm256_add_epi16(m0_copy,m3_copy);
	m0_copy=_mm256_add_epi16(m0_copy,m4_copy);
	m0_copy=_mm256_add_epi16(m0_copy,m5_copy);
	m0_copy=_mm256_add_epi16(m0_copy,m6_copy);
	m0_copy=_mm256_add_epi16(m0_copy,m7_copy);
	m0_copy=_mm256_add_epi16(m0_copy,m8_copy);

	//zero the 4th out of 16 elements
	m0=_mm256_and_si256(m0,mask_unz);
	m1=_mm256_and_si256(m1,mask_unz);
	m2=_mm256_and_si256(m2,mask_unz);
	m3=_mm256_and_si256(m3,mask_unz);
	m4=_mm256_and_si256(m4,mask_unz);
	m5=_mm256_and_si256(m5,mask_unz);
	m6=_mm256_and_si256(m6,mask_unz);
	m7=_mm256_and_si256(m7,mask_unz);
	m8=_mm256_and_si256(m8,mask_unz);


//make 16-bit values 32-bit (horizontal add)
m0=_mm256_madd_epi16(m0,ones);
m1=_mm256_madd_epi16(m1,ones);
m2=_mm256_madd_epi16(m2,ones);
m3=_mm256_madd_epi16(m3,ones);
m4=_mm256_madd_epi16(m4,ones);
m5=_mm256_madd_epi16(m5,ones);
m6=_mm256_madd_epi16(m6,ones);
m7=_mm256_madd_epi16(m7,ones);
m8=_mm256_madd_epi16(m8,ones);

//vertical add
m0=_mm256_add_epi32(m0,m1);
m0=_mm256_add_epi32(m0,m2);
m0=_mm256_add_epi32(m0,m3);
m0=_mm256_add_epi32(m0,m4);
m0=_mm256_add_epi32(m0,m5);
m0=_mm256_add_epi32(m0,m6);
m0=_mm256_add_epi32(m0,m7);
m0=_mm256_add_epi32(m0,m8);

//horizontal add
m2=_mm256_srli_si256(m0,4);
	m2=_mm256_add_epi32(m0,m2);

	//keep only 4th and 7th out of 8 elements
	m1=_mm256_and_si256(m0,mask_32_1);
	m1=_mm256_add_epi32(m1,m0_copy);

	//shift m1 and add with m2 (complex shift is needed)
	m3=_mm256_srli_si256(m1,8);
	m4=_mm256_and_si256(m1,mask_32_2);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,8);
	m3=_mm256_add_epi32(m3,m4);
	m2=_mm256_add_epi32(m2,m3);
	return _mm256_and_si256(m2,mask_32_a);//in 32-bit format keep only 0,2,5 and discard the others

	}


//this is for computing row=3 and row=N-4 only
void prelude_block_3_9x9(unsigned char **frame1,unsigned char **filt,const unsigned int row, const unsigned int load_row,const unsigned int M, const unsigned int division_case, const __m256i f, const __m256i c0,const __m256i c1,const __m256i c2,const __m256i c3,const __m256i c4,const __m256i c5,const __m256i c6,const __m256i c7){

	unsigned int col;
    __m256i rr0,rr1,rr2,rr3,rr4,rr5,rr6,rr7,r0,r1,r2,r3,r4,r5,r6,r7,out1,out2,output1,output2,output3,output4,output5,m0,m1,m2,m3,m4;


	const __m256i out_mask_1    = _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255);//0,20
	const __m256i out_mask_2    = _mm256_set_epi8(0,0,0,0,0,0,0,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255,0,0,0,0);//4,24
	const __m256i out_mask_3    = _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, 0,0,0,0,0,0,0,255,0,0,0,0,0,0,0,0);//8
	const __m256i out_mask_4    = _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, 0,0,0,255,0,0,0,0,0,0,0,0,0,0,0,0);//12
	const __m256i out_mask_5    = _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, 0,0,0,0,255,255,0,0,0,0,0,0,0,0,0,0);//10,11


	const __m256i mask_prelude   = _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0);
	const __m256i mask_prelude1  = _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0);
	const __m256i mask_prelude2  = _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0);
	const __m256i mask_prelude3  = _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0);


	  for (col = 0; col <= M-32-5; col+=30){

		  if (col==0){
			  //1st col
			rr0=_mm256_load_si256( (__m256i *) &frame1[load_row][0]);
			rr1=_mm256_load_si256( (__m256i *) &frame1[load_row+1][0]);
			rr2=_mm256_load_si256( (__m256i *) &frame1[load_row+2][0]);
			rr3=_mm256_load_si256( (__m256i *) &frame1[load_row+3][0]);
			rr4=_mm256_load_si256( (__m256i *) &frame1[load_row+4][0]);
			rr5=_mm256_load_si256( (__m256i *) &frame1[load_row+5][0]);
			rr6=_mm256_load_si256( (__m256i *) &frame1[load_row+6][0]);
			rr7=_mm256_load_si256( (__m256i *) &frame1[load_row+7][0]);

			//START - extra code needed for prelude
		 r0=insert_four_zeros_front(rr0,mask_prelude);
		 r1=insert_four_zeros_front(rr1,mask_prelude);
		 r2=insert_four_zeros_front(rr2,mask_prelude);
		 r3=insert_four_zeros_front(rr3,mask_prelude);
		 r4=insert_four_zeros_front(rr4,mask_prelude);
		 r5=insert_four_zeros_front(rr5,mask_prelude);
		 r6=insert_four_zeros_front(rr6,mask_prelude);
		 r7=insert_four_zeros_front(rr7,mask_prelude);

		out1=prelude_block_pre_9x9_3(r0,r1,r2,r3,r4,r5,r6,r7,c0,c1,c2,c3,c4,c5,c6,c7);

		  //2nd col

			//START - extra code needed for prelude
		  r0=insert_three_zeros_front(rr0,mask_prelude1);
		  r1=insert_three_zeros_front(rr1,mask_prelude1);
		  r2=insert_three_zeros_front(rr2,mask_prelude1);
		  r3=insert_three_zeros_front(rr3,mask_prelude1);
		  r4=insert_three_zeros_front(rr4,mask_prelude1);
		  r5=insert_three_zeros_front(rr5,mask_prelude1);
		  r6=insert_three_zeros_front(rr6,mask_prelude1);
		  r7=insert_three_zeros_front(rr7,mask_prelude1);

		out2=prelude_block_pre_9x9_3(r0,r1,r2,r3,r4,r5,r6,r7,c0,c1,c2,c3,c4,c5,c6,c7);

			//merge out1 and out2 and then divide
			out2=_mm256_slli_si256(out2,4);
			out1=_mm256_add_epi32(out1,out2);

			out1 = division_32(division_case,out1,f);

			//elements must be re-arranged before store
			m0=_mm256_and_si256(out1,out_mask_1);
			m1=_mm256_and_si256(out1,out_mask_2);
			m1=_mm256_srli_si256(m1,3);
			m2=_mm256_and_si256(out1,out_mask_3);
			m2=_mm256_slli_si256(m2,2);
			m3=_mm256_and_si256(out1,out_mask_4);
			m3=_mm256_srli_si256(m3,1);
			m0=_mm256_add_epi8(m0,m1);
			m0=_mm256_add_epi8(m0,m2);
			output1=_mm256_add_epi8(m0,m3);

		//3rd col
		//START - extra code needed for prelude
		r0=insert_two_zeros_front(rr0,mask_prelude2);
		r1=insert_two_zeros_front(rr1,mask_prelude2);
		r2=insert_two_zeros_front(rr2,mask_prelude2);
		r3=insert_two_zeros_front(rr3,mask_prelude2);
		r4=insert_two_zeros_front(rr4,mask_prelude2);
		r5=insert_two_zeros_front(rr5,mask_prelude2);
		r6=insert_two_zeros_front(rr6,mask_prelude2);
		r7=insert_two_zeros_front(rr7,mask_prelude2);

		out1=prelude_block_pre_9x9_3(r0,r1,r2,r3,r4,r5,r6,r7,c0,c1,c2,c3,c4,c5,c6,c7);

		//4th col
		r0=insert_one_zeros_front(rr0,mask_prelude3);
		r1=insert_one_zeros_front(rr1,mask_prelude3);
		r2=insert_one_zeros_front(rr2,mask_prelude3);
		r3=insert_one_zeros_front(rr3,mask_prelude3);
		r4=insert_one_zeros_front(rr4,mask_prelude3);
		r5=insert_one_zeros_front(rr5,mask_prelude3);
		r6=insert_one_zeros_front(rr6,mask_prelude3);
		r7=insert_one_zeros_front(rr7,mask_prelude3);

		out2 = prelude_block_pre_9x9_3(r0,r1,r2,r3,r4,r5,r6,r7,c0,c1,c2,c3,c4,c5,c6,c7);

		//merge out1 and out2 and then divide
		out2=_mm256_slli_si256(out2,4);
		out1=_mm256_add_epi32(out1,out2);

		out1 = division_32(division_case,out1,f);

		//elements must be re-arranged before store
		m0=_mm256_and_si256(out1,out_mask_1);
		m1=_mm256_and_si256(out1,out_mask_2);
		m1=_mm256_srli_si256(m1,3);
		m2=_mm256_and_si256(out1,out_mask_3);
		m2=_mm256_slli_si256(m2,2);
		m3=_mm256_and_si256(out1,out_mask_4);
		m3=_mm256_srli_si256(m3,1);
		m0=_mm256_add_epi8(m0,m1);
		m0=_mm256_add_epi8(m0,m2);
		output2=_mm256_add_epi8(m0,m3);

		//5th col iteration
		out1 = prelude_block_pre_9x9_3(rr0,rr1,rr2,rr3,rr4,rr5,rr6,rr7,c0,c1,c2,c3,c4,c5,c6,c7);

		//6th col iteration
		r0=_mm256_loadu_si256( (__m256i *) &frame1[load_row][1]);
		r1=_mm256_loadu_si256( (__m256i *) &frame1[load_row+1][1]);
		r2=_mm256_loadu_si256( (__m256i *) &frame1[load_row+2][1]);
		r3=_mm256_loadu_si256( (__m256i *) &frame1[load_row+3][1]);
		r4=_mm256_loadu_si256( (__m256i *) &frame1[load_row+4][1]);
		r5=_mm256_loadu_si256( (__m256i *) &frame1[load_row+5][1]);
		r6=_mm256_loadu_si256( (__m256i *) &frame1[load_row+6][1]);
		r7=_mm256_loadu_si256( (__m256i *) &frame1[load_row+7][1]);

		out2 = prelude_block_pre_9x9_3(r0,r1,r2,r3,r4,r5,r6,r7,c0,c1,c2,c3,c4,c5,c6,c7);

		//merge out1 and out2 and then divide
		out2=_mm256_slli_si256(out2,4);
		out1=_mm256_add_epi32(out1,out2);

		out1 = division_32(division_case,out1,f);

		//elements must be re-arranged before store
		m0=_mm256_and_si256(out1,out_mask_1);
		m1=_mm256_and_si256(out1,out_mask_2);
		m1=_mm256_srli_si256(m1,3);
		m2=_mm256_and_si256(out1,out_mask_3);
		m2=_mm256_slli_si256(m2,2);
		m3=_mm256_and_si256(out1,out_mask_4);
		m3=_mm256_srli_si256(m3,1);
		m0=_mm256_add_epi8(m0,m1);
		m0=_mm256_add_epi8(m0,m2);
		output3=_mm256_add_epi8(m0,m3);

		//7th col iteration
		r0=_mm256_loadu_si256( (__m256i *) &frame1[load_row][2]);
		r1=_mm256_loadu_si256( (__m256i *) &frame1[load_row+1][2]);
		r2=_mm256_loadu_si256( (__m256i *) &frame1[load_row+2][2]);
		r3=_mm256_loadu_si256( (__m256i *) &frame1[load_row+3][2]);
		r4=_mm256_loadu_si256( (__m256i *) &frame1[load_row+4][2]);
		r5=_mm256_loadu_si256( (__m256i *) &frame1[load_row+5][2]);
		r6=_mm256_loadu_si256( (__m256i *) &frame1[load_row+6][2]);
		r7=_mm256_loadu_si256( (__m256i *) &frame1[load_row+7][2]);

		out1 = prelude_block_pre_9x9_3(r0,r1,r2,r3,r4,r5,r6,r7,c0,c1,c2,c3,c4,c5,c6,c7);

		//8th col iteration
		r0=_mm256_loadu_si256( (__m256i *) &frame1[load_row][3]);
		r1=_mm256_loadu_si256( (__m256i *) &frame1[load_row+1][3]);
		r2=_mm256_loadu_si256( (__m256i *) &frame1[load_row+2][3]);
		r3=_mm256_loadu_si256( (__m256i *) &frame1[load_row+3][3]);
		r4=_mm256_loadu_si256( (__m256i *) &frame1[load_row+4][3]);
		r5=_mm256_loadu_si256( (__m256i *) &frame1[load_row+5][3]);
		r6=_mm256_loadu_si256( (__m256i *) &frame1[load_row+6][3]);
		r7=_mm256_loadu_si256( (__m256i *) &frame1[load_row+7][3]);

		out2 = prelude_block_pre_9x9_3(r0,r1,r2,r3,r4,r5,r6,r7,c0,c1,c2,c3,c4,c5,c6,c7);

		//merge out1 and out2 and then divide
		out2=_mm256_slli_si256(out2,4);
		out1=_mm256_add_epi32(out1,out2);

		out1 = division_32(division_case,out1,f);

		//elements must be re-arranged before store
		m0=_mm256_and_si256(out1,out_mask_1);
		m1=_mm256_and_si256(out1,out_mask_2);
		m1=_mm256_srli_si256(m1,3);
		m2=_mm256_and_si256(out1,out_mask_3);
		m2=_mm256_slli_si256(m2,2);
		m3=_mm256_and_si256(out1,out_mask_4);
		m3=_mm256_srli_si256(m3,1);
		m0=_mm256_add_epi8(m0,m1);
		m0=_mm256_add_epi8(m0,m2);
		output4=_mm256_add_epi8(m0,m3);

		//9th col iteration
		r0=_mm256_loadu_si256( (__m256i *) &frame1[load_row][4]);
		r1=_mm256_loadu_si256( (__m256i *) &frame1[load_row+1][4]);
		r2=_mm256_loadu_si256( (__m256i *) &frame1[load_row+2][4]);
		r3=_mm256_loadu_si256( (__m256i *) &frame1[load_row+3][4]);
		r4=_mm256_loadu_si256( (__m256i *) &frame1[load_row+4][4]);
		r5=_mm256_loadu_si256( (__m256i *) &frame1[load_row+5][4]);
		r6=_mm256_loadu_si256( (__m256i *) &frame1[load_row+6][4]);
		r7=_mm256_loadu_si256( (__m256i *) &frame1[load_row+7][4]);

		out1 = prelude_block_pre_9x9_3(r0,r1,r2,r3,r4,r5,r6,r7,c0,c1,c2,c3,c4,c5,c6,c7);

		//10th col iteration
		r0=_mm256_loadu_si256( (__m256i *) &frame1[load_row][5]);
		r1=_mm256_loadu_si256( (__m256i *) &frame1[load_row+1][5]);
		r2=_mm256_loadu_si256( (__m256i *) &frame1[load_row+2][5]);
		r3=_mm256_loadu_si256( (__m256i *) &frame1[load_row+3][5]);
		r4=_mm256_loadu_si256( (__m256i *) &frame1[load_row+4][5]);
		r5=_mm256_loadu_si256( (__m256i *) &frame1[load_row+5][5]);
		r6=_mm256_loadu_si256( (__m256i *) &frame1[load_row+6][5]);
		r7=_mm256_loadu_si256( (__m256i *) &frame1[load_row+7][5]);

		out2 = prelude_block_pre_9x9_3(r0,r1,r2,r3,r4,r5,r6,r7,c0,c1,c2,c3,c4,c5,c6,c7);

		//merge out1 and out2 and then divide
		out2=_mm256_slli_si256(out2,4);
		out1=_mm256_add_epi32(out1,out2);

		out1 = division_32(division_case,out1,f);

		//elements must be re-arranged before store
		m0=_mm256_and_si256(out1,out_mask_1);
		m1=_mm256_and_si256(out1,out_mask_2);
		m1=_mm256_srli_si256(m1,3);
		m2=_mm256_and_si256(out1,out_mask_3);
		m2=_mm256_slli_si256(m2,2);
		m3=_mm256_and_si256(out1,out_mask_4);
		m3=_mm256_srli_si256(m3,1);
		m0=_mm256_add_epi8(m0,m1);
		m0=_mm256_add_epi8(m0,m2);
		output5=_mm256_add_epi8(m0,m3);

			//blend the output1:output5
			output2=_mm256_slli_si256(output2,2);
			output3=_mm256_slli_si256(output3,4);

			m3=_mm256_slli_si256(output4,6);
			m4=_mm256_and_si256(output4,out_mask_5);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_srli_si256(m4,10);
			output4=_mm256_add_epi32(m3,m4);

			m3=_mm256_slli_si256(output5,8);
			m4=_mm256_and_si256(output5,out_mask_5);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_srli_si256(m4,8);
			output5=_mm256_add_epi32(m3,m4);

			output1=_mm256_add_epi8(output1,output2);
			output1=_mm256_add_epi8(output1,output3);
			output1=_mm256_add_epi8(output1,output4);
			output1=_mm256_add_epi8(output1,output5);

				_mm256_store_si256( (__m256i *) &filt[row][0],output1 );
		  	}
		  else {
			  //1st col

		out1=prelude_block_pre_9x9_3m(frame1,load_row,col-4,c0,c1,c2,c3,c4,c5,c6,c7);

		  //2nd col

		out2=prelude_block_pre_9x9_3m(frame1,load_row,col-3,c0,c1,c2,c3,c4,c5,c6,c7);

			//merge out1 and out2 and then divide
			out2=_mm256_slli_si256(out2,4);
			out1=_mm256_add_epi32(out1,out2);

			out1 = division_32(division_case,out1,f);

			//elements must be re-arranged before store
			m0=_mm256_and_si256(out1,out_mask_1);
			m1=_mm256_and_si256(out1,out_mask_2);
			m1=_mm256_srli_si256(m1,3);
			m2=_mm256_and_si256(out1,out_mask_3);
			m2=_mm256_slli_si256(m2,2);
			m3=_mm256_and_si256(out1,out_mask_4);
			m3=_mm256_srli_si256(m3,1);
			m0=_mm256_add_epi8(m0,m1);
			m0=_mm256_add_epi8(m0,m2);
			output1=_mm256_add_epi8(m0,m3);

		//3rd col
		out1=prelude_block_pre_9x9_3m(frame1,load_row,col-2,c0,c1,c2,c3,c4,c5,c6,c7);

		//4th col
		out2=prelude_block_pre_9x9_3m(frame1,load_row,col-1,c0,c1,c2,c3,c4,c5,c6,c7);

		//merge out1 and out2 and then divide
		out2=_mm256_slli_si256(out2,4);
		out1=_mm256_add_epi32(out1,out2);

		out1 = division_32(division_case,out1,f);

		//elements must be re-arranged before store
		m0=_mm256_and_si256(out1,out_mask_1);
		m1=_mm256_and_si256(out1,out_mask_2);
		m1=_mm256_srli_si256(m1,3);
		m2=_mm256_and_si256(out1,out_mask_3);
		m2=_mm256_slli_si256(m2,2);
		m3=_mm256_and_si256(out1,out_mask_4);
		m3=_mm256_srli_si256(m3,1);
		m0=_mm256_add_epi8(m0,m1);
		m0=_mm256_add_epi8(m0,m2);
		output2=_mm256_add_epi8(m0,m3);

		//5th col iteration
		out1=prelude_block_pre_9x9_3m(frame1,load_row,col,c0,c1,c2,c3,c4,c5,c6,c7);

		//6th col iteration
		out2=prelude_block_pre_9x9_3m(frame1,load_row,col+1,c0,c1,c2,c3,c4,c5,c6,c7);

		//merge out1 and out2 and then divide
		out2=_mm256_slli_si256(out2,4);
		out1=_mm256_add_epi32(out1,out2);

		out1 = division_32(division_case,out1,f);

		//elements must be re-arranged before store
		m0=_mm256_and_si256(out1,out_mask_1);
		m1=_mm256_and_si256(out1,out_mask_2);
		m1=_mm256_srli_si256(m1,3);
		m2=_mm256_and_si256(out1,out_mask_3);
		m2=_mm256_slli_si256(m2,2);
		m3=_mm256_and_si256(out1,out_mask_4);
		m3=_mm256_srli_si256(m3,1);
		m0=_mm256_add_epi8(m0,m1);
		m0=_mm256_add_epi8(m0,m2);
		output3=_mm256_add_epi8(m0,m3);

		//7th col iteration
		out1=prelude_block_pre_9x9_3m(frame1,load_row,col+2,c0,c1,c2,c3,c4,c5,c6,c7);


		//8th col iteration
		out2=prelude_block_pre_9x9_3m(frame1,load_row,col+3,c0,c1,c2,c3,c4,c5,c6,c7);


		//merge out1 and out2 and then divide
		out2=_mm256_slli_si256(out2,4);
		out1=_mm256_add_epi32(out1,out2);

		out1 = division_32(division_case,out1,f);

		//elements must be re-arranged before store
		m0=_mm256_and_si256(out1,out_mask_1);
		m1=_mm256_and_si256(out1,out_mask_2);
		m1=_mm256_srli_si256(m1,3);
		m2=_mm256_and_si256(out1,out_mask_3);
		m2=_mm256_slli_si256(m2,2);
		m3=_mm256_and_si256(out1,out_mask_4);
		m3=_mm256_srli_si256(m3,1);
		m0=_mm256_add_epi8(m0,m1);
		m0=_mm256_add_epi8(m0,m2);
		output4=_mm256_add_epi8(m0,m3);

		//9th col iteration
		out1=prelude_block_pre_9x9_3m(frame1,load_row,col+4,c0,c1,c2,c3,c4,c5,c6,c7);


		//10th col iteration
		out2=prelude_block_pre_9x9_3m(frame1,load_row,col+5,c0,c1,c2,c3,c4,c5,c6,c7);


		//merge out1 and out2 and then divide
		out2=_mm256_slli_si256(out2,4);
		out1=_mm256_add_epi32(out1,out2);

		out1 = division_32(division_case,out1,f);

		//elements must be re-arranged before store
		m0=_mm256_and_si256(out1,out_mask_1);
		m1=_mm256_and_si256(out1,out_mask_2);
		m1=_mm256_srli_si256(m1,3);
		m2=_mm256_and_si256(out1,out_mask_3);
		m2=_mm256_slli_si256(m2,2);
		m3=_mm256_and_si256(out1,out_mask_4);
		m3=_mm256_srli_si256(m3,1);
		m0=_mm256_add_epi8(m0,m1);
		m0=_mm256_add_epi8(m0,m2);
		output5=_mm256_add_epi8(m0,m3);

			//blend the output1:output5
			output2=_mm256_slli_si256(output2,2);
			output3=_mm256_slli_si256(output3,4);

			m3=_mm256_slli_si256(output4,6);
			m4=_mm256_and_si256(output4,out_mask_5);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_srli_si256(m4,10);
			output4=_mm256_add_epi32(m3,m4);

			m3=_mm256_slli_si256(output5,8);
			m4=_mm256_and_si256(output5,out_mask_5);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_srli_si256(m4,8);
			output5=_mm256_add_epi32(m3,m4);

			output1=_mm256_add_epi8(output1,output2);
			output1=_mm256_add_epi8(output1,output3);
			output1=_mm256_add_epi8(output1,output4);
			output1=_mm256_add_epi8(output1,output5);

			_mm256_storeu_si256( (__m256i *) &filt[row][col],output1 );

		  }
}
}



__m256i prelude_block_pre_9x9_3(const __m256i r0,const __m256i r1,const __m256i r2,const __m256i r3,const __m256i r4,const __m256i r5,const __m256i r6,const __m256i r7, const __m256i c0,const __m256i c1,const __m256i c2,const __m256i c3,const __m256i c4,const __m256i c5,const __m256i c6,const __m256i c7){

	__m256i m0,m1,m2,m3,m4,m5,m6,m7;
	const __m256i mask_z    = _mm256_set_epi16(0,0,0,0,0,0,0,0,0,0,0,65535,0,0,0,0);
	const __m256i mask_unz    = _mm256_set_epi16(65535,65535,65535,65535,65535,65535,65535,65535,65535,65535,65535,0,65535,65535,65535,65535);
	const __m256i mask_32_1    = _mm256_set_epi32(0xffffffff,0,0,0xffffffff,0,0,0,0);
	const __m256i mask_32_2    = _mm256_set_epi32(0,0,0,0xffffffff,0,0,0,0);
	const __m256i mask_32_a    = _mm256_set_epi32(0,0,0xffffffff,0,0,0xffffffff,0,0xffffffff);
	const __m256i ones=_mm256_set1_epi16(1);


//multiply by the mask
m0=_mm256_maddubs_epi16(r0,c0);
m1=_mm256_maddubs_epi16(r1,c1);
m2=_mm256_maddubs_epi16(r2,c2);
m3=_mm256_maddubs_epi16(r3,c3);
m4=_mm256_maddubs_epi16(r4,c4);
m5=_mm256_maddubs_epi16(r5,c5);
m6=_mm256_maddubs_epi16(r6,c6);
m7=_mm256_maddubs_epi16(r7,c7);

	//keep a copy of the 4th element
__m256i	m0_copy=_mm256_and_si256(m0,mask_z);
__m256i	m1_copy=_mm256_and_si256(m1,mask_z);
__m256i	m2_copy=_mm256_and_si256(m2,mask_z);
__m256i	m3_copy=_mm256_and_si256(m3,mask_z);
__m256i	m4_copy=_mm256_and_si256(m4,mask_z);
__m256i	m5_copy=_mm256_and_si256(m5,mask_z);
__m256i	m6_copy=_mm256_and_si256(m6,mask_z);
__m256i	m7_copy=_mm256_and_si256(m7,mask_z);

	//add all the 4th elements above
	//16-bit addition here
	m0_copy=_mm256_add_epi16(m0_copy,m1_copy);
	m0_copy=_mm256_add_epi16(m0_copy,m2_copy);
	m0_copy=_mm256_add_epi16(m0_copy,m3_copy);
	m0_copy=_mm256_add_epi16(m0_copy,m4_copy);
	m0_copy=_mm256_add_epi16(m0_copy,m5_copy);
	m0_copy=_mm256_add_epi16(m0_copy,m6_copy);
	m0_copy=_mm256_add_epi16(m0_copy,m7_copy);

	//zero the 4th out of 16 elements
	m0=_mm256_and_si256(m0,mask_unz);
	m1=_mm256_and_si256(m1,mask_unz);
	m2=_mm256_and_si256(m2,mask_unz);
	m3=_mm256_and_si256(m3,mask_unz);
	m4=_mm256_and_si256(m4,mask_unz);
	m5=_mm256_and_si256(m5,mask_unz);
	m6=_mm256_and_si256(m6,mask_unz);
	m7=_mm256_and_si256(m7,mask_unz);


//make 16-bit values 32-bit (horizontal add)
m0=_mm256_madd_epi16(m0,ones);
m1=_mm256_madd_epi16(m1,ones);
m2=_mm256_madd_epi16(m2,ones);
m3=_mm256_madd_epi16(m3,ones);
m4=_mm256_madd_epi16(m4,ones);
m5=_mm256_madd_epi16(m5,ones);
m6=_mm256_madd_epi16(m6,ones);
m7=_mm256_madd_epi16(m7,ones);

//vertical add
m0=_mm256_add_epi32(m0,m1);
m0=_mm256_add_epi32(m0,m2);
m0=_mm256_add_epi32(m0,m3);
m0=_mm256_add_epi32(m0,m4);
m0=_mm256_add_epi32(m0,m5);
m0=_mm256_add_epi32(m0,m6);
m0=_mm256_add_epi32(m0,m7);

//horizontal add
m2=_mm256_srli_si256(m0,4);
	m2=_mm256_add_epi32(m0,m2);

	//keep only 4th and 7th out of 8 elements
	m1=_mm256_and_si256(m0,mask_32_1);
	m1=_mm256_add_epi32(m1,m0_copy);

	//shift m1 and add with m2 (complex shift is needed)
	m3=_mm256_srli_si256(m1,8);
	m4=_mm256_and_si256(m1,mask_32_2);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,8);
	m3=_mm256_add_epi32(m3,m4);
	m2=_mm256_add_epi32(m2,m3);
	return _mm256_and_si256(m2,mask_32_a);//in 32-bit format keep only 0,2,5 and discard the others

	}


__m256i prelude_block_pre_9x9_3m(unsigned char **frame1,const unsigned int load_row, const unsigned int load_col, const __m256i c0,const __m256i c1,const __m256i c2,const __m256i c3,const __m256i c4,const __m256i c5,const __m256i c6,const __m256i c7){

	__m256i m0,m1,m2,m3,m4,m5,m6,m7;
	const __m256i mask_z    = _mm256_set_epi16(0,0,0,0,0,0,0,0,0,0,0,65535,0,0,0,0);
	const __m256i mask_unz    = _mm256_set_epi16(65535,65535,65535,65535,65535,65535,65535,65535,65535,65535,65535,0,65535,65535,65535,65535);
	const __m256i mask_32_1    = _mm256_set_epi32(0xffffffff,0,0,0xffffffff,0,0,0,0);
	const __m256i mask_32_2    = _mm256_set_epi32(0,0,0,0xffffffff,0,0,0,0);
	const __m256i mask_32_a    = _mm256_set_epi32(0,0,0xffffffff,0,0,0xffffffff,0,0xffffffff);
	const __m256i ones=_mm256_set1_epi16(1);

__m256i		r0=_mm256_loadu_si256( (__m256i *) &frame1[load_row][load_col]);
__m256i		r1=_mm256_loadu_si256( (__m256i *) &frame1[load_row+1][load_col]);
__m256i		r2=_mm256_loadu_si256( (__m256i *) &frame1[load_row+2][load_col]);
__m256i		r3=_mm256_loadu_si256( (__m256i *) &frame1[load_row+3][load_col]);
__m256i		r4=_mm256_loadu_si256( (__m256i *) &frame1[load_row+4][load_col]);
__m256i		r5=_mm256_loadu_si256( (__m256i *) &frame1[load_row+5][load_col]);
__m256i		r6=_mm256_loadu_si256( (__m256i *) &frame1[load_row+6][load_col]);
__m256i		r7=_mm256_loadu_si256( (__m256i *) &frame1[load_row+7][load_col]);

//multiply by the mask
m0=_mm256_maddubs_epi16(r0,c0);
m1=_mm256_maddubs_epi16(r1,c1);
m2=_mm256_maddubs_epi16(r2,c2);
m3=_mm256_maddubs_epi16(r3,c3);
m4=_mm256_maddubs_epi16(r4,c4);
m5=_mm256_maddubs_epi16(r5,c5);
m6=_mm256_maddubs_epi16(r6,c6);
m7=_mm256_maddubs_epi16(r7,c7);

	//keep a copy of the 4th element
__m256i	m0_copy=_mm256_and_si256(m0,mask_z);
__m256i	m1_copy=_mm256_and_si256(m1,mask_z);
__m256i	m2_copy=_mm256_and_si256(m2,mask_z);
__m256i	m3_copy=_mm256_and_si256(m3,mask_z);
__m256i	m4_copy=_mm256_and_si256(m4,mask_z);
__m256i	m5_copy=_mm256_and_si256(m5,mask_z);
__m256i	m6_copy=_mm256_and_si256(m6,mask_z);
__m256i	m7_copy=_mm256_and_si256(m7,mask_z);

	//add all the 4th elements above
	//16-bit addition here
	m0_copy=_mm256_add_epi16(m0_copy,m1_copy);
	m0_copy=_mm256_add_epi16(m0_copy,m2_copy);
	m0_copy=_mm256_add_epi16(m0_copy,m3_copy);
	m0_copy=_mm256_add_epi16(m0_copy,m4_copy);
	m0_copy=_mm256_add_epi16(m0_copy,m5_copy);
	m0_copy=_mm256_add_epi16(m0_copy,m6_copy);
	m0_copy=_mm256_add_epi16(m0_copy,m7_copy);

	//zero the 4th out of 16 elements
	m0=_mm256_and_si256(m0,mask_unz);
	m1=_mm256_and_si256(m1,mask_unz);
	m2=_mm256_and_si256(m2,mask_unz);
	m3=_mm256_and_si256(m3,mask_unz);
	m4=_mm256_and_si256(m4,mask_unz);
	m5=_mm256_and_si256(m5,mask_unz);
	m6=_mm256_and_si256(m6,mask_unz);
	m7=_mm256_and_si256(m7,mask_unz);


//make 16-bit values 32-bit (horizontal add)
m0=_mm256_madd_epi16(m0,ones);
m1=_mm256_madd_epi16(m1,ones);
m2=_mm256_madd_epi16(m2,ones);
m3=_mm256_madd_epi16(m3,ones);
m4=_mm256_madd_epi16(m4,ones);
m5=_mm256_madd_epi16(m5,ones);
m6=_mm256_madd_epi16(m6,ones);
m7=_mm256_madd_epi16(m7,ones);

//vertical add
m0=_mm256_add_epi32(m0,m1);
m0=_mm256_add_epi32(m0,m2);
m0=_mm256_add_epi32(m0,m3);
m0=_mm256_add_epi32(m0,m4);
m0=_mm256_add_epi32(m0,m5);
m0=_mm256_add_epi32(m0,m6);
m0=_mm256_add_epi32(m0,m7);

//horizontal add
m2=_mm256_srli_si256(m0,4);
	m2=_mm256_add_epi32(m0,m2);

	//keep only 4th and 7th out of 8 elements
	m1=_mm256_and_si256(m0,mask_32_1);
	m1=_mm256_add_epi32(m1,m0_copy);

	//shift m1 and add with m2 (complex shift is needed)
	m3=_mm256_srli_si256(m1,8);
	m4=_mm256_and_si256(m1,mask_32_2);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,8);
	m3=_mm256_add_epi32(m3,m4);
	m2=_mm256_add_epi32(m2,m3);
	return _mm256_and_si256(m2,mask_32_a);//in 32-bit format keep only 0,2,5 and discard the others

	}




//this is for computing row=2 and row=N-3 only
void prelude_block_2_9x9(unsigned char **frame1,unsigned char **filt,const unsigned int row, const unsigned int load_row,const unsigned int M, const unsigned int division_case, const __m256i f, const __m256i c0,const __m256i c1,const __m256i c2,const __m256i c3,const __m256i c4,const __m256i c5,const __m256i c6){

	unsigned int col;
    __m256i rr0,rr1,rr2,rr3,rr4,rr5,rr6,r0,r1,r2,r3,r4,r5,r6,out1,out2,output1,output2,output3,output4,output5,m0,m1,m2,m3,m4;


	const __m256i out_mask_1    = _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255);//0,20
	const __m256i out_mask_2    = _mm256_set_epi8(0,0,0,0,0,0,0,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255,0,0,0,0);//4,24
	const __m256i out_mask_3    = _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, 0,0,0,0,0,0,0,255,0,0,0,0,0,0,0,0);//8
	const __m256i out_mask_4    = _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, 0,0,0,255,0,0,0,0,0,0,0,0,0,0,0,0);//12
	const __m256i out_mask_5    = _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, 0,0,0,0,255,255,0,0,0,0,0,0,0,0,0,0);//10,11


	const __m256i mask_prelude   = _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0);
	const __m256i mask_prelude1  = _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0);
	const __m256i mask_prelude2  = _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0);
	const __m256i mask_prelude3  = _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0);


	  for (col = 0; col <= M-32-5; col+=30){

		  if (col==0){
			  //1st col
			rr0=_mm256_load_si256( (__m256i *) &frame1[load_row][0]);
			rr1=_mm256_load_si256( (__m256i *) &frame1[load_row+1][0]);
			rr2=_mm256_load_si256( (__m256i *) &frame1[load_row+2][0]);
			rr3=_mm256_load_si256( (__m256i *) &frame1[load_row+3][0]);
			rr4=_mm256_load_si256( (__m256i *) &frame1[load_row+4][0]);
			rr5=_mm256_load_si256( (__m256i *) &frame1[load_row+5][0]);
			rr6=_mm256_load_si256( (__m256i *) &frame1[load_row+6][0]);

			//START - extra code needed for prelude
		 r0=insert_four_zeros_front(rr0,mask_prelude);
		 r1=insert_four_zeros_front(rr1,mask_prelude);
		 r2=insert_four_zeros_front(rr2,mask_prelude);
		 r3=insert_four_zeros_front(rr3,mask_prelude);
		 r4=insert_four_zeros_front(rr4,mask_prelude);
		 r5=insert_four_zeros_front(rr5,mask_prelude);
		 r6=insert_four_zeros_front(rr6,mask_prelude);

		out1=prelude_block_pre_9x9_2(r0,r1,r2,r3,r4,r5,r6,c0,c1,c2,c3,c4,c5,c6);

		  //2nd col

			//START - extra code needed for prelude
		  r0=insert_three_zeros_front(rr0,mask_prelude1);
		  r1=insert_three_zeros_front(rr1,mask_prelude1);
		  r2=insert_three_zeros_front(rr2,mask_prelude1);
		  r3=insert_three_zeros_front(rr3,mask_prelude1);
		  r4=insert_three_zeros_front(rr4,mask_prelude1);
		  r5=insert_three_zeros_front(rr5,mask_prelude1);
		  r6=insert_three_zeros_front(rr6,mask_prelude1);

		out2=prelude_block_pre_9x9_2(r0,r1,r2,r3,r4,r5,r6,c0,c1,c2,c3,c4,c5,c6);

			//merge out1 and out2 and then divide
			out2=_mm256_slli_si256(out2,4);
			out1=_mm256_add_epi32(out1,out2);

			out1 = division_32(division_case,out1,f);

			//elements must be re-arranged before store
			m0=_mm256_and_si256(out1,out_mask_1);
			m1=_mm256_and_si256(out1,out_mask_2);
			m1=_mm256_srli_si256(m1,3);
			m2=_mm256_and_si256(out1,out_mask_3);
			m2=_mm256_slli_si256(m2,2);
			m3=_mm256_and_si256(out1,out_mask_4);
			m3=_mm256_srli_si256(m3,1);
			m0=_mm256_add_epi8(m0,m1);
			m0=_mm256_add_epi8(m0,m2);
			output1=_mm256_add_epi8(m0,m3);

		//3rd col
		//START - extra code needed for prelude
		r0=insert_two_zeros_front(rr0,mask_prelude2);
		r1=insert_two_zeros_front(rr1,mask_prelude2);
		r2=insert_two_zeros_front(rr2,mask_prelude2);
		r3=insert_two_zeros_front(rr3,mask_prelude2);
		r4=insert_two_zeros_front(rr4,mask_prelude2);
		r5=insert_two_zeros_front(rr5,mask_prelude2);
		r6=insert_two_zeros_front(rr6,mask_prelude2);

		out1=prelude_block_pre_9x9_2(r0,r1,r2,r3,r4,r5,r6,c0,c1,c2,c3,c4,c5,c6);

		//4th col
		r0=insert_one_zeros_front(rr0,mask_prelude3);
		r1=insert_one_zeros_front(rr1,mask_prelude3);
		r2=insert_one_zeros_front(rr2,mask_prelude3);
		r3=insert_one_zeros_front(rr3,mask_prelude3);
		r4=insert_one_zeros_front(rr4,mask_prelude3);
		r5=insert_one_zeros_front(rr5,mask_prelude3);
		r6=insert_one_zeros_front(rr6,mask_prelude3);

		out2 = prelude_block_pre_9x9_2(r0,r1,r2,r3,r4,r5,r6,c0,c1,c2,c3,c4,c5,c6);

		//merge out1 and out2 and then divide
		out2=_mm256_slli_si256(out2,4);
		out1=_mm256_add_epi32(out1,out2);

		out1 = division_32(division_case,out1,f);

		//elements must be re-arranged before store
		m0=_mm256_and_si256(out1,out_mask_1);
		m1=_mm256_and_si256(out1,out_mask_2);
		m1=_mm256_srli_si256(m1,3);
		m2=_mm256_and_si256(out1,out_mask_3);
		m2=_mm256_slli_si256(m2,2);
		m3=_mm256_and_si256(out1,out_mask_4);
		m3=_mm256_srli_si256(m3,1);
		m0=_mm256_add_epi8(m0,m1);
		m0=_mm256_add_epi8(m0,m2);
		output2=_mm256_add_epi8(m0,m3);

		//5th col iteration
		out1 = prelude_block_pre_9x9_2(rr0,rr1,rr2,rr3,rr4,rr5,rr6,c0,c1,c2,c3,c4,c5,c6);

		//6th col iteration
		r0=_mm256_loadu_si256( (__m256i *) &frame1[load_row][1]);
		r1=_mm256_loadu_si256( (__m256i *) &frame1[load_row+1][1]);
		r2=_mm256_loadu_si256( (__m256i *) &frame1[load_row+2][1]);
		r3=_mm256_loadu_si256( (__m256i *) &frame1[load_row+3][1]);
		r4=_mm256_loadu_si256( (__m256i *) &frame1[load_row+4][1]);
		r5=_mm256_loadu_si256( (__m256i *) &frame1[load_row+5][1]);
		r6=_mm256_loadu_si256( (__m256i *) &frame1[load_row+6][1]);

		out2 = prelude_block_pre_9x9_2(r0,r1,r2,r3,r4,r5,r6,c0,c1,c2,c3,c4,c5,c6);

		//merge out1 and out2 and then divide
		out2=_mm256_slli_si256(out2,4);
		out1=_mm256_add_epi32(out1,out2);

		out1 = division_32(division_case,out1,f);

		//elements must be re-arranged before store
		m0=_mm256_and_si256(out1,out_mask_1);
		m1=_mm256_and_si256(out1,out_mask_2);
		m1=_mm256_srli_si256(m1,3);
		m2=_mm256_and_si256(out1,out_mask_3);
		m2=_mm256_slli_si256(m2,2);
		m3=_mm256_and_si256(out1,out_mask_4);
		m3=_mm256_srli_si256(m3,1);
		m0=_mm256_add_epi8(m0,m1);
		m0=_mm256_add_epi8(m0,m2);
		output3=_mm256_add_epi8(m0,m3);

		//7th col iteration
		r0=_mm256_loadu_si256( (__m256i *) &frame1[load_row][2]);
		r1=_mm256_loadu_si256( (__m256i *) &frame1[load_row+1][2]);
		r2=_mm256_loadu_si256( (__m256i *) &frame1[load_row+2][2]);
		r3=_mm256_loadu_si256( (__m256i *) &frame1[load_row+3][2]);
		r4=_mm256_loadu_si256( (__m256i *) &frame1[load_row+4][2]);
		r5=_mm256_loadu_si256( (__m256i *) &frame1[load_row+5][2]);
		r6=_mm256_loadu_si256( (__m256i *) &frame1[load_row+6][2]);

		out1 = prelude_block_pre_9x9_2(r0,r1,r2,r3,r4,r5,r6,c0,c1,c2,c3,c4,c5,c6);

		//8th col iteration
		r0=_mm256_loadu_si256( (__m256i *) &frame1[load_row][3]);
		r1=_mm256_loadu_si256( (__m256i *) &frame1[load_row+1][3]);
		r2=_mm256_loadu_si256( (__m256i *) &frame1[load_row+2][3]);
		r3=_mm256_loadu_si256( (__m256i *) &frame1[load_row+3][3]);
		r4=_mm256_loadu_si256( (__m256i *) &frame1[load_row+4][3]);
		r5=_mm256_loadu_si256( (__m256i *) &frame1[load_row+5][3]);
		r6=_mm256_loadu_si256( (__m256i *) &frame1[load_row+6][3]);

		out2 = prelude_block_pre_9x9_2(r0,r1,r2,r3,r4,r5,r6,c0,c1,c2,c3,c4,c5,c6);

		//merge out1 and out2 and then divide
		out2=_mm256_slli_si256(out2,4);
		out1=_mm256_add_epi32(out1,out2);

		out1 = division_32(division_case,out1,f);

		//elements must be re-arranged before store
		m0=_mm256_and_si256(out1,out_mask_1);
		m1=_mm256_and_si256(out1,out_mask_2);
		m1=_mm256_srli_si256(m1,3);
		m2=_mm256_and_si256(out1,out_mask_3);
		m2=_mm256_slli_si256(m2,2);
		m3=_mm256_and_si256(out1,out_mask_4);
		m3=_mm256_srli_si256(m3,1);
		m0=_mm256_add_epi8(m0,m1);
		m0=_mm256_add_epi8(m0,m2);
		output4=_mm256_add_epi8(m0,m3);

		//9th col iteration
		r0=_mm256_loadu_si256( (__m256i *) &frame1[load_row][4]);
		r1=_mm256_loadu_si256( (__m256i *) &frame1[load_row+1][4]);
		r2=_mm256_loadu_si256( (__m256i *) &frame1[load_row+2][4]);
		r3=_mm256_loadu_si256( (__m256i *) &frame1[load_row+3][4]);
		r4=_mm256_loadu_si256( (__m256i *) &frame1[load_row+4][4]);
		r5=_mm256_loadu_si256( (__m256i *) &frame1[load_row+5][4]);
		r6=_mm256_loadu_si256( (__m256i *) &frame1[load_row+6][4]);

		out1 = prelude_block_pre_9x9_2(r0,r1,r2,r3,r4,r5,r6,c0,c1,c2,c3,c4,c5,c6);

		//10th col iteration
		r0=_mm256_loadu_si256( (__m256i *) &frame1[load_row][5]);
		r1=_mm256_loadu_si256( (__m256i *) &frame1[load_row+1][5]);
		r2=_mm256_loadu_si256( (__m256i *) &frame1[load_row+2][5]);
		r3=_mm256_loadu_si256( (__m256i *) &frame1[load_row+3][5]);
		r4=_mm256_loadu_si256( (__m256i *) &frame1[load_row+4][5]);
		r5=_mm256_loadu_si256( (__m256i *) &frame1[load_row+5][5]);
		r6=_mm256_loadu_si256( (__m256i *) &frame1[load_row+6][5]);

		out2 = prelude_block_pre_9x9_2(r0,r1,r2,r3,r4,r5,r6,c0,c1,c2,c3,c4,c5,c6);

		//merge out1 and out2 and then divide
		out2=_mm256_slli_si256(out2,4);
		out1=_mm256_add_epi32(out1,out2);

		out1 = division_32(division_case,out1,f);

		//elements must be re-arranged before store
		m0=_mm256_and_si256(out1,out_mask_1);
		m1=_mm256_and_si256(out1,out_mask_2);
		m1=_mm256_srli_si256(m1,3);
		m2=_mm256_and_si256(out1,out_mask_3);
		m2=_mm256_slli_si256(m2,2);
		m3=_mm256_and_si256(out1,out_mask_4);
		m3=_mm256_srli_si256(m3,1);
		m0=_mm256_add_epi8(m0,m1);
		m0=_mm256_add_epi8(m0,m2);
		output5=_mm256_add_epi8(m0,m3);

			//blend the output1:output5
			output2=_mm256_slli_si256(output2,2);
			output3=_mm256_slli_si256(output3,4);

			m3=_mm256_slli_si256(output4,6);
			m4=_mm256_and_si256(output4,out_mask_5);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_srli_si256(m4,10);
			output4=_mm256_add_epi32(m3,m4);

			m3=_mm256_slli_si256(output5,8);
			m4=_mm256_and_si256(output5,out_mask_5);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_srli_si256(m4,8);
			output5=_mm256_add_epi32(m3,m4);

			output1=_mm256_add_epi8(output1,output2);
			output1=_mm256_add_epi8(output1,output3);
			output1=_mm256_add_epi8(output1,output4);
			output1=_mm256_add_epi8(output1,output5);

				_mm256_store_si256( (__m256i *) &filt[row][0],output1 );
		  	}
		  else {
			  //1st col

		out1=prelude_block_pre_9x9_2m(frame1,load_row,col-4,c0,c1,c2,c3,c4,c5,c6);

		  //2nd col

		out2=prelude_block_pre_9x9_2m(frame1,load_row,col-3,c0,c1,c2,c3,c4,c5,c6);

			//merge out1 and out2 and then divide
			out2=_mm256_slli_si256(out2,4);
			out1=_mm256_add_epi32(out1,out2);

			out1 = division_32(division_case,out1,f);

			//elements must be re-arranged before store
			m0=_mm256_and_si256(out1,out_mask_1);
			m1=_mm256_and_si256(out1,out_mask_2);
			m1=_mm256_srli_si256(m1,3);
			m2=_mm256_and_si256(out1,out_mask_3);
			m2=_mm256_slli_si256(m2,2);
			m3=_mm256_and_si256(out1,out_mask_4);
			m3=_mm256_srli_si256(m3,1);
			m0=_mm256_add_epi8(m0,m1);
			m0=_mm256_add_epi8(m0,m2);
			output1=_mm256_add_epi8(m0,m3);

		//3rd col
		out1=prelude_block_pre_9x9_2m(frame1,load_row,col-2,c0,c1,c2,c3,c4,c5,c6);

		//4th col
		out2=prelude_block_pre_9x9_2m(frame1,load_row,col-1,c0,c1,c2,c3,c4,c5,c6);

		//merge out1 and out2 and then divide
		out2=_mm256_slli_si256(out2,4);
		out1=_mm256_add_epi32(out1,out2);

		out1 = division_32(division_case,out1,f);

		//elements must be re-arranged before store
		m0=_mm256_and_si256(out1,out_mask_1);
		m1=_mm256_and_si256(out1,out_mask_2);
		m1=_mm256_srli_si256(m1,3);
		m2=_mm256_and_si256(out1,out_mask_3);
		m2=_mm256_slli_si256(m2,2);
		m3=_mm256_and_si256(out1,out_mask_4);
		m3=_mm256_srli_si256(m3,1);
		m0=_mm256_add_epi8(m0,m1);
		m0=_mm256_add_epi8(m0,m2);
		output2=_mm256_add_epi8(m0,m3);

		//5th col iteration
		out1=prelude_block_pre_9x9_2m(frame1,load_row,col,c0,c1,c2,c3,c4,c5,c6);

		//6th col iteration
		out2=prelude_block_pre_9x9_2m(frame1,load_row,col+1,c0,c1,c2,c3,c4,c5,c6);

		//merge out1 and out2 and then divide
		out2=_mm256_slli_si256(out2,4);
		out1=_mm256_add_epi32(out1,out2);

		out1 = division_32(division_case,out1,f);

		//elements must be re-arranged before store
		m0=_mm256_and_si256(out1,out_mask_1);
		m1=_mm256_and_si256(out1,out_mask_2);
		m1=_mm256_srli_si256(m1,3);
		m2=_mm256_and_si256(out1,out_mask_3);
		m2=_mm256_slli_si256(m2,2);
		m3=_mm256_and_si256(out1,out_mask_4);
		m3=_mm256_srli_si256(m3,1);
		m0=_mm256_add_epi8(m0,m1);
		m0=_mm256_add_epi8(m0,m2);
		output3=_mm256_add_epi8(m0,m3);

		//7th col iteration
		out1=prelude_block_pre_9x9_2m(frame1,load_row,col+2,c0,c1,c2,c3,c4,c5,c6);


		//8th col iteration
		out2=prelude_block_pre_9x9_2m(frame1,load_row,col+3,c0,c1,c2,c3,c4,c5,c6);


		//merge out1 and out2 and then divide
		out2=_mm256_slli_si256(out2,4);
		out1=_mm256_add_epi32(out1,out2);

		out1 = division_32(division_case,out1,f);

		//elements must be re-arranged before store
		m0=_mm256_and_si256(out1,out_mask_1);
		m1=_mm256_and_si256(out1,out_mask_2);
		m1=_mm256_srli_si256(m1,3);
		m2=_mm256_and_si256(out1,out_mask_3);
		m2=_mm256_slli_si256(m2,2);
		m3=_mm256_and_si256(out1,out_mask_4);
		m3=_mm256_srli_si256(m3,1);
		m0=_mm256_add_epi8(m0,m1);
		m0=_mm256_add_epi8(m0,m2);
		output4=_mm256_add_epi8(m0,m3);

		//9th col iteration
		out1=prelude_block_pre_9x9_2m(frame1,load_row,col+4,c0,c1,c2,c3,c4,c5,c6);


		//10th col iteration
		out2=prelude_block_pre_9x9_2m(frame1,load_row,col+5,c0,c1,c2,c3,c4,c5,c6);


		//merge out1 and out2 and then divide
		out2=_mm256_slli_si256(out2,4);
		out1=_mm256_add_epi32(out1,out2);

		out1 = division_32(division_case,out1,f);

		//elements must be re-arranged before store
		m0=_mm256_and_si256(out1,out_mask_1);
		m1=_mm256_and_si256(out1,out_mask_2);
		m1=_mm256_srli_si256(m1,3);
		m2=_mm256_and_si256(out1,out_mask_3);
		m2=_mm256_slli_si256(m2,2);
		m3=_mm256_and_si256(out1,out_mask_4);
		m3=_mm256_srli_si256(m3,1);
		m0=_mm256_add_epi8(m0,m1);
		m0=_mm256_add_epi8(m0,m2);
		output5=_mm256_add_epi8(m0,m3);

			//blend the output1:output5
			output2=_mm256_slli_si256(output2,2);
			output3=_mm256_slli_si256(output3,4);

			m3=_mm256_slli_si256(output4,6);
			m4=_mm256_and_si256(output4,out_mask_5);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_srli_si256(m4,10);
			output4=_mm256_add_epi32(m3,m4);

			m3=_mm256_slli_si256(output5,8);
			m4=_mm256_and_si256(output5,out_mask_5);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_srli_si256(m4,8);
			output5=_mm256_add_epi32(m3,m4);

			output1=_mm256_add_epi8(output1,output2);
			output1=_mm256_add_epi8(output1,output3);
			output1=_mm256_add_epi8(output1,output4);
			output1=_mm256_add_epi8(output1,output5);

			_mm256_storeu_si256( (__m256i *) &filt[row][col],output1 );

		  }
}
}




__m256i prelude_block_pre_9x9_2(const __m256i r0,const __m256i r1,const __m256i r2,const __m256i r3,const __m256i r4,const __m256i r5,const __m256i r6, const __m256i c0,const __m256i c1,const __m256i c2,const __m256i c3,const __m256i c4,const __m256i c5,const __m256i c6){

	__m256i m0,m1,m2,m3,m4,m5,m6;
	const __m256i mask_z    = _mm256_set_epi16(0,0,0,0,0,0,0,0,0,0,0,65535,0,0,0,0);
	const __m256i mask_unz    = _mm256_set_epi16(65535,65535,65535,65535,65535,65535,65535,65535,65535,65535,65535,0,65535,65535,65535,65535);
	const __m256i mask_32_1    = _mm256_set_epi32(0xffffffff,0,0,0xffffffff,0,0,0,0);
	const __m256i mask_32_2    = _mm256_set_epi32(0,0,0,0xffffffff,0,0,0,0);
	const __m256i mask_32_a    = _mm256_set_epi32(0,0,0xffffffff,0,0,0xffffffff,0,0xffffffff);
	const __m256i ones=_mm256_set1_epi16(1);


//multiply by the mask
m0=_mm256_maddubs_epi16(r0,c0);
m1=_mm256_maddubs_epi16(r1,c1);
m2=_mm256_maddubs_epi16(r2,c2);
m3=_mm256_maddubs_epi16(r3,c3);
m4=_mm256_maddubs_epi16(r4,c4);
m5=_mm256_maddubs_epi16(r5,c5);
m6=_mm256_maddubs_epi16(r6,c6);

	//keep a copy of the 4th element
__m256i	m0_copy=_mm256_and_si256(m0,mask_z);
__m256i	m1_copy=_mm256_and_si256(m1,mask_z);
__m256i	m2_copy=_mm256_and_si256(m2,mask_z);
__m256i	m3_copy=_mm256_and_si256(m3,mask_z);
__m256i	m4_copy=_mm256_and_si256(m4,mask_z);
__m256i	m5_copy=_mm256_and_si256(m5,mask_z);
__m256i	m6_copy=_mm256_and_si256(m6,mask_z);

	//add all the 4th elements above
	//16-bit addition here
	m0_copy=_mm256_add_epi16(m0_copy,m1_copy);
	m0_copy=_mm256_add_epi16(m0_copy,m2_copy);
	m0_copy=_mm256_add_epi16(m0_copy,m3_copy);
	m0_copy=_mm256_add_epi16(m0_copy,m4_copy);
	m0_copy=_mm256_add_epi16(m0_copy,m5_copy);
	m0_copy=_mm256_add_epi16(m0_copy,m6_copy);

	//zero the 4th out of 16 elements
	m0=_mm256_and_si256(m0,mask_unz);
	m1=_mm256_and_si256(m1,mask_unz);
	m2=_mm256_and_si256(m2,mask_unz);
	m3=_mm256_and_si256(m3,mask_unz);
	m4=_mm256_and_si256(m4,mask_unz);
	m5=_mm256_and_si256(m5,mask_unz);
	m6=_mm256_and_si256(m6,mask_unz);


//make 16-bit values 32-bit (horizontal add)
m0=_mm256_madd_epi16(m0,ones);
m1=_mm256_madd_epi16(m1,ones);
m2=_mm256_madd_epi16(m2,ones);
m3=_mm256_madd_epi16(m3,ones);
m4=_mm256_madd_epi16(m4,ones);
m5=_mm256_madd_epi16(m5,ones);
m6=_mm256_madd_epi16(m6,ones);

//vertical add
m0=_mm256_add_epi32(m0,m1);
m0=_mm256_add_epi32(m0,m2);
m0=_mm256_add_epi32(m0,m3);
m0=_mm256_add_epi32(m0,m4);
m0=_mm256_add_epi32(m0,m5);
m0=_mm256_add_epi32(m0,m6);

//horizontal add
m2=_mm256_srli_si256(m0,4);
	m2=_mm256_add_epi32(m0,m2);

	//keep only 4th and 7th out of 8 elements
	m1=_mm256_and_si256(m0,mask_32_1);
	m1=_mm256_add_epi32(m1,m0_copy);

	//shift m1 and add with m2 (complex shift is needed)
	m3=_mm256_srli_si256(m1,8);
	m4=_mm256_and_si256(m1,mask_32_2);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,8);
	m3=_mm256_add_epi32(m3,m4);
	m2=_mm256_add_epi32(m2,m3);
	return _mm256_and_si256(m2,mask_32_a);//in 32-bit format keep only 0,2,5 and discard the others

	}


__m256i prelude_block_pre_9x9_2m(unsigned char **frame1,const unsigned int load_row, const unsigned int load_col, const __m256i c0,const __m256i c1,const __m256i c2,const __m256i c3,const __m256i c4,const __m256i c5,const __m256i c6){

	__m256i m0,m1,m2,m3,m4,m5,m6;
	const __m256i mask_z    = _mm256_set_epi16(0,0,0,0,0,0,0,0,0,0,0,65535,0,0,0,0);
	const __m256i mask_unz    = _mm256_set_epi16(65535,65535,65535,65535,65535,65535,65535,65535,65535,65535,65535,0,65535,65535,65535,65535);
	const __m256i mask_32_1    = _mm256_set_epi32(0xffffffff,0,0,0xffffffff,0,0,0,0);
	const __m256i mask_32_2    = _mm256_set_epi32(0,0,0,0xffffffff,0,0,0,0);
	const __m256i mask_32_a    = _mm256_set_epi32(0,0,0xffffffff,0,0,0xffffffff,0,0xffffffff);
	const __m256i ones=_mm256_set1_epi16(1);

__m256i		r0=_mm256_loadu_si256( (__m256i *) &frame1[load_row][load_col]);
__m256i		r1=_mm256_loadu_si256( (__m256i *) &frame1[load_row+1][load_col]);
__m256i		r2=_mm256_loadu_si256( (__m256i *) &frame1[load_row+2][load_col]);
__m256i		r3=_mm256_loadu_si256( (__m256i *) &frame1[load_row+3][load_col]);
__m256i		r4=_mm256_loadu_si256( (__m256i *) &frame1[load_row+4][load_col]);
__m256i		r5=_mm256_loadu_si256( (__m256i *) &frame1[load_row+5][load_col]);
__m256i		r6=_mm256_loadu_si256( (__m256i *) &frame1[load_row+6][load_col]);

//multiply by the mask
m0=_mm256_maddubs_epi16(r0,c0);
m1=_mm256_maddubs_epi16(r1,c1);
m2=_mm256_maddubs_epi16(r2,c2);
m3=_mm256_maddubs_epi16(r3,c3);
m4=_mm256_maddubs_epi16(r4,c4);
m5=_mm256_maddubs_epi16(r5,c5);
m6=_mm256_maddubs_epi16(r6,c6);

	//keep a copy of the 4th element
__m256i	m0_copy=_mm256_and_si256(m0,mask_z);
__m256i	m1_copy=_mm256_and_si256(m1,mask_z);
__m256i	m2_copy=_mm256_and_si256(m2,mask_z);
__m256i	m3_copy=_mm256_and_si256(m3,mask_z);
__m256i	m4_copy=_mm256_and_si256(m4,mask_z);
__m256i	m5_copy=_mm256_and_si256(m5,mask_z);
__m256i	m6_copy=_mm256_and_si256(m6,mask_z);

	//add all the 4th elements above
	//16-bit addition here
	m0_copy=_mm256_add_epi16(m0_copy,m1_copy);
	m0_copy=_mm256_add_epi16(m0_copy,m2_copy);
	m0_copy=_mm256_add_epi16(m0_copy,m3_copy);
	m0_copy=_mm256_add_epi16(m0_copy,m4_copy);
	m0_copy=_mm256_add_epi16(m0_copy,m5_copy);
	m0_copy=_mm256_add_epi16(m0_copy,m6_copy);

	//zero the 4th out of 16 elements
	m0=_mm256_and_si256(m0,mask_unz);
	m1=_mm256_and_si256(m1,mask_unz);
	m2=_mm256_and_si256(m2,mask_unz);
	m3=_mm256_and_si256(m3,mask_unz);
	m4=_mm256_and_si256(m4,mask_unz);
	m5=_mm256_and_si256(m5,mask_unz);
	m6=_mm256_and_si256(m6,mask_unz);


//make 16-bit values 32-bit (horizontal add)
m0=_mm256_madd_epi16(m0,ones);
m1=_mm256_madd_epi16(m1,ones);
m2=_mm256_madd_epi16(m2,ones);
m3=_mm256_madd_epi16(m3,ones);
m4=_mm256_madd_epi16(m4,ones);
m5=_mm256_madd_epi16(m5,ones);
m6=_mm256_madd_epi16(m6,ones);

//vertical add
m0=_mm256_add_epi32(m0,m1);
m0=_mm256_add_epi32(m0,m2);
m0=_mm256_add_epi32(m0,m3);
m0=_mm256_add_epi32(m0,m4);
m0=_mm256_add_epi32(m0,m5);
m0=_mm256_add_epi32(m0,m6);

//horizontal add
m2=_mm256_srli_si256(m0,4);
	m2=_mm256_add_epi32(m0,m2);

	//keep only 4th and 7th out of 8 elements
	m1=_mm256_and_si256(m0,mask_32_1);
	m1=_mm256_add_epi32(m1,m0_copy);

	//shift m1 and add with m2 (complex shift is needed)
	m3=_mm256_srli_si256(m1,8);
	m4=_mm256_and_si256(m1,mask_32_2);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,8);
	m3=_mm256_add_epi32(m3,m4);
	m2=_mm256_add_epi32(m2,m3);
	return _mm256_and_si256(m2,mask_32_a);//in 32-bit format keep only 0,2,5 and discard the others

	}




//this is for computing row=1 and row=N-2 only
void prelude_block_1_9x9(unsigned char **frame1,unsigned char **filt,const unsigned int row, const unsigned int load_row,const unsigned int M, const unsigned int division_case, const __m256i f, const __m256i c0,const __m256i c1,const __m256i c2,const __m256i c3,const __m256i c4,const __m256i c5){

	unsigned int col;
    __m256i rr0,rr1,rr2,rr3,rr4,rr5,r0,r1,r2,r3,r4,r5,out1,out2,output1,output2,output3,output4,output5,m0,m1,m2,m3,m4;


	const __m256i out_mask_1    = _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255);//0,20
	const __m256i out_mask_2    = _mm256_set_epi8(0,0,0,0,0,0,0,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255,0,0,0,0);//4,24
	const __m256i out_mask_3    = _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, 0,0,0,0,0,0,0,255,0,0,0,0,0,0,0,0);//8
	const __m256i out_mask_4    = _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, 0,0,0,255,0,0,0,0,0,0,0,0,0,0,0,0);//12
	const __m256i out_mask_5    = _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, 0,0,0,0,255,255,0,0,0,0,0,0,0,0,0,0);//10,11


	const __m256i mask_prelude   = _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0);
	const __m256i mask_prelude1  = _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0);
	const __m256i mask_prelude2  = _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0);
	const __m256i mask_prelude3  = _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0);


	  for (col = 0; col <= M-32-5; col+=30){

		  if (col==0){
			  //1st col
			rr0=_mm256_load_si256( (__m256i *) &frame1[load_row][0]);
			rr1=_mm256_load_si256( (__m256i *) &frame1[load_row+1][0]);
			rr2=_mm256_load_si256( (__m256i *) &frame1[load_row+2][0]);
			rr3=_mm256_load_si256( (__m256i *) &frame1[load_row+3][0]);
			rr4=_mm256_load_si256( (__m256i *) &frame1[load_row+4][0]);
			rr5=_mm256_load_si256( (__m256i *) &frame1[load_row+5][0]);

			//START - extra code needed for prelude
		 r0=insert_four_zeros_front(rr0,mask_prelude);
		 r1=insert_four_zeros_front(rr1,mask_prelude);
		 r2=insert_four_zeros_front(rr2,mask_prelude);
		 r3=insert_four_zeros_front(rr3,mask_prelude);
		 r4=insert_four_zeros_front(rr4,mask_prelude);
		 r5=insert_four_zeros_front(rr5,mask_prelude);

		out1=prelude_block_pre_9x9_1(r0,r1,r2,r3,r4,r5,c0,c1,c2,c3,c4,c5);

		  //2nd col

			//START - extra code needed for prelude
		  r0=insert_three_zeros_front(rr0,mask_prelude1);
		  r1=insert_three_zeros_front(rr1,mask_prelude1);
		  r2=insert_three_zeros_front(rr2,mask_prelude1);
		  r3=insert_three_zeros_front(rr3,mask_prelude1);
		  r4=insert_three_zeros_front(rr4,mask_prelude1);
		  r5=insert_three_zeros_front(rr5,mask_prelude1);

		out2=prelude_block_pre_9x9_1(r0,r1,r2,r3,r4,r5,c0,c1,c2,c3,c4,c5);

			//merge out1 and out2 and then divide
			out2=_mm256_slli_si256(out2,4);
			out1=_mm256_add_epi32(out1,out2);

			out1 = division_32(division_case,out1,f);

			//elements must be re-arranged before store
			m0=_mm256_and_si256(out1,out_mask_1);
			m1=_mm256_and_si256(out1,out_mask_2);
			m1=_mm256_srli_si256(m1,3);
			m2=_mm256_and_si256(out1,out_mask_3);
			m2=_mm256_slli_si256(m2,2);
			m3=_mm256_and_si256(out1,out_mask_4);
			m3=_mm256_srli_si256(m3,1);
			m0=_mm256_add_epi8(m0,m1);
			m0=_mm256_add_epi8(m0,m2);
			output1=_mm256_add_epi8(m0,m3);

		//3rd col
		//START - extra code needed for prelude
		r0=insert_two_zeros_front(rr0,mask_prelude2);
		r1=insert_two_zeros_front(rr1,mask_prelude2);
		r2=insert_two_zeros_front(rr2,mask_prelude2);
		r3=insert_two_zeros_front(rr3,mask_prelude2);
		r4=insert_two_zeros_front(rr4,mask_prelude2);
		r5=insert_two_zeros_front(rr5,mask_prelude2);

		out1=prelude_block_pre_9x9_1(r0,r1,r2,r3,r4,r5,c0,c1,c2,c3,c4,c5);

		//4th col
		r0=insert_one_zeros_front(rr0,mask_prelude3);
		r1=insert_one_zeros_front(rr1,mask_prelude3);
		r2=insert_one_zeros_front(rr2,mask_prelude3);
		r3=insert_one_zeros_front(rr3,mask_prelude3);
		r4=insert_one_zeros_front(rr4,mask_prelude3);
		r5=insert_one_zeros_front(rr5,mask_prelude3);

		out2 = prelude_block_pre_9x9_1(r0,r1,r2,r3,r4,r5,c0,c1,c2,c3,c4,c5);

		//merge out1 and out2 and then divide
		out2=_mm256_slli_si256(out2,4);
		out1=_mm256_add_epi32(out1,out2);

		out1 = division_32(division_case,out1,f);

		//elements must be re-arranged before store
		m0=_mm256_and_si256(out1,out_mask_1);
		m1=_mm256_and_si256(out1,out_mask_2);
		m1=_mm256_srli_si256(m1,3);
		m2=_mm256_and_si256(out1,out_mask_3);
		m2=_mm256_slli_si256(m2,2);
		m3=_mm256_and_si256(out1,out_mask_4);
		m3=_mm256_srli_si256(m3,1);
		m0=_mm256_add_epi8(m0,m1);
		m0=_mm256_add_epi8(m0,m2);
		output2=_mm256_add_epi8(m0,m3);

		//5th col iteration
		out1 = prelude_block_pre_9x9_1(rr0,rr1,rr2,rr3,rr4,rr5,c0,c1,c2,c3,c4,c5);

		//6th col iteration
		r0=_mm256_loadu_si256( (__m256i *) &frame1[load_row][1]);
		r1=_mm256_loadu_si256( (__m256i *) &frame1[load_row+1][1]);
		r2=_mm256_loadu_si256( (__m256i *) &frame1[load_row+2][1]);
		r3=_mm256_loadu_si256( (__m256i *) &frame1[load_row+3][1]);
		r4=_mm256_loadu_si256( (__m256i *) &frame1[load_row+4][1]);
		r5=_mm256_loadu_si256( (__m256i *) &frame1[load_row+5][1]);

		out2 = prelude_block_pre_9x9_1(r0,r1,r2,r3,r4,r5,c0,c1,c2,c3,c4,c5);

		//merge out1 and out2 and then divide
		out2=_mm256_slli_si256(out2,4);
		out1=_mm256_add_epi32(out1,out2);

		out1 = division_32(division_case,out1,f);

		//elements must be re-arranged before store
		m0=_mm256_and_si256(out1,out_mask_1);
		m1=_mm256_and_si256(out1,out_mask_2);
		m1=_mm256_srli_si256(m1,3);
		m2=_mm256_and_si256(out1,out_mask_3);
		m2=_mm256_slli_si256(m2,2);
		m3=_mm256_and_si256(out1,out_mask_4);
		m3=_mm256_srli_si256(m3,1);
		m0=_mm256_add_epi8(m0,m1);
		m0=_mm256_add_epi8(m0,m2);
		output3=_mm256_add_epi8(m0,m3);

		//7th col iteration
		r0=_mm256_loadu_si256( (__m256i *) &frame1[load_row][2]);
		r1=_mm256_loadu_si256( (__m256i *) &frame1[load_row+1][2]);
		r2=_mm256_loadu_si256( (__m256i *) &frame1[load_row+2][2]);
		r3=_mm256_loadu_si256( (__m256i *) &frame1[load_row+3][2]);
		r4=_mm256_loadu_si256( (__m256i *) &frame1[load_row+4][2]);
		r5=_mm256_loadu_si256( (__m256i *) &frame1[load_row+5][2]);

		out1 = prelude_block_pre_9x9_1(r0,r1,r2,r3,r4,r5,c0,c1,c2,c3,c4,c5);

		//8th col iteration
		r0=_mm256_loadu_si256( (__m256i *) &frame1[load_row][3]);
		r1=_mm256_loadu_si256( (__m256i *) &frame1[load_row+1][3]);
		r2=_mm256_loadu_si256( (__m256i *) &frame1[load_row+2][3]);
		r3=_mm256_loadu_si256( (__m256i *) &frame1[load_row+3][3]);
		r4=_mm256_loadu_si256( (__m256i *) &frame1[load_row+4][3]);
		r5=_mm256_loadu_si256( (__m256i *) &frame1[load_row+5][3]);

		out2 = prelude_block_pre_9x9_1(r0,r1,r2,r3,r4,r5,c0,c1,c2,c3,c4,c5);

		//merge out1 and out2 and then divide
		out2=_mm256_slli_si256(out2,4);
		out1=_mm256_add_epi32(out1,out2);

		out1 = division_32(division_case,out1,f);

		//elements must be re-arranged before store
		m0=_mm256_and_si256(out1,out_mask_1);
		m1=_mm256_and_si256(out1,out_mask_2);
		m1=_mm256_srli_si256(m1,3);
		m2=_mm256_and_si256(out1,out_mask_3);
		m2=_mm256_slli_si256(m2,2);
		m3=_mm256_and_si256(out1,out_mask_4);
		m3=_mm256_srli_si256(m3,1);
		m0=_mm256_add_epi8(m0,m1);
		m0=_mm256_add_epi8(m0,m2);
		output4=_mm256_add_epi8(m0,m3);

		//9th col iteration
		r0=_mm256_loadu_si256( (__m256i *) &frame1[load_row][4]);
		r1=_mm256_loadu_si256( (__m256i *) &frame1[load_row+1][4]);
		r2=_mm256_loadu_si256( (__m256i *) &frame1[load_row+2][4]);
		r3=_mm256_loadu_si256( (__m256i *) &frame1[load_row+3][4]);
		r4=_mm256_loadu_si256( (__m256i *) &frame1[load_row+4][4]);
		r5=_mm256_loadu_si256( (__m256i *) &frame1[load_row+5][4]);

		out1 = prelude_block_pre_9x9_1(r0,r1,r2,r3,r4,r5,c0,c1,c2,c3,c4,c5);

		//10th col iteration
		r0=_mm256_loadu_si256( (__m256i *) &frame1[load_row][5]);
		r1=_mm256_loadu_si256( (__m256i *) &frame1[load_row+1][5]);
		r2=_mm256_loadu_si256( (__m256i *) &frame1[load_row+2][5]);
		r3=_mm256_loadu_si256( (__m256i *) &frame1[load_row+3][5]);
		r4=_mm256_loadu_si256( (__m256i *) &frame1[load_row+4][5]);
		r5=_mm256_loadu_si256( (__m256i *) &frame1[load_row+5][5]);

		out2 = prelude_block_pre_9x9_1(r0,r1,r2,r3,r4,r5,c0,c1,c2,c3,c4,c5);

		//merge out1 and out2 and then divide
		out2=_mm256_slli_si256(out2,4);
		out1=_mm256_add_epi32(out1,out2);

		out1 = division_32(division_case,out1,f);

		//elements must be re-arranged before store
		m0=_mm256_and_si256(out1,out_mask_1);
		m1=_mm256_and_si256(out1,out_mask_2);
		m1=_mm256_srli_si256(m1,3);
		m2=_mm256_and_si256(out1,out_mask_3);
		m2=_mm256_slli_si256(m2,2);
		m3=_mm256_and_si256(out1,out_mask_4);
		m3=_mm256_srli_si256(m3,1);
		m0=_mm256_add_epi8(m0,m1);
		m0=_mm256_add_epi8(m0,m2);
		output5=_mm256_add_epi8(m0,m3);

			//blend the output1:output5
			output2=_mm256_slli_si256(output2,2);
			output3=_mm256_slli_si256(output3,4);

			m3=_mm256_slli_si256(output4,6);
			m4=_mm256_and_si256(output4,out_mask_5);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_srli_si256(m4,10);
			output4=_mm256_add_epi32(m3,m4);

			m3=_mm256_slli_si256(output5,8);
			m4=_mm256_and_si256(output5,out_mask_5);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_srli_si256(m4,8);
			output5=_mm256_add_epi32(m3,m4);

			output1=_mm256_add_epi8(output1,output2);
			output1=_mm256_add_epi8(output1,output3);
			output1=_mm256_add_epi8(output1,output4);
			output1=_mm256_add_epi8(output1,output5);

				_mm256_store_si256( (__m256i *) &filt[row][0],output1 );
		  	}
		  else {
			  //1st col

		out1=prelude_block_pre_9x9_1m(frame1,load_row,col-4,c0,c1,c2,c3,c4,c5);

		  //2nd col

		out2=prelude_block_pre_9x9_1m(frame1,load_row,col-3,c0,c1,c2,c3,c4,c5);

			//merge out1 and out2 and then divide
			out2=_mm256_slli_si256(out2,4);
			out1=_mm256_add_epi32(out1,out2);

			out1 = division_32(division_case,out1,f);

			//elements must be re-arranged before store
			m0=_mm256_and_si256(out1,out_mask_1);
			m1=_mm256_and_si256(out1,out_mask_2);
			m1=_mm256_srli_si256(m1,3);
			m2=_mm256_and_si256(out1,out_mask_3);
			m2=_mm256_slli_si256(m2,2);
			m3=_mm256_and_si256(out1,out_mask_4);
			m3=_mm256_srli_si256(m3,1);
			m0=_mm256_add_epi8(m0,m1);
			m0=_mm256_add_epi8(m0,m2);
			output1=_mm256_add_epi8(m0,m3);

		//3rd col
		out1=prelude_block_pre_9x9_1m(frame1,load_row,col-2,c0,c1,c2,c3,c4,c5);

		//4th col
		out2=prelude_block_pre_9x9_1m(frame1,load_row,col-1,c0,c1,c2,c3,c4,c5);

		//merge out1 and out2 and then divide
		out2=_mm256_slli_si256(out2,4);
		out1=_mm256_add_epi32(out1,out2);

		out1 = division_32(division_case,out1,f);

		//elements must be re-arranged before store
		m0=_mm256_and_si256(out1,out_mask_1);
		m1=_mm256_and_si256(out1,out_mask_2);
		m1=_mm256_srli_si256(m1,3);
		m2=_mm256_and_si256(out1,out_mask_3);
		m2=_mm256_slli_si256(m2,2);
		m3=_mm256_and_si256(out1,out_mask_4);
		m3=_mm256_srli_si256(m3,1);
		m0=_mm256_add_epi8(m0,m1);
		m0=_mm256_add_epi8(m0,m2);
		output2=_mm256_add_epi8(m0,m3);

		//5th col iteration
		out1=prelude_block_pre_9x9_1m(frame1,load_row,col,c0,c1,c2,c3,c4,c5);

		//6th col iteration
		out2=prelude_block_pre_9x9_1m(frame1,load_row,col+1,c0,c1,c2,c3,c4,c5);

		//merge out1 and out2 and then divide
		out2=_mm256_slli_si256(out2,4);
		out1=_mm256_add_epi32(out1,out2);

		out1 = division_32(division_case,out1,f);

		//elements must be re-arranged before store
		m0=_mm256_and_si256(out1,out_mask_1);
		m1=_mm256_and_si256(out1,out_mask_2);
		m1=_mm256_srli_si256(m1,3);
		m2=_mm256_and_si256(out1,out_mask_3);
		m2=_mm256_slli_si256(m2,2);
		m3=_mm256_and_si256(out1,out_mask_4);
		m3=_mm256_srli_si256(m3,1);
		m0=_mm256_add_epi8(m0,m1);
		m0=_mm256_add_epi8(m0,m2);
		output3=_mm256_add_epi8(m0,m3);

		//7th col iteration
		out1=prelude_block_pre_9x9_1m(frame1,load_row,col+2,c0,c1,c2,c3,c4,c5);


		//8th col iteration
		out2=prelude_block_pre_9x9_1m(frame1,load_row,col+3,c0,c1,c2,c3,c4,c5);


		//merge out1 and out2 and then divide
		out2=_mm256_slli_si256(out2,4);
		out1=_mm256_add_epi32(out1,out2);

		out1 = division_32(division_case,out1,f);

		//elements must be re-arranged before store
		m0=_mm256_and_si256(out1,out_mask_1);
		m1=_mm256_and_si256(out1,out_mask_2);
		m1=_mm256_srli_si256(m1,3);
		m2=_mm256_and_si256(out1,out_mask_3);
		m2=_mm256_slli_si256(m2,2);
		m3=_mm256_and_si256(out1,out_mask_4);
		m3=_mm256_srli_si256(m3,1);
		m0=_mm256_add_epi8(m0,m1);
		m0=_mm256_add_epi8(m0,m2);
		output4=_mm256_add_epi8(m0,m3);

		//9th col iteration
		out1=prelude_block_pre_9x9_1m(frame1,load_row,col+4,c0,c1,c2,c3,c4,c5);


		//10th col iteration
		out2=prelude_block_pre_9x9_1m(frame1,load_row,col+5,c0,c1,c2,c3,c4,c5);


		//merge out1 and out2 and then divide
		out2=_mm256_slli_si256(out2,4);
		out1=_mm256_add_epi32(out1,out2);

		out1 = division_32(division_case,out1,f);

		//elements must be re-arranged before store
		m0=_mm256_and_si256(out1,out_mask_1);
		m1=_mm256_and_si256(out1,out_mask_2);
		m1=_mm256_srli_si256(m1,3);
		m2=_mm256_and_si256(out1,out_mask_3);
		m2=_mm256_slli_si256(m2,2);
		m3=_mm256_and_si256(out1,out_mask_4);
		m3=_mm256_srli_si256(m3,1);
		m0=_mm256_add_epi8(m0,m1);
		m0=_mm256_add_epi8(m0,m2);
		output5=_mm256_add_epi8(m0,m3);

			//blend the output1:output5
			output2=_mm256_slli_si256(output2,2);
			output3=_mm256_slli_si256(output3,4);

			m3=_mm256_slli_si256(output4,6);
			m4=_mm256_and_si256(output4,out_mask_5);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_srli_si256(m4,10);
			output4=_mm256_add_epi32(m3,m4);

			m3=_mm256_slli_si256(output5,8);
			m4=_mm256_and_si256(output5,out_mask_5);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_srli_si256(m4,8);
			output5=_mm256_add_epi32(m3,m4);

			output1=_mm256_add_epi8(output1,output2);
			output1=_mm256_add_epi8(output1,output3);
			output1=_mm256_add_epi8(output1,output4);
			output1=_mm256_add_epi8(output1,output5);

			_mm256_storeu_si256( (__m256i *) &filt[row][col],output1 );

		  }
}
}




__m256i prelude_block_pre_9x9_1(const __m256i r0,const __m256i r1,const __m256i r2,const __m256i r3,const __m256i r4,const __m256i r5, const __m256i c0,const __m256i c1,const __m256i c2,const __m256i c3,const __m256i c4,const __m256i c5){

	__m256i m0,m1,m2,m3,m4,m5;
	const __m256i mask_z    = _mm256_set_epi16(0,0,0,0,0,0,0,0,0,0,0,65535,0,0,0,0);
	const __m256i mask_unz    = _mm256_set_epi16(65535,65535,65535,65535,65535,65535,65535,65535,65535,65535,65535,0,65535,65535,65535,65535);
	const __m256i mask_32_1    = _mm256_set_epi32(0xffffffff,0,0,0xffffffff,0,0,0,0);
	const __m256i mask_32_2    = _mm256_set_epi32(0,0,0,0xffffffff,0,0,0,0);
	const __m256i mask_32_a    = _mm256_set_epi32(0,0,0xffffffff,0,0,0xffffffff,0,0xffffffff);
	const __m256i ones=_mm256_set1_epi16(1);


//multiply by the mask
m0=_mm256_maddubs_epi16(r0,c0);
m1=_mm256_maddubs_epi16(r1,c1);
m2=_mm256_maddubs_epi16(r2,c2);
m3=_mm256_maddubs_epi16(r3,c3);
m4=_mm256_maddubs_epi16(r4,c4);
m5=_mm256_maddubs_epi16(r5,c5);

	//keep a copy of the 4th element
__m256i	m0_copy=_mm256_and_si256(m0,mask_z);
__m256i	m1_copy=_mm256_and_si256(m1,mask_z);
__m256i	m2_copy=_mm256_and_si256(m2,mask_z);
__m256i	m3_copy=_mm256_and_si256(m3,mask_z);
__m256i	m4_copy=_mm256_and_si256(m4,mask_z);
__m256i	m5_copy=_mm256_and_si256(m5,mask_z);

	//add all the 4th elements above
	//16-bit addition here
	m0_copy=_mm256_add_epi16(m0_copy,m1_copy);
	m0_copy=_mm256_add_epi16(m0_copy,m2_copy);
	m0_copy=_mm256_add_epi16(m0_copy,m3_copy);
	m0_copy=_mm256_add_epi16(m0_copy,m4_copy);
	m0_copy=_mm256_add_epi16(m0_copy,m5_copy);

	//zero the 4th out of 16 elements
	m0=_mm256_and_si256(m0,mask_unz);
	m1=_mm256_and_si256(m1,mask_unz);
	m2=_mm256_and_si256(m2,mask_unz);
	m3=_mm256_and_si256(m3,mask_unz);
	m4=_mm256_and_si256(m4,mask_unz);
	m5=_mm256_and_si256(m5,mask_unz);


//make 16-bit values 32-bit (horizontal add)
m0=_mm256_madd_epi16(m0,ones);
m1=_mm256_madd_epi16(m1,ones);
m2=_mm256_madd_epi16(m2,ones);
m3=_mm256_madd_epi16(m3,ones);
m4=_mm256_madd_epi16(m4,ones);
m5=_mm256_madd_epi16(m5,ones);

//vertical add
m0=_mm256_add_epi32(m0,m1);
m0=_mm256_add_epi32(m0,m2);
m0=_mm256_add_epi32(m0,m3);
m0=_mm256_add_epi32(m0,m4);
m0=_mm256_add_epi32(m0,m5);

//horizontal add
m2=_mm256_srli_si256(m0,4);
	m2=_mm256_add_epi32(m0,m2);

	//keep only 4th and 7th out of 8 elements
	m1=_mm256_and_si256(m0,mask_32_1);
	m1=_mm256_add_epi32(m1,m0_copy);

	//shift m1 and add with m2 (complex shift is needed)
	m3=_mm256_srli_si256(m1,8);
	m4=_mm256_and_si256(m1,mask_32_2);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,8);
	m3=_mm256_add_epi32(m3,m4);
	m2=_mm256_add_epi32(m2,m3);
	return _mm256_and_si256(m2,mask_32_a);//in 32-bit format keep only 0,2,5 and discard the others

	}


__m256i prelude_block_pre_9x9_1m(unsigned char **frame1,const unsigned int load_row, const unsigned int load_col, const __m256i c0,const __m256i c1,const __m256i c2,const __m256i c3,const __m256i c4,const __m256i c5){

	__m256i m0,m1,m2,m3,m4,m5;
	const __m256i mask_z    = _mm256_set_epi16(0,0,0,0,0,0,0,0,0,0,0,65535,0,0,0,0);
	const __m256i mask_unz    = _mm256_set_epi16(65535,65535,65535,65535,65535,65535,65535,65535,65535,65535,65535,0,65535,65535,65535,65535);
	const __m256i mask_32_1    = _mm256_set_epi32(0xffffffff,0,0,0xffffffff,0,0,0,0);
	const __m256i mask_32_2    = _mm256_set_epi32(0,0,0,0xffffffff,0,0,0,0);
	const __m256i mask_32_a    = _mm256_set_epi32(0,0,0xffffffff,0,0,0xffffffff,0,0xffffffff);
	const __m256i ones=_mm256_set1_epi16(1);

__m256i		r0=_mm256_loadu_si256( (__m256i *) &frame1[load_row][load_col]);
__m256i		r1=_mm256_loadu_si256( (__m256i *) &frame1[load_row+1][load_col]);
__m256i		r2=_mm256_loadu_si256( (__m256i *) &frame1[load_row+2][load_col]);
__m256i		r3=_mm256_loadu_si256( (__m256i *) &frame1[load_row+3][load_col]);
__m256i		r4=_mm256_loadu_si256( (__m256i *) &frame1[load_row+4][load_col]);
__m256i		r5=_mm256_loadu_si256( (__m256i *) &frame1[load_row+5][load_col]);

//multiply by the mask
m0=_mm256_maddubs_epi16(r0,c0);
m1=_mm256_maddubs_epi16(r1,c1);
m2=_mm256_maddubs_epi16(r2,c2);
m3=_mm256_maddubs_epi16(r3,c3);
m4=_mm256_maddubs_epi16(r4,c4);
m5=_mm256_maddubs_epi16(r5,c5);

	//keep a copy of the 4th element
__m256i	m0_copy=_mm256_and_si256(m0,mask_z);
__m256i	m1_copy=_mm256_and_si256(m1,mask_z);
__m256i	m2_copy=_mm256_and_si256(m2,mask_z);
__m256i	m3_copy=_mm256_and_si256(m3,mask_z);
__m256i	m4_copy=_mm256_and_si256(m4,mask_z);
__m256i	m5_copy=_mm256_and_si256(m5,mask_z);

	//add all the 4th elements above
	//16-bit addition here
	m0_copy=_mm256_add_epi16(m0_copy,m1_copy);
	m0_copy=_mm256_add_epi16(m0_copy,m2_copy);
	m0_copy=_mm256_add_epi16(m0_copy,m3_copy);
	m0_copy=_mm256_add_epi16(m0_copy,m4_copy);
	m0_copy=_mm256_add_epi16(m0_copy,m5_copy);

	//zero the 4th out of 16 elements
	m0=_mm256_and_si256(m0,mask_unz);
	m1=_mm256_and_si256(m1,mask_unz);
	m2=_mm256_and_si256(m2,mask_unz);
	m3=_mm256_and_si256(m3,mask_unz);
	m4=_mm256_and_si256(m4,mask_unz);
	m5=_mm256_and_si256(m5,mask_unz);


//make 16-bit values 32-bit (horizontal add)
m0=_mm256_madd_epi16(m0,ones);
m1=_mm256_madd_epi16(m1,ones);
m2=_mm256_madd_epi16(m2,ones);
m3=_mm256_madd_epi16(m3,ones);
m4=_mm256_madd_epi16(m4,ones);
m5=_mm256_madd_epi16(m5,ones);

//vertical add
m0=_mm256_add_epi32(m0,m1);
m0=_mm256_add_epi32(m0,m2);
m0=_mm256_add_epi32(m0,m3);
m0=_mm256_add_epi32(m0,m4);
m0=_mm256_add_epi32(m0,m5);

//horizontal add
m2=_mm256_srli_si256(m0,4);
	m2=_mm256_add_epi32(m0,m2);

	//keep only 4th and 7th out of 8 elements
	m1=_mm256_and_si256(m0,mask_32_1);
	m1=_mm256_add_epi32(m1,m0_copy);

	//shift m1 and add with m2 (complex shift is needed)
	m3=_mm256_srli_si256(m1,8);
	m4=_mm256_and_si256(m1,mask_32_2);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,8);
	m3=_mm256_add_epi32(m3,m4);
	m2=_mm256_add_epi32(m2,m3);
	return _mm256_and_si256(m2,mask_32_a);//in 32-bit format keep only 0,2,5 and discard the others

	}




//this is for computing row=0 and row=N-1 only
void prelude_block_0_9x9(unsigned char **frame1,unsigned char **filt,const unsigned int row, const unsigned int load_row,const unsigned int M, const unsigned int division_case, const __m256i f, const __m256i c0,const __m256i c1,const __m256i c2,const __m256i c3,const __m256i c4){

	unsigned int col;
    __m256i rr0,rr1,rr2,rr3,rr4,r0,r1,r2,r3,r4,out1,out2,output1,output2,output3,output4,output5,m0,m1,m2,m3,m4;


	const __m256i out_mask_1    = _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255);//0,20
	const __m256i out_mask_2    = _mm256_set_epi8(0,0,0,0,0,0,0,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255,0,0,0,0);//4,24
	const __m256i out_mask_3    = _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, 0,0,0,0,0,0,0,255,0,0,0,0,0,0,0,0);//8
	const __m256i out_mask_4    = _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, 0,0,0,255,0,0,0,0,0,0,0,0,0,0,0,0);//12
	const __m256i out_mask_5    = _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, 0,0,0,0,255,255,0,0,0,0,0,0,0,0,0,0);//10,11


	const __m256i mask_prelude   = _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0);
	const __m256i mask_prelude1  = _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0);
	const __m256i mask_prelude2  = _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0);
	const __m256i mask_prelude3  = _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0);


	  for (col = 0; col <= M-32-5; col+=30){

		  if (col==0){
			  //1st col
			rr0=_mm256_load_si256( (__m256i *) &frame1[load_row][0]);
			rr1=_mm256_load_si256( (__m256i *) &frame1[load_row+1][0]);
			rr2=_mm256_load_si256( (__m256i *) &frame1[load_row+2][0]);
			rr3=_mm256_load_si256( (__m256i *) &frame1[load_row+3][0]);
			rr4=_mm256_load_si256( (__m256i *) &frame1[load_row+4][0]);

			//START - extra code needed for prelude
		 r0=insert_four_zeros_front(rr0,mask_prelude);
		 r1=insert_four_zeros_front(rr1,mask_prelude);
		 r2=insert_four_zeros_front(rr2,mask_prelude);
		 r3=insert_four_zeros_front(rr3,mask_prelude);
		 r4=insert_four_zeros_front(rr4,mask_prelude);

		out1=prelude_block_pre_9x9_0(r0,r1,r2,r3,r4,c0,c1,c2,c3,c4);

		  //2nd col

			//START - extra code needed for prelude
		  r0=insert_three_zeros_front(rr0,mask_prelude1);
		  r1=insert_three_zeros_front(rr1,mask_prelude1);
		  r2=insert_three_zeros_front(rr2,mask_prelude1);
		  r3=insert_three_zeros_front(rr3,mask_prelude1);
		  r4=insert_three_zeros_front(rr4,mask_prelude1);

		out2=prelude_block_pre_9x9_0(r0,r1,r2,r3,r4,c0,c1,c2,c3,c4);


			//merge out1 and out2 and then divide
			out2=_mm256_slli_si256(out2,4);
			out1=_mm256_add_epi32(out1,out2);

			out1 = division_32(division_case,out1,f);

			//elements must be re-arranged before store
			m0=_mm256_and_si256(out1,out_mask_1);
			m1=_mm256_and_si256(out1,out_mask_2);
			m1=_mm256_srli_si256(m1,3);
			m2=_mm256_and_si256(out1,out_mask_3);
			m2=_mm256_slli_si256(m2,2);
			m3=_mm256_and_si256(out1,out_mask_4);
			m3=_mm256_srli_si256(m3,1);
			m0=_mm256_add_epi8(m0,m1);
			m0=_mm256_add_epi8(m0,m2);
			output1=_mm256_add_epi8(m0,m3);

		//3rd col
		//START - extra code needed for prelude
		r0=insert_two_zeros_front(rr0,mask_prelude2);
		r1=insert_two_zeros_front(rr1,mask_prelude2);
		r2=insert_two_zeros_front(rr2,mask_prelude2);
		r3=insert_two_zeros_front(rr3,mask_prelude2);
		r4=insert_two_zeros_front(rr4,mask_prelude2);

		out1=prelude_block_pre_9x9_0(r0,r1,r2,r3,r4,c0,c1,c2,c3,c4);


		//4th col
		r0=insert_one_zeros_front(rr0,mask_prelude3);
		r1=insert_one_zeros_front(rr1,mask_prelude3);
		r2=insert_one_zeros_front(rr2,mask_prelude3);
		r3=insert_one_zeros_front(rr3,mask_prelude3);
		r4=insert_one_zeros_front(rr4,mask_prelude3);

		out2 = prelude_block_pre_9x9_0(r0,r1,r2,r3,r4,c0,c1,c2,c3,c4);


		//merge out1 and out2 and then divide
		out2=_mm256_slli_si256(out2,4);
		out1=_mm256_add_epi32(out1,out2);

		out1 = division_32(division_case,out1,f);

		//elements must be re-arranged before store
		m0=_mm256_and_si256(out1,out_mask_1);
		m1=_mm256_and_si256(out1,out_mask_2);
		m1=_mm256_srli_si256(m1,3);
		m2=_mm256_and_si256(out1,out_mask_3);
		m2=_mm256_slli_si256(m2,2);
		m3=_mm256_and_si256(out1,out_mask_4);
		m3=_mm256_srli_si256(m3,1);
		m0=_mm256_add_epi8(m0,m1);
		m0=_mm256_add_epi8(m0,m2);
		output2=_mm256_add_epi8(m0,m3);

		//5th col iteration
		out1 = prelude_block_pre_9x9_0(rr0,rr1,rr2,rr3,rr4,c0,c1,c2,c3,c4);

		//6th col iteration
		r0=_mm256_loadu_si256( (__m256i *) &frame1[load_row][1]);
		r1=_mm256_loadu_si256( (__m256i *) &frame1[load_row+1][1]);
		r2=_mm256_loadu_si256( (__m256i *) &frame1[load_row+2][1]);
		r3=_mm256_loadu_si256( (__m256i *) &frame1[load_row+3][1]);
		r4=_mm256_loadu_si256( (__m256i *) &frame1[load_row+4][1]);

		out2 = prelude_block_pre_9x9_0(r0,r1,r2,r3,r4,c0,c1,c2,c3,c4);


		//merge out1 and out2 and then divide
		out2=_mm256_slli_si256(out2,4);
		out1=_mm256_add_epi32(out1,out2);

		out1 = division_32(division_case,out1,f);

		//elements must be re-arranged before store
		m0=_mm256_and_si256(out1,out_mask_1);
		m1=_mm256_and_si256(out1,out_mask_2);
		m1=_mm256_srli_si256(m1,3);
		m2=_mm256_and_si256(out1,out_mask_3);
		m2=_mm256_slli_si256(m2,2);
		m3=_mm256_and_si256(out1,out_mask_4);
		m3=_mm256_srli_si256(m3,1);
		m0=_mm256_add_epi8(m0,m1);
		m0=_mm256_add_epi8(m0,m2);
		output3=_mm256_add_epi8(m0,m3);

		//7th col iteration
		r0=_mm256_loadu_si256( (__m256i *) &frame1[load_row][2]);
		r1=_mm256_loadu_si256( (__m256i *) &frame1[load_row+1][2]);
		r2=_mm256_loadu_si256( (__m256i *) &frame1[load_row+2][2]);
		r3=_mm256_loadu_si256( (__m256i *) &frame1[load_row+3][2]);
		r4=_mm256_loadu_si256( (__m256i *) &frame1[load_row+4][2]);

		out1 = prelude_block_pre_9x9_0(r0,r1,r2,r3,r4,c0,c1,c2,c3,c4);

		//8th col iteration
		r0=_mm256_loadu_si256( (__m256i *) &frame1[load_row][3]);
		r1=_mm256_loadu_si256( (__m256i *) &frame1[load_row+1][3]);
		r2=_mm256_loadu_si256( (__m256i *) &frame1[load_row+2][3]);
		r3=_mm256_loadu_si256( (__m256i *) &frame1[load_row+3][3]);
		r4=_mm256_loadu_si256( (__m256i *) &frame1[load_row+4][3]);

		out2 = prelude_block_pre_9x9_0(r0,r1,r2,r3,r4,c0,c1,c2,c3,c4);

		//merge out1 and out2 and then divide
		out2=_mm256_slli_si256(out2,4);
		out1=_mm256_add_epi32(out1,out2);

		out1 = division_32(division_case,out1,f);

		//elements must be re-arranged before store
		m0=_mm256_and_si256(out1,out_mask_1);
		m1=_mm256_and_si256(out1,out_mask_2);
		m1=_mm256_srli_si256(m1,3);
		m2=_mm256_and_si256(out1,out_mask_3);
		m2=_mm256_slli_si256(m2,2);
		m3=_mm256_and_si256(out1,out_mask_4);
		m3=_mm256_srli_si256(m3,1);
		m0=_mm256_add_epi8(m0,m1);
		m0=_mm256_add_epi8(m0,m2);
		output4=_mm256_add_epi8(m0,m3);

		//9th col iteration
		r0=_mm256_loadu_si256( (__m256i *) &frame1[load_row][4]);
		r1=_mm256_loadu_si256( (__m256i *) &frame1[load_row+1][4]);
		r2=_mm256_loadu_si256( (__m256i *) &frame1[load_row+2][4]);
		r3=_mm256_loadu_si256( (__m256i *) &frame1[load_row+3][4]);
		r4=_mm256_loadu_si256( (__m256i *) &frame1[load_row+4][4]);

		out1 = prelude_block_pre_9x9_0(r0,r1,r2,r3,r4,c0,c1,c2,c3,c4);

		//10th col iteration
		r0=_mm256_loadu_si256( (__m256i *) &frame1[load_row][5]);
		r1=_mm256_loadu_si256( (__m256i *) &frame1[load_row+1][5]);
		r2=_mm256_loadu_si256( (__m256i *) &frame1[load_row+2][5]);
		r3=_mm256_loadu_si256( (__m256i *) &frame1[load_row+3][5]);
		r4=_mm256_loadu_si256( (__m256i *) &frame1[load_row+4][5]);

		out2 = prelude_block_pre_9x9_0(r0,r1,r2,r3,r4,c0,c1,c2,c3,c4);

		//merge out1 and out2 and then divide
		out2=_mm256_slli_si256(out2,4);
		out1=_mm256_add_epi32(out1,out2);

		out1 = division_32(division_case,out1,f);

		//elements must be re-arranged before store
		m0=_mm256_and_si256(out1,out_mask_1);
		m1=_mm256_and_si256(out1,out_mask_2);
		m1=_mm256_srli_si256(m1,3);
		m2=_mm256_and_si256(out1,out_mask_3);
		m2=_mm256_slli_si256(m2,2);
		m3=_mm256_and_si256(out1,out_mask_4);
		m3=_mm256_srli_si256(m3,1);
		m0=_mm256_add_epi8(m0,m1);
		m0=_mm256_add_epi8(m0,m2);
		output5=_mm256_add_epi8(m0,m3);

			//blend the output1:output5
			output2=_mm256_slli_si256(output2,2);
			output3=_mm256_slli_si256(output3,4);

			m3=_mm256_slli_si256(output4,6);
			m4=_mm256_and_si256(output4,out_mask_5);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_srli_si256(m4,10);
			output4=_mm256_add_epi32(m3,m4);

			m3=_mm256_slli_si256(output5,8);
			m4=_mm256_and_si256(output5,out_mask_5);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_srli_si256(m4,8);
			output5=_mm256_add_epi32(m3,m4);

			output1=_mm256_add_epi8(output1,output2);
			output1=_mm256_add_epi8(output1,output3);
			output1=_mm256_add_epi8(output1,output4);
			output1=_mm256_add_epi8(output1,output5);

				_mm256_store_si256( (__m256i *) &filt[row][0],output1 );
		  	}
		  else {
			  //1st col

		out1=prelude_block_pre_9x9_0m(frame1,load_row,col-4,c0,c1,c2,c3,c4);

		  //2nd col

		out2=prelude_block_pre_9x9_0m(frame1,load_row,col-3,c0,c1,c2,c3,c4);

			//merge out1 and out2 and then divide
			out2=_mm256_slli_si256(out2,4);
			out1=_mm256_add_epi32(out1,out2);

			out1 = division_32(division_case,out1,f);

			//elements must be re-arranged before store
			m0=_mm256_and_si256(out1,out_mask_1);
			m1=_mm256_and_si256(out1,out_mask_2);
			m1=_mm256_srli_si256(m1,3);
			m2=_mm256_and_si256(out1,out_mask_3);
			m2=_mm256_slli_si256(m2,2);
			m3=_mm256_and_si256(out1,out_mask_4);
			m3=_mm256_srli_si256(m3,1);
			m0=_mm256_add_epi8(m0,m1);
			m0=_mm256_add_epi8(m0,m2);
			output1=_mm256_add_epi8(m0,m3);

		//3rd col
		out1=prelude_block_pre_9x9_0m(frame1,load_row,col-2,c0,c1,c2,c3,c4);

		//4th col
		out2=prelude_block_pre_9x9_0m(frame1,load_row,col-1,c0,c1,c2,c3,c4);

		//merge out1 and out2 and then divide
		out2=_mm256_slli_si256(out2,4);
		out1=_mm256_add_epi32(out1,out2);

		out1 = division_32(division_case,out1,f);

		//elements must be re-arranged before store
		m0=_mm256_and_si256(out1,out_mask_1);
		m1=_mm256_and_si256(out1,out_mask_2);
		m1=_mm256_srli_si256(m1,3);
		m2=_mm256_and_si256(out1,out_mask_3);
		m2=_mm256_slli_si256(m2,2);
		m3=_mm256_and_si256(out1,out_mask_4);
		m3=_mm256_srli_si256(m3,1);
		m0=_mm256_add_epi8(m0,m1);
		m0=_mm256_add_epi8(m0,m2);
		output2=_mm256_add_epi8(m0,m3);

		//5th col iteration
		out1=prelude_block_pre_9x9_0m(frame1,load_row,col,c0,c1,c2,c3,c4);

		//6th col iteration
		out2=prelude_block_pre_9x9_0m(frame1,load_row,col+1,c0,c1,c2,c3,c4);

		//merge out1 and out2 and then divide
		out2=_mm256_slli_si256(out2,4);
		out1=_mm256_add_epi32(out1,out2);

		out1 = division_32(division_case,out1,f);

		//elements must be re-arranged before store
		m0=_mm256_and_si256(out1,out_mask_1);
		m1=_mm256_and_si256(out1,out_mask_2);
		m1=_mm256_srli_si256(m1,3);
		m2=_mm256_and_si256(out1,out_mask_3);
		m2=_mm256_slli_si256(m2,2);
		m3=_mm256_and_si256(out1,out_mask_4);
		m3=_mm256_srli_si256(m3,1);
		m0=_mm256_add_epi8(m0,m1);
		m0=_mm256_add_epi8(m0,m2);
		output3=_mm256_add_epi8(m0,m3);

		//7th col iteration
		out1=prelude_block_pre_9x9_0m(frame1,load_row,col+2,c0,c1,c2,c3,c4);


		//8th col iteration
		out2=prelude_block_pre_9x9_0m(frame1,load_row,col+3,c0,c1,c2,c3,c4);


		//merge out1 and out2 and then divide
		out2=_mm256_slli_si256(out2,4);
		out1=_mm256_add_epi32(out1,out2);

		out1 = division_32(division_case,out1,f);

		//elements must be re-arranged before store
		m0=_mm256_and_si256(out1,out_mask_1);
		m1=_mm256_and_si256(out1,out_mask_2);
		m1=_mm256_srli_si256(m1,3);
		m2=_mm256_and_si256(out1,out_mask_3);
		m2=_mm256_slli_si256(m2,2);
		m3=_mm256_and_si256(out1,out_mask_4);
		m3=_mm256_srli_si256(m3,1);
		m0=_mm256_add_epi8(m0,m1);
		m0=_mm256_add_epi8(m0,m2);
		output4=_mm256_add_epi8(m0,m3);

		//9th col iteration
		out1=prelude_block_pre_9x9_0m(frame1,load_row,col+4,c0,c1,c2,c3,c4);

		//10th col iteration
		out2=prelude_block_pre_9x9_0m(frame1,load_row,col+5,c0,c1,c2,c3,c4);


		//merge out1 and out2 and then divide
		out2=_mm256_slli_si256(out2,4);
		out1=_mm256_add_epi32(out1,out2);

		out1 = division_32(division_case,out1,f);

		//elements must be re-arranged before store
		m0=_mm256_and_si256(out1,out_mask_1);
		m1=_mm256_and_si256(out1,out_mask_2);
		m1=_mm256_srli_si256(m1,3);
		m2=_mm256_and_si256(out1,out_mask_3);
		m2=_mm256_slli_si256(m2,2);
		m3=_mm256_and_si256(out1,out_mask_4);
		m3=_mm256_srli_si256(m3,1);
		m0=_mm256_add_epi8(m0,m1);
		m0=_mm256_add_epi8(m0,m2);
		output5=_mm256_add_epi8(m0,m3);

			//blend the output1:output5
			output2=_mm256_slli_si256(output2,2);
			output3=_mm256_slli_si256(output3,4);

			m3=_mm256_slli_si256(output4,6);
			m4=_mm256_and_si256(output4,out_mask_5);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_srli_si256(m4,10);
			output4=_mm256_add_epi32(m3,m4);

			m3=_mm256_slli_si256(output5,8);
			m4=_mm256_and_si256(output5,out_mask_5);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_srli_si256(m4,8);
			output5=_mm256_add_epi32(m3,m4);

			output1=_mm256_add_epi8(output1,output2);
			output1=_mm256_add_epi8(output1,output3);
			output1=_mm256_add_epi8(output1,output4);
			output1=_mm256_add_epi8(output1,output5);

			_mm256_storeu_si256( (__m256i *) &filt[row][col],output1 );

		  }
}
}




__m256i prelude_block_pre_9x9_0(const __m256i r0,const __m256i r1,const __m256i r2,const __m256i r3,const __m256i r4, const __m256i c0,const __m256i c1,const __m256i c2,const __m256i c3,const __m256i c4){

	__m256i m0,m1,m2,m3,m4;
	const __m256i mask_z    = _mm256_set_epi16(0,0,0,0,0,0,0,0,0,0,0,65535,0,0,0,0);
	const __m256i mask_unz    = _mm256_set_epi16(65535,65535,65535,65535,65535,65535,65535,65535,65535,65535,65535,0,65535,65535,65535,65535);
	const __m256i mask_32_1    = _mm256_set_epi32(0xffffffff,0,0,0xffffffff,0,0,0,0);
	const __m256i mask_32_2    = _mm256_set_epi32(0,0,0,0xffffffff,0,0,0,0);
	const __m256i mask_32_a    = _mm256_set_epi32(0,0,0xffffffff,0,0,0xffffffff,0,0xffffffff);
	const __m256i ones=_mm256_set1_epi16(1);


//multiply by the mask
m0=_mm256_maddubs_epi16(r0,c0);
m1=_mm256_maddubs_epi16(r1,c1);
m2=_mm256_maddubs_epi16(r2,c2);
m3=_mm256_maddubs_epi16(r3,c3);
m4=_mm256_maddubs_epi16(r4,c4);

	//keep a copy of the 4th element
__m256i	m0_copy=_mm256_and_si256(m0,mask_z);
__m256i	m1_copy=_mm256_and_si256(m1,mask_z);
__m256i	m2_copy=_mm256_and_si256(m2,mask_z);
__m256i	m3_copy=_mm256_and_si256(m3,mask_z);
__m256i	m4_copy=_mm256_and_si256(m4,mask_z);

	//add all the 4th elements above
	//16-bit addition here
	m0_copy=_mm256_add_epi16(m0_copy,m1_copy);
	m0_copy=_mm256_add_epi16(m0_copy,m2_copy);
	m0_copy=_mm256_add_epi16(m0_copy,m3_copy);
	m0_copy=_mm256_add_epi16(m0_copy,m4_copy);

	//zero the 4th out of 16 elements
	m0=_mm256_and_si256(m0,mask_unz);
	m1=_mm256_and_si256(m1,mask_unz);
	m2=_mm256_and_si256(m2,mask_unz);
	m3=_mm256_and_si256(m3,mask_unz);
	m4=_mm256_and_si256(m4,mask_unz);


//make 16-bit values 32-bit (horizontal add)
m0=_mm256_madd_epi16(m0,ones);
m1=_mm256_madd_epi16(m1,ones);
m2=_mm256_madd_epi16(m2,ones);
m3=_mm256_madd_epi16(m3,ones);
m4=_mm256_madd_epi16(m4,ones);

//vertical add
m0=_mm256_add_epi32(m0,m1);
m0=_mm256_add_epi32(m0,m2);
m0=_mm256_add_epi32(m0,m3);
m0=_mm256_add_epi32(m0,m4);

//horizontal add
m2=_mm256_srli_si256(m0,4);
	m2=_mm256_add_epi32(m0,m2);

	//keep only 4th and 7th out of 8 elements
	m1=_mm256_and_si256(m0,mask_32_1);
	m1=_mm256_add_epi32(m1,m0_copy);

	//shift m1 and add with m2 (complex shift is needed)
	m3=_mm256_srli_si256(m1,8);
	m4=_mm256_and_si256(m1,mask_32_2);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,8);
	m3=_mm256_add_epi32(m3,m4);
	m2=_mm256_add_epi32(m2,m3);
	return _mm256_and_si256(m2,mask_32_a);//in 32-bit format keep only 0,2,5 and discard the others

	}


__m256i prelude_block_pre_9x9_0m(unsigned char **frame1,const unsigned int load_row, const unsigned int load_col, const __m256i c0,const __m256i c1,const __m256i c2,const __m256i c3,const __m256i c4){

	__m256i m0,m1,m2,m3,m4;
	const __m256i mask_z    = _mm256_set_epi16(0,0,0,0,0,0,0,0,0,0,0,65535,0,0,0,0);
	const __m256i mask_unz    = _mm256_set_epi16(65535,65535,65535,65535,65535,65535,65535,65535,65535,65535,65535,0,65535,65535,65535,65535);
	const __m256i mask_32_1    = _mm256_set_epi32(0xffffffff,0,0,0xffffffff,0,0,0,0);
	const __m256i mask_32_2    = _mm256_set_epi32(0,0,0,0xffffffff,0,0,0,0);
	const __m256i mask_32_a    = _mm256_set_epi32(0,0,0xffffffff,0,0,0xffffffff,0,0xffffffff);
	const __m256i ones=_mm256_set1_epi16(1);

__m256i		r0=_mm256_loadu_si256( (__m256i *) &frame1[load_row][load_col]);
__m256i		r1=_mm256_loadu_si256( (__m256i *) &frame1[load_row+1][load_col]);
__m256i		r2=_mm256_loadu_si256( (__m256i *) &frame1[load_row+2][load_col]);
__m256i		r3=_mm256_loadu_si256( (__m256i *) &frame1[load_row+3][load_col]);
__m256i		r4=_mm256_loadu_si256( (__m256i *) &frame1[load_row+4][load_col]);

//multiply by the mask
m0=_mm256_maddubs_epi16(r0,c0);
m1=_mm256_maddubs_epi16(r1,c1);
m2=_mm256_maddubs_epi16(r2,c2);
m3=_mm256_maddubs_epi16(r3,c3);
m4=_mm256_maddubs_epi16(r4,c4);

	//keep a copy of the 4th element
__m256i	m0_copy=_mm256_and_si256(m0,mask_z);
__m256i	m1_copy=_mm256_and_si256(m1,mask_z);
__m256i	m2_copy=_mm256_and_si256(m2,mask_z);
__m256i	m3_copy=_mm256_and_si256(m3,mask_z);
__m256i	m4_copy=_mm256_and_si256(m4,mask_z);

	//add all the 4th elements above
	//16-bit addition here
	m0_copy=_mm256_add_epi16(m0_copy,m1_copy);
	m0_copy=_mm256_add_epi16(m0_copy,m2_copy);
	m0_copy=_mm256_add_epi16(m0_copy,m3_copy);
	m0_copy=_mm256_add_epi16(m0_copy,m4_copy);

	//zero the 4th out of 16 elements
	m0=_mm256_and_si256(m0,mask_unz);
	m1=_mm256_and_si256(m1,mask_unz);
	m2=_mm256_and_si256(m2,mask_unz);
	m3=_mm256_and_si256(m3,mask_unz);
	m4=_mm256_and_si256(m4,mask_unz);


//make 16-bit values 32-bit (horizontal add)
m0=_mm256_madd_epi16(m0,ones);
m1=_mm256_madd_epi16(m1,ones);
m2=_mm256_madd_epi16(m2,ones);
m3=_mm256_madd_epi16(m3,ones);
m4=_mm256_madd_epi16(m4,ones);

//vertical add
m0=_mm256_add_epi32(m0,m1);
m0=_mm256_add_epi32(m0,m2);
m0=_mm256_add_epi32(m0,m3);
m0=_mm256_add_epi32(m0,m4);

//horizontal add
m2=_mm256_srli_si256(m0,4);
	m2=_mm256_add_epi32(m0,m2);

	//keep only 4th and 7th out of 8 elements
	m1=_mm256_and_si256(m0,mask_32_1);
	m1=_mm256_add_epi32(m1,m0_copy);

	//shift m1 and add with m2 (complex shift is needed)
	m3=_mm256_srli_si256(m1,8);
	m4=_mm256_and_si256(m1,mask_32_2);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,8);
	m3=_mm256_add_epi32(m3,m4);
	m2=_mm256_add_epi32(m2,m3);
	return _mm256_and_si256(m2,mask_32_a);//in 32-bit format keep only 0,2,5 and discard the others

	}



void loop_reminder_9x9(unsigned char **frame1,unsigned char **filt,const unsigned int row, const unsigned int col,const unsigned int N, const unsigned int M,const unsigned int REMINDER_ITERATIONS,const unsigned int division_case,const signed char mask_vector[][32], const __m256i f,const unsigned short int divisor,signed char **filter9x9){

	const __m256i c0=_mm256_load_si256( (__m256i *) &mask_vector[0][0]);
	const __m256i c1=_mm256_load_si256( (__m256i *) &mask_vector[1][0]);
	const __m256i c2=_mm256_load_si256( (__m256i *) &mask_vector[2][0]);
	const __m256i c3=_mm256_load_si256( (__m256i *) &mask_vector[3][0]);
	const __m256i c4=_mm256_load_si256( (__m256i *) &mask_vector[4][0]);
	const __m256i c5=_mm256_load_si256( (__m256i *) &mask_vector[5][0]);
	const __m256i c6=_mm256_load_si256( (__m256i *) &mask_vector[6][0]);
	const __m256i c7=_mm256_load_si256( (__m256i *) &mask_vector[7][0]);
	const __m256i c8=_mm256_load_si256( (__m256i *) &mask_vector[8][0]);


	const __m256i out_mask_1    = _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255);//0,20
	const __m256i out_mask_2    = _mm256_set_epi8(0,0,0,0,0,0,0,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255,0,0,0,0);//4,24
	const __m256i out_mask_3    = _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, 0,0,0,0,0,0,0,255,0,0,0,0,0,0,0,0);//8
	const __m256i out_mask_4    = _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, 0,0,0,255,0,0,0,0,0,0,0,0,0,0,0,0);//12
	const __m256i out_mask_5    = _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, 0,0,0,0,255,255,0,0,0,0,0,0,0,0,0,0);//10,11


	__m256i r0,r1,r2,r3,r4,r5,r6,r7,r8,m0,m1,m2,m3,m4,out1,out2;
	__m256i output1,output2,output3,output4,output5,reminder_mask1;


//important:  //REMINDER_ITERATIONS ranges [7,36]. If >30, two iterations are needed, one for computing the first 30 output pixels and another for computing the rest


		//1st col iteration
	    r0=_mm256_loadu_si256( (__m256i *) &frame1[row-4][col-4]);
		r1=_mm256_loadu_si256( (__m256i *) &frame1[row-3][col-4]);
		r2=_mm256_loadu_si256( (__m256i *) &frame1[row-2][col-4]);
		r3=_mm256_loadu_si256( (__m256i *) &frame1[row-1][col-4]);
		r4=_mm256_loadu_si256( (__m256i *) &frame1[row][col-4]);
		r5=_mm256_loadu_si256( (__m256i *) &frame1[row+1][col-4]);
		r6=_mm256_loadu_si256( (__m256i *) &frame1[row+2][col-4]);
		r7=_mm256_loadu_si256( (__m256i *) &frame1[row+3][col-4]);
		r8=_mm256_loadu_si256( (__m256i *) &frame1[row+4][col-4]);

		reminder_mask1=_mm256_load_si256( (__m256i *) &reminder_msk_9x9[REMINDER_ITERATIONS-1][0]);

		//zero the values that exceed the array bounds
		r0=_mm256_and_si256(r0,reminder_mask1);
		r1=_mm256_and_si256(r1,reminder_mask1);
		r2=_mm256_and_si256(r2,reminder_mask1);
		r3=_mm256_and_si256(r3,reminder_mask1);
		r4=_mm256_and_si256(r4,reminder_mask1);
		r5=_mm256_and_si256(r5,reminder_mask1);
		r6=_mm256_and_si256(r6,reminder_mask1);
		r7=_mm256_and_si256(r7,reminder_mask1);
		r8=_mm256_and_si256(r8,reminder_mask1);

		out1=main_block_pre_9x9(r0,r1,r2,r3,r4,r5,r6,r7,r8,c0,c1,c2,c3,c4,c5,c6,c7,c8);

		  //2nd col

	    r0=_mm256_loadu_si256( (__m256i *) &frame1[row-4][col-3]);
		r1=_mm256_loadu_si256( (__m256i *) &frame1[row-3][col-3]);
		r2=_mm256_loadu_si256( (__m256i *) &frame1[row-2][col-3]);
		r3=_mm256_loadu_si256( (__m256i *) &frame1[row-1][col-3]);
		r4=_mm256_loadu_si256( (__m256i *) &frame1[row][col-3]);
		r5=_mm256_loadu_si256( (__m256i *) &frame1[row+1][col-3]);
		r6=_mm256_loadu_si256( (__m256i *) &frame1[row+2][col-3]);
		r7=_mm256_loadu_si256( (__m256i *) &frame1[row+3][col-3]);
		r8=_mm256_loadu_si256( (__m256i *) &frame1[row+4][col-3]);

		reminder_mask1=_mm256_load_si256( (__m256i *) &reminder_msk_9x9[REMINDER_ITERATIONS-2][0]);

		//zero the values that exceed the array bounds
		r0=_mm256_and_si256(r0,reminder_mask1);
		r1=_mm256_and_si256(r1,reminder_mask1);
		r2=_mm256_and_si256(r2,reminder_mask1);
		r3=_mm256_and_si256(r3,reminder_mask1);
		r4=_mm256_and_si256(r4,reminder_mask1);
		r5=_mm256_and_si256(r5,reminder_mask1);
		r6=_mm256_and_si256(r6,reminder_mask1);
		r7=_mm256_and_si256(r7,reminder_mask1);
		r8=_mm256_and_si256(r8,reminder_mask1);

		out2=main_block_pre_9x9(r0,r1,r2,r3,r4,r5,r6,r7,r8,c0,c1,c2,c3,c4,c5,c6,c7,c8);

			//merge out1 and out2 and then divide
			out2=_mm256_slli_si256(out2,4);
			out1=_mm256_add_epi32(out1,out2);

			out1 = division_32(division_case,out1,f);

			//elements must be re-arranged before store
			m0=_mm256_and_si256(out1,out_mask_1);
			m1=_mm256_and_si256(out1,out_mask_2);
			m1=_mm256_srli_si256(m1,3);
			m2=_mm256_and_si256(out1,out_mask_3);
			m2=_mm256_slli_si256(m2,2);
			m3=_mm256_and_si256(out1,out_mask_4);
			m3=_mm256_srli_si256(m3,1);
			m0=_mm256_add_epi8(m0,m1);
			m0=_mm256_add_epi8(m0,m2);
			output1=_mm256_add_epi8(m0,m3);

		//3rd col
		    r0=_mm256_loadu_si256( (__m256i *) &frame1[row-4][col-2]);
			r1=_mm256_loadu_si256( (__m256i *) &frame1[row-3][col-2]);
			r2=_mm256_loadu_si256( (__m256i *) &frame1[row-2][col-2]);
			r3=_mm256_loadu_si256( (__m256i *) &frame1[row-1][col-2]);
			r4=_mm256_loadu_si256( (__m256i *) &frame1[row][col-2]);
			r5=_mm256_loadu_si256( (__m256i *) &frame1[row+1][col-2]);
			r6=_mm256_loadu_si256( (__m256i *) &frame1[row+2][col-2]);
			r7=_mm256_loadu_si256( (__m256i *) &frame1[row+3][col-2]);
			r8=_mm256_loadu_si256( (__m256i *) &frame1[row+4][col-2]);

			reminder_mask1=_mm256_load_si256( (__m256i *) &reminder_msk_9x9[REMINDER_ITERATIONS-3][0]);

			//zero the values that exceed the array bounds
			r0=_mm256_and_si256(r0,reminder_mask1);
			r1=_mm256_and_si256(r1,reminder_mask1);
			r2=_mm256_and_si256(r2,reminder_mask1);
			r3=_mm256_and_si256(r3,reminder_mask1);
			r4=_mm256_and_si256(r4,reminder_mask1);
			r5=_mm256_and_si256(r5,reminder_mask1);
			r6=_mm256_and_si256(r6,reminder_mask1);
			r7=_mm256_and_si256(r7,reminder_mask1);
			r8=_mm256_and_si256(r8,reminder_mask1);

		out1=main_block_pre_9x9(r0,r1,r2,r3,r4,r5,r6,r7,r8,c0,c1,c2,c3,c4,c5,c6,c7,c8);

		//4th col
	    r0=_mm256_loadu_si256( (__m256i *) &frame1[row-4][col-1]);
		r1=_mm256_loadu_si256( (__m256i *) &frame1[row-3][col-1]);
		r2=_mm256_loadu_si256( (__m256i *) &frame1[row-2][col-1]);
		r3=_mm256_loadu_si256( (__m256i *) &frame1[row-1][col-1]);
		r4=_mm256_loadu_si256( (__m256i *) &frame1[row][col-1]);
		r5=_mm256_loadu_si256( (__m256i *) &frame1[row+1][col-1]);
		r6=_mm256_loadu_si256( (__m256i *) &frame1[row+2][col-1]);
		r7=_mm256_loadu_si256( (__m256i *) &frame1[row+3][col-1]);
		r8=_mm256_loadu_si256( (__m256i *) &frame1[row+4][col-1]);

		reminder_mask1=_mm256_load_si256( (__m256i *) &reminder_msk_9x9[REMINDER_ITERATIONS-4][0]);

		//zero the values that exceed the array bounds
		r0=_mm256_and_si256(r0,reminder_mask1);
		r1=_mm256_and_si256(r1,reminder_mask1);
		r2=_mm256_and_si256(r2,reminder_mask1);
		r3=_mm256_and_si256(r3,reminder_mask1);
		r4=_mm256_and_si256(r4,reminder_mask1);
		r5=_mm256_and_si256(r5,reminder_mask1);
		r6=_mm256_and_si256(r6,reminder_mask1);
		r7=_mm256_and_si256(r7,reminder_mask1);
		r8=_mm256_and_si256(r8,reminder_mask1);

		out2 = main_block_pre_9x9(r0,r1,r2,r3,r4,r5,r6,r7,r8,c0,c1,c2,c3,c4,c5,c6,c7,c8);

		//merge out1 and out2 and then divide
		out2=_mm256_slli_si256(out2,4);
		out1=_mm256_add_epi32(out1,out2);

		out1 = division_32(division_case,out1,f);

		//elements must be re-arranged before store
		m0=_mm256_and_si256(out1,out_mask_1);
		m1=_mm256_and_si256(out1,out_mask_2);
		m1=_mm256_srli_si256(m1,3);
		m2=_mm256_and_si256(out1,out_mask_3);
		m2=_mm256_slli_si256(m2,2);
		m3=_mm256_and_si256(out1,out_mask_4);
		m3=_mm256_srli_si256(m3,1);
		m0=_mm256_add_epi8(m0,m1);
		m0=_mm256_add_epi8(m0,m2);
		output2=_mm256_add_epi8(m0,m3);

		//5th col iteration
	    r0=_mm256_loadu_si256( (__m256i *) &frame1[row-4][col]);
		r1=_mm256_loadu_si256( (__m256i *) &frame1[row-3][col]);
		r2=_mm256_loadu_si256( (__m256i *) &frame1[row-2][col]);
		r3=_mm256_loadu_si256( (__m256i *) &frame1[row-1][col]);
		r4=_mm256_loadu_si256( (__m256i *) &frame1[row][col]);
		r5=_mm256_loadu_si256( (__m256i *) &frame1[row+1][col]);
		r6=_mm256_loadu_si256( (__m256i *) &frame1[row+2][col]);
		r7=_mm256_loadu_si256( (__m256i *) &frame1[row+3][col]);
		r8=_mm256_loadu_si256( (__m256i *) &frame1[row+4][col]);

		reminder_mask1=_mm256_load_si256( (__m256i *) &reminder_msk_9x9[REMINDER_ITERATIONS-5][0]);

		//zero the values that exceed the array bounds
		r0=_mm256_and_si256(r0,reminder_mask1);
		r1=_mm256_and_si256(r1,reminder_mask1);
		r2=_mm256_and_si256(r2,reminder_mask1);
		r3=_mm256_and_si256(r3,reminder_mask1);
		r4=_mm256_and_si256(r4,reminder_mask1);
		r5=_mm256_and_si256(r5,reminder_mask1);
		r6=_mm256_and_si256(r6,reminder_mask1);
		r7=_mm256_and_si256(r7,reminder_mask1);
		r8=_mm256_and_si256(r8,reminder_mask1);

		out1 = main_block_pre_9x9(r0,r1,r2,r3,r4,r5,r6,r7,r8,c0,c1,c2,c3,c4,c5,c6,c7,c8);

		//6th col iteration
	    r0=_mm256_loadu_si256( (__m256i *) &frame1[row-4][col+1]);
		r1=_mm256_loadu_si256( (__m256i *) &frame1[row-3][col+1]);
		r2=_mm256_loadu_si256( (__m256i *) &frame1[row-2][col+1]);
		r3=_mm256_loadu_si256( (__m256i *) &frame1[row-1][col+1]);
		r4=_mm256_loadu_si256( (__m256i *) &frame1[row][col+1]);
		r5=_mm256_loadu_si256( (__m256i *) &frame1[row+1][col+1]);
		r6=_mm256_loadu_si256( (__m256i *) &frame1[row+2][col+1]);
		r7=_mm256_loadu_si256( (__m256i *) &frame1[row+3][col+1]);
		r8=_mm256_loadu_si256( (__m256i *) &frame1[row+4][col+1]);

		reminder_mask1=_mm256_load_si256( (__m256i *) &reminder_msk_9x9[REMINDER_ITERATIONS-6][0]);

		//zero the values that exceed the array bounds
		r0=_mm256_and_si256(r0,reminder_mask1);
		r1=_mm256_and_si256(r1,reminder_mask1);
		r2=_mm256_and_si256(r2,reminder_mask1);
		r3=_mm256_and_si256(r3,reminder_mask1);
		r4=_mm256_and_si256(r4,reminder_mask1);
		r5=_mm256_and_si256(r5,reminder_mask1);
		r6=_mm256_and_si256(r6,reminder_mask1);
		r7=_mm256_and_si256(r7,reminder_mask1);
		r8=_mm256_and_si256(r8,reminder_mask1);

		out2 = main_block_pre_9x9(r0,r1,r2,r3,r4,r5,r6,r7,r8,c0,c1,c2,c3,c4,c5,c6,c7,c8);

		//merge out1 and out2 and then divide
		out2=_mm256_slli_si256(out2,4);
		out1=_mm256_add_epi32(out1,out2);

		out1 = division_32(division_case,out1,f);

		//elements must be re-arranged before store
		m0=_mm256_and_si256(out1,out_mask_1);
		m1=_mm256_and_si256(out1,out_mask_2);
		m1=_mm256_srli_si256(m1,3);
		m2=_mm256_and_si256(out1,out_mask_3);
		m2=_mm256_slli_si256(m2,2);
		m3=_mm256_and_si256(out1,out_mask_4);
		m3=_mm256_srli_si256(m3,1);
		m0=_mm256_add_epi8(m0,m1);
		m0=_mm256_add_epi8(m0,m2);
		output3=_mm256_add_epi8(m0,m3);

		//7th col iteration
	    r0=_mm256_loadu_si256( (__m256i *) &frame1[row-4][col+2]);
		r1=_mm256_loadu_si256( (__m256i *) &frame1[row-3][col+2]);
		r2=_mm256_loadu_si256( (__m256i *) &frame1[row-2][col+2]);
		r3=_mm256_loadu_si256( (__m256i *) &frame1[row-1][col+2]);
		r4=_mm256_loadu_si256( (__m256i *) &frame1[row][col+2]);
		r5=_mm256_loadu_si256( (__m256i *) &frame1[row+1][col+2]);
		r6=_mm256_loadu_si256( (__m256i *) &frame1[row+2][col+2]);
		r7=_mm256_loadu_si256( (__m256i *) &frame1[row+3][col+2]);
		r8=_mm256_loadu_si256( (__m256i *) &frame1[row+4][col+2]);

		reminder_mask1=_mm256_load_si256( (__m256i *) &reminder_msk_9x9[REMINDER_ITERATIONS-7][0]);

		//zero the values that exceed the array bounds
		r0=_mm256_and_si256(r0,reminder_mask1);
		r1=_mm256_and_si256(r1,reminder_mask1);
		r2=_mm256_and_si256(r2,reminder_mask1);
		r3=_mm256_and_si256(r3,reminder_mask1);
		r4=_mm256_and_si256(r4,reminder_mask1);
		r5=_mm256_and_si256(r5,reminder_mask1);
		r6=_mm256_and_si256(r6,reminder_mask1);
		r7=_mm256_and_si256(r7,reminder_mask1);
		r8=_mm256_and_si256(r8,reminder_mask1);

		out1 = main_block_pre_9x9(r0,r1,r2,r3,r4,r5,r6,r7,r8,c0,c1,c2,c3,c4,c5,c6,c7,c8);

	if (REMINDER_ITERATIONS>7){
		//8th col iteration
	    r0=_mm256_loadu_si256( (__m256i *) &frame1[row-4][col+3]);
		r1=_mm256_loadu_si256( (__m256i *) &frame1[row-3][col+3]);
		r2=_mm256_loadu_si256( (__m256i *) &frame1[row-2][col+3]);
		r3=_mm256_loadu_si256( (__m256i *) &frame1[row-1][col+3]);
		r4=_mm256_loadu_si256( (__m256i *) &frame1[row][col+3]);
		r5=_mm256_loadu_si256( (__m256i *) &frame1[row+1][col+3]);
		r6=_mm256_loadu_si256( (__m256i *) &frame1[row+2][col+3]);
		r7=_mm256_loadu_si256( (__m256i *) &frame1[row+3][col+3]);
		r8=_mm256_loadu_si256( (__m256i *) &frame1[row+4][col+3]);

		reminder_mask1=_mm256_load_si256( (__m256i *) &reminder_msk_9x9[REMINDER_ITERATIONS-8][0]);

		//zero the values that exceed the array bounds
		r0=_mm256_and_si256(r0,reminder_mask1);
		r1=_mm256_and_si256(r1,reminder_mask1);
		r2=_mm256_and_si256(r2,reminder_mask1);
		r3=_mm256_and_si256(r3,reminder_mask1);
		r4=_mm256_and_si256(r4,reminder_mask1);
		r5=_mm256_and_si256(r5,reminder_mask1);
		r6=_mm256_and_si256(r6,reminder_mask1);
		r7=_mm256_and_si256(r7,reminder_mask1);
		r8=_mm256_and_si256(r8,reminder_mask1);

		out2 = main_block_pre_9x9(r0,r1,r2,r3,r4,r5,r6,r7,r8,c0,c1,c2,c3,c4,c5,c6,c7,c8);

		//merge out1 and out2 and then divide
		out2=_mm256_slli_si256(out2,4);
		out1=_mm256_add_epi32(out1,out2);

		out1 = division_32(division_case,out1,f);

		//elements must be re-arranged before store
		m0=_mm256_and_si256(out1,out_mask_1);
		m1=_mm256_and_si256(out1,out_mask_2);
		m1=_mm256_srli_si256(m1,3);
		m2=_mm256_and_si256(out1,out_mask_3);
		m2=_mm256_slli_si256(m2,2);
		m3=_mm256_and_si256(out1,out_mask_4);
		m3=_mm256_srli_si256(m3,1);
		m0=_mm256_add_epi8(m0,m1);
		m0=_mm256_add_epi8(m0,m2);
		output4=_mm256_add_epi8(m0,m3);
	}
	else{
		out1 = division_32(division_case,out1,f);

		//elements must be re-arranged before store
		m0=_mm256_and_si256(out1,out_mask_1);
		m1=_mm256_and_si256(out1,out_mask_2);
		m1=_mm256_srli_si256(m1,3);
		m2=_mm256_and_si256(out1,out_mask_3);
		m2=_mm256_slli_si256(m2,2);
		m3=_mm256_and_si256(out1,out_mask_4);
		m3=_mm256_srli_si256(m3,1);
		m0=_mm256_add_epi8(m0,m1);
		m0=_mm256_add_epi8(m0,m2);
		output4=_mm256_add_epi8(m0,m3);
	}

	if (REMINDER_ITERATIONS>8){
		//9th col iteration
	    r0=_mm256_loadu_si256( (__m256i *) &frame1[row-4][col+4]);
		r1=_mm256_loadu_si256( (__m256i *) &frame1[row-3][col+4]);
		r2=_mm256_loadu_si256( (__m256i *) &frame1[row-2][col+4]);
		r3=_mm256_loadu_si256( (__m256i *) &frame1[row-1][col+4]);
		r4=_mm256_loadu_si256( (__m256i *) &frame1[row][col+4]);
		r5=_mm256_loadu_si256( (__m256i *) &frame1[row+1][col+4]);
		r6=_mm256_loadu_si256( (__m256i *) &frame1[row+2][col+4]);
		r7=_mm256_loadu_si256( (__m256i *) &frame1[row+3][col+4]);
		r8=_mm256_loadu_si256( (__m256i *) &frame1[row+4][col+4]);

		reminder_mask1=_mm256_load_si256( (__m256i *) &reminder_msk_9x9[REMINDER_ITERATIONS-9][0]);

		//zero the values that exceed the array bounds
		r0=_mm256_and_si256(r0,reminder_mask1);
		r1=_mm256_and_si256(r1,reminder_mask1);
		r2=_mm256_and_si256(r2,reminder_mask1);
		r3=_mm256_and_si256(r3,reminder_mask1);
		r4=_mm256_and_si256(r4,reminder_mask1);
		r5=_mm256_and_si256(r5,reminder_mask1);
		r6=_mm256_and_si256(r6,reminder_mask1);
		r7=_mm256_and_si256(r7,reminder_mask1);
		r8=_mm256_and_si256(r8,reminder_mask1);

		out1 = main_block_pre_9x9(r0,r1,r2,r3,r4,r5,r6,r7,r8,c0,c1,c2,c3,c4,c5,c6,c7,c8);
	}
	else {
		output5=_mm256_setzero_si256();
	}

	if (REMINDER_ITERATIONS>9){
		//10th col iteration
	    r0=_mm256_loadu_si256( (__m256i *) &frame1[row-4][col+5]);
		r1=_mm256_loadu_si256( (__m256i *) &frame1[row-3][col+5]);
		r2=_mm256_loadu_si256( (__m256i *) &frame1[row-2][col+5]);
		r3=_mm256_loadu_si256( (__m256i *) &frame1[row-1][col+5]);
		r4=_mm256_loadu_si256( (__m256i *) &frame1[row][col+5]);
		r5=_mm256_loadu_si256( (__m256i *) &frame1[row+1][col+5]);
		r6=_mm256_loadu_si256( (__m256i *) &frame1[row+2][col+5]);
		r7=_mm256_loadu_si256( (__m256i *) &frame1[row+3][col+5]);
		r8=_mm256_loadu_si256( (__m256i *) &frame1[row+4][col+5]);

		reminder_mask1=_mm256_load_si256( (__m256i *) &reminder_msk_9x9[REMINDER_ITERATIONS-10][0]);

		//zero the values that exceed the array bounds
		r0=_mm256_and_si256(r0,reminder_mask1);
		r1=_mm256_and_si256(r1,reminder_mask1);
		r2=_mm256_and_si256(r2,reminder_mask1);
		r3=_mm256_and_si256(r3,reminder_mask1);
		r4=_mm256_and_si256(r4,reminder_mask1);
		r5=_mm256_and_si256(r5,reminder_mask1);
		r6=_mm256_and_si256(r6,reminder_mask1);
		r7=_mm256_and_si256(r7,reminder_mask1);
		r8=_mm256_and_si256(r8,reminder_mask1);

		out2 = main_block_pre_9x9(r0,r1,r2,r3,r4,r5,r6,r7,r8,c0,c1,c2,c3,c4,c5,c6,c7,c8);

		//merge out1 and out2 and then divide
		out2=_mm256_slli_si256(out2,4);
		out1=_mm256_add_epi32(out1,out2);

		out1 = division_32(division_case,out1,f);

		//elements must be re-arranged before store
		m0=_mm256_and_si256(out1,out_mask_1);
		m1=_mm256_and_si256(out1,out_mask_2);
		m1=_mm256_srli_si256(m1,3);
		m2=_mm256_and_si256(out1,out_mask_3);
		m2=_mm256_slli_si256(m2,2);
		m3=_mm256_and_si256(out1,out_mask_4);
		m3=_mm256_srli_si256(m3,1);
		m0=_mm256_add_epi8(m0,m1);
		m0=_mm256_add_epi8(m0,m2);
		output5=_mm256_add_epi8(m0,m3);
	}
	else {
		out1 = division_32(division_case,out1,f);

				//elements must be re-arranged before store
				m0=_mm256_and_si256(out1,out_mask_1);
				m1=_mm256_and_si256(out1,out_mask_2);
				m1=_mm256_srli_si256(m1,3);
				m2=_mm256_and_si256(out1,out_mask_3);
				m2=_mm256_slli_si256(m2,2);
				m3=_mm256_and_si256(out1,out_mask_4);
				m3=_mm256_srli_si256(m3,1);
				m0=_mm256_add_epi8(m0,m1);
				m0=_mm256_add_epi8(m0,m2);
				output5=_mm256_add_epi8(m0,m3);
	}
			//blend the output1:output5
  			output2=_mm256_slli_si256(output2,2);
  			output3=_mm256_slli_si256(output3,4);

  			m3=_mm256_slli_si256(output4,6);
  			m4=_mm256_and_si256(output4,out_mask_5);
  			m4=_mm256_permute2f128_si256(m4,m4,1);
  			m4=_mm256_srli_si256(m4,10);
  			output4=_mm256_add_epi32(m3,m4);

  			m3=_mm256_slli_si256(output5,8);
  			m4=_mm256_and_si256(output5,out_mask_5);
  			m4=_mm256_permute2f128_si256(m4,m4,1);
  			m4=_mm256_srli_si256(m4,8);
  			output5=_mm256_add_epi32(m3,m4);

  			output1=_mm256_add_epi8(output1,output2);
  			output1=_mm256_add_epi8(output1,output3);
  			output1=_mm256_add_epi8(output1,output4);
  			output1=_mm256_add_epi8(output1,output5);


	switch (REMINDER_ITERATIONS){

	case 7:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output1,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output1,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output1,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output1,6);
		  break;
	case 8:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output1,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output1,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output1,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output1,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output1,7);
		  break;
	case 9:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output1,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output1,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output1,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output1,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output1,7);
		filt[row][col+8] = (unsigned char) _mm256_extract_epi8(output1,8);
		  break;
	case 10:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output1,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output1,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output1,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output1,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output1,7);
		filt[row][col+8] = (unsigned char) _mm256_extract_epi8(output1,8);
		filt[row][col+9] = (unsigned char) _mm256_extract_epi8(output1,9);
		  break;
	case 11:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output1,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output1,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output1,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output1,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output1,7);
		filt[row][col+8] = (unsigned char) _mm256_extract_epi8(output1,8);
		filt[row][col+9] = (unsigned char) _mm256_extract_epi8(output1,9);
		filt[row][col+10] = (unsigned char) _mm256_extract_epi8(output1,10);
		  break;
	case 12:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output1,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output1,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output1,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output1,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output1,7);
		filt[row][col+8] = (unsigned char) _mm256_extract_epi8(output1,8);
		filt[row][col+9] = (unsigned char) _mm256_extract_epi8(output1,9);
		filt[row][col+10] = (unsigned char) _mm256_extract_epi8(output1,10);
		filt[row][col+11] = (unsigned char) _mm256_extract_epi8(output1,11);
		  break;
	case 13:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output1,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output1,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output1,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output1,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output1,7);
		filt[row][col+8] = (unsigned char) _mm256_extract_epi8(output1,8);
		filt[row][col+9] = (unsigned char) _mm256_extract_epi8(output1,9);
		filt[row][col+10] = (unsigned char) _mm256_extract_epi8(output1,10);
		filt[row][col+11] = (unsigned char) _mm256_extract_epi8(output1,11);
		filt[row][col+12] = (unsigned char) _mm256_extract_epi8(output1,12);
		  break;
	case 14:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output1,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output1,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output1,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output1,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output1,7);
		filt[row][col+8] = (unsigned char) _mm256_extract_epi8(output1,8);
		filt[row][col+9] = (unsigned char) _mm256_extract_epi8(output1,9);
		filt[row][col+10] = (unsigned char) _mm256_extract_epi8(output1,10);
		filt[row][col+11] = (unsigned char) _mm256_extract_epi8(output1,11);
		filt[row][col+12] = (unsigned char) _mm256_extract_epi8(output1,12);
		filt[row][col+13] = (unsigned char) _mm256_extract_epi8(output1,13);
		  break;
	case 15:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output1,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output1,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output1,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output1,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output1,7);
		filt[row][col+8] = (unsigned char) _mm256_extract_epi8(output1,8);
		filt[row][col+9] = (unsigned char) _mm256_extract_epi8(output1,9);
		filt[row][col+10] = (unsigned char) _mm256_extract_epi8(output1,10);
		filt[row][col+11] = (unsigned char) _mm256_extract_epi8(output1,11);
		filt[row][col+12] = (unsigned char) _mm256_extract_epi8(output1,12);
		filt[row][col+13] = (unsigned char) _mm256_extract_epi8(output1,13);
		filt[row][col+14] = (unsigned char) _mm256_extract_epi8(output1,14);
		  break;
	case 16:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
		  break;
	case 17:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
		  break;
	case 18:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
		  break;
	case 19:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
		  break;
	case 20:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output1,19);
		  break;
	case 21:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output1,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output1,20);
		  break;
	case 22:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output1,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output1,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output1,21);
		break;
	case 23:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output1,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output1,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output1,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output1,22);
		  break;
	case 24:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output1,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output1,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output1,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output1,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output1,23);
		  break;
	case 25:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output1,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output1,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output1,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output1,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output1,23);
	filt[row][col+24] = (unsigned char) _mm256_extract_epi8(output1,24);
		  break;
	case 26:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output1,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output1,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output1,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output1,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output1,23);
	filt[row][col+24] = (unsigned char) _mm256_extract_epi8(output1,24);
	filt[row][col+25] = (unsigned char) _mm256_extract_epi8(output1,25);
		  break;
	case 27:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output1,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output1,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output1,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output1,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output1,23);
	filt[row][col+24] = (unsigned char) _mm256_extract_epi8(output1,24);
	filt[row][col+25] = (unsigned char) _mm256_extract_epi8(output1,25);
	filt[row][col+26] = (unsigned char) _mm256_extract_epi8(output1,26);
		  break;
	case 28:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output1,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output1,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output1,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output1,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output1,23);
	filt[row][col+24] = (unsigned char) _mm256_extract_epi8(output1,24);
	filt[row][col+25] = (unsigned char) _mm256_extract_epi8(output1,25);
	filt[row][col+26] = (unsigned char) _mm256_extract_epi8(output1,26);
	filt[row][col+27] = (unsigned char) _mm256_extract_epi8(output1,27);
		  break;
	case 29:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output1,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output1,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output1,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output1,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output1,23);
	filt[row][col+24] = (unsigned char) _mm256_extract_epi8(output1,24);
	filt[row][col+25] = (unsigned char) _mm256_extract_epi8(output1,25);
	filt[row][col+26] = (unsigned char) _mm256_extract_epi8(output1,26);
	filt[row][col+27] = (unsigned char) _mm256_extract_epi8(output1,27);
	filt[row][col+28] = (unsigned char) _mm256_extract_epi8(output1,28);
		  break;
	case 30:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output1,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output1,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output1,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output1,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output1,23);
	filt[row][col+24] = (unsigned char) _mm256_extract_epi8(output1,24);
	filt[row][col+25] = (unsigned char) _mm256_extract_epi8(output1,25);
	filt[row][col+26] = (unsigned char) _mm256_extract_epi8(output1,26);
	filt[row][col+27] = (unsigned char) _mm256_extract_epi8(output1,27);
	filt[row][col+28] = (unsigned char) _mm256_extract_epi8(output1,28);
	filt[row][col+29] = (unsigned char) _mm256_extract_epi8(output1,29);
		  break;
	case 31:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output1,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output1,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output1,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output1,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output1,23);
	filt[row][col+24] = (unsigned char) _mm256_extract_epi8(output1,24);
	filt[row][col+25] = (unsigned char) _mm256_extract_epi8(output1,25);
	filt[row][col+26] = (unsigned char) _mm256_extract_epi8(output1,26);
	filt[row][col+27] = (unsigned char) _mm256_extract_epi8(output1,27);
	filt[row][col+28] = (unsigned char) _mm256_extract_epi8(output1,28);
	filt[row][col+29] = (unsigned char) _mm256_extract_epi8(output1,29);
	//another iteration is needed to compute filt[row][col+30]
	default_9x9(frame1,filt,row,col+30, M, divisor, filter9x9);

		  break;
	case 32:
		_mm256_storeu_si256( (__m256i *) &filt[row][col],output1 );
		//another iteration is needed to compute filt[row][col+30:31]
		default_9x9(frame1,filt,row,col+30, M, divisor, filter9x9);

		  break;
	case 33:
		_mm256_storeu_si256( (__m256i *) &filt[row][col],output1 );
		//another iteration is needed to compute filt[row][col+30:32]
		default_9x9(frame1,filt,row,col+30, M, divisor, filter9x9);

		  break;
	case 34:
		_mm256_storeu_si256( (__m256i *) &filt[row][col],output1 );
		//another iteration is needed to compute filt[row][col+30:33]
		default_9x9(frame1,filt,row,col+30, M, divisor, filter9x9);

		  break;
	case 35:
		_mm256_storeu_si256( (__m256i *) &filt[row][col],output1 );
		//another iteration is needed to compute filt[row][col+30:34]
		default_9x9(frame1,filt,row,col+30, M, divisor, filter9x9);

		  break;
	case 36:
		_mm256_storeu_si256( (__m256i *) &filt[row][col],output1 );
		//another iteration is needed to compute filt[row][col+30:35]
		default_9x9(frame1,filt,row,col+30, M, divisor, filter9x9);

		  break;
	default:
		printf("\nsomething went wrong");
		exit(EXIT_FAILURE);
 	}




}


void default_9x9_pre(unsigned char **frame1,unsigned char **filt, const unsigned int row,const unsigned int col_off, const unsigned int M, const unsigned int N,const unsigned int divisor,signed char **filter){

	int rowOffset,colOffset;
	int newPixel;
	unsigned char pix;
	unsigned int col;

		for (col = col_off; col < M; col++) {
				newPixel = 0;
				for (rowOffset=-4; rowOffset<=4; rowOffset++) {
					for (colOffset=-4; colOffset<=4; colOffset++) {

						if ( (row+rowOffset<0) || (row+rowOffset>=N) || (col+colOffset>=M) )
							pix=0;
						else
							pix=frame1[row+rowOffset][col+colOffset];

	                   newPixel += pix * filter[4 + rowOffset][4 + colOffset];

					}
				        }
			filt[row][col] = (unsigned char) (newPixel / divisor);

			}

}


void default_9x9(unsigned char **frame1,unsigned char **filt,const unsigned int row,const unsigned int col_off, const unsigned int M, const unsigned int divisor,signed char **filter){

	int rowOffset;
	int newPixel;
	unsigned char pix;
	unsigned int col;

	for (col = col_off; col < M; col++) {
				newPixel = 0;
              for (rowOffset=-4;rowOffset<=4;rowOffset++){

                newPixel += frame1[row+rowOffset][col-4] * filter[4+rowOffset][4-4];
                newPixel += frame1[row+rowOffset][col-3] * filter[4+rowOffset][4-3];
                newPixel += frame1[row+rowOffset][col-2] * filter[4+rowOffset][4-2];
                newPixel += frame1[row+rowOffset][col-1] * filter[4+rowOffset][4-1];
                newPixel += frame1[row+rowOffset][col-0] * filter[4+rowOffset][4-0];
                if (col+1>=M){
                	pix=0;
                	newPixel += pix * filter[4+rowOffset][4+1];
                }
                else {
                	pix=frame1[row+rowOffset][col+1];
                	newPixel += pix * filter[4+rowOffset][4+1];
                }

                if (col+2>=M){
                	pix=0;
                	newPixel += pix * filter[4+rowOffset][4+2];
                }
                else {
                	pix=frame1[row+rowOffset][col+2];
                	newPixel += pix * filter[4+rowOffset][4+2];
                }

                if (col+3>=M){
                	pix=0;
                	newPixel += pix * filter[4+rowOffset][4+3];
                }
                else {
                	pix=frame1[row+rowOffset][col+3];
                	newPixel += pix * filter[4+rowOffset][4+3];
                }

                if (col+4>=M){
                	pix=0;
                	newPixel += pix * filter[4+rowOffset][4+4];
                }
                else {
                	pix=frame1[row+rowOffset][col+4];
                	newPixel += pix * filter[4+rowOffset][4+4];
                }

              }

			filt[row][col] = (unsigned char) (newPixel / divisor);

			}

}



void loop_reminder_9x9_row_boundaries(unsigned char **frame1,unsigned char **filt,const unsigned int row, const unsigned int N, const unsigned int M,const unsigned int REMINDER_ITERATIONS,const unsigned int division_case,const signed char mask_vector[][32], const __m256i f,const unsigned short int divisor,signed char **filter9x9){


	const __m256i out_mask_1    = _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255);//0,20
	const __m256i out_mask_2    = _mm256_set_epi8(0,0,0,0,0,0,0,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255,0,0,0,0);//4,24
	const __m256i out_mask_3    = _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, 0,0,0,0,0,0,0,255,0,0,0,0,0,0,0,0);//8
	const __m256i out_mask_4    = _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, 0,0,0,255,0,0,0,0,0,0,0,0,0,0,0,0);//12
	const __m256i out_mask_5    = _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, 0,0,0,0,255,255,0,0,0,0,0,0,0,0,0,0);//10,11


	__m256i m0,m1,m2,m3,m4,out1,out2;
	__m256i output1,output2,output3,output4,output5;

unsigned int col=M-REMINDER_ITERATIONS;

//important:  //REMINDER_ITERATIONS ranges [7,36]. If >30, two iterations are needed, one for computing the first 30 output pixels and another for computing the rest

		out1=load_rows(frame1,-4, 0, row, col, N, REMINDER_ITERATIONS, mask_vector );

		out2=load_rows(frame1,-3, 1, row, col, N, REMINDER_ITERATIONS, mask_vector );

			//merge out1 and out2 and then divide
			out2=_mm256_slli_si256(out2,4);
			out1=_mm256_add_epi32(out1,out2);

			out1 = division_32(division_case,out1,f);

			//elements must be re-arranged before store
			m0=_mm256_and_si256(out1,out_mask_1);
			m1=_mm256_and_si256(out1,out_mask_2);
			m1=_mm256_srli_si256(m1,3);
			m2=_mm256_and_si256(out1,out_mask_3);
			m2=_mm256_slli_si256(m2,2);
			m3=_mm256_and_si256(out1,out_mask_4);
			m3=_mm256_srli_si256(m3,1);
			m0=_mm256_add_epi8(m0,m1);
			m0=_mm256_add_epi8(m0,m2);
			output1=_mm256_add_epi8(m0,m3);

		//3rd col

		out1=load_rows(frame1,-2, 2, row, col, N, REMINDER_ITERATIONS, mask_vector );

		//4th col

		out2 =load_rows(frame1,-1, 3, row, col, N, REMINDER_ITERATIONS, mask_vector );

		//merge out1 and out2 and then divide
		out2=_mm256_slli_si256(out2,4);
		out1=_mm256_add_epi32(out1,out2);

		out1 = division_32(division_case,out1,f);

		//elements must be re-arranged before store
		m0=_mm256_and_si256(out1,out_mask_1);
		m1=_mm256_and_si256(out1,out_mask_2);
		m1=_mm256_srli_si256(m1,3);
		m2=_mm256_and_si256(out1,out_mask_3);
		m2=_mm256_slli_si256(m2,2);
		m3=_mm256_and_si256(out1,out_mask_4);
		m3=_mm256_srli_si256(m3,1);
		m0=_mm256_add_epi8(m0,m1);
		m0=_mm256_add_epi8(m0,m2);
		output2=_mm256_add_epi8(m0,m3);

		//5th col iteration

		out1 =load_rows(frame1,0,4, row, col, N, REMINDER_ITERATIONS, mask_vector );

		//6th col iteration


		out2 =load_rows(frame1,+1,5, row, col, N, REMINDER_ITERATIONS, mask_vector );

		//merge out1 and out2 and then divide
		out2=_mm256_slli_si256(out2,4);
		out1=_mm256_add_epi32(out1,out2);

		out1 = division_32(division_case,out1,f);

		//elements must be re-arranged before store
		m0=_mm256_and_si256(out1,out_mask_1);
		m1=_mm256_and_si256(out1,out_mask_2);
		m1=_mm256_srli_si256(m1,3);
		m2=_mm256_and_si256(out1,out_mask_3);
		m2=_mm256_slli_si256(m2,2);
		m3=_mm256_and_si256(out1,out_mask_4);
		m3=_mm256_srli_si256(m3,1);
		m0=_mm256_add_epi8(m0,m1);
		m0=_mm256_add_epi8(m0,m2);
		output3=_mm256_add_epi8(m0,m3);

		//7th col iteration

		out1 =load_rows(frame1,+2,6, row, col, N, REMINDER_ITERATIONS, mask_vector );

		//8th col iteration
		if (REMINDER_ITERATIONS>7){
			out2 =load_rows(frame1,+3,7, row, col, N, REMINDER_ITERATIONS, mask_vector);
		}
		else{
			out2=_mm256_setzero_si256();
		}

		//merge out1 and out2 and then divide
		out2=_mm256_slli_si256(out2,4);
		out1=_mm256_add_epi32(out1,out2);

		out1 = division_32(division_case,out1,f);

		//elements must be re-arranged before store
		m0=_mm256_and_si256(out1,out_mask_1);
		m1=_mm256_and_si256(out1,out_mask_2);
		m1=_mm256_srli_si256(m1,3);
		m2=_mm256_and_si256(out1,out_mask_3);
		m2=_mm256_slli_si256(m2,2);
		m3=_mm256_and_si256(out1,out_mask_4);
		m3=_mm256_srli_si256(m3,1);
		m0=_mm256_add_epi8(m0,m1);
		m0=_mm256_add_epi8(m0,m2);
		output4=_mm256_add_epi8(m0,m3);

		//9th col iteration
		if (REMINDER_ITERATIONS>8){
	      out1 =load_rows(frame1,+4,8, row, col, N, REMINDER_ITERATIONS,mask_vector );
		}
		else {
			output5=_mm256_setzero_si256();
		}

		//10th col iteration
		if (REMINDER_ITERATIONS>9){
		 out2 =load_rows(frame1,+5,9, row, col, N, REMINDER_ITERATIONS, mask_vector );}
		else{
			out2=_mm256_setzero_si256();
		}

		//merge out1 and out2 and then divide
		out2=_mm256_slli_si256(out2,4);
		out1=_mm256_add_epi32(out1,out2);

		out1 = division_32(division_case,out1,f);

		//elements must be re-arranged before store
		m0=_mm256_and_si256(out1,out_mask_1);
		m1=_mm256_and_si256(out1,out_mask_2);
		m1=_mm256_srli_si256(m1,3);
		m2=_mm256_and_si256(out1,out_mask_3);
		m2=_mm256_slli_si256(m2,2);
		m3=_mm256_and_si256(out1,out_mask_4);
		m3=_mm256_srli_si256(m3,1);
		m0=_mm256_add_epi8(m0,m1);
		m0=_mm256_add_epi8(m0,m2);
		output5=_mm256_add_epi8(m0,m3);

			//blend the output1:output5
  			output2=_mm256_slli_si256(output2,2);
  			output3=_mm256_slli_si256(output3,4);

  			m3=_mm256_slli_si256(output4,6);
  			m4=_mm256_and_si256(output4,out_mask_5);
  			m4=_mm256_permute2f128_si256(m4,m4,1);
  			m4=_mm256_srli_si256(m4,10);
  			output4=_mm256_add_epi32(m3,m4);

  			m3=_mm256_slli_si256(output5,8);
  			m4=_mm256_and_si256(output5,out_mask_5);
  			m4=_mm256_permute2f128_si256(m4,m4,1);
  			m4=_mm256_srli_si256(m4,8);
  			output5=_mm256_add_epi32(m3,m4);

  			output1=_mm256_add_epi8(output1,output2);
  			output1=_mm256_add_epi8(output1,output3);
  			output1=_mm256_add_epi8(output1,output4);
  			output1=_mm256_add_epi8(output1,output5);



	switch (REMINDER_ITERATIONS){

	case 7:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output1,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output1,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output1,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output1,6);
		  break;
	case 8:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output1,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output1,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output1,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output1,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output1,7);
		  break;
	case 9:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output1,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output1,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output1,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output1,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output1,7);
		filt[row][col+8] = (unsigned char) _mm256_extract_epi8(output1,8);
		  break;
	case 10:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output1,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output1,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output1,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output1,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output1,7);
		filt[row][col+8] = (unsigned char) _mm256_extract_epi8(output1,8);
		filt[row][col+9] = (unsigned char) _mm256_extract_epi8(output1,9);
		  break;
	case 11:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output1,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output1,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output1,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output1,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output1,7);
		filt[row][col+8] = (unsigned char) _mm256_extract_epi8(output1,8);
		filt[row][col+9] = (unsigned char) _mm256_extract_epi8(output1,9);
		filt[row][col+10] = (unsigned char) _mm256_extract_epi8(output1,10);
		  break;
	case 12:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output1,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output1,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output1,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output1,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output1,7);
		filt[row][col+8] = (unsigned char) _mm256_extract_epi8(output1,8);
		filt[row][col+9] = (unsigned char) _mm256_extract_epi8(output1,9);
		filt[row][col+10] = (unsigned char) _mm256_extract_epi8(output1,10);
		filt[row][col+11] = (unsigned char) _mm256_extract_epi8(output1,11);
		  break;
	case 13:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output1,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output1,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output1,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output1,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output1,7);
		filt[row][col+8] = (unsigned char) _mm256_extract_epi8(output1,8);
		filt[row][col+9] = (unsigned char) _mm256_extract_epi8(output1,9);
		filt[row][col+10] = (unsigned char) _mm256_extract_epi8(output1,10);
		filt[row][col+11] = (unsigned char) _mm256_extract_epi8(output1,11);
		filt[row][col+12] = (unsigned char) _mm256_extract_epi8(output1,12);
		  break;
	case 14:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output1,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output1,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output1,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output1,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output1,7);
		filt[row][col+8] = (unsigned char) _mm256_extract_epi8(output1,8);
		filt[row][col+9] = (unsigned char) _mm256_extract_epi8(output1,9);
		filt[row][col+10] = (unsigned char) _mm256_extract_epi8(output1,10);
		filt[row][col+11] = (unsigned char) _mm256_extract_epi8(output1,11);
		filt[row][col+12] = (unsigned char) _mm256_extract_epi8(output1,12);
		filt[row][col+13] = (unsigned char) _mm256_extract_epi8(output1,13);
		  break;
	case 15:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output1,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output1,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output1,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output1,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output1,7);
		filt[row][col+8] = (unsigned char) _mm256_extract_epi8(output1,8);
		filt[row][col+9] = (unsigned char) _mm256_extract_epi8(output1,9);
		filt[row][col+10] = (unsigned char) _mm256_extract_epi8(output1,10);
		filt[row][col+11] = (unsigned char) _mm256_extract_epi8(output1,11);
		filt[row][col+12] = (unsigned char) _mm256_extract_epi8(output1,12);
		filt[row][col+13] = (unsigned char) _mm256_extract_epi8(output1,13);
		filt[row][col+14] = (unsigned char) _mm256_extract_epi8(output1,14);
		  break;
	case 16:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
		  break;
	case 17:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
		  break;
	case 18:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
		  break;
	case 19:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
		  break;
	case 20:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output1,19);
		  break;
	case 21:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output1,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output1,20);
		  break;
	case 22:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output1,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output1,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output1,21);
		break;
	case 23:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output1,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output1,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output1,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output1,22);
		  break;
	case 24:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output1,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output1,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output1,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output1,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output1,23);
		  break;
	case 25:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output1,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output1,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output1,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output1,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output1,23);
	filt[row][col+24] = (unsigned char) _mm256_extract_epi8(output1,24);
		  break;
	case 26:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output1,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output1,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output1,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output1,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output1,23);
	filt[row][col+24] = (unsigned char) _mm256_extract_epi8(output1,24);
	filt[row][col+25] = (unsigned char) _mm256_extract_epi8(output1,25);
		  break;
	case 27:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output1,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output1,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output1,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output1,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output1,23);
	filt[row][col+24] = (unsigned char) _mm256_extract_epi8(output1,24);
	filt[row][col+25] = (unsigned char) _mm256_extract_epi8(output1,25);
	filt[row][col+26] = (unsigned char) _mm256_extract_epi8(output1,26);
		  break;
	case 28:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output1,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output1,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output1,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output1,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output1,23);
	filt[row][col+24] = (unsigned char) _mm256_extract_epi8(output1,24);
	filt[row][col+25] = (unsigned char) _mm256_extract_epi8(output1,25);
	filt[row][col+26] = (unsigned char) _mm256_extract_epi8(output1,26);
	filt[row][col+27] = (unsigned char) _mm256_extract_epi8(output1,27);
		  break;
	case 29:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output1,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output1,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output1,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output1,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output1,23);
	filt[row][col+24] = (unsigned char) _mm256_extract_epi8(output1,24);
	filt[row][col+25] = (unsigned char) _mm256_extract_epi8(output1,25);
	filt[row][col+26] = (unsigned char) _mm256_extract_epi8(output1,26);
	filt[row][col+27] = (unsigned char) _mm256_extract_epi8(output1,27);
	filt[row][col+28] = (unsigned char) _mm256_extract_epi8(output1,28);
		  break;
	case 30:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output1,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output1,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output1,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output1,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output1,23);
	filt[row][col+24] = (unsigned char) _mm256_extract_epi8(output1,24);
	filt[row][col+25] = (unsigned char) _mm256_extract_epi8(output1,25);
	filt[row][col+26] = (unsigned char) _mm256_extract_epi8(output1,26);
	filt[row][col+27] = (unsigned char) _mm256_extract_epi8(output1,27);
	filt[row][col+28] = (unsigned char) _mm256_extract_epi8(output1,28);
	filt[row][col+29] = (unsigned char) _mm256_extract_epi8(output1,29);
		  break;
	case 31:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output1,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output1,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output1,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output1,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output1,23);
	filt[row][col+24] = (unsigned char) _mm256_extract_epi8(output1,24);
	filt[row][col+25] = (unsigned char) _mm256_extract_epi8(output1,25);
	filt[row][col+26] = (unsigned char) _mm256_extract_epi8(output1,26);
	filt[row][col+27] = (unsigned char) _mm256_extract_epi8(output1,27);
	filt[row][col+28] = (unsigned char) _mm256_extract_epi8(output1,28);
	filt[row][col+29] = (unsigned char) _mm256_extract_epi8(output1,29);
	//another iteration is needed to compute filt[row][col+30]
	default_9x9_pre(frame1,filt,row,col+30, M,N, divisor, filter9x9);

		  break;
	case 32:
		_mm256_storeu_si256( (__m256i *) &filt[row][col],output1 );
		//another iteration is needed to compute filt[row][col+30:31]
		default_9x9_pre(frame1,filt,row,col+30, M,N, divisor, filter9x9);

		  break;
	case 33:
		_mm256_storeu_si256( (__m256i *) &filt[row][col],output1 );
		//another iteration is needed to compute filt[row][col+30:32]
		default_9x9_pre(frame1,filt,row,col+30, M,N, divisor, filter9x9);

		  break;
	case 34:
		_mm256_storeu_si256( (__m256i *) &filt[row][col],output1 );
		//another iteration is needed to compute filt[row][col+30:33]
		default_9x9_pre(frame1,filt,row,col+30, M,N, divisor, filter9x9);

		  break;
	case 35:
		_mm256_storeu_si256( (__m256i *) &filt[row][col],output1 );
		//another iteration is needed to compute filt[row][col+30:34]
		default_9x9_pre(frame1,filt,row,col+30, M,N, divisor, filter9x9);

		  break;
	case 36:
		_mm256_storeu_si256( (__m256i *) &filt[row][col],output1 );
		//another iteration is needed to compute filt[row][col+30:35]
		default_9x9_pre(frame1,filt,row,col+30, M,N, divisor, filter9x9);

		  break;
	default:
		printf("\nsomething went wrong");
		exit(EXIT_FAILURE);
 	}

}


__m256i load_rows(unsigned char **frame1, const int col_off, const unsigned int it, const unsigned int row, const unsigned int col, const unsigned int N, const unsigned int REMINDER_ITERATIONS,const signed char mask_vector[][32]){

	 __m256i c0=_mm256_load_si256( (__m256i *) &mask_vector[0][0]);
	__m256i c1=_mm256_load_si256( (__m256i *) &mask_vector[1][0]);
	__m256i c2=_mm256_load_si256( (__m256i *) &mask_vector[2][0]);
	__m256i c3=_mm256_load_si256( (__m256i *) &mask_vector[3][0]);
	 __m256i c4=_mm256_load_si256( (__m256i *) &mask_vector[4][0]);
	 __m256i c5=_mm256_load_si256( (__m256i *) &mask_vector[5][0]);
	 __m256i c6=_mm256_load_si256( (__m256i *) &mask_vector[6][0]);
	 __m256i c7=_mm256_load_si256( (__m256i *) &mask_vector[7][0]);
	 __m256i c8=_mm256_load_si256( (__m256i *) &mask_vector[8][0]);

	 __m256i r0,r1,r2,r3,r4,r5,r6,r7,r8;

	if (row==0){
		r0=_mm256_loadu_si256( (__m256i *) &frame1[0][col+col_off]);
		r1=_mm256_loadu_si256( (__m256i *) &frame1[1][col+col_off]);
		r2=_mm256_loadu_si256( (__m256i *) &frame1[2][col+col_off]);
		r3=_mm256_loadu_si256( (__m256i *) &frame1[3][col+col_off]);
		r4=_mm256_loadu_si256( (__m256i *) &frame1[4][col+col_off]);
	    r5=_mm256_setzero_si256();
	    r6=_mm256_setzero_si256();
	    r7=_mm256_setzero_si256();
	    r8=_mm256_setzero_si256();
	    c0=c4; c1=c5; c2=c6;c3=c7;c4=c8;
	}
	else if (row==1){
		r0=_mm256_loadu_si256( (__m256i *) &frame1[0][col+col_off]);
		r1=_mm256_loadu_si256( (__m256i *) &frame1[1][col+col_off]);
		r2=_mm256_loadu_si256( (__m256i *) &frame1[2][col+col_off]);
		r3=_mm256_loadu_si256( (__m256i *) &frame1[3][col+col_off]);
		r4=_mm256_loadu_si256( (__m256i *) &frame1[4][col+col_off]);
		r5=_mm256_loadu_si256( (__m256i *) &frame1[5][col+col_off]);
	    r6=_mm256_setzero_si256();
	    r7=_mm256_setzero_si256();
	    r8=_mm256_setzero_si256();
	    c0=c3; c1=c4; c2=c5;c3=c6;c4=c7;c5=c8;
	}
	else if (row==2){
		r0=_mm256_loadu_si256( (__m256i *) &frame1[0][col+col_off]);
		r1=_mm256_loadu_si256( (__m256i *) &frame1[1][col+col_off]);
		r2=_mm256_loadu_si256( (__m256i *) &frame1[2][col+col_off]);
		r3=_mm256_loadu_si256( (__m256i *) &frame1[3][col+col_off]);
		r4=_mm256_loadu_si256( (__m256i *) &frame1[4][col+col_off]);
		r5=_mm256_loadu_si256( (__m256i *) &frame1[5][col+col_off]);
		r6=_mm256_loadu_si256( (__m256i *) &frame1[6][col+col_off]);
	    r7=_mm256_setzero_si256();
	    r8=_mm256_setzero_si256();
	    c0=c2; c1=c3; c2=c4;c3=c5;c4=c6;c5=c7;c6=c8;
	}
	else if (row==3){
		r0=_mm256_loadu_si256( (__m256i *) &frame1[0][col+col_off]);
		r1=_mm256_loadu_si256( (__m256i *) &frame1[1][col+col_off]);
		r2=_mm256_loadu_si256( (__m256i *) &frame1[2][col+col_off]);
		r3=_mm256_loadu_si256( (__m256i *) &frame1[3][col+col_off]);
		r4=_mm256_loadu_si256( (__m256i *) &frame1[4][col+col_off]);
		r5=_mm256_loadu_si256( (__m256i *) &frame1[5][col+col_off]);
		r6=_mm256_loadu_si256( (__m256i *) &frame1[6][col+col_off]);
		r7=_mm256_loadu_si256( (__m256i *) &frame1[7][col+col_off]);
	    r8=_mm256_setzero_si256();
	    c0=c1; c1=c2; c2=c3;c3=c4;c4=c5;c5=c6;c6=c7;c7=c8;
	}
	else if (row==N-4){
	    r0=_mm256_setzero_si256();
	    r1=_mm256_loadu_si256( (__m256i *) &frame1[N-8][col+col_off]);
		r2=_mm256_loadu_si256( (__m256i *) &frame1[N-7][col+col_off]);
		r3=_mm256_loadu_si256( (__m256i *) &frame1[N-6][col+col_off]);
		r4=_mm256_loadu_si256( (__m256i *) &frame1[N-5][col+col_off]);
		r5=_mm256_loadu_si256( (__m256i *) &frame1[N-4][col+col_off]);
		r6=_mm256_loadu_si256( (__m256i *) &frame1[N-3][col+col_off]);
		r7=_mm256_loadu_si256( (__m256i *) &frame1[N-2][col+col_off]);
		r8=_mm256_loadu_si256( (__m256i *) &frame1[N-1][col+col_off]);
	    c8=c7; c7=c6; c6=c5; c5=c4; c4=c3; c3=c2; c2=c1; c1=c0;
	}
	else if (row==N-3){
	    r0=_mm256_setzero_si256();
	    r1=_mm256_setzero_si256();
		r2=_mm256_loadu_si256( (__m256i *) &frame1[N-7][col+col_off]);
		r3=_mm256_loadu_si256( (__m256i *) &frame1[N-6][col+col_off]);
		r4=_mm256_loadu_si256( (__m256i *) &frame1[N-5][col+col_off]);
		r5=_mm256_loadu_si256( (__m256i *) &frame1[N-4][col+col_off]);
		r6=_mm256_loadu_si256( (__m256i *) &frame1[N-3][col+col_off]);
		r7=_mm256_loadu_si256( (__m256i *) &frame1[N-2][col+col_off]);
		r8=_mm256_loadu_si256( (__m256i *) &frame1[N-1][col+col_off]);
	    c8=c6; c7=c5; c6=c4; c5=c3; c4=c2; c3=c1; c2=c0;
	}
	else if (row==N-2){
	    r0=_mm256_setzero_si256();
	    r1=_mm256_setzero_si256();
		r2=_mm256_setzero_si256();
		r3=_mm256_loadu_si256( (__m256i *) &frame1[N-6][col+col_off]);
		r4=_mm256_loadu_si256( (__m256i *) &frame1[N-5][col+col_off]);
		r5=_mm256_loadu_si256( (__m256i *) &frame1[N-4][col+col_off]);
		r6=_mm256_loadu_si256( (__m256i *) &frame1[N-3][col+col_off]);
		r7=_mm256_loadu_si256( (__m256i *) &frame1[N-2][col+col_off]);
		r8=_mm256_loadu_si256( (__m256i *) &frame1[N-1][col+col_off]);
		c8=c5; c7=c4; c6=c3; c5=c2; c4=c1; c3=c0;
	}
	else if (row==N-1){
	    r0=_mm256_setzero_si256();
	    r1=_mm256_setzero_si256();
		r2=_mm256_setzero_si256();
		r3=_mm256_setzero_si256();
		r4=_mm256_loadu_si256( (__m256i *) &frame1[N-5][col+col_off]);
		r5=_mm256_loadu_si256( (__m256i *) &frame1[N-4][col+col_off]);
		r6=_mm256_loadu_si256( (__m256i *) &frame1[N-3][col+col_off]);
		r7=_mm256_loadu_si256( (__m256i *) &frame1[N-2][col+col_off]);
		r8=_mm256_loadu_si256( (__m256i *) &frame1[N-1][col+col_off]);
		c8=c4; c7=c3; c6=c2; c5=c1; c4=c0;
	}
	else {
		printf("\nsomething went wrong");
		exit(EXIT_FAILURE);
	}


	__m256i reminder_mask1=_mm256_load_si256( (__m256i *) &reminder_msk_9x9[REMINDER_ITERATIONS-1-it][0]);

	//zero the values that exceed the array bounds
	r0=_mm256_and_si256(r0,reminder_mask1);
	r1=_mm256_and_si256(r1,reminder_mask1);
	r2=_mm256_and_si256(r2,reminder_mask1);
	r3=_mm256_and_si256(r3,reminder_mask1);
	r4=_mm256_and_si256(r4,reminder_mask1);
	r5=_mm256_and_si256(r5,reminder_mask1);
	r6=_mm256_and_si256(r6,reminder_mask1);
	r7=_mm256_and_si256(r7,reminder_mask1);
	r8=_mm256_and_si256(r8,reminder_mask1);

	return main_block_pre_9x9(r0,r1,r2,r3,r4,r5,r6,r7,r8,c0,c1,c2,c3,c4,c5,c6,c7,c8);

}






void convolution_optimized_5x5_old(unsigned char **frame1,unsigned char **filt,const unsigned int M, const unsigned int N, const unsigned short int divisor, signed char **filter5x5){

const signed char f00=filter5x5[0][0];
const signed char f01=filter5x5[0][1];
const signed char f02=filter5x5[0][2];
const signed char f03=filter5x5[0][3];
const signed char f04=filter5x5[0][4];

const signed char f10=filter5x5[1][0];
const signed char f11=filter5x5[1][1];
const signed char f12=filter5x5[1][2];
const signed char f13=filter5x5[1][3];
const signed char f14=filter5x5[1][4];

const signed char f20=filter5x5[2][0];
const signed char f21=filter5x5[2][1];
const signed char f22=filter5x5[2][2];
const signed char f23=filter5x5[2][3];
const signed char f24=filter5x5[2][4];

const signed char f30=filter5x5[3][0];
const signed char f31=filter5x5[3][1];
const signed char f32=filter5x5[3][2];
const signed char f33=filter5x5[3][3];
const signed char f34=filter5x5[3][4];

const signed char f40=filter5x5[4][0];
const signed char f41=filter5x5[4][1];
const signed char f42=filter5x5[4][2];
const signed char f43=filter5x5[4][3];
const signed char f44=filter5x5[4][4];

const signed char mask_vector[30][32] __attribute__((aligned(64))) ={
		{f00,f01,f02,f03,f04,0,f00,f01,f02,f03,f04,0,f00,f01,f02,f03,f04,0,f00,f01,f02,f03,f04,0,f00,f01,f02,f03,f04,0,0,0},
		{f10,f11,f12,f13,f14,0,f10,f11,f12,f13,f14,0,f10,f11,f12,f13,f14,0,f10,f11,f12,f13,f14,0,f10,f11,f12,f13,f14,0,0,0},
		{f20,f21,f22,f23,f24,0,f20,f21,f22,f23,f24,0,f20,f21,f22,f23,f24,0,f20,f21,f22,f23,f24,0,f20,f21,f22,f23,f24,0,0,0},
		{f30,f31,f32,f33,f34,0,f30,f31,f32,f33,f34,0,f30,f31,f32,f33,f34,0,f30,f31,f32,f33,f34,0,f30,f31,f32,f33,f34,0,0,0},
		{f40,f41,f42,f43,f44,0,f40,f41,f42,f43,f44,0,f40,f41,f42,f43,f44,0,f40,f41,f42,f43,f44,0,f40,f41,f42,f43,f44,0,0,0},

		{0,f00,f01,f02,f03,f04,0,f00,f01,f02,f03,f04,0,f00,f01,f02,f03,f04,0,f00,f01,f02,f03,f04,0,f00,f01,f02,f03,f04,0,0},
		{0,f10,f11,f12,f13,f14,0,f10,f11,f12,f13,f14,0,f10,f11,f12,f13,f14,0,f10,f11,f12,f13,f14,0,f10,f11,f12,f13,f14,0,0},
		{0,f20,f21,f22,f23,f24,0,f20,f21,f22,f23,f24,0,f20,f21,f22,f23,f24,0,f20,f21,f22,f23,f24,0,f20,f21,f22,f23,f24,0,0},
		{0,f30,f31,f32,f33,f34,0,f30,f31,f32,f33,f34,0,f30,f31,f32,f33,f34,0,f30,f31,f32,f33,f34,0,f30,f31,f32,f33,f34,0,0},
		{0,f40,f41,f42,f43,f44,0,f40,f41,f42,f43,f44,0,f40,f41,f42,f43,f44,0,f40,f41,f42,f43,f44,0,f40,f41,f42,f43,f44,0,0},

		{0,0,f00,f01,f02,f03,f04,0,f00,f01,f02,f03,f04,0,f00,f01,f02,f03,f04,0,f00,f01,f02,f03,f04,0,f00,f01,f02,f03,f04,0},
		{0,0,f10,f11,f12,f13,f14,0,f10,f11,f12,f13,f14,0,f10,f11,f12,f13,f14,0,f10,f11,f12,f13,f14,0,f10,f11,f12,f13,f14,0},
		{0,0,f20,f21,f22,f23,f24,0,f20,f21,f22,f23,f24,0,f20,f21,f22,f23,f24,0,f20,f21,f22,f23,f24,0,f20,f21,f22,f23,f24,0},
		{0,0,f30,f31,f32,f33,f34,0,f30,f31,f32,f33,f34,0,f30,f31,f32,f33,f34,0,f30,f31,f32,f33,f34,0,f30,f31,f32,f33,f34,0},
		{0,0,f40,f41,f42,f43,f44,0,f40,f41,f42,f43,f44,0,f40,f41,f42,f43,f44,0,f40,f41,f42,f43,f44,0,f40,f41,f42,f43,f44,0},

		{0,0,0,f00,f01,f02,f03,f04,0,f00,f01,f02,f03,f04,0,f00,f01,f02,f03,f04,0,f00,f01,f02,f03,f04,0,f00,f01,f02,f03,f04},
		{0,0,0,f10,f11,f12,f13,f14,0,f10,f11,f12,f13,f14,0,f10,f11,f12,f13,f14,0,f10,f11,f12,f13,f14,0,f10,f11,f12,f13,f14},
		{0,0,0,f20,f21,f22,f23,f24,0,f20,f21,f22,f23,f24,0,f20,f21,f22,f23,f24,0,f20,f21,f22,f23,f24,0,f20,f21,f22,f23,f24},
		{0,0,0,f30,f31,f32,f33,f34,0,f30,f31,f32,f33,f34,0,f30,f31,f32,f33,f34,0,f30,f31,f32,f33,f34,0,f30,f31,f32,f33,f34},
		{0,0,0,f40,f41,f42,f43,f44,0,f40,f41,f42,f43,f44,0,f40,f41,f42,f43,f44,0,f40,f41,f42,f43,f44,0,f40,f41,f42,f43,f44},

		{0,0,0,0,f00,f01,f02,f03,f04,0,f00,f01,f02,f03,f04,0,f00,f01,f02,f03,f04,0,f00,f01,f02,f03,f04,0,0,0,0,0},
		{0,0,0,0,f10,f11,f12,f13,f14,0,f10,f11,f12,f13,f14,0,f10,f11,f12,f13,f14,0,f10,f11,f12,f13,f14,0,0,0,0,0},
		{0,0,0,0,f20,f21,f22,f23,f24,0,f20,f21,f22,f23,f24,0,f20,f21,f22,f23,f24,0,f20,f21,f22,f23,f24,0,0,0,0,0},
		{0,0,0,0,f30,f31,f32,f33,f34,0,f30,f31,f32,f33,f34,0,f30,f31,f32,f33,f34,0,f30,f31,f32,f33,f34,0,0,0,0,0},
		{0,0,0,0,f40,f41,f42,f43,f44,0,f40,f41,f42,f43,f44,0,f40,f41,f42,f43,f44,0,f40,f41,f42,f43,f44,0,0,0,0,0},

		{0,0,0,0,0,f00,f01,f02,f03,f04,0,f00,f01,f02,f03,f04,0,f00,f01,f02,f03,f04,0,f00,f01,f02,f03,f04,0,0,0,0},
		{0,0,0,0,0,f10,f11,f12,f13,f14,0,f10,f11,f12,f13,f14,0,f10,f11,f12,f13,f14,0,f10,f11,f12,f13,f14,0,0,0,0},
		{0,0,0,0,0,f20,f21,f22,f23,f24,0,f20,f21,f22,f23,f24,0,f20,f21,f22,f23,f24,0,f20,f21,f22,f23,f24,0,0,0,0},
		{0,0,0,0,0,f30,f31,f32,f33,f34,0,f30,f31,f32,f33,f34,0,f30,f31,f32,f33,f34,0,f30,f31,f32,f33,f34,0,0,0,0},
		{0,0,0,0,0,f40,f41,f42,f43,f44,0,f40,f41,f42,f43,f44,0,f40,f41,f42,f43,f44,0,f40,f41,f42,f43,f44,0,0,0,0},
};


	const __m256i c0=_mm256_load_si256( (__m256i *) &mask_vector[0][0]);
	const __m256i c1=_mm256_load_si256( (__m256i *) &mask_vector[1][0]);
	const __m256i c2=_mm256_load_si256( (__m256i *) &mask_vector[2][0]);
	const __m256i c3=_mm256_load_si256( (__m256i *) &mask_vector[3][0]);
	const __m256i c4=_mm256_load_si256( (__m256i *) &mask_vector[4][0]);


	const __m256i mask_prelude2  = _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0);
	const __m256i mask_prelude1  = _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0);


	//const __m256i f  = _mm256_set_epi16(52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759);
	//const __m256i mask  = _mm256_set_epi8(0,0,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255);
	//const __m256i mask2  = _mm256_set_epi8(0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0);
	//const __m256i mask6  = _mm256_set_epi8(0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0);
	const __m256i mask2  = _mm256_set_epi16(0,0,0,0,0,0,0,0,0,65535,0,0,0,0,0,0);

	const __m256i mask3  = _mm256_set_epi16(0,0,0,0,0,0,0,65535,0,0,0,0,0,0,0,0);

	const __m256i output_mask      = _mm256_set_epi16(0,0,0,65535,0,0,65535,0,0,65535,0,0,65535,0,0,65535);




	//const unsigned int REMINDER_ITERATIONS =  (M-((((M-32)/30)*30)+30)); //M-(last_col_value+28)
	//printf("\n%d",REMINDER_ITERATIONS);


	const unsigned int division_case=prepare_for_division(divisor); //determine which is the division case (A, B or C)
	const __m256i f = _mm256_load_si256( (__m256i *) &f_vector[0]);// initialize the division vector

//printf("\n%d %d",division_case,b);


#pragma omp parallel
{

unsigned int row,col;
register __m256i r0,r1,r2,r3,r4,m0,m1,m2,m3,m4,rr0,rr1,rr2,rr3,rr4;
__m256i output_even,output_odd;


/*---------------------- Gaussian Blur ---------------------------------*/

    #pragma omp for schedule(static)
	for (row = 1; row <= N-2; row++) {

		if (row==1){//in this case I calculate filt[0][:] and filt[1][:] too. No extra loads or multiplications are required for this
		}
		else if (row==N-2){
		}
		else {


	  for (col = 0; col <= M-32; col+=30){
	 //last col value that does not read outside of the array bounds is (col<M-29)

		  if (col==0){
				//1st col iteration
				//load the 5 rows
				rr0=_mm256_load_si256( (__m256i *) &frame1[row-2][0]);
				rr1=_mm256_load_si256( (__m256i *) &frame1[row-1][0]);
				rr2=_mm256_load_si256( (__m256i *) &frame1[row][0]);
				rr3=_mm256_load_si256( (__m256i *) &frame1[row+1][0]);
				rr4=_mm256_load_si256( (__m256i *) &frame1[row+2][0]);

				  r0=insert_two_zeros_front(rr0,mask_prelude2);
				  r1=insert_two_zeros_front(rr1,mask_prelude2);
				  r2=insert_two_zeros_front(rr2,mask_prelude2);
				  r3=insert_two_zeros_front(rr3,mask_prelude2);
				  r4=insert_two_zeros_front(rr4,mask_prelude2);


				//multiply with the mask
				m0=_mm256_maddubs_epi16(r0,c0);
				m1=_mm256_maddubs_epi16(r1,c1);
				m2=_mm256_maddubs_epi16(r2,c2);
				m3=_mm256_maddubs_epi16(r3,c3);
				m4=_mm256_maddubs_epi16(r4,c4);

				//vertical add
				m0=_mm256_add_epi16(m0,m1);
				m2=_mm256_add_epi16(m2,m3);
				m0=_mm256_add_epi16(m0,m2);
				m0=_mm256_add_epi16(m0,m4);


				m1=_mm256_srli_si256(m0,2);
				m2=_mm256_add_epi16(m1,m0);

				m1=_mm256_srli_si256(m0,4);
				m2=_mm256_add_epi16(m1,m2);

				//after shifts the 15th element is lost as it cannot be propagated to the 16th position (AVX registers are managed as two seperate SSE registers)
				m4=_mm256_and_si256(m0,mask3);
				m4=_mm256_permute2f128_si256(m4,m4,1);
				m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions
				m2=_mm256_add_epi16(m2,m4);

				//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
				output_even=_mm256_and_si256(m2,output_mask);


				//2nd col iteration

				  r0=insert_one_zeros_front(rr0,mask_prelude1);
				  r1=insert_one_zeros_front(rr1,mask_prelude1);
				  r2=insert_one_zeros_front(rr2,mask_prelude1);
				  r3=insert_one_zeros_front(rr3,mask_prelude1);
				  r4=insert_one_zeros_front(rr4,mask_prelude1);


				//multiply with the mask
				m0=_mm256_maddubs_epi16(r0,c0);
				m1=_mm256_maddubs_epi16(r1,c1);
				m2=_mm256_maddubs_epi16(r2,c2);
				m3=_mm256_maddubs_epi16(r3,c3);
				m4=_mm256_maddubs_epi16(r4,c4);

				//vertical add
				m0=_mm256_add_epi16(m0,m1);
				m2=_mm256_add_epi16(m2,m3);
				m0=_mm256_add_epi16(m0,m2);
				m0=_mm256_add_epi16(m0,m4);


				m1=_mm256_srli_si256(m0,2);
				m2=_mm256_add_epi16(m1,m0);

				m1=_mm256_srli_si256(m0,4);
				m2=_mm256_add_epi16(m1,m2);

				//after shifts the 15th element is lost as it cannot be propagated to the 16th position (AVX registers are managed as two seperate SSE registers)
				m4=_mm256_and_si256(m0,mask3);
				m4=_mm256_permute2f128_si256(m4,m4,1);
				m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions
				m2=_mm256_add_epi16(m2,m4);

				//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
				output_odd=_mm256_and_si256(m2,output_mask);



		         //3rd col iteration

				//multiply with the mask
				m0=_mm256_maddubs_epi16(rr0,c0);
				m1=_mm256_maddubs_epi16(rr1,c1);
				m2=_mm256_maddubs_epi16(rr2,c2);
				m3=_mm256_maddubs_epi16(rr3,c3);
				m4=_mm256_maddubs_epi16(rr4,c4);

				//vertical add
				m0=_mm256_add_epi16(m0,m1);
				m2=_mm256_add_epi16(m2,m3);
				m0=_mm256_add_epi16(m0,m2);
				m0=_mm256_add_epi16(m0,m4);


				m1=_mm256_srli_si256(m0,2);
				m2=_mm256_add_epi16(m1,m0);

				m1=_mm256_srli_si256(m0,4);
				m2=_mm256_add_epi16(m1,m2);

				//after shifts the 15th element is lost as it cannot be propagated to the 16th position (AVX registers are managed as two seperate SSE registers)
				m4=_mm256_and_si256(m0,mask3);
				m4=_mm256_permute2f128_si256(m4,m4,1);
				m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions
				m2=_mm256_add_epi16(m2,m4);

				//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
				m2=_mm256_and_si256(m2,output_mask);

				m2=_mm256_slli_si256(m2,2);
				output_even=_mm256_add_epi16(output_even,m2);

				//4th col iteration

				//load the 5 rows
				r0=_mm256_loadu_si256( (__m256i *) &frame1[row-2][col+1]);
				r1=_mm256_loadu_si256( (__m256i *) &frame1[row-1][col+1]);
				r2=_mm256_loadu_si256( (__m256i *) &frame1[row][col+1]);
				r3=_mm256_loadu_si256( (__m256i *) &frame1[row+1][col+1]);
				r4=_mm256_loadu_si256( (__m256i *) &frame1[row+2][col+1]);

				//multiply with the mask
				m0=_mm256_maddubs_epi16(r0,c0);
				m1=_mm256_maddubs_epi16(r1,c1);
				m2=_mm256_maddubs_epi16(r2,c2);
				m3=_mm256_maddubs_epi16(r3,c3);
				m4=_mm256_maddubs_epi16(r4,c4);

				//vertical add
				m0=_mm256_add_epi16(m0,m1);
				m2=_mm256_add_epi16(m2,m3);
				m0=_mm256_add_epi16(m0,m2);
				m0=_mm256_add_epi16(m0,m4);


				m1=_mm256_srli_si256(m0,2);
				m2=_mm256_add_epi16(m1,m0);

				m1=_mm256_srli_si256(m0,4);
				m2=_mm256_add_epi16(m1,m2);

				//after shifts the 15th element is lost as it cannot be propagated to the 16th position (AVX registers are managed as two seperate SSE registers)
				m4=_mm256_and_si256(m0,mask3);
				m4=_mm256_permute2f128_si256(m4,m4,1);
				m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions
				m2=_mm256_add_epi16(m2,m4);

				//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
				m2=_mm256_and_si256(m2,output_mask);

				m2=_mm256_slli_si256(m2,2);
				output_odd=_mm256_add_epi16(output_odd,m2);

				//5th col iteration
				//load the 5 rows
				r0=_mm256_loadu_si256( (__m256i *) &frame1[row-2][col+2]);
				r1=_mm256_loadu_si256( (__m256i *) &frame1[row-1][col+2]);
				r2=_mm256_loadu_si256( (__m256i *) &frame1[row][col+2]);
				r3=_mm256_loadu_si256( (__m256i *) &frame1[row+1][col+2]);
				r4=_mm256_loadu_si256( (__m256i *) &frame1[row+2][col+2]);

				//multiply with the mask
				m0=_mm256_maddubs_epi16(r0,c0);
				m1=_mm256_maddubs_epi16(r1,c1);
				m2=_mm256_maddubs_epi16(r2,c2);
				m3=_mm256_maddubs_epi16(r3,c3);
				m4=_mm256_maddubs_epi16(r4,c4);

				//vertical add
				m0=_mm256_add_epi16(m0,m1);
				m2=_mm256_add_epi16(m2,m3);
				m0=_mm256_add_epi16(m0,m2);
				m0=_mm256_add_epi16(m0,m4);


				m1=_mm256_srli_si256(m0,2);
				m2=_mm256_add_epi16(m1,m0);

				m1=_mm256_srli_si256(m0,4);
				m2=_mm256_add_epi16(m1,m2);

				//after shifts the 15th element is lost as it cannot be propagated to the 16th position (AVX registers are managed as two seperate SSE registers)
				m4=_mm256_and_si256(m0,mask3);
				m4=_mm256_permute2f128_si256(m4,m4,1);
				m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions
				m2=_mm256_add_epi16(m2,m4);

				//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
				m2=_mm256_and_si256(m2,output_mask);

				m3=_mm256_slli_si256(m2,4);
				m4=_mm256_and_si256(m2,mask2);
				m4=_mm256_permute2f128_si256(m4,m4,1);
				m4=_mm256_srli_si256(m4,12);//shift 6 short int positions or 12 char positions
				m2=_mm256_add_epi16(m3,m4);

				output_even=_mm256_add_epi16(output_even,m2);

		                 //6th col iteration
				//load the 5 rows
				r0=_mm256_loadu_si256( (__m256i *) &frame1[row-2][col+3]);
				r1=_mm256_loadu_si256( (__m256i *) &frame1[row-1][col+3]);
				r2=_mm256_loadu_si256( (__m256i *) &frame1[row][col+3]);
				r3=_mm256_loadu_si256( (__m256i *) &frame1[row+1][col+3]);
				r4=_mm256_loadu_si256( (__m256i *) &frame1[row+2][col+3]);

				//multiply with the mask
				m0=_mm256_maddubs_epi16(r0,c0);
				m1=_mm256_maddubs_epi16(r1,c1);
				m2=_mm256_maddubs_epi16(r2,c2);
				m3=_mm256_maddubs_epi16(r3,c3);
				m4=_mm256_maddubs_epi16(r4,c4);

				//vertical add
				m0=_mm256_add_epi16(m0,m1);
				m2=_mm256_add_epi16(m2,m3);
				m0=_mm256_add_epi16(m0,m2);
				m0=_mm256_add_epi16(m0,m4);


				m1=_mm256_srli_si256(m0,2);
				m2=_mm256_add_epi16(m1,m0);

				m1=_mm256_srli_si256(m0,4);
				m2=_mm256_add_epi16(m1,m2);

				//after shifts the 15th element is lost as it cannot be propagated to the 16th position (AVX registers are managed as two seperate SSE registers)
				m4=_mm256_and_si256(m0,mask3);
				m4=_mm256_permute2f128_si256(m4,m4,1);
				m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions
				m2=_mm256_add_epi16(m2,m4);

				//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
				m2=_mm256_and_si256(m2,output_mask);

				m3=_mm256_slli_si256(m2,4);
				m4=_mm256_and_si256(m2,mask2);
				m4=_mm256_permute2f128_si256(m4,m4,1);
				m4=_mm256_srli_si256(m4,12);//shift 6 short int positions or 12 char positions
				m2=_mm256_add_epi16(m3,m4);

				output_odd=_mm256_add_epi16(output_odd,m2);


				//now division follows
				output_even=division(division_case,output_even,f);
				output_odd=division(division_case,output_odd,f);

				//shift odd 1 position and add to even
				output_odd = _mm256_slli_si256(output_odd,1);
				output_even = _mm256_add_epi8(output_even,output_odd);


				_mm256_store_si256( (__m256i *) &filt[row][0],output_even );

		  		}
		  else {



		//col iteration computes output pixels of    2,8,14,20,26
		// col+1 iteration computes output pixels of 3,9,15,21,27
		// col+2 iteration computes output pixels of 4,10,16,22,28
		// col+3 iteration computes output pixels of 5,11,17,23,29
		// col+4 iteration computes output pixels of 6,12,18,24,30
		// col+5 iteration computes output pixels of 7,13,19,25,31
		//afterwards, col becomes 30 and repeat the above process

		//1st col iteration
		//load the 5 rows
		r0=_mm256_loadu_si256( (__m256i *) &frame1[row-2][col-2]);
		r1=_mm256_loadu_si256( (__m256i *) &frame1[row-1][col-2]);
		r2=_mm256_loadu_si256( (__m256i *) &frame1[row][col-2]);
		r3=_mm256_loadu_si256( (__m256i *) &frame1[row+1][col-2]);
		r4=_mm256_loadu_si256( (__m256i *) &frame1[row+2][col-2]);

		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,c0);
		m1=_mm256_maddubs_epi16(r1,c1);
		m2=_mm256_maddubs_epi16(r2,c2);
		m3=_mm256_maddubs_epi16(r3,c3);
		m4=_mm256_maddubs_epi16(r4,c4);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m2=_mm256_add_epi16(m2,m3);
		m0=_mm256_add_epi16(m0,m2);
		m0=_mm256_add_epi16(m0,m4);


		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		m1=_mm256_srli_si256(m0,4);
		m2=_mm256_add_epi16(m1,m2);

		//after shifts the 15th element is lost as it cannot be propagated to the 16th position (AVX registers are managed as two seperate SSE registers)
		m4=_mm256_and_si256(m0,mask3);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions
		m2=_mm256_add_epi16(m2,m4);

		//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
		output_even=_mm256_and_si256(m2,output_mask);


		//2nd col iteration
		//load the 5 rows
		r0=_mm256_loadu_si256( (__m256i *) &frame1[row-2][col-1]);
		r1=_mm256_loadu_si256( (__m256i *) &frame1[row-1][col-1]);
		r2=_mm256_loadu_si256( (__m256i *) &frame1[row][col-1]);
		r3=_mm256_loadu_si256( (__m256i *) &frame1[row+1][col-1]);
		r4=_mm256_loadu_si256( (__m256i *) &frame1[row+2][col-1]);

		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,c0);
		m1=_mm256_maddubs_epi16(r1,c1);
		m2=_mm256_maddubs_epi16(r2,c2);
		m3=_mm256_maddubs_epi16(r3,c3);
		m4=_mm256_maddubs_epi16(r4,c4);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m2=_mm256_add_epi16(m2,m3);
		m0=_mm256_add_epi16(m0,m2);
		m0=_mm256_add_epi16(m0,m4);


		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		m1=_mm256_srli_si256(m0,4);
		m2=_mm256_add_epi16(m1,m2);

		//after shifts the 15th element is lost as it cannot be propagated to the 16th position (AVX registers are managed as two seperate SSE registers)
		m4=_mm256_and_si256(m0,mask3);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions
		m2=_mm256_add_epi16(m2,m4);

		//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
		output_odd=_mm256_and_si256(m2,output_mask);



         //3rd col iteration
		//load the 5 rows
		r0=_mm256_loadu_si256( (__m256i *) &frame1[row-2][col-0]);
		r1=_mm256_loadu_si256( (__m256i *) &frame1[row-1][col-0]);
		r2=_mm256_loadu_si256( (__m256i *) &frame1[row][col-0]);
		r3=_mm256_loadu_si256( (__m256i *) &frame1[row+1][col-0]);
		r4=_mm256_loadu_si256( (__m256i *) &frame1[row+2][col-0]);

		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,c0);
		m1=_mm256_maddubs_epi16(r1,c1);
		m2=_mm256_maddubs_epi16(r2,c2);
		m3=_mm256_maddubs_epi16(r3,c3);
		m4=_mm256_maddubs_epi16(r4,c4);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m2=_mm256_add_epi16(m2,m3);
		m0=_mm256_add_epi16(m0,m2);
		m0=_mm256_add_epi16(m0,m4);


		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		m1=_mm256_srli_si256(m0,4);
		m2=_mm256_add_epi16(m1,m2);

		//after shifts the 15th element is lost as it cannot be propagated to the 16th position (AVX registers are managed as two seperate SSE registers)
		m4=_mm256_and_si256(m0,mask3);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions
		m2=_mm256_add_epi16(m2,m4);

		//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
		m2=_mm256_and_si256(m2,output_mask);

		m2=_mm256_slli_si256(m2,2);
		output_even=_mm256_add_epi16(output_even,m2);

		//4th col iteration

		//load the 5 rows
		r0=_mm256_loadu_si256( (__m256i *) &frame1[row-2][col+1]);
		r1=_mm256_loadu_si256( (__m256i *) &frame1[row-1][col+1]);
		r2=_mm256_loadu_si256( (__m256i *) &frame1[row][col+1]);
		r3=_mm256_loadu_si256( (__m256i *) &frame1[row+1][col+1]);
		r4=_mm256_loadu_si256( (__m256i *) &frame1[row+2][col+1]);

		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,c0);
		m1=_mm256_maddubs_epi16(r1,c1);
		m2=_mm256_maddubs_epi16(r2,c2);
		m3=_mm256_maddubs_epi16(r3,c3);
		m4=_mm256_maddubs_epi16(r4,c4);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m2=_mm256_add_epi16(m2,m3);
		m0=_mm256_add_epi16(m0,m2);
		m0=_mm256_add_epi16(m0,m4);


		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		m1=_mm256_srli_si256(m0,4);
		m2=_mm256_add_epi16(m1,m2);

		//after shifts the 15th element is lost as it cannot be propagated to the 16th position (AVX registers are managed as two seperate SSE registers)
		m4=_mm256_and_si256(m0,mask3);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions
		m2=_mm256_add_epi16(m2,m4);

		//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
		m2=_mm256_and_si256(m2,output_mask);

		m2=_mm256_slli_si256(m2,2);
		output_odd=_mm256_add_epi16(output_odd,m2);

		//5th col iteration
		//load the 5 rows
		r0=_mm256_loadu_si256( (__m256i *) &frame1[row-2][col+2]);
		r1=_mm256_loadu_si256( (__m256i *) &frame1[row-1][col+2]);
		r2=_mm256_loadu_si256( (__m256i *) &frame1[row][col+2]);
		r3=_mm256_loadu_si256( (__m256i *) &frame1[row+1][col+2]);
		r4=_mm256_loadu_si256( (__m256i *) &frame1[row+2][col+2]);

		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,c0);
		m1=_mm256_maddubs_epi16(r1,c1);
		m2=_mm256_maddubs_epi16(r2,c2);
		m3=_mm256_maddubs_epi16(r3,c3);
		m4=_mm256_maddubs_epi16(r4,c4);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m2=_mm256_add_epi16(m2,m3);
		m0=_mm256_add_epi16(m0,m2);
		m0=_mm256_add_epi16(m0,m4);


		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		m1=_mm256_srli_si256(m0,4);
		m2=_mm256_add_epi16(m1,m2);

		//after shifts the 15th element is lost as it cannot be propagated to the 16th position (AVX registers are managed as two seperate SSE registers)
		m4=_mm256_and_si256(m0,mask3);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions
		m2=_mm256_add_epi16(m2,m4);

		//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
		m2=_mm256_and_si256(m2,output_mask);

		m3=_mm256_slli_si256(m2,4);
		m4=_mm256_and_si256(m2,mask2);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_srli_si256(m4,12);//shift 6 short int positions or 12 char positions
		m2=_mm256_add_epi16(m3,m4);

		output_even=_mm256_add_epi16(output_even,m2);

                 //6th col iteration
		//load the 5 rows
		r0=_mm256_loadu_si256( (__m256i *) &frame1[row-2][col+3]);
		r1=_mm256_loadu_si256( (__m256i *) &frame1[row-1][col+3]);
		r2=_mm256_loadu_si256( (__m256i *) &frame1[row][col+3]);
		r3=_mm256_loadu_si256( (__m256i *) &frame1[row+1][col+3]);
		r4=_mm256_loadu_si256( (__m256i *) &frame1[row+2][col+3]);

		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,c0);
		m1=_mm256_maddubs_epi16(r1,c1);
		m2=_mm256_maddubs_epi16(r2,c2);
		m3=_mm256_maddubs_epi16(r3,c3);
		m4=_mm256_maddubs_epi16(r4,c4);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m2=_mm256_add_epi16(m2,m3);
		m0=_mm256_add_epi16(m0,m2);
		m0=_mm256_add_epi16(m0,m4);


		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		m1=_mm256_srli_si256(m0,4);
		m2=_mm256_add_epi16(m1,m2);

		//after shifts the 15th element is lost as it cannot be propagated to the 16th position (AVX registers are managed as two seperate SSE registers)
		m4=_mm256_and_si256(m0,mask3);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions
		m2=_mm256_add_epi16(m2,m4);

		//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
		m2=_mm256_and_si256(m2,output_mask);

		m3=_mm256_slli_si256(m2,4);
		m4=_mm256_and_si256(m2,mask2);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_srli_si256(m4,12);//shift 6 short int positions or 12 char positions
		m2=_mm256_add_epi16(m3,m4);

		output_odd=_mm256_add_epi16(output_odd,m2);


		//now division follows
		output_even=division(division_case,output_even,f);
		output_odd=division(division_case,output_odd,f);

		//shift odd 1 position and add to even
		output_odd = _mm256_slli_si256(output_odd,1);
		output_even = _mm256_add_epi8(output_even,output_odd);



		_mm256_storeu_si256( (__m256i *) &filt[row][col],output_even );
		  }

		}


		}
}

}//end of parallel


}




void Filter2D_3x3_16(unsigned char **frame1,unsigned char **filt,const unsigned int M, const unsigned int N, const unsigned short int divisor, signed char **filter){

const signed char f00=filter[0][0];
const signed char f01=filter[0][1];
const signed char f02=filter[0][2];

const signed char f10=filter[1][0];
const signed char f11=filter[1][1];
const signed char f12=filter[1][2];

const signed char f20=filter[2][0];
const signed char f21=filter[2][1];
const signed char f22=filter[2][2];

const __m256i c0=_mm256_set_epi8(0,f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00);
const __m256i c1=_mm256_set_epi8(0,f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10);
const __m256i c2=_mm256_set_epi8(0,f22,f21,f20,0,f22,f21,f20,0,f22,f21,f20,0,f22,f21,f20,0,f22,f21,f20,0,f22,f21,f20,0,f22,f21,f20,0,f22,f21,f20);



	//const __m256i mask3  = _mm256_set_epi16(0,0,0,0,0,0,0,65535,0,0,0,0,0,0,0,0);
	const __m256i mask_prelude  = _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0);

	const __m256i output_mask      = _mm256_set_epi16(0,65535,0,65535,0,65535,0,65535,0,65535,0,65535,0,65535,0,65535);
	//const __m256i output_mask_sh1  = _mm256_set_epi16(0,0,65535,0,65535,0,65535,0,65535,0,65535,0,65535,0,65535,0);



   //REMINDER_ITERATIONS value ranges from 2 to 33
	const unsigned int REMINDER_ITERATIONS =  (M-((((M-32-2)/32)*32)+32)); //M-(last_col_value+32)
//	printf("\n%d",REMINDER_ITERATIONS);


	const unsigned int division_case=prepare_for_division(divisor); //determine the division case (A, B or C)
	const __m256i f = _mm256_load_si256( (__m256i *) &f_vector[0]);// initialize the division vector

//printf("\n%d %d",division_case,b);


#pragma omp parallel
{

unsigned int row,col;
register __m256i r0,r1,r2,m0,m1,m2,rr0,rr1,rr2;
//__m256i output_row0_even,output_row1_even,output_row0_odd,output_row1_odd; //these are for processing row #0 and #1 only.
__m256i output_even,output_odd;
//__m256i m0_prelude_row0,m0_prelude_row1;


/*---------------------- Gaussian Blur ---------------------------------*/

    #pragma omp for schedule(static)
	for (row = 0; row < N; row++) {

	if (row==0){//special case compute filt[0:1][:]

		  for (col = 0; col <= M-32-2; col+=32){

			  if (col==0){

					//load the 2 rows only
					r0=_mm256_load_si256( (__m256i *) &frame1[0][0]);
					r1=_mm256_load_si256( (__m256i *) &frame1[1][0]);

		  			//START - extra code needed for prelude
			  					m0=_mm256_slli_si256(r0,1);//shift 1 elements left - equivalent to filling with one zero inthe beginning
			  					m1=_mm256_slli_si256(r1,1);//shift 1 elements left - equivalent to filling with one zero inthe beginning

			  					//preserve the element being lost because of the shift above
			  					r0=_mm256_and_si256(r0,mask_prelude);
			  					r0=_mm256_permute2f128_si256(r0,r0,1);
			  					r0=_mm256_srli_si256(r0,15);//shift 15 elements
			  					r0=_mm256_add_epi16(m0,r0);

			  					r1=_mm256_and_si256(r1,mask_prelude);
			  					r1=_mm256_permute2f128_si256(r1,r1,1);
			  					r1=_mm256_srli_si256(r1,15);//shift 15 elements
			  					r1=_mm256_add_epi16(m1,r1);

					//multiply with the mask
					m0=_mm256_maddubs_epi16(r0,c1);
					m1=_mm256_maddubs_epi16(r1,c2);

					//vertical add
					m0=_mm256_add_epi16(m0,m1);

					//hozizontal additions
					m1=_mm256_srli_si256(m0,2);
					m2=_mm256_add_epi16(m1,m0);

					//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others. later on they will be stored into 0,4,8,12,16,20,24,28 positions (8bit register).
					output_even=_mm256_and_si256(m2,output_mask);

					//2ND col iteration

					//load the 3 rows
					r0=_mm256_load_si256( (__m256i *) &frame1[0][0]);
					r1=_mm256_load_si256( (__m256i *) &frame1[1][0]);

					//multiply with the mask
					m0=_mm256_maddubs_epi16(r0,c1);
					m1=_mm256_maddubs_epi16(r1,c2);

					//vertical add
					m0=_mm256_add_epi16(m0,m1);

					//hozizontal additions
					m1=_mm256_srli_si256(m0,2);
					m2=_mm256_add_epi16(m1,m0);

					//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others. later on they will be stored into 0,4,8,12,16,20,24,28 positions (8bit register).
					output_odd=_mm256_and_si256(m2,output_mask);


					//3rd col iteration

					//load the 3 rows
					r0=_mm256_loadu_si256( (__m256i *) &frame1[0][1]);
					r1=_mm256_loadu_si256( (__m256i *) &frame1[1][1]);

					//multiply with the mask
					m0=_mm256_maddubs_epi16(r0,c1);
					m1=_mm256_maddubs_epi16(r1,c2);

					//vertical add
					m0=_mm256_add_epi16(m0,m1);

					//hozizontal additions
					m1=_mm256_srli_si256(m0,2);
					m2=_mm256_add_epi16(m1,m0);

					//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others. later on they will be stored into 0,4,8,12,16,20,24,28 positions (8bit register).
					m2=_mm256_and_si256(m2,output_mask);
					m2=_mm256_slli_si256(m2,2);
					output_even=_mm256_add_epi16(output_even,m2);

					//4th col iteration

					//load the 3 rows
					r0=_mm256_loadu_si256( (__m256i *) &frame1[0][2]);
					r1=_mm256_loadu_si256( (__m256i *) &frame1[1][2]);

					//multiply with the mask
					m0=_mm256_maddubs_epi16(r0,c1);
					m1=_mm256_maddubs_epi16(r1,c2);

					//vertical add
					m0=_mm256_add_epi16(m0,m1);

					//hozizontal additions
					m1=_mm256_srli_si256(m0,2);
					m2=_mm256_add_epi16(m1,m0);

					//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others. later on they will be stored into 0,4,8,12,16,20,24,28 positions (8bit register).
					m2=_mm256_and_si256(m2,output_mask);
					m2=_mm256_slli_si256(m2,2);
					output_odd=_mm256_add_epi16(output_odd,m2);


					//now division follows
					output_even=division(division_case,output_even,f);
					output_odd=division(division_case,output_odd,f);

					//shift odd 1 position and add to even
					output_odd = _mm256_slli_si256(output_odd,1);
					output_even = _mm256_add_epi8(output_even,output_odd);


					_mm256_store_si256( (__m256i *) &filt[0][0],output_even );
			  			//END - extra code needed for prelude
			  		}
			  else {


					//col iteration computes output pixels of    1,5,9,13,17,21,25,29
					// col+1 iteration computes output pixels of 2,6,10,14,18,22,26,30
					// col+2 iteration computes output pixels of 3,7,11,15,19,23,27
					// col+3 iteration computes output pixels of 4,8,12,16,20,24,28
					//afterwards, col becomes 30 and repeat the above process

			//1st col iteration

			//load the 3 rows
			r0=_mm256_loadu_si256( (__m256i *) &frame1[0][col-1]);
			r1=_mm256_loadu_si256( (__m256i *) &frame1[1][col-1]);

			//multiply with the mask
			m0=_mm256_maddubs_epi16(r0,c1);
			m1=_mm256_maddubs_epi16(r1,c2);

			//vertical add
			m0=_mm256_add_epi16(m0,m1);

			//hozizontal additions
			m1=_mm256_srli_si256(m0,2);
			m2=_mm256_add_epi16(m1,m0);

			//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others. later on they will be stored into 0,4,8,12,16,20,24,28 positions (8bit register).
			output_even=_mm256_and_si256(m2,output_mask);

			//2ND col iteration

			//load the 3 rows
			r0=_mm256_load_si256( (__m256i *) &frame1[0][col]);//these loads are always aligned
			r1=_mm256_load_si256( (__m256i *) &frame1[1][col]);

			//multiply with the mask
			m0=_mm256_maddubs_epi16(r0,c1);
			m1=_mm256_maddubs_epi16(r1,c2);

			//vertical add
			m0=_mm256_add_epi16(m0,m1);

			//hozizontal additions
			m1=_mm256_srli_si256(m0,2);
			m2=_mm256_add_epi16(m1,m0);

			//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others. later on they will be stored into 0,4,8,12,16,20,24,28 positions (8bit register).
			output_odd=_mm256_and_si256(m2,output_mask);


			//3rd col iteration

			//load the 3 rows
			r0=_mm256_loadu_si256( (__m256i *) &frame1[0][col+1]);
			r1=_mm256_loadu_si256( (__m256i *) &frame1[1][col+1]);

			//multiply with the mask
			m0=_mm256_maddubs_epi16(r0,c1);
			m1=_mm256_maddubs_epi16(r1,c2);

			//vertical add
			m0=_mm256_add_epi16(m0,m1);

			//hozizontal additions
			m1=_mm256_srli_si256(m0,2);
			m2=_mm256_add_epi16(m1,m0);

			//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others. later on they will be stored into 0,4,8,12,16,20,24,28 positions (8bit register).
			m2=_mm256_and_si256(m2,output_mask);
			m2=_mm256_slli_si256(m2,2);
			output_even=_mm256_add_epi16(output_even,m2);

			//4th col iteration

			//load the 3 rows
			r0=_mm256_loadu_si256( (__m256i *) &frame1[0][col+2]);
			r1=_mm256_loadu_si256( (__m256i *) &frame1[1][col+2]);

			//multiply with the mask
			m0=_mm256_maddubs_epi16(r0,c1);
			m1=_mm256_maddubs_epi16(r1,c2);

			//vertical add
			m0=_mm256_add_epi16(m0,m1);

			//hozizontal additions
			m1=_mm256_srli_si256(m0,2);
			m2=_mm256_add_epi16(m1,m0);

			//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others. later on they will be stored into 0,4,8,12,16,20,24,28 positions (8bit register).
			m2=_mm256_and_si256(m2,output_mask);
			m2=_mm256_slli_si256(m2,2);
			output_odd=_mm256_add_epi16(output_odd,m2);


			//now division follows
			output_even=division(division_case,output_even,f);
			output_odd=division(division_case,output_odd,f);

			//shift odd 1 position and add to even
			output_odd = _mm256_slli_si256(output_odd,1);
			output_even = _mm256_add_epi8(output_even,output_odd);


			_mm256_store_si256( (__m256i *) &filt[0][col],output_even );

			  }

			}
		  Filter2D_loop_reminder_3x3_first_last_rows(frame1,filt,M,N,0,col,REMINDER_ITERATIONS,division_case,divisor,filter,c0,c1,c2,f);

	}

	else if (row==N-1){//special case compute filt[N-2:N-1][:]
		  for (col = 0; col <= M-32-2; col+=32){

			  if (col==0){

					//load the 3 rows
					r0=_mm256_load_si256( (__m256i *) &frame1[N-2][0]);
					r1=_mm256_load_si256( (__m256i *) &frame1[N-1][0]);

		  			//START - extra code needed for prelude
			  					m0=_mm256_slli_si256(r0,1);//shift 1 elements left - equivalent to filling with one zero inthe beginning
			  					m1=_mm256_slli_si256(r1,1);//shift 1 elements left - equivalent to filling with one zero inthe beginning

			  					//preserve the element being lost because of the shift above
			  					r0=_mm256_and_si256(r0,mask_prelude);
			  					r0=_mm256_permute2f128_si256(r0,r0,1);
			  					r0=_mm256_srli_si256(r0,15);//shift 15 elements
			  					r0=_mm256_add_epi16(m0,r0);

			  					r1=_mm256_and_si256(r1,mask_prelude);
			  					r1=_mm256_permute2f128_si256(r1,r1,1);
			  					r1=_mm256_srli_si256(r1,15);//shift 15 elements
			  					r1=_mm256_add_epi16(m1,r1);

					//multiply with the mask
					m0=_mm256_maddubs_epi16(r0,c0);
					m1=_mm256_maddubs_epi16(r1,c1);

					//vertical add
					m0=_mm256_add_epi16(m0,m1);

					//hozizontal additions
					m1=_mm256_srli_si256(m0,2);
					m2=_mm256_add_epi16(m1,m0);

					//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others. later on they will be stored into 0,4,8,12,16,20,24,28 positions (8bit register).
					output_even=_mm256_and_si256(m2,output_mask);

					//2ND col iteration

					//load the 3 rows
					r0=_mm256_load_si256( (__m256i *) &frame1[N-2][0]);
					r1=_mm256_load_si256( (__m256i *) &frame1[N-1][0]);

					//multiply with the mask
					m0=_mm256_maddubs_epi16(r0,c0);
					m1=_mm256_maddubs_epi16(r1,c1);

					//vertical add
					m0=_mm256_add_epi16(m0,m1);

					//hozizontal additions
					m1=_mm256_srli_si256(m0,2);
					m2=_mm256_add_epi16(m1,m0);

					//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others. later on they will be stored into 0,4,8,12,16,20,24,28 positions (8bit register).
					output_odd=_mm256_and_si256(m2,output_mask);


					//3rd col iteration

					//load the 3 rows
					r0=_mm256_loadu_si256( (__m256i *) &frame1[N-2][1]);
					r1=_mm256_loadu_si256( (__m256i *) &frame1[N-1][1]);

					//multiply with the mask
					m0=_mm256_maddubs_epi16(r0,c0);
					m1=_mm256_maddubs_epi16(r1,c1);

					//vertical add
					m0=_mm256_add_epi16(m0,m1);

					//hozizontal additions
					m1=_mm256_srli_si256(m0,2);
					m2=_mm256_add_epi16(m1,m0);

					//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others. later on they will be stored into 0,4,8,12,16,20,24,28 positions (8bit register).
					m2=_mm256_and_si256(m2,output_mask);
					m2=_mm256_slli_si256(m2,2);
					output_even=_mm256_add_epi16(output_even,m2);

					//4th col iteration

					//load the 3 rows
					r0=_mm256_loadu_si256( (__m256i *) &frame1[N-2][2]);
					r1=_mm256_loadu_si256( (__m256i *) &frame1[N-1][2]);

					//multiply with the mask
					m0=_mm256_maddubs_epi16(r0,c0);
					m1=_mm256_maddubs_epi16(r1,c1);

					//vertical add
					m0=_mm256_add_epi16(m0,m1);

					//hozizontal additions
					m1=_mm256_srli_si256(m0,2);
					m2=_mm256_add_epi16(m1,m0);

					//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others. later on they will be stored into 0,4,8,12,16,20,24,28 positions (8bit register).
					m2=_mm256_and_si256(m2,output_mask);
					m2=_mm256_slli_si256(m2,2);
					output_odd=_mm256_add_epi16(output_odd,m2);


					//now division follows
					output_even=division(division_case,output_even,f);
					output_odd=division(division_case,output_odd,f);

					//shift odd 1 position and add to even
					output_odd = _mm256_slli_si256(output_odd,1);
					output_even = _mm256_add_epi8(output_even,output_odd);


					_mm256_store_si256( (__m256i *) &filt[N-1][0],output_even );
			  			//END - extra code needed for prelude
			  		}
			  else {


					//col iteration computes output pixels of    1,5,9,13,17,21,25,29
					// col+1 iteration computes output pixels of 2,6,10,14,18,22,26,30
					// col+2 iteration computes output pixels of 3,7,11,15,19,23,27
					// col+3 iteration computes output pixels of 4,8,12,16,20,24,28
					//afterwards, col becomes 30 and repeat the above process

			//1st col iteration

			//load the 3 rows
			r0=_mm256_loadu_si256( (__m256i *) &frame1[N-2][col-1]);
			r1=_mm256_loadu_si256( (__m256i *) &frame1[N-1][col-1]);

			//multiply with the mask
			m0=_mm256_maddubs_epi16(r0,c0);
			m1=_mm256_maddubs_epi16(r1,c1);

			//vertical add
			m0=_mm256_add_epi16(m0,m1);

			//hozizontal additions
			m1=_mm256_srli_si256(m0,2);
			m2=_mm256_add_epi16(m1,m0);

			//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others. later on they will be stored into 0,4,8,12,16,20,24,28 positions (8bit register).
			output_even=_mm256_and_si256(m2,output_mask);

			//2ND col iteration

			//load the 3 rows
			r0=_mm256_load_si256( (__m256i *) &frame1[N-2][col]);//these loads are always aligned
			r1=_mm256_load_si256( (__m256i *) &frame1[N-1][col]);

			//multiply with the mask
			m0=_mm256_maddubs_epi16(r0,c0);
			m1=_mm256_maddubs_epi16(r1,c1);

			//vertical add
			m0=_mm256_add_epi16(m0,m1);

			//hozizontal additions
			m1=_mm256_srli_si256(m0,2);
			m2=_mm256_add_epi16(m1,m0);

			//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others. later on they will be stored into 0,4,8,12,16,20,24,28 positions (8bit register).
			output_odd=_mm256_and_si256(m2,output_mask);


			//3rd col iteration

			//load the 3 rows
			r0=_mm256_loadu_si256( (__m256i *) &frame1[N-2][col+1]);
			r1=_mm256_loadu_si256( (__m256i *) &frame1[N-1][col+1]);

			//multiply with the mask
			m0=_mm256_maddubs_epi16(r0,c0);
			m1=_mm256_maddubs_epi16(r1,c1);

			//vertical add
			m0=_mm256_add_epi16(m0,m1);

			//hozizontal additions
			m1=_mm256_srli_si256(m0,2);
			m2=_mm256_add_epi16(m1,m0);

			//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others. later on they will be stored into 0,4,8,12,16,20,24,28 positions (8bit register).
			m2=_mm256_and_si256(m2,output_mask);
			m2=_mm256_slli_si256(m2,2);
			output_even=_mm256_add_epi16(output_even,m2);

			//4th col iteration

			//load the 3 rows
			r0=_mm256_loadu_si256( (__m256i *) &frame1[N-2][col+2]);
			r1=_mm256_loadu_si256( (__m256i *) &frame1[N-1][col+2]);

			//multiply with the mask
			m0=_mm256_maddubs_epi16(r0,c0);
			m1=_mm256_maddubs_epi16(r1,c1);

			//vertical add
			m0=_mm256_add_epi16(m0,m1);

			//hozizontal additions
			m1=_mm256_srli_si256(m0,2);
			m2=_mm256_add_epi16(m1,m0);

			//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others. later on they will be stored into 0,4,8,12,16,20,24,28 positions (8bit register).
			m2=_mm256_and_si256(m2,output_mask);
			m2=_mm256_slli_si256(m2,2);
			output_odd=_mm256_add_epi16(output_odd,m2);


			//now division follows
			output_even=division(division_case,output_even,f);
			output_odd=division(division_case,output_odd,f);

			//shift odd 1 position and add to even
			output_odd = _mm256_slli_si256(output_odd,1);
			output_even = _mm256_add_epi8(output_even,output_odd);


			_mm256_store_si256( (__m256i *) &filt[N-1][col],output_even );

			  }

			}
		  Filter2D_loop_reminder_3x3_first_last_rows(frame1,filt,M,N,N-1,col,REMINDER_ITERATIONS,division_case,divisor,filter,c0,c1,c2,f);

	}

	else {//main loop

	  for (col = 0; col <= M-32-2; col+=32){

		  if (col==0){

				//load the 3 rows
				rr0=_mm256_load_si256( (__m256i *) &frame1[row-1][0]);
				rr1=_mm256_load_si256( (__m256i *) &frame1[row][0]);
				rr2=_mm256_load_si256( (__m256i *) &frame1[row+1][0]);

	  			//START - extra code needed for prelude
		  					m0=_mm256_slli_si256(rr0,1);//shift 1 elements left - equivalent to filling with one zero inthe beginning
		  					m1=_mm256_slli_si256(rr1,1);//shift 1 elements left - equivalent to filling with one zero inthe beginning
		  					m2=_mm256_slli_si256(rr2,1);//shift 1 elements left - equivalent to filling with one zero inthe beginning

		  					//preserve the element being lost because of the shift above
		  					r0=_mm256_and_si256(rr0,mask_prelude);
		  					r0=_mm256_permute2f128_si256(r0,r0,1);
		  					r0=_mm256_srli_si256(r0,15);//shift 15 elements
		  					r0=_mm256_add_epi16(m0,r0);

		  					r1=_mm256_and_si256(rr1,mask_prelude);
		  					r1=_mm256_permute2f128_si256(r1,r1,1);
		  					r1=_mm256_srli_si256(r1,15);//shift 15 elements
		  					r1=_mm256_add_epi16(m1,r1);

		  					r2=_mm256_and_si256(rr2,mask_prelude);
		  					r2=_mm256_permute2f128_si256(r2,r2,1);
		  					r2=_mm256_srli_si256(r2,15);//shift 15 elements
		  					r2=_mm256_add_epi16(m2,r2);

				//multiply with the mask
				m0=_mm256_maddubs_epi16(r0,c0);
				m1=_mm256_maddubs_epi16(r1,c1);
				m2=_mm256_maddubs_epi16(r2,c2);

				//vertical add
				m0=_mm256_add_epi16(m0,m1);
				m0=_mm256_add_epi16(m0,m2);

				//hozizontal additions
				m1=_mm256_srli_si256(m0,2);
				m2=_mm256_add_epi16(m1,m0);

				//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others. later on they will be stored into 0,4,8,12,16,20,24,28 positions (8bit register).
				output_even=_mm256_and_si256(m2,output_mask);

				//2ND col iteration

				//load the 3 rows
				//r0=_mm256_load_si256( (__m256i *) &frame1[row-1][0]);
				//r1=_mm256_load_si256( (__m256i *) &frame1[row][0]);
				//r2=_mm256_load_si256( (__m256i *) &frame1[row+1][0]);

				//multiply with the mask
				m0=_mm256_maddubs_epi16(rr0,c0);
				m1=_mm256_maddubs_epi16(rr1,c1);
				m2=_mm256_maddubs_epi16(rr2,c2);

				//vertical add
				m0=_mm256_add_epi16(m0,m1);
				m0=_mm256_add_epi16(m0,m2);

				//hozizontal additions
				m1=_mm256_srli_si256(m0,2);
				m2=_mm256_add_epi16(m1,m0);

				//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others. later on they will be stored into 0,4,8,12,16,20,24,28 positions (8bit register).
				output_odd=_mm256_and_si256(m2,output_mask);


				//3rd col iteration

				//load the 3 rows
				r0=_mm256_loadu_si256( (__m256i *) &frame1[row-1][1]);
				r1=_mm256_loadu_si256( (__m256i *) &frame1[row][1]);
				r2=_mm256_loadu_si256( (__m256i *) &frame1[row+1][1]);

				//multiply with the mask
				m0=_mm256_maddubs_epi16(r0,c0);
				m1=_mm256_maddubs_epi16(r1,c1);
				m2=_mm256_maddubs_epi16(r2,c2);

				//vertical add
				m0=_mm256_add_epi16(m0,m1);
				m0=_mm256_add_epi16(m0,m2);

				//hozizontal additions
				m1=_mm256_srli_si256(m0,2);
				m2=_mm256_add_epi16(m1,m0);

				//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others. later on they will be stored into 0,4,8,12,16,20,24,28 positions (8bit register).
				m2=_mm256_and_si256(m2,output_mask);
				m2=_mm256_slli_si256(m2,2);
				output_even=_mm256_add_epi16(output_even,m2);

				//4th col iteration

				//load the 3 rows
				r0=_mm256_loadu_si256( (__m256i *) &frame1[row-1][2]);
				r1=_mm256_loadu_si256( (__m256i *) &frame1[row][2]);
				r2=_mm256_loadu_si256( (__m256i *) &frame1[row+1][2]);

				//multiply with the mask
				m0=_mm256_maddubs_epi16(r0,c0);
				m1=_mm256_maddubs_epi16(r1,c1);
				m2=_mm256_maddubs_epi16(r2,c2);

				//vertical add
				m0=_mm256_add_epi16(m0,m1);
				m0=_mm256_add_epi16(m0,m2);

				//hozizontal additions
				m1=_mm256_srli_si256(m0,2);
				m2=_mm256_add_epi16(m1,m0);

				//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others. later on they will be stored into 0,4,8,12,16,20,24,28 positions (8bit register).
				m2=_mm256_and_si256(m2,output_mask);
				m2=_mm256_slli_si256(m2,2);
				output_odd=_mm256_add_epi16(output_odd,m2);


				//now division follows
				output_even=division(division_case,output_even,f);
				output_odd=division(division_case,output_odd,f);

				//shift odd 1 position and add to even
				output_odd = _mm256_slli_si256(output_odd,1);
				output_even = _mm256_add_epi8(output_even,output_odd);


				_mm256_store_si256( (__m256i *) &filt[row][0],output_even );
		  			//END - extra code needed for prelude
		  		}
		  else {


				//col iteration computes output pixels of    1,5,9,13,17,21,25,29
				// col+1 iteration computes output pixels of 2,6,10,14,18,22,26,30
				// col+2 iteration computes output pixels of 3,7,11,15,19,23,27
				// col+3 iteration computes output pixels of 4,8,12,16,20,24,28
				//afterwards, col becomes 30 and repeat the above process

		//1st col iteration

		//load the 3 rows
		r0=_mm256_loadu_si256( (__m256i *) &frame1[row-1][col-1]);
		r1=_mm256_loadu_si256( (__m256i *) &frame1[row][col-1]);
		r2=_mm256_loadu_si256( (__m256i *) &frame1[row+1][col-1]);

		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,c0);
		m1=_mm256_maddubs_epi16(r1,c1);
		m2=_mm256_maddubs_epi16(r2,c2);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m0=_mm256_add_epi16(m0,m2);

		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others. later on they will be stored into 0,4,8,12,16,20,24,28 positions (8bit register).
		output_even=_mm256_and_si256(m2,output_mask);

		//2ND col iteration

		//load the 3 rows
		r0=_mm256_load_si256( (__m256i *) &frame1[row-1][col]);//these loads are always aligned
		r1=_mm256_load_si256( (__m256i *) &frame1[row][col]);
		r2=_mm256_load_si256( (__m256i *) &frame1[row+1][col]);

		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,c0);
		m1=_mm256_maddubs_epi16(r1,c1);
		m2=_mm256_maddubs_epi16(r2,c2);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m0=_mm256_add_epi16(m0,m2);

		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others. later on they will be stored into 0,4,8,12,16,20,24,28 positions (8bit register).
		output_odd=_mm256_and_si256(m2,output_mask);


		//3rd col iteration

		//load the 3 rows
		r0=_mm256_loadu_si256( (__m256i *) &frame1[row-1][col+1]);
		r1=_mm256_loadu_si256( (__m256i *) &frame1[row][col+1]);
		r2=_mm256_loadu_si256( (__m256i *) &frame1[row+1][col+1]);

		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,c0);
		m1=_mm256_maddubs_epi16(r1,c1);
		m2=_mm256_maddubs_epi16(r2,c2);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m0=_mm256_add_epi16(m0,m2);

		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others. later on they will be stored into 0,4,8,12,16,20,24,28 positions (8bit register).
		m2=_mm256_and_si256(m2,output_mask);
		m2=_mm256_slli_si256(m2,2);
		output_even=_mm256_add_epi16(output_even,m2);

		//4th col iteration

		//load the 3 rows
		r0=_mm256_loadu_si256( (__m256i *) &frame1[row-1][col+2]);
		r1=_mm256_loadu_si256( (__m256i *) &frame1[row][col+2]);
		r2=_mm256_loadu_si256( (__m256i *) &frame1[row+1][col+2]);

		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,c0);
		m1=_mm256_maddubs_epi16(r1,c1);
		m2=_mm256_maddubs_epi16(r2,c2);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m0=_mm256_add_epi16(m0,m2);

		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others. later on they will be stored into 0,4,8,12,16,20,24,28 positions (8bit register).
		m2=_mm256_and_si256(m2,output_mask);
		m2=_mm256_slli_si256(m2,2);
		output_odd=_mm256_add_epi16(output_odd,m2);


		//now division follows
		output_even=division(division_case,output_even,f);
		output_odd=division(division_case,output_odd,f);

		//shift odd 1 position and add to even
		output_odd = _mm256_slli_si256(output_odd,1);
		output_even = _mm256_add_epi8(output_even,output_odd);


		_mm256_store_si256( (__m256i *) &filt[row][col],output_even );

		  }

		}
	  Filter2D_loop_reminder_3x3(frame1,filt,M,N,row,col,REMINDER_ITERATIONS,division_case,divisor,filter,c0,c1,c2,f);

	}

}

}//end of parallel


}



int Filter2D_loop_reminder_3x3(unsigned char **frame1,unsigned char **filt,const unsigned int M, const unsigned int N,const unsigned int row, const unsigned int col,const unsigned int REMINDER_ITERATIONS,const unsigned int division_case,const unsigned short int divisor,signed char **filter,const __m256i c0,const __m256i c1,const __m256i c2,const __m256i f){

	register	__m256i r0,r1,r2,m0,m1,m2,output_even,output_odd;


	const __m256i output_mask      = _mm256_set_epi16(0,65535,0,65535,0,65535,0,65535,0,65535,0,65535,0,65535,0,65535);

	 __m256i reminder_mask1;



	//1st col iteration

		//load the 3 rows
		r0=_mm256_loadu_si256( (__m256i *) &frame1[row-1][col-1]);
		r1=_mm256_loadu_si256( (__m256i *) &frame1[row][col-1]);
		r2=_mm256_loadu_si256( (__m256i *) &frame1[row+1][col-1]);

		//AND r0-r3 with reminder_mask
		reminder_mask1=_mm256_load_si256( (__m256i *) &reminder_msk1_3x3[REMINDER_ITERATIONS-1][0]);
		r0=_mm256_and_si256(r0,reminder_mask1);
		r1=_mm256_and_si256(r1,reminder_mask1);
		r2=_mm256_and_si256(r2,reminder_mask1);

		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,c0);
		m1=_mm256_maddubs_epi16(r1,c1);
		m2=_mm256_maddubs_epi16(r2,c2);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m0=_mm256_add_epi16(m0,m2);

		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others. later on they will be stored into 0,4,8,12,16,20,24,28 positions (8bit register).
		output_even=_mm256_and_si256(m2,output_mask);

		//2ND col iteration

		//load the 3 rows
		r0=_mm256_loadu_si256( (__m256i *) &frame1[row-1][col]);
		r1=_mm256_loadu_si256( (__m256i *) &frame1[row][col]);
		r2=_mm256_loadu_si256( (__m256i *) &frame1[row+1][col]);

		//AND r0-r3 with reminder_mask
		reminder_mask1=_mm256_load_si256( (__m256i *) &reminder_msk1_3x3[REMINDER_ITERATIONS-2][0]);
		r0=_mm256_and_si256(r0,reminder_mask1);
		r1=_mm256_and_si256(r1,reminder_mask1);
		r2=_mm256_and_si256(r2,reminder_mask1);

		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,c0);
		m1=_mm256_maddubs_epi16(r1,c1);
		m2=_mm256_maddubs_epi16(r2,c2);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m0=_mm256_add_epi16(m0,m2);

		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others. later on they will be stored into 0,4,8,12,16,20,24,28 positions (8bit register).
		output_odd=_mm256_and_si256(m2,output_mask);



if (REMINDER_ITERATIONS>2){//this prevents  reminder_msk1_3x3[REMINDER_ITERATIONS-3][0]) access -1
	//3rd col iteration
	//load the 3 rows
		r0=_mm256_loadu_si256( (__m256i *) &frame1[row-1][col+1]);
		r1=_mm256_loadu_si256( (__m256i *) &frame1[row][col+1]);
		r2=_mm256_loadu_si256( (__m256i *) &frame1[row+1][col+1]);

		//AND r0-r3 with reminder_mask
		reminder_mask1=_mm256_load_si256( (__m256i *) &reminder_msk1_3x3[REMINDER_ITERATIONS-3][0]);
		r0=_mm256_and_si256(r0,reminder_mask1);
		r1=_mm256_and_si256(r1,reminder_mask1);
		r2=_mm256_and_si256(r2,reminder_mask1);

		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,c0);
		m1=_mm256_maddubs_epi16(r1,c1);
		m2=_mm256_maddubs_epi16(r2,c2);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m0=_mm256_add_epi16(m0,m2);

		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others. later on they will be stored into 0,4,8,12,16,20,24,28 positions (8bit register).
		m2=_mm256_and_si256(m2,output_mask);
		m2=_mm256_slli_si256(m2,2);
		output_even=_mm256_add_epi16(output_even,m2);
}


if (REMINDER_ITERATIONS>3){
		//4th col iteration

		//load the 3 rows
		r0=_mm256_loadu_si256( (__m256i *) &frame1[row-1][col+2]);
		r1=_mm256_loadu_si256( (__m256i *) &frame1[row][col+2]);
		r2=_mm256_loadu_si256( (__m256i *) &frame1[row+1][col+2]);

		//AND r0-r3 with reminder_mask
		reminder_mask1=_mm256_load_si256( (__m256i *) &reminder_msk1_3x3[REMINDER_ITERATIONS-4][0]);
		r0=_mm256_and_si256(r0,reminder_mask1);
		r1=_mm256_and_si256(r1,reminder_mask1);
		r2=_mm256_and_si256(r2,reminder_mask1);

		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,c0);
		m1=_mm256_maddubs_epi16(r1,c1);
		m2=_mm256_maddubs_epi16(r2,c2);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m0=_mm256_add_epi16(m0,m2);

		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others. later on they will be stored into 0,4,8,12,16,20,24,28 positions (8bit register).
		m2=_mm256_and_si256(m2,output_mask);
		m2=_mm256_slli_si256(m2,2);
		output_odd=_mm256_add_epi16(output_odd,m2);
}

		//now division follows
		output_even=division(division_case,output_even,f);
		output_odd=division(division_case,output_odd,f);

		//shift odd 1 position and add to even
		output_odd = _mm256_slli_si256(output_odd,1);
		output_even = _mm256_add_epi8(output_even,output_odd);



 	switch (REMINDER_ITERATIONS){

	case 2:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		  break;
	case 3:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		  break;
	case 4:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output_even,3);
		  break;
	case 5:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output_even,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output_even,4);
		  break;
	case 6:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output_even,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output_even,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output_even,5);
		  break;
	case 7:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output_even,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output_even,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output_even,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output_even,6);
		  break;
	case 8:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output_even,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output_even,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output_even,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output_even,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output_even,7);
		  break;
	case 9:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output_even,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output_even,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output_even,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output_even,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output_even,7);
		filt[row][col+8] = (unsigned char) _mm256_extract_epi8(output_even,8);
		  break;
	case 10:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output_even,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output_even,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output_even,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output_even,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output_even,7);
		filt[row][col+8] = (unsigned char) _mm256_extract_epi8(output_even,8);
		filt[row][col+9] = (unsigned char) _mm256_extract_epi8(output_even,9);
		  break;
	case 11:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output_even,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output_even,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output_even,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output_even,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output_even,7);
		filt[row][col+8] = (unsigned char) _mm256_extract_epi8(output_even,8);
		filt[row][col+9] = (unsigned char) _mm256_extract_epi8(output_even,9);
		filt[row][col+10] = (unsigned char) _mm256_extract_epi8(output_even,10);
		  break;
	case 12:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output_even,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output_even,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output_even,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output_even,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output_even,7);
		filt[row][col+8] = (unsigned char) _mm256_extract_epi8(output_even,8);
		filt[row][col+9] = (unsigned char) _mm256_extract_epi8(output_even,9);
		filt[row][col+10] = (unsigned char) _mm256_extract_epi8(output_even,10);
		filt[row][col+11] = (unsigned char) _mm256_extract_epi8(output_even,11);
		  break;
	case 13:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output_even,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output_even,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output_even,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output_even,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output_even,7);
		filt[row][col+8] = (unsigned char) _mm256_extract_epi8(output_even,8);
		filt[row][col+9] = (unsigned char) _mm256_extract_epi8(output_even,9);
		filt[row][col+10] = (unsigned char) _mm256_extract_epi8(output_even,10);
		filt[row][col+11] = (unsigned char) _mm256_extract_epi8(output_even,11);
		filt[row][col+12] = (unsigned char) _mm256_extract_epi8(output_even,12);
		  break;
	case 14:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output_even,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output_even,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output_even,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output_even,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output_even,7);
		filt[row][col+8] = (unsigned char) _mm256_extract_epi8(output_even,8);
		filt[row][col+9] = (unsigned char) _mm256_extract_epi8(output_even,9);
		filt[row][col+10] = (unsigned char) _mm256_extract_epi8(output_even,10);
		filt[row][col+11] = (unsigned char) _mm256_extract_epi8(output_even,11);
		filt[row][col+12] = (unsigned char) _mm256_extract_epi8(output_even,12);
		filt[row][col+13] = (unsigned char) _mm256_extract_epi8(output_even,13);
		  break;
	case 15:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output_even,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output_even,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output_even,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output_even,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output_even,7);
		filt[row][col+8] = (unsigned char) _mm256_extract_epi8(output_even,8);
		filt[row][col+9] = (unsigned char) _mm256_extract_epi8(output_even,9);
		filt[row][col+10] = (unsigned char) _mm256_extract_epi8(output_even,10);
		filt[row][col+11] = (unsigned char) _mm256_extract_epi8(output_even,11);
		filt[row][col+12] = (unsigned char) _mm256_extract_epi8(output_even,12);
		filt[row][col+13] = (unsigned char) _mm256_extract_epi8(output_even,13);
		filt[row][col+14] = (unsigned char) _mm256_extract_epi8(output_even,14);
		  break;
	case 16:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
		  break;
	case 17:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
		  break;
	case 18:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
		  break;
	case 19:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
		  break;
	case 20:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output_even,19);
		  break;
	case 21:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output_even,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output_even,20);
		  break;
	case 22:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output_even,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output_even,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output_even,21);
		break;
	case 23:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output_even,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output_even,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output_even,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output_even,22);
		  break;
	case 24:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output_even,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output_even,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output_even,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output_even,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output_even,23);
		  break;
	case 25:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output_even,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output_even,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output_even,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output_even,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output_even,23);
	filt[row][col+24] = (unsigned char) _mm256_extract_epi8(output_even,24);
		  break;
	case 26:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output_even,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output_even,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output_even,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output_even,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output_even,23);
	filt[row][col+24] = (unsigned char) _mm256_extract_epi8(output_even,24);
	filt[row][col+25] = (unsigned char) _mm256_extract_epi8(output_even,25);
		  break;
	case 27:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output_even,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output_even,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output_even,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output_even,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output_even,23);
	filt[row][col+24] = (unsigned char) _mm256_extract_epi8(output_even,24);
	filt[row][col+25] = (unsigned char) _mm256_extract_epi8(output_even,25);
	filt[row][col+26] = (unsigned char) _mm256_extract_epi8(output_even,26);
		  break;
	case 28:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output_even,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output_even,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output_even,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output_even,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output_even,23);
	filt[row][col+24] = (unsigned char) _mm256_extract_epi8(output_even,24);
	filt[row][col+25] = (unsigned char) _mm256_extract_epi8(output_even,25);
	filt[row][col+26] = (unsigned char) _mm256_extract_epi8(output_even,26);
	filt[row][col+27] = (unsigned char) _mm256_extract_epi8(output_even,27);
		  break;
	case 29:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output_even,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output_even,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output_even,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output_even,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output_even,23);
	filt[row][col+24] = (unsigned char) _mm256_extract_epi8(output_even,24);
	filt[row][col+25] = (unsigned char) _mm256_extract_epi8(output_even,25);
	filt[row][col+26] = (unsigned char) _mm256_extract_epi8(output_even,26);
	filt[row][col+27] = (unsigned char) _mm256_extract_epi8(output_even,27);
	filt[row][col+28] = (unsigned char) _mm256_extract_epi8(output_even,28);
		  break;
	case 30:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output_even,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output_even,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output_even,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output_even,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output_even,23);
	filt[row][col+24] = (unsigned char) _mm256_extract_epi8(output_even,24);
	filt[row][col+25] = (unsigned char) _mm256_extract_epi8(output_even,25);
	filt[row][col+26] = (unsigned char) _mm256_extract_epi8(output_even,26);
	filt[row][col+27] = (unsigned char) _mm256_extract_epi8(output_even,27);
	filt[row][col+28] = (unsigned char) _mm256_extract_epi8(output_even,28);
	filt[row][col+29] = (unsigned char) _mm256_extract_epi8(output_even,29);

		  break;

	case 31:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output_even,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output_even,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output_even,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output_even,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output_even,23);
	filt[row][col+24] = (unsigned char) _mm256_extract_epi8(output_even,24);
	filt[row][col+25] = (unsigned char) _mm256_extract_epi8(output_even,25);
	filt[row][col+26] = (unsigned char) _mm256_extract_epi8(output_even,26);
	filt[row][col+27] = (unsigned char) _mm256_extract_epi8(output_even,27);
	filt[row][col+28] = (unsigned char) _mm256_extract_epi8(output_even,28);
	filt[row][col+29] = (unsigned char) _mm256_extract_epi8(output_even,29);
	filt[row][col+30] = (unsigned char) _mm256_extract_epi8(output_even,30);

		  break;
	case 32:
		_mm256_storeu_si256( (__m256i *) &filt[row][col],output_even );

		  break;
	case 33:
		_mm256_storeu_si256( (__m256i *) &filt[row][col],output_even );

		//the filt[row][col+32] is computed unvectorized
		int newPixel = 0;
		newPixel += frame1[row-1][M-1-1] * filter[0][0];
		newPixel += frame1[row-1][M-1] * filter[0][1];

		newPixel += frame1[row][M-1-1] * filter[1][0];
		newPixel += frame1[row][M-1] * filter[1][1];

		newPixel += frame1[row+1][M-1-1] * filter[2][0];
		newPixel += frame1[row+1][M-1] * filter[2][1];

		filt[row][M-1] = (unsigned char) (newPixel / divisor);
		  break;
	default:
		printf("\nsomething went wrong");
		return -1;
 	}

return 0;


}



//this routine works only for row=0 and row=N-1
int Filter2D_loop_reminder_3x3_first_last_rows(unsigned char **frame1,unsigned char **filt,const unsigned int M, const unsigned int N,const unsigned int row, const unsigned int col,const unsigned int REMINDER_ITERATIONS,const unsigned int division_case,const unsigned short int divisor,signed char **filter,const __m256i c0,const __m256i c1,const __m256i c2,const __m256i f){

	register	__m256i r0,r1,m0,m1,m2,output_even,output_odd;


	const __m256i output_mask      = _mm256_set_epi16(0,65535,0,65535,0,65535,0,65535,0,65535,0,65535,0,65535,0,65535);

	 __m256i reminder_mask1;

	//1st col iteration
	if (row==0){
		r0=_mm256_loadu_si256( (__m256i *) &frame1[0][col-1]);
		r1=_mm256_loadu_si256( (__m256i *) &frame1[1][col-1]);
		reminder_mask1=_mm256_load_si256( (__m256i *) &reminder_msk1_3x3[REMINDER_ITERATIONS-1][0]);
		r0=_mm256_and_si256(r0,reminder_mask1);
		r1=_mm256_and_si256(r1,reminder_mask1);

		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,c1);
		m1=_mm256_maddubs_epi16(r1,c2);
	}
	else {//if row=N-1
		r0=_mm256_loadu_si256( (__m256i *) &frame1[N-2][col-1]);
		r1=_mm256_loadu_si256( (__m256i *) &frame1[N-1][col-1]);
		reminder_mask1=_mm256_load_si256( (__m256i *) &reminder_msk1_3x3[REMINDER_ITERATIONS-1][0]);
		r0=_mm256_and_si256(r0,reminder_mask1);
		r1=_mm256_and_si256(r1,reminder_mask1);

		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,c0);
		m1=_mm256_maddubs_epi16(r1,c1);
	}

		//vertical add
		m0=_mm256_add_epi16(m0,m1);

		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others. later on they will be stored into 0,4,8,12,16,20,24,28 positions (8bit register).
		output_even=_mm256_and_si256(m2,output_mask);

		//2ND col iteration
		if (row==0){
			r0=_mm256_loadu_si256( (__m256i *) &frame1[0][col]);
			r1=_mm256_loadu_si256( (__m256i *) &frame1[1][col]);
			reminder_mask1=_mm256_load_si256( (__m256i *) &reminder_msk1_3x3[REMINDER_ITERATIONS-2][0]);
			r0=_mm256_and_si256(r0,reminder_mask1);
			r1=_mm256_and_si256(r1,reminder_mask1);

			//multiply with the mask
			m0=_mm256_maddubs_epi16(r0,c1);
			m1=_mm256_maddubs_epi16(r1,c2);
		}
		else {//if row=N-1
			r0=_mm256_loadu_si256( (__m256i *) &frame1[N-2][col]);
			r1=_mm256_loadu_si256( (__m256i *) &frame1[N-1][col]);
			reminder_mask1=_mm256_load_si256( (__m256i *) &reminder_msk1_3x3[REMINDER_ITERATIONS-2][0]);
			r0=_mm256_and_si256(r0,reminder_mask1);
			r1=_mm256_and_si256(r1,reminder_mask1);

			//multiply with the mask
			m0=_mm256_maddubs_epi16(r0,c0);
			m1=_mm256_maddubs_epi16(r1,c1);
		}


		//vertical add
		m0=_mm256_add_epi16(m0,m1);

		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others. later on they will be stored into 0,4,8,12,16,20,24,28 positions (8bit register).
		output_odd=_mm256_and_si256(m2,output_mask);



if (REMINDER_ITERATIONS>2){//this prevents  reminder_msk1_3x3[REMINDER_ITERATIONS-3][0]) access -1
	//3rd col iteration
	if (row==0){
		r0=_mm256_loadu_si256( (__m256i *) &frame1[0][col+1]);
		r1=_mm256_loadu_si256( (__m256i *) &frame1[1][col+1]);
		reminder_mask1=_mm256_load_si256( (__m256i *) &reminder_msk1_3x3[REMINDER_ITERATIONS-3][0]);
		r0=_mm256_and_si256(r0,reminder_mask1);
		r1=_mm256_and_si256(r1,reminder_mask1);

		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,c1);
		m1=_mm256_maddubs_epi16(r1,c2);
	}
	else {//if row=N-1
		r0=_mm256_loadu_si256( (__m256i *) &frame1[N-2][col+1]);
		r1=_mm256_loadu_si256( (__m256i *) &frame1[N-1][col+1]);
		reminder_mask1=_mm256_load_si256( (__m256i *) &reminder_msk1_3x3[REMINDER_ITERATIONS-3][0]);
		r0=_mm256_and_si256(r0,reminder_mask1);
		r1=_mm256_and_si256(r1,reminder_mask1);

		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,c0);
		m1=_mm256_maddubs_epi16(r1,c1);
	}


		//vertical add
		m0=_mm256_add_epi16(m0,m1);

		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others. later on they will be stored into 0,4,8,12,16,20,24,28 positions (8bit register).
		m2=_mm256_and_si256(m2,output_mask);
		m2=_mm256_slli_si256(m2,2);
		output_even=_mm256_add_epi16(output_even,m2);
}


if (REMINDER_ITERATIONS>3){
		//4th col iteration

	if (row==0){
		r0=_mm256_loadu_si256( (__m256i *) &frame1[0][col+2]);
		r1=_mm256_loadu_si256( (__m256i *) &frame1[1][col+2]);
		reminder_mask1=_mm256_load_si256( (__m256i *) &reminder_msk1_3x3[REMINDER_ITERATIONS-4][0]);
		r0=_mm256_and_si256(r0,reminder_mask1);
		r1=_mm256_and_si256(r1,reminder_mask1);

		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,c1);
		m1=_mm256_maddubs_epi16(r1,c2);
	}
	else {//if row=N-1
		r0=_mm256_loadu_si256( (__m256i *) &frame1[N-2][col+2]);
		r1=_mm256_loadu_si256( (__m256i *) &frame1[N-1][col+2]);
		reminder_mask1=_mm256_load_si256( (__m256i *) &reminder_msk1_3x3[REMINDER_ITERATIONS-4][0]);
		r0=_mm256_and_si256(r0,reminder_mask1);
		r1=_mm256_and_si256(r1,reminder_mask1);

		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,c0);
		m1=_mm256_maddubs_epi16(r1,c1);
	}

		//vertical add
		m0=_mm256_add_epi16(m0,m1);

		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others. later on they will be stored into 0,4,8,12,16,20,24,28 positions (8bit register).
		m2=_mm256_and_si256(m2,output_mask);
		m2=_mm256_slli_si256(m2,2);
		output_odd=_mm256_add_epi16(output_odd,m2);
}

		//now division follows
		output_even=division(division_case,output_even,f);
		output_odd=division(division_case,output_odd,f);

		//shift odd 1 position and add to even
		output_odd = _mm256_slli_si256(output_odd,1);
		output_even = _mm256_add_epi8(output_even,output_odd);



 	switch (REMINDER_ITERATIONS){

	case 2:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		  break;
	case 3:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		  break;
	case 4:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output_even,3);
		  break;
	case 5:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output_even,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output_even,4);
		  break;
	case 6:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output_even,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output_even,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output_even,5);
		  break;
	case 7:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output_even,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output_even,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output_even,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output_even,6);
		  break;
	case 8:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output_even,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output_even,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output_even,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output_even,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output_even,7);
		  break;
	case 9:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output_even,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output_even,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output_even,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output_even,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output_even,7);
		filt[row][col+8] = (unsigned char) _mm256_extract_epi8(output_even,8);
		  break;
	case 10:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output_even,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output_even,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output_even,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output_even,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output_even,7);
		filt[row][col+8] = (unsigned char) _mm256_extract_epi8(output_even,8);
		filt[row][col+9] = (unsigned char) _mm256_extract_epi8(output_even,9);
		  break;
	case 11:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output_even,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output_even,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output_even,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output_even,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output_even,7);
		filt[row][col+8] = (unsigned char) _mm256_extract_epi8(output_even,8);
		filt[row][col+9] = (unsigned char) _mm256_extract_epi8(output_even,9);
		filt[row][col+10] = (unsigned char) _mm256_extract_epi8(output_even,10);
		  break;
	case 12:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output_even,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output_even,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output_even,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output_even,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output_even,7);
		filt[row][col+8] = (unsigned char) _mm256_extract_epi8(output_even,8);
		filt[row][col+9] = (unsigned char) _mm256_extract_epi8(output_even,9);
		filt[row][col+10] = (unsigned char) _mm256_extract_epi8(output_even,10);
		filt[row][col+11] = (unsigned char) _mm256_extract_epi8(output_even,11);
		  break;
	case 13:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output_even,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output_even,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output_even,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output_even,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output_even,7);
		filt[row][col+8] = (unsigned char) _mm256_extract_epi8(output_even,8);
		filt[row][col+9] = (unsigned char) _mm256_extract_epi8(output_even,9);
		filt[row][col+10] = (unsigned char) _mm256_extract_epi8(output_even,10);
		filt[row][col+11] = (unsigned char) _mm256_extract_epi8(output_even,11);
		filt[row][col+12] = (unsigned char) _mm256_extract_epi8(output_even,12);
		  break;
	case 14:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output_even,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output_even,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output_even,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output_even,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output_even,7);
		filt[row][col+8] = (unsigned char) _mm256_extract_epi8(output_even,8);
		filt[row][col+9] = (unsigned char) _mm256_extract_epi8(output_even,9);
		filt[row][col+10] = (unsigned char) _mm256_extract_epi8(output_even,10);
		filt[row][col+11] = (unsigned char) _mm256_extract_epi8(output_even,11);
		filt[row][col+12] = (unsigned char) _mm256_extract_epi8(output_even,12);
		filt[row][col+13] = (unsigned char) _mm256_extract_epi8(output_even,13);
		  break;
	case 15:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output_even,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output_even,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output_even,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output_even,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output_even,7);
		filt[row][col+8] = (unsigned char) _mm256_extract_epi8(output_even,8);
		filt[row][col+9] = (unsigned char) _mm256_extract_epi8(output_even,9);
		filt[row][col+10] = (unsigned char) _mm256_extract_epi8(output_even,10);
		filt[row][col+11] = (unsigned char) _mm256_extract_epi8(output_even,11);
		filt[row][col+12] = (unsigned char) _mm256_extract_epi8(output_even,12);
		filt[row][col+13] = (unsigned char) _mm256_extract_epi8(output_even,13);
		filt[row][col+14] = (unsigned char) _mm256_extract_epi8(output_even,14);
		  break;
	case 16:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
		  break;
	case 17:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
		  break;
	case 18:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
		  break;
	case 19:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
		  break;
	case 20:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output_even,19);
		  break;
	case 21:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output_even,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output_even,20);
		  break;
	case 22:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output_even,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output_even,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output_even,21);
		break;
	case 23:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output_even,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output_even,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output_even,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output_even,22);
		  break;
	case 24:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output_even,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output_even,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output_even,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output_even,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output_even,23);
		  break;
	case 25:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output_even,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output_even,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output_even,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output_even,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output_even,23);
	filt[row][col+24] = (unsigned char) _mm256_extract_epi8(output_even,24);
		  break;
	case 26:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output_even,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output_even,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output_even,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output_even,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output_even,23);
	filt[row][col+24] = (unsigned char) _mm256_extract_epi8(output_even,24);
	filt[row][col+25] = (unsigned char) _mm256_extract_epi8(output_even,25);
		  break;
	case 27:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output_even,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output_even,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output_even,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output_even,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output_even,23);
	filt[row][col+24] = (unsigned char) _mm256_extract_epi8(output_even,24);
	filt[row][col+25] = (unsigned char) _mm256_extract_epi8(output_even,25);
	filt[row][col+26] = (unsigned char) _mm256_extract_epi8(output_even,26);
		  break;
	case 28:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output_even,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output_even,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output_even,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output_even,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output_even,23);
	filt[row][col+24] = (unsigned char) _mm256_extract_epi8(output_even,24);
	filt[row][col+25] = (unsigned char) _mm256_extract_epi8(output_even,25);
	filt[row][col+26] = (unsigned char) _mm256_extract_epi8(output_even,26);
	filt[row][col+27] = (unsigned char) _mm256_extract_epi8(output_even,27);
		  break;
	case 29:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output_even,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output_even,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output_even,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output_even,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output_even,23);
	filt[row][col+24] = (unsigned char) _mm256_extract_epi8(output_even,24);
	filt[row][col+25] = (unsigned char) _mm256_extract_epi8(output_even,25);
	filt[row][col+26] = (unsigned char) _mm256_extract_epi8(output_even,26);
	filt[row][col+27] = (unsigned char) _mm256_extract_epi8(output_even,27);
	filt[row][col+28] = (unsigned char) _mm256_extract_epi8(output_even,28);
		  break;
	case 30:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output_even,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output_even,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output_even,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output_even,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output_even,23);
	filt[row][col+24] = (unsigned char) _mm256_extract_epi8(output_even,24);
	filt[row][col+25] = (unsigned char) _mm256_extract_epi8(output_even,25);
	filt[row][col+26] = (unsigned char) _mm256_extract_epi8(output_even,26);
	filt[row][col+27] = (unsigned char) _mm256_extract_epi8(output_even,27);
	filt[row][col+28] = (unsigned char) _mm256_extract_epi8(output_even,28);
	filt[row][col+29] = (unsigned char) _mm256_extract_epi8(output_even,29);

		  break;

	case 31:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output_even,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output_even,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output_even,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output_even,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output_even,23);
	filt[row][col+24] = (unsigned char) _mm256_extract_epi8(output_even,24);
	filt[row][col+25] = (unsigned char) _mm256_extract_epi8(output_even,25);
	filt[row][col+26] = (unsigned char) _mm256_extract_epi8(output_even,26);
	filt[row][col+27] = (unsigned char) _mm256_extract_epi8(output_even,27);
	filt[row][col+28] = (unsigned char) _mm256_extract_epi8(output_even,28);
	filt[row][col+29] = (unsigned char) _mm256_extract_epi8(output_even,29);
	filt[row][col+30] = (unsigned char) _mm256_extract_epi8(output_even,30);

		  break;
	case 32:
		_mm256_storeu_si256( (__m256i *) &filt[row][col],output_even );

		  break;
	case 33:
		_mm256_storeu_si256( (__m256i *) &filt[row][col],output_even );

		//the filt[row][col+32] is computed unvectorized
		int newPixel = 0;

		if (row==N-1){
		newPixel += frame1[row-1][M-1-1] * filter[0][0];
		newPixel += frame1[row-1][M-1] * filter[0][1];}

		newPixel += frame1[row][M-1-1] * filter[1][0];
		newPixel += frame1[row][M-1] * filter[1][1];

		if (row==0){
		newPixel += frame1[row+1][M-1-1] * filter[2][0];
		newPixel += frame1[row+1][M-1] * filter[2][1];}

		filt[row][M-1] = (unsigned char) (newPixel / divisor);
		  break;
	default:
		printf("\nsomething went wrong");
		return -1;
 	}

return 0;


}



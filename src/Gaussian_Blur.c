#include "Gaussian_Blur.h"



 //after every 5 mask elements I insert a zero. This is because there is no mul command for 8bit in AVX/SSE. The only command I can use is maddubs which mults and adds the intermediate results. Thus, the 5th element will be added with the 6th which is always zero.
signed char gaussian_filter_5x5[9][32] __attribute__((aligned(64))) ={
		{2,4,5,4,2,0,2,4,5,4,2,0,2,4,5,4,2,0,2,4,5,4,2,0,2,4,5,4,2,0,0,0},
		{4,9,12,9,4,0,4,9,12,9,4,0,4,9,12,9,4,0,4,9,12,9,4,0,4,9,12,9,4,0,0,0},
		{5,12,15,12,5,0,5,12,15,12,5,0,5,12,15,12,5,0,5,12,15,12,5,0,5,12,15,12,5,0,0,0},

		{0,2,4,5,4,2,0,2,4,5,4,2,0,2,4,5,4,2,0,2,4,5,4,2,0,2,4,5,4,2,0,0},
		{0,4,9,12,9,4,0,4,9,12,9,4,0,4,9,12,9,4,0,4,9,12,9,4,0,4,9,12,9,4,0,0},
		{0,5,12,15,12,5,0,5,12,15,12,5,0,5,12,15,12,5,0,5,12,15,12,5,0,5,12,15,12,5,0,0},

		{0,0,2,4,5,4,2,0,2,4,5,4,2,0,2,4,5,4,2,0,2,4,5,4,2,0,2,4,5,4,2,0},
		{0,0,4,9,12,9,4,0,4,9,12,9,4,0,4,9,12,9,4,0,4,9,12,9,4,0,4,9,12,9,4,0},
		{0,0,5,12,15,12,5,0,5,12,15,12,5,0,5,12,15,12,5,0,5,12,15,12,5,0,5,12,15,12,5,0},
};





__attribute__((aligned(64))) unsigned short int f_vector[16]; //initialized by 'prepare_for_division()' routine, for 2D routines
__attribute__((aligned(64))) unsigned short int f_vector_x[16]; //initialized by 'prepare_for_division()' routine, for separable routines only
__attribute__((aligned(64))) unsigned short int f_vector_y[16]; //initialized by 'prepare_for_division()' routine, for separable routines only


unsigned int b;

//this is for 9x9 case Y mask only
const unsigned char reminder_mask_9x9[31][32] __attribute__((aligned(64))) ={//this lookup table is used in the loop reminder
		{255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0},
		{255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0},
		{255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0},
		{255,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0},
		{255,255,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0},
		{255,255,255,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0},
		{255,255,255,255,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0},
		{255,255,255,255,255,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0},
		{255,255,255,255,255,255,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0},
		{255,255,255,255,255,255,255,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0},
		{255,255,255,255,255,255,255,255,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0},
		{255,255,255,255,255,255,255,255,255,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,0,0,0,0,0,0,0,0,0,0,0},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,0,0,0,0,0,0,0,0,0,0},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,0,0,0,0,0,0,0,0,0},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,0,0,0,0,0,0,0,0},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,0,0,0,0,0,0,0},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,0,0,0,0,0,0},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,0,0,0,0,0},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,0,0,0,0},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,0,0,0},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,0,0},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,0},
} ;


//this is for 7x7 case
const unsigned char reminder_mask_7x7[32][32] __attribute__((aligned(64))) ={//this lookup table is used in the loop reminder
		{255,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0},
		{255,255,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0},
		{255,255,255,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0},
		{255,255,255,255,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0},
		{255,255,255,255,255,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0},
		{255,255,255,255,255,255,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0},
		{255,255,255,255,255,255,255,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0},
		{255,255,255,255,255,255,255,255,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0},
		{255,255,255,255,255,255,255,255,255,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,0,0,0,0,0,0,0,0,0,0,0},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,0,0,0,0,0,0,0,0,0,0},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,0,0,0,0,0,0,0,0,0},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,0,0,0,0,0,0,0,0},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,0,0,0,0,0,0,0},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,0,0,0,0,0,0},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,0,0,0,0,0},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,0,0,0,0},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,0,0,0},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,0,0},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,0},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255},
} ;


//this is for 5x5 case
const unsigned char reminder_msk1[31][32] __attribute__((aligned(64))) ={//this lookup table is used in the loop reminder
		{255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0},
		{255,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0},
		{255,255,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0},
		{255,255,255,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0},
		{255,255,255,255,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0},
		{255,255,255,255,255,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0},
		{255,255,255,255,255,255,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0},
		{255,255,255,255,255,255,255,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0},
		{255,255,255,255,255,255,255,255,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0},
		{255,255,255,255,255,255,255,255,255,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,0,0,0,0,0,0,0,0,0,0,0},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,0,0,0,0,0,0,0,0,0,0},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,0,0,0,0,0,0,0,0,0},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,0,0,0,0,0,0,0,0},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,0,0,0,0,0,0,0},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,0,0,0,0,0,0},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,0,0,0,0,0},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,0,0,0,0},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,0,0,0},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,0,0},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,0},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255},
} ;

//this is for 5x5 case
const unsigned char reminder_msk2[31][32] __attribute__((aligned(64))) ={//this lookup table is used in the loop reminder
		{0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0},
		{255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0},
		{255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0},
		{255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0},
		{255,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0},
		{255,255,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0},
		{255,255,255,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0},
		{255,255,255,255,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0},
		{255,255,255,255,255,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0},
		{255,255,255,255,255,255,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0},
		{255,255,255,255,255,255,255,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0},
		{255,255,255,255,255,255,255,255,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0},
		{255,255,255,255,255,255,255,255,255,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,0,0,0,0,0,0,0,0,0,0,0},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,0,0,0,0,0,0,0,0,0,0},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,0,0,0,0,0,0,0,0,0},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,0,0,0,0,0,0,0,0},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,0,0,0,0,0,0,0},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,0,0,0,0,0,0},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,0,0,0,0,0},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,0,0,0,0},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,0,0,0},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,0,0},
};



const unsigned char reminder_msk1_3x3[33][32] __attribute__((aligned(64))) ={//this lookup table is used in the loop reminder
		{255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0},
		{255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0},
		{255,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0},
		{255,255,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0},
		{255,255,255,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0},
		{255,255,255,255,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0},
		{255,255,255,255,255,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0},
		{255,255,255,255,255,255,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0},
		{255,255,255,255,255,255,255,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0},
		{255,255,255,255,255,255,255,255,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0},
		{255,255,255,255,255,255,255,255,255,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,0,0,0,0,0,0,0,0,0,0,0},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,0,0,0,0,0,0,0,0,0,0},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,0,0,0,0,0,0,0,0,0},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,0,0,0,0,0,0,0,0},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,0,0,0,0,0,0,0},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,0,0,0,0,0,0},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,0,0,0,0,0},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,0,0,0,0},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,0,0,0},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,0,0},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,0},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255},
		{255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255},

} ;



const signed char gaussianMask_1d[5]={1,4,6,4,1};

signed char gaussian_filter_5x5_seperable_y[6][32] __attribute__((aligned(64))) ={
		{1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0},
		{4,0,4,0,4,0,4,0,4,0,4,0,4,0,4,0,4,0,4,0,4,0,4,0,4,0,4,0,4,0,4,0},
		{6,0,6,0,6,0,6,0,6,0,6,0,6,0,6,0,6,0,6,0,6,0,6,0,6,0,6,0,6,0,6,0},
		{0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1},
		{0,4,0,4,0,4,0,4,0,4,0,4,0,4,0,4,0,4,0,4,0,4,0,4,0,4,0,4,0,4,0,4},
		{0,6,0,6,0,6,0,6,0,6,0,6,0,6,0,6,0,6,0,6,0,6,0,6,0,6,0,6,0,6,0,6},
};

signed char gaussian_filter_5x5_seperable_x[6][32] __attribute__((aligned(64))) ={
		{1,4,6,4,1,0,1,4,6,4,1,0,1,4,6,4,1,0,1,4,6,4,1,0,1,4,6,4,1,0,0,0},
		{0,1,4,6,4,1,0,1,4,6,4,1,0,1,4,6,4,1,0,1,4,6,4,1,0,1,4,6,4,1,0,0},
		{0,0,1,4,6,4,1,0,1,4,6,4,1,0,1,4,6,4,1,0,1,4,6,4,1,0,1,4,6,4,1,0},
		{0,0,0,1,4,6,4,1,0,1,4,6,4,1,0,1,4,6,4,1,0,1,4,6,4,1,0,1,4,6,4,1},
		{0,0,0,0,1,4,6,4,1,0,1,4,6,4,1,0,1,4,6,4,1,0,1,4,6,4,1,0,0,0,0,0},
		{0,0,0,0,0,1,4,6,4,1,0,1,4,6,4,1,0,1,4,6,4,1,0,1,4,6,4,1,0,0,0,0},
};



void Conv_default(unsigned char **frame1,unsigned char **filt,const unsigned int M, const unsigned int N,const unsigned int filter_size, const unsigned int divisor,signed char **filter){

	int row,col,rowOffset,colOffset;
	int newPixel;
	unsigned char pix;
	const unsigned short int size=filter_size/2;

	/*---------------------- Gaussian Blur ---------------------------------*/
		for (row = 0; row < N; row++) {
			for (col = 0; col < M; col++) {
				newPixel = 0;
				for (rowOffset=-size; rowOffset<=size; rowOffset++) {
					for (colOffset=-size; colOffset<=size; colOffset++) {

						if ( (row+rowOffset<0) || (row+rowOffset>=N) || (col+colOffset<0) || (col+colOffset>=M) )
							pix=0;
						else
							pix=frame1[row+rowOffset][col+colOffset];

	                   newPixel += pix * filter[size + rowOffset][size + colOffset];

					}
				        }
			filt[row][col] = (unsigned char) (newPixel / divisor);

			}
		}

		//printf("\n Def= %d %d %d",filt[N-3][M-3],filt[N-3][M-2],filt[N-3][M-1]);
}



void Conv_default_separable(unsigned char **frame1,unsigned char **filt,const unsigned int M, const unsigned int N, const unsigned int kernel_size, signed char *kernel_y, signed char *kernel_x, const unsigned int divisor_xy){


	int row,col,rowOffset,colOffset,newPixel;
	unsigned char pix;
	unsigned char temp[M];
	const unsigned short int size=kernel_size/2;

	/*---------------------- Gaussian Blur ---------------------------------*/
		for (row = 0; row < N; row++) {
			for (col = 0; col < M; col++) {
				newPixel = 0;
				for (rowOffset=-size; rowOffset<=size; rowOffset++) {

						if ( (row+rowOffset<0) || (row+rowOffset>=N) )
							pix=0;
						else
							pix=frame1[row+rowOffset][col];

	                   newPixel += pix * kernel_y[size + rowOffset];
					}

			temp[col] = (unsigned char) (newPixel / divisor_xy);
			}

			for (col = 0; col < M; col++) {
							newPixel = 0;
							for (colOffset=-size; colOffset<=size; colOffset++) {

									if ( (col+colOffset<0) || (col+colOffset>=M) )
										pix=0;
									else
										pix=temp[col+colOffset];

				                   newPixel += pix * kernel_x[size + colOffset];
								}

						filt[row][col] = (unsigned char) (newPixel / divisor_xy);
						}

		}


}





// (x/divisor) is transformed into ((x * ceil(f) ) >> w+b) by using the following algorithm. Since the divisor does not change I apply the following procedure just once
const unsigned int prepare_for_division(const unsigned short int divisor){
/*
 * * Mathematical formula, used for unsigned division with fixed divisor:
* (From Terje Mathisen, unpublished)
* x = dividend
* d = divisor
* w = integer word size, bits
* b = floor(log2(d)) = bit_scan_reverse(d)
* f = 2^(w+b) / d                                [exact division]
* If f is an integer then d is a power of 2 then go to case A
* If the fractional part of f is < 0.5 then go to case B
* If the fractional part of f is > 0.5 then go to case C
* Case A:  [shift only]
* result = x >> b
* Case B:  [round down f and compensate by adding one to x]
* result = ((x+1)*floor(f)) >> (w+b)             [high part of unsigned multiplication with 2w bits]
* Case C:  [round up f, no compensation for rounding error]
* result = (x*ceil(f)) >> (w+b)                  [high part of unsigned multiplication with 2w bits]
*
 */
__m256i f_vec;
unsigned short int tmp;

if (divisor==0){
	printf("\n cannot divide with zero, process is terminated");
	exit(EXIT_FAILURE);
}
else if (divisor==1)
	return 4;


const unsigned int w=16; //integer word size in bits of the division operands
b = (unsigned int) floorf(log2f(divisor));
float f=powf(2,w+b)/divisor;
int integer_part_f = (int) f;
float float_part_f = f - integer_part_f;

if (float_part_f < 0.0001){ //if f==0.0
	return 1; //case A
}
else if (float_part_f < 0.5){
	tmp=(unsigned short int) floorf(f);
	f_vec = _mm256_set1_epi16(tmp);
	_mm256_store_si256( (__m256i *) &f_vector[0],f_vec);
	return 2; //case B
}
else{
	tmp=(unsigned short int) ceilf(f);
	f_vec = _mm256_set1_epi16(tmp);
	_mm256_store_si256( (__m256i *) &f_vector[0],f_vec);
	return 3; //case C
}


}



// (x/divisor) is transformed into ((x * ceil(f) ) >> w+b) by using the following algorithm. Since the divisor does not change I apply the following procedure just once
const unsigned int prepare_for_division_32(const unsigned int divisor){
/*
 * * Mathematical formula, used for unsigned division with fixed divisor:
* (From Terje Mathisen, unpublished)
* x = dividend
* d = divisor
* w = integer word size, bits
* b = floor(log2(d)) = bit_scan_reverse(d)
* f = 2^(w+b) / d                                [exact division]
* If f is an integer then d is a power of 2 then go to case A
* If the fractional part of f is < 0.5 then go to case B
* If the fractional part of f is > 0.5 then go to case C
* Case A:  [shift only]
* result = x >> b
* Case B:  [round down f and compensate by adding one to x]
* result = ((x+1)*floor(f)) >> (w+b)             [high part of unsigned multiplication with 2w bits]
* Case C:  [round up f, no compensation for rounding error]
* result = (x*ceil(f)) >> (w+b)                  [high part of unsigned multiplication with 2w bits]
*
 */
__m256i f_vec;
unsigned int tmp;


if (divisor==0){
	printf("\n cannot divide with zero, process is terminated");
	exit(EXIT_FAILURE);
}
else if (divisor==1)
	return 4;


const unsigned int w=32; //integer word size in bits of the division operands
b = (unsigned int) floorf(log2f(divisor));
double f=pow(2,w+b)/divisor; //printf("\n f=%f, w+b=%d, div=%d",f,w+b,divisor);
unsigned long int integer_part_f = (unsigned long int) f;
double float_part_f = f - integer_part_f; //printf("\n int part=%lu, Fp part %f",integer_part_f, float_part_f);


if (float_part_f < 0.0001){ //if f==0.0
	return 1; //case A
}
else if (float_part_f < 0.5){
	tmp=(unsigned int) floor(f); //printf("\n tmp=%u, %f",tmp,floor(f));
	f_vec = _mm256_set1_epi32(tmp);
	_mm256_store_si256( (__m256i *) &f_vector[0],f_vec);
	return 2; //case B
}
else{
	tmp=(unsigned int) ceil(f); //printf("\n tmp=%u",tmp);
	f_vec = _mm256_set1_epi32(tmp);
	_mm256_store_si256( (__m256i *) &f_vector[0],f_vec);
	return 3; //case C
}


}


inline __m256i division(const unsigned int division_case, __m256i m2, const __m256i f){
	__m256i m1,m3;

	if (division_case==1) { //case A
		return (_mm256_srli_epi16(m2, b));            // shift right logical with b
	}
	else if (division_case==2) { //case B
		m2 = _mm256_add_epi16(m2, _mm256_set1_epi16(1)); //m2=m2+1

		m3 = _mm256_mulhi_epu16(m2, f);             // multiply high unsigned words
		m1 = _mm256_sub_epi16(m2, m3);                     // subtract
		m1 = _mm256_srli_epi16(m1, 16);             // shift right logical with w
		m3 = _mm256_add_epi16(m3, m1);                    // add
		return (_mm256_srli_epi16(m3, b));            // shift right logical with b
	}
	else if (division_case==3){//case C
		m3 = _mm256_mulhi_epu16(m2, f);             // multiply high unsigned words
		m1 = _mm256_sub_epi16(m2, m3);                     // subtract
		m1 = _mm256_srli_epi16(m1, 16);             // shift right logical with w
		m3 = _mm256_add_epi16(m3, m1);                    // add
		return (_mm256_srli_epi16(m3, b));            // shift right logical with b
	}
	else //division with 1
		return m2;

}




inline __m256i division_32(const unsigned int division_case, __m256i m2, const __m256i f){
	__m256i m1,m3;

	if (division_case==1) { //case A
		return (_mm256_srli_epi32(m2, b));            // shift right logical with b
	}
	else if (division_case==2) { //case B
		m2 = _mm256_add_epi32(m2, _mm256_set1_epi32(1)); //m2=m2+1

	    m1 = _mm256_mul_epu32(m2, f);               // 32x32->64 bit unsigned multiplication of a[0] and a[2]
	    m1 = _mm256_srli_epi64(m1, 32);                   // high dword of result 0 and 2
	    m3 = _mm256_srli_epi64(m2, 32);                    // get a[1] and a[3] into position for multiplication
	    m3 = _mm256_mul_epu32(m3, f);              // 32x32->64 bit unsigned multiplication of a[1] and a[3]
	    m3 = _mm256_blend_epi16(m1, m3, 0xCC);            // blend two results
	    m1 = _mm256_sub_epi32(m2, m3);                     // subtract
	    m1 = _mm256_srli_epi32(m1, 32);             // shift right logical
	    m1 = _mm256_add_epi32(m3, m1);                   // add
	    return        _mm256_srli_epi32(m1, b);           // shift right logical
	}
	else if (division_case==3) {//case C
	    m1 = _mm256_mul_epu32(m2, f);               // 32x32->64 bit unsigned multiplication of a[0] and a[2]
	    m1 = _mm256_srli_epi64(m1, 32);                   // high dword of result 0 and 2
	    m3 = _mm256_srli_epi64(m2, 32);                    // get a[1] and a[3] into position for multiplication
	    m3 = _mm256_mul_epu32(m3, f);              // 32x32->64 bit unsigned multiplication of a[1] and a[3]
	    m3 = _mm256_blend_epi16(m1, m3, 0xCC);            // blend two results
	    m1 = _mm256_sub_epi32(m2, m3);                     // subtract
	    m1 = _mm256_srli_epi32(m1, 32);             // shift right logical
	    m1 = _mm256_add_epi32(m3, m1);                   // add
	    return        _mm256_srli_epi32(m1, b);           // shift right logical
	}
	else //division with 1
		return m2;

}



//for loop reminder<16 only
int loop_reminder_low_reminder_values_seperable(unsigned char **frame1,unsigned char **filt,const unsigned int M, const unsigned int N,const unsigned int row, const unsigned int col, const unsigned int REMINDER_ITERATIONS,const unsigned int division_case, const __m256i cx,const __m256i cx_sh1,const __m256i cx_sh2,const __m256i cx_sh3,const __m256i cx_sh4,const __m256i cx_sh5,const __m256i cy0, const __m256i cy1,const __m256i cy2,const __m256i cy0_sh1,const __m256i cy1_sh1, const __m256i cy2_sh1,const __m256i f ){

register	__m256i r0,r1,r2,r3,r4,m0,m1,m2,m3,m4,even,odd;

//const __m256i mask2  = _mm256_set_epi8(0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0);
const __m256i mask3  = _mm256_set_epi16(0,0,0,0,0,0,0,65535,0,0,0,0,0,0,0,0);

	//const __m256i f  = _mm256_set_epi16(52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759);


 __m256i reminder_mask1;//,reminder_mask2;

if (REMINDER_ITERATIONS == 0){
	return 0; //no loop reminder is needed
}

	reminder_mask1=_mm256_load_si256( (__m256i *) &reminder_msk1[REMINDER_ITERATIONS-1][0]);
	//reminder_mask2=_mm256_load_si256( (__m256i *) &reminder_msk2[REMINDER_ITERATIONS-1][0]);


	 //1st col iteration
	//load the 5 rows
	r0=_mm256_loadu_si256( (__m256i *) &frame1[row-2][col-2]);
	r1=_mm256_loadu_si256( (__m256i *) &frame1[row-1][col-2]);
	r2=_mm256_loadu_si256( (__m256i *) &frame1[row][col-2]);
	r3=_mm256_loadu_si256( (__m256i *) &frame1[row+1][col-2]);
	r4=_mm256_loadu_si256( (__m256i *) &frame1[row+2][col-2]);

	/*
	 * The above load operations load outside of the array bounds; these elements are filled with zeros just after
	 * Furthermore, I add two extra zeros in the end to compute col=N-2 and col=N-1. The last value of col is N-1, not N
	 */

	//AND r0-r4 with reminder_mask
	r0=_mm256_and_si256(r0,reminder_mask1);
	r1=_mm256_and_si256(r1,reminder_mask1);
	r2=_mm256_and_si256(r2,reminder_mask1);
	r3=_mm256_and_si256(r3,reminder_mask1);
	r4=_mm256_and_si256(r4,reminder_mask1);

	  //y filter begins

		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,cy0);
		m1=_mm256_maddubs_epi16(r1,cy1);
		m2=_mm256_maddubs_epi16(r2,cy2);
		m3=_mm256_maddubs_epi16(r3,cy1);
		m4=_mm256_maddubs_epi16(r4,cy0);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m2=_mm256_add_epi16(m2,m3);
		m0=_mm256_add_epi16(m0,m2);
		m0=_mm256_add_epi16(m0,m4);

		even=division(division_case,m0,f);//even results

		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,cy0_sh1);
		m1=_mm256_maddubs_epi16(r1,cy1_sh1);
		m2=_mm256_maddubs_epi16(r2,cy2_sh1);
		m3=_mm256_maddubs_epi16(r3,cy1_sh1);
		m4=_mm256_maddubs_epi16(r4,cy0_sh1);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m2=_mm256_add_epi16(m2,m3);
		m0=_mm256_add_epi16(m0,m2);
		m0=_mm256_add_epi16(m0,m4);

		odd=division(division_case,m0,f);//odd results

		//pack to one register
		odd=_mm256_slli_si256(odd,1); //shift one position right
		r0=_mm256_add_epi8(even,odd); //add the odd with the even

		//y filter ends - r0 has the data to be processed by x filter

	//multiply with the mask
	m0=_mm256_maddubs_epi16(r0,cx);

	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);

	m1=_mm256_srli_si256(m0,4);
	m2=_mm256_add_epi16(m1,m2);

	//prepei na prosthesw to pixel pu thelw sto r2 kai meta na kanw diairesi me ola mazi
	m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
	m2=_mm256_add_epi16(m2,m4);

	//now division of m2/divisor follows
	m2=division(division_case,m2,f);


if (REMINDER_ITERATIONS > 12){
	filt[row][col] = (unsigned char) _mm256_extract_epi16(m2,0);
	filt[row][col+6] = (unsigned char) _mm256_extract_epi16(m2,3);
	filt[row][col+12] = (unsigned char) _mm256_extract_epi16(m2,6);
}
else if (REMINDER_ITERATIONS > 6){
	filt[row][col] = (unsigned char) _mm256_extract_epi16(m2,0);
	filt[row][col+6] = (unsigned char) _mm256_extract_epi16(m2,3);
}
else if (REMINDER_ITERATIONS > 0)
	filt[row][col] = (unsigned char) _mm256_extract_epi16(m2,0);

/*
if (REMINDER_ITERATIONS > 18)
	filt[row][col+18] = (unsigned char) _mm256_extract_epi16(m2,9);

if (REMINDER_ITERATIONS > 24)
	filt[row][col+24] = (unsigned char) _mm256_extract_epi16(m2,12);
*/


if (REMINDER_ITERATIONS ==1)
	return 0;


    //2nd col iteration

	//multiply with the mask
m0=_mm256_maddubs_epi16(r0,cx_sh1);

	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);

	m1=_mm256_srli_si256(m0,4);
	m2=_mm256_add_epi16(m1,m2);

	//prepei na prosthesw to pixel pu thelw sto r2 kai meta na kanw diairesi me ola mazi
	m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
	m2=_mm256_add_epi16(m2,m4);

	//now division of m2/divisor follows
	m2=division(division_case,m2,f);




if (REMINDER_ITERATIONS > 13){
	filt[row][col+1] = (unsigned char) _mm256_extract_epi16(m2,0);
	filt[row][col+6+1] = (unsigned char) _mm256_extract_epi16(m2,3);
	filt[row][col+12+1] = (unsigned char) _mm256_extract_epi16(m2,6);
}
else if (REMINDER_ITERATIONS > 7){
	filt[row][col+1] = (unsigned char) _mm256_extract_epi16(m2,0);
	filt[row][col+6+1] = (unsigned char) _mm256_extract_epi16(m2,3);
}
else if (REMINDER_ITERATIONS > 1)
	filt[row][col+1] = (unsigned char) _mm256_extract_epi16(m2,0);

/*
if (REMINDER_ITERATIONS > 19)
	filt[row][col+18+1] = (unsigned char) _mm256_extract_epi16(m2,9);

if (REMINDER_ITERATIONS > 25)
	filt[row][col+24+1] = (unsigned char) _mm256_extract_epi16(m2,12);
*/


if (REMINDER_ITERATIONS ==2)
	return 0;


     //3rd col iteration

	//multiply with the mask
m0=_mm256_maddubs_epi16(r0,cx_sh2);

	//hozizontal additions
	//hadd(0:5)   and store filt[row[col]
	//hadd(6:11)   and store filt[row[col+8]
	//hadd(12:17) and store filt[row[col+14]
	//hadd(18:23) and store filt[row[col+20]
	//hadd(24:30) and store filt[row[col+26]

	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);
	m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,_mm256_extract_epi16(m2,8),0,0,0,0,0,0,0);

	m1=_mm256_srli_si256(m0,4);
	m2=_mm256_add_epi16(m1,m2);

	//prepei na prosthesw to pixel pu thelw sto r2 kai meta na kanw diairesi me ola mazi

	m2=_mm256_add_epi16(m2,m4);

	//now division of m2/divisor follows
	m2=division(division_case,m2,f);



if (REMINDER_ITERATIONS > 14){
	filt[row][col+2] = (unsigned char) _mm256_extract_epi16(m2,1);
	filt[row][col+6+2] = (unsigned char) _mm256_extract_epi16(m2,4);
	filt[row][col+12+2] = (unsigned char) _mm256_extract_epi16(m2,7);
}
else if (REMINDER_ITERATIONS > 8){
	filt[row][col+2] = (unsigned char) _mm256_extract_epi16(m2,1);
	filt[row][col+6+2] = (unsigned char) _mm256_extract_epi16(m2,4);
}
else if (REMINDER_ITERATIONS > 2)
	filt[row][col+2] = (unsigned char) _mm256_extract_epi16(m2,1);


/*
if (REMINDER_ITERATIONS > 20)
	filt[row][col+18+2] = (unsigned char) _mm256_extract_epi16(m2,10);

if (REMINDER_ITERATIONS > 26)
	filt[row][col+24+2] = (unsigned char) _mm256_extract_epi16(m2,13);
*/

if (REMINDER_ITERATIONS ==3){
	return 0;
}


	 //4th col iteration
m0=_mm256_maddubs_epi16(r0,cx_sh3);


	m1=_mm256_srli_si256(m0,2);
	m4=_mm256_add_epi16(m1,m0);

	m1=_mm256_srli_si256(m0,4);
	m2=_mm256_add_epi16(m1,m4);

	//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
	//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
	m4=_mm256_and_si256(m4,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions

	m2=_mm256_add_epi16(m2,m4);

	//now division of r2/159 follows
	m2=division(division_case,m2,f);



if (REMINDER_ITERATIONS > 9){
	filt[row][col+3] = (unsigned char) _mm256_extract_epi16(m2,1);
	filt[row][col+6+3] = (unsigned char) _mm256_extract_epi16(m2,4);
}
else if (REMINDER_ITERATIONS > 3)
	filt[row][col+3] = (unsigned char) _mm256_extract_epi16(m2,1);
/*
if (REMINDER_ITERATIONS > 15)
	filt[row][col+12+3] = (unsigned char) _mm256_extract_epi16(m2,6);

if (REMINDER_ITERATIONS > 21)
	filt[row][col+18+3] = (unsigned char) _mm256_extract_epi16(m2,9);

if (REMINDER_ITERATIONS > 27)
	filt[row][col+24+3] = (unsigned char) _mm256_extract_epi16(m2,12);
*/


if (REMINDER_ITERATIONS ==4)
	return 0;


   //5th col iteration

	//multiply with the mask
m0=_mm256_maddubs_epi16(r0,cx_sh4);

	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);

	m1=_mm256_srli_si256(m0,4);
	m2=_mm256_add_epi16(m1,m2);


	//now division of m2/divisor follows
	m2=division(division_case,m2,f);




if (REMINDER_ITERATIONS > 10){
	filt[row][col+1+3] = (unsigned char) _mm256_extract_epi16(m2,2);
	filt[row][col+6+1+3] = (unsigned char) _mm256_extract_epi16(m2,5);
}
else if (REMINDER_ITERATIONS > 4)
	filt[row][col+1+3] = (unsigned char) _mm256_extract_epi16(m2,2);

/*
if (REMINDER_ITERATIONS > 16)
	filt[row][col+12+1+3] = (unsigned char) _mm256_extract_epi16(m2,6);

if (REMINDER_ITERATIONS > 22)
	filt[row][col+18+1+3] = (unsigned char) _mm256_extract_epi16(m2,9);

if (REMINDER_ITERATIONS > 28)
	filt[row][col+24+1+3] = (unsigned char) _mm256_extract_epi16(m2,12);
*/


if (REMINDER_ITERATIONS ==5)
	return 0;


    //6th col iteration

	//multiply with the mask
m0=_mm256_maddubs_epi16(r0,cx_sh5);

m1=_mm256_srli_si256(m0,2);
m2=_mm256_add_epi16(m1,m0);

m1=_mm256_srli_si256(m0,4);
m2=_mm256_add_epi16(m1,m2);


	//now division of m2/divisor follows
	m2=division(division_case,m2,f);



if (REMINDER_ITERATIONS > 11){
	filt[row][col+2+3] = (unsigned char) _mm256_extract_epi16(m2,2);
	filt[row][col+6+2+3] = (unsigned char) _mm256_extract_epi16(m2,5);
}
else if (REMINDER_ITERATIONS > 5)
	filt[row][col+2+3] = (unsigned char) _mm256_extract_epi16(m2,2);
/*
if (REMINDER_ITERATIONS > 17)
	filt[row][col+12+2+3] = (unsigned char) _mm256_extract_epi16(m2,7);

if (REMINDER_ITERATIONS > 23)
	filt[row][col+18+2+3] = (unsigned char) _mm256_extract_epi16(m2,10);

if (REMINDER_ITERATIONS > 29)
	filt[row][col+24+2+3] = (unsigned char) _mm256_extract_epi16(m2,13);
*/
/*
if (REMINDER_ITERATIONS == 31){

	int newPixel = 0;
	int col2=M-1;
	for (int rowOffset=-2; rowOffset<=2; rowOffset++) {
		for (int colOffset=-2; colOffset<=2; colOffset++) {

		   if ( ((row+rowOffset)<N) && ((col2+colOffset)<M) && ((row+rowOffset)>=0) && ((col2+colOffset)>=0) )
              newPixel += frame1[row+rowOffset][col2+colOffset] * gaussianMask[2 + rowOffset][2 + colOffset];

		      }
	        }
filt[row][M-1] = (unsigned char) (newPixel / 159);
}
*/
return 0;

}




//for REMINDER_ITERATIONS >=16 only
int loop_reminder_high_reminder_values_seperable(unsigned char **frame1,unsigned char **filt,const unsigned int M, const unsigned int N,const unsigned int row, const unsigned int col,const unsigned int REMINDER_ITERATIONS,const unsigned int division_case,const __m256i cx,const __m256i cx_sh1,const __m256i cx_sh2,const __m256i cx_sh3,const __m256i cx_sh4,const __m256i cx_sh5,const __m256i cy0, const __m256i cy1,const __m256i cy2,const __m256i cy0_sh1,const __m256i cy1_sh1, const __m256i cy2_sh1,const __m256i f,const unsigned int divisor){

register	__m256i r0,r1,r2,r3,r4,m0,m1,m2,m3,m4,output,even,odd;


		//const __m256i f  = _mm256_set_epi16(52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759);
		const __m256i mask  = _mm256_set_epi8(0,0,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255);
		const __m256i mask2  = _mm256_set_epi8(0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0);
		const __m256i mask3  = _mm256_set_epi16(0,0,0,0,0,0,0,65535,0,0,0,0,0,0,0,0);
//		const __m256i mask4  = _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255,0,0,0,0,0,0,0,0,0,0,0,0);
//		const __m256i mask5  = _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0);
		const __m256i mask6  = _mm256_set_epi8(0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0);

 __m256i reminder_mask1;
		unsigned char temp[5],pix;
		unsigned int col2;
		int newPixel;

	reminder_mask1=_mm256_load_si256( (__m256i *) &reminder_msk1[REMINDER_ITERATIONS-1][0]);
//	reminder_mask2=_mm256_load_si256( (__m256i *) &reminder_msk2[REMINDER_ITERATIONS-1][0]);




	 //1st col iteration
	//load the 5 rows
	r0=_mm256_loadu_si256( (__m256i *) &frame1[row-2][col-2]);
	r1=_mm256_loadu_si256( (__m256i *) &frame1[row-1][col-2]);
	r2=_mm256_loadu_si256( (__m256i *) &frame1[row][col-2]);
	r3=_mm256_loadu_si256( (__m256i *) &frame1[row+1][col-2]);
	r4=_mm256_loadu_si256( (__m256i *) &frame1[row+2][col-2]);

	/*
	 * The above load operations load outside of the array bounds; these elements are filled with zeros just after
	 * Furthermore, I add two extra zeros in the end to compute col=N-2 and col=N-1. The last value of col is N-1, not N
	 */

	//AND r0-r4 with reminder_mask
	r0=_mm256_and_si256(r0,reminder_mask1);
	r1=_mm256_and_si256(r1,reminder_mask1);
	r2=_mm256_and_si256(r2,reminder_mask1);
	r3=_mm256_and_si256(r3,reminder_mask1);
	r4=_mm256_and_si256(r4,reminder_mask1);

	  //y filter begins

			//multiply with the mask
			m0=_mm256_maddubs_epi16(r0,cy0);
			m1=_mm256_maddubs_epi16(r1,cy1);
			m2=_mm256_maddubs_epi16(r2,cy2);
			m3=_mm256_maddubs_epi16(r3,cy1);
			m4=_mm256_maddubs_epi16(r4,cy0);

			//vertical add
			m0=_mm256_add_epi16(m0,m1);
			m2=_mm256_add_epi16(m2,m3);
			m0=_mm256_add_epi16(m0,m2);
			m0=_mm256_add_epi16(m0,m4);

			even=division(division_case,m0,f);//even results

			//multiply with the mask
			m0=_mm256_maddubs_epi16(r0,cy0_sh1);
			m1=_mm256_maddubs_epi16(r1,cy1_sh1);
			m2=_mm256_maddubs_epi16(r2,cy2_sh1);
			m3=_mm256_maddubs_epi16(r3,cy1_sh1);
			m4=_mm256_maddubs_epi16(r4,cy0_sh1);

			//vertical add
			m0=_mm256_add_epi16(m0,m1);
			m2=_mm256_add_epi16(m2,m3);
			m0=_mm256_add_epi16(m0,m2);
			m0=_mm256_add_epi16(m0,m4);

			odd=division(division_case,m0,f);//odd results

			//pack to one register
			odd=_mm256_slli_si256(odd,1); //shift one position right
			r0=_mm256_add_epi8(even,odd); //add the odd with the even

			//y filter ends - r0 has the data to be processed by x filter


	//multiply with the mask
	m0=_mm256_maddubs_epi16(r0,cx);

	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);

	m1=_mm256_srli_si256(m0,4);
	m2=_mm256_add_epi16(m1,m2);

	//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
	//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
	m4=_mm256_and_si256(m0,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions

	m2=_mm256_add_epi16(m2,m4);

	//now division of r2/159 follows
	m2=division(division_case,m2,f);

	//and with mask to keep just 0,6,12,18,24
	output = _mm256_and_si256(m2,mask);


	//2nd col iteration
	//multiply with the mask
	m0=_mm256_maddubs_epi16(r0,cx_sh1);

	//hozizontal additions
	//hadd(0:5)   and store filt[row[col]
	//hadd(6:11)   and store filt[row[col+8]
	//hadd(12:17) and store filt[row[col+14]
	//hadd(18:23) and store filt[row[col+20]
	//hadd(24:30) and store filt[row[col+26]

	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);

	m1=_mm256_srli_si256(m0,4);
	m2=_mm256_add_epi16(m1,m2);

	//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
	//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
	m4=_mm256_and_si256(m0,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions
	m2=_mm256_add_epi16(m2,m4);

	//now division of r2/159 follows
	m2=division(division_case,m2,f);

	//and with mask to keep just 0,6,12,18,24
	m2 = _mm256_and_si256(m2,mask);
	m2 = _mm256_slli_si256(m2,1);
	output = _mm256_add_epi8(output,m2);


             //3rd col iteration
	//multiply with the mask
	m0=_mm256_maddubs_epi16(r0,cx_sh2);

	//hozizontal additions
	//hadd(0:5)   and store filt[row[col]
	//hadd(6:11)   and store filt[row[col+8]
	//hadd(12:17) and store filt[row[col+14]
	//hadd(18:23) and store filt[row[col+20]
	//hadd(24:30) and store filt[row[col+26]

	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);

	//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,_mm256_extract_epi16(m2,8),0,0,0,0,0,0,0);
	m4=_mm256_and_si256(m2,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions

	m1=_mm256_srli_si256(m0,4);
	m2=_mm256_add_epi16(m1,m2);

	//prepei na prosthesw to pixel pu thelw sto r2 kai meta na kanw diairesi me ola mazi

	m2=_mm256_add_epi16(m2,m4);

	//now division of r2/159 follows
	m2=division(division_case,m2,f);

	//and with mask to keep just 2,8,14,20,26
	m2 = _mm256_and_si256(m2,mask2);
	output = _mm256_add_epi8(output,m2);

	//4th col iteration
	m0=_mm256_maddubs_epi16(r0,cx_sh3);

	m1=_mm256_srli_si256(m0,2);
	m4=_mm256_add_epi16(m1,m0);

	m1=_mm256_srli_si256(m0,4);
	m2=_mm256_add_epi16(m1,m4);

	//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
	//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
	m4=_mm256_and_si256(m4,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions

	m2=_mm256_add_epi16(m2,m4);

	//now division of r2/159 follows
	m2=division(division_case,m2,f);


	//and with mask to keep just 0,6,12,18,24
	m2 = _mm256_and_si256(m2,mask2);
	m2 = _mm256_slli_si256(m2,1);
	output = _mm256_add_epi8(output,m2);




	//5th col iteration
	//multiply with the mask
	m0=_mm256_maddubs_epi16(r0,cx_sh4);


	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);

	m1=_mm256_srli_si256(m0,4);
	m2=_mm256_add_epi16(m1,m2);


	//now division of r2/159 follows
	m2=division(division_case,m2,f);

	//and with mask to keep just 0,6,12,18,24
	m2 = _mm256_and_si256(m2,mask6);
	//m1 = _mm256_slli_si256(m2,4);
	output = _mm256_add_epi8(output,m2);

             //6th col iteration
	//multiply with the mask
	m0=_mm256_maddubs_epi16(r0,cx_sh5);


	m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		m1=_mm256_srli_si256(m0,4);
		m2=_mm256_add_epi16(m1,m2);

		//now division of r2/159 follows
		m2=division(division_case,m2,f);

		//and with mask to keep just 2,8,14,20,26
		m2 = _mm256_and_si256(m2,mask6);
		m1 = _mm256_slli_si256(m2,1);
		output = _mm256_add_epi8(output,m1);

	//_mm256_storeu_si256( (__m256i *) &filt[row][col],output);
	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output, 0)); //store low 128bit - 16pixels

	switch (REMINDER_ITERATIONS){
	case 16:
	  break;
	case 17:
		filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output,16);
		  break;
	case 18:
		filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output,16);
		filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output,17);
		  break;
	case 19:
		filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output,16);
		filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output,17);
		filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output,18);
		  break;
	case 20:
		filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output,16);
		filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output,17);
		filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output,18);
		filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output,19);
		  break;
	case 21:
		filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output,16);
		filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output,17);
		filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output,18);
		filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output,19);
		filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output,20);
		  break;
	case 22:
		filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output,16);
		filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output,17);
		filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output,18);
		filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output,19);
		filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output,20);
		filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output,21);
		  break;
	case 23:
		filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output,16);
		filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output,17);
		filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output,18);
		filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output,19);
		filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output,20);
		filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output,21);
		filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output,22);
		  break;
	case 24:
		//_mm_storeu_si64( (__m128i *) &filt[row][col+16],_mm256_extractf128_si256(output, 0)); //store low 128bit - 16pixels
		filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output,16);
		filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output,17);
		filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output,18);
		filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output,19);
		filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output,20);
		filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output,21);
		filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output,22);
		filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output,23);
		  break;
	case 25:
		filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output,16);
		filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output,17);
		filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output,18);
		filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output,19);
		filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output,20);
		filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output,21);
		filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output,22);
		filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output,23);
		filt[row][col+24] = (unsigned char) _mm256_extract_epi8(output,24);
		  break;
	case 26:
		filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output,16);
		filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output,17);
		filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output,18);
		filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output,19);
		filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output,20);
		filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output,21);
		filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output,22);
		filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output,23);
		filt[row][col+24] = (unsigned char) _mm256_extract_epi8(output,24);
		filt[row][col+25] = (unsigned char) _mm256_extract_epi8(output,25);
		  break;
	case 27:
		filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output,16);
		filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output,17);
		filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output,18);
		filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output,19);
		filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output,20);
		filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output,21);
		filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output,22);
		filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output,23);
		filt[row][col+24] = (unsigned char) _mm256_extract_epi8(output,24);
		filt[row][col+25] = (unsigned char) _mm256_extract_epi8(output,25);
		filt[row][col+26] = (unsigned char) _mm256_extract_epi8(output,26);
		  break;
	case 28:
		filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output,16);
		filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output,17);
		filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output,18);
		filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output,19);
		filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output,20);
		filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output,21);
		filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output,22);
		filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output,23);
		filt[row][col+24] = (unsigned char) _mm256_extract_epi8(output,24);
		filt[row][col+25] = (unsigned char) _mm256_extract_epi8(output,25);
		filt[row][col+26] = (unsigned char) _mm256_extract_epi8(output,26);
		filt[row][col+27] = (unsigned char) _mm256_extract_epi8(output,27);
		  break;
	case 29:
		filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output,16);
		filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output,17);
		filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output,18);
		filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output,19);
		filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output,20);
		filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output,21);
		filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output,22);
		filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output,23);
		filt[row][col+24] = (unsigned char) _mm256_extract_epi8(output,24);
		filt[row][col+25] = (unsigned char) _mm256_extract_epi8(output,25);
		filt[row][col+26] = (unsigned char) _mm256_extract_epi8(output,26);
		filt[row][col+27] = (unsigned char) _mm256_extract_epi8(output,27);

		//I need to calculate the y filter for elements  prior to the M-3, otherwise the x filter cannot work
	for (col2 = col+26; col2 <=col+28; col2++) {
		newPixel = 0;
		for (int offs=-2; offs<=2; offs++) {

               newPixel += frame1[row+offs][col2] * gaussianMask_1d[2 + offs];

			}

	temp[col2-(col+26)] = (unsigned char) (newPixel / divisor);

	}
	temp[3]=0;temp[4]=0;

	for (col2 = col+28; col2 <= col+28; col2++) {
					newPixel = 0;
					for (int off=-2; off<=2; off++) {
						//printf("\n %d",col2-(col+26)+off);
							if ( (col2-(col+26)+off>=3) )
								pix=0;
							else
								pix=temp[col2-(col+26)+off];

		                   newPixel += pix * gaussianMask_1d[2 + off];
						}

				filt[row][col2] = (unsigned char) (newPixel / divisor);

				}

		  break;
	case 30:
		filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output,16);
		filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output,17);
		filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output,18);
		filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output,19);
		filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output,20);
		filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output,21);
		filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output,22);
		filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output,23);
		filt[row][col+24] = (unsigned char) _mm256_extract_epi8(output,24);
		filt[row][col+25] = (unsigned char) _mm256_extract_epi8(output,25);
		filt[row][col+26] = (unsigned char) _mm256_extract_epi8(output,26);
		filt[row][col+27] = (unsigned char) _mm256_extract_epi8(output,27);

		//I need to calculate the y filter for elements  prior to the M-3, otherwise the x filter cannot work
	for (col2 = col+26; col2 <=col+29; col2++) {
		newPixel = 0;
		for (int offs=-2; offs<=2; offs++) {

               newPixel += frame1[row+offs][col2] * gaussianMask_1d[2 + offs];

			}

	temp[col2-(col+26)] = (unsigned char) (newPixel / divisor);
	}
	temp[5]=0;

	for (col2 = col+28; col2 <= col+29; col2++) {
					newPixel = 0;
					for (int off=-2; off<=2; off++) {
						//printf("\n %d",col2-(col+26)+off);
							if ( (col2-(col+26)+off>=4) )
								pix=0;
							else
								pix=temp[col2-(col+26)+off];

		                   newPixel += pix * gaussianMask_1d[2 + off];
						}

				filt[row][col2] = (unsigned char) (newPixel / divisor);

				}

		  break;
	case 31:
		filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output,16);
		filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output,17);
		filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output,18);
		filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output,19);
		filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output,20);
		filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output,21);
		filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output,22);
		filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output,23);
		filt[row][col+24] = (unsigned char) _mm256_extract_epi8(output,24);
		filt[row][col+25] = (unsigned char) _mm256_extract_epi8(output,25);
		filt[row][col+26] = (unsigned char) _mm256_extract_epi8(output,26);
		filt[row][col+27] = (unsigned char) _mm256_extract_epi8(output,27);

			//I need to calculate the y filter for elements  prior to the M-3, otherwise the x filter cannot work
		for (col2 = col+26; col2 <=col+30; col2++) {
			newPixel = 0;
			for (int offs=-2; offs<=2; offs++) {

                   newPixel += frame1[row+offs][col2] * gaussianMask_1d[2 + offs];

				}

		temp[col2-(col+26)] = (unsigned char) (newPixel / divisor);

		}

		for (col2 = col+28; col2 <= col+30; col2++) {
						newPixel = 0;
						for (int off=-2; off<=2; off++) {
							//printf("\n %d",col2-(col+26)+off);
								if ( (col2-(col+26)+off>=5) )
									pix=0;
								else
									pix=temp[col2-(col+26)+off];

			                   newPixel += pix * gaussianMask_1d[2 + off];
							}

					filt[row][col2] = (unsigned char) (newPixel / divisor);

					}



		  break;
	default:
		printf("\n something went wrong");
	}

return 0;

}




void Gaussian_Blur_optimized_5x5_16(unsigned char **frame1,unsigned char **filt,const unsigned int M, const unsigned int N, const unsigned short int divisor, signed char **filter5x5){

const signed char f00=filter5x5[0][0];
const signed char f01=filter5x5[0][1];
const signed char f02=filter5x5[0][2];
const signed char f03=filter5x5[0][3];
const signed char f04=filter5x5[0][4];

const signed char f10=filter5x5[1][0];
const signed char f11=filter5x5[1][1];
const signed char f12=filter5x5[1][2];
const signed char f13=filter5x5[1][3];
const signed char f14=filter5x5[1][4];

const signed char f20=filter5x5[2][0];
const signed char f21=filter5x5[2][1];
const signed char f22=filter5x5[2][2];
const signed char f23=filter5x5[2][3];
const signed char f24=filter5x5[2][4];


	const __m256i c0=_mm256_set_epi8(0,0,0,f04,f03,f02,f01,f00,0,f04,f03,f02,f01,f00,0,f04,f03,f02,f01,f00,0,f04,f03,f02,f01,f00,0,f04,f03,f02,f01,f00);
	const __m256i c1=_mm256_set_epi8(0,0,0,f14,f13,f12,f11,f10,0,f14,f13,f12,f11,f10,0,f14,f13,f12,f11,f10,0,f14,f13,f12,f11,f10,0,f14,f13,f12,f11,f10);
	const __m256i c2=_mm256_set_epi8(0,0,0,f24,f23,f22,f21,f20,0,f24,f23,f22,f21,f20,0,f24,f23,f22,f21,f20,0,f24,f23,f22,f21,f20,0,f24,f23,f22,f21,f20);

	const __m256i c0_sh1=_mm256_set_epi8(0,0,f04,f03,f02,f01,f00,0,f04,f03,f02,f01,f00,0,f04,f03,f02,f01,f00,0,f04,f03,f02,f01,f00,0,f04,f03,f02,f01,f00,0);
	const __m256i c1_sh1=_mm256_set_epi8(0,0,f14,f13,f12,f11,f10,0,f14,f13,f12,f11,f10,0,f14,f13,f12,f11,f10,0,f14,f13,f12,f11,f10,0,f14,f13,f12,f11,f10,0);
	const __m256i c2_sh1=_mm256_set_epi8(0,0,f24,f23,f22,f21,f20,0,f24,f23,f22,f21,f20,0,f24,f23,f22,f21,f20,0,f24,f23,f22,f21,f20,0,f24,f23,f22,f21,f20,0);

	const __m256i c0_sh2=_mm256_set_epi8(0,f04,f03,f02,f01,f00,0,f04,f03,f02,f01,f00,0,f04,f03,f02,f01,f00,0,f04,f03,f02,f01,f00,0,f04,f03,f02,f01,f00,0,0);
	const __m256i c1_sh2=_mm256_set_epi8(0,f14,f13,f12,f11,f10,0,f14,f13,f12,f11,f10,0,f14,f13,f12,f11,f10,0,f14,f13,f12,f11,f10,0,f14,f13,f12,f11,f10,0,0);
	const __m256i c2_sh2=_mm256_set_epi8(0,f24,f23,f22,f21,f20,0,f24,f23,f22,f21,f20,0,f24,f23,f22,f21,f20,0,f24,f23,f22,f21,f20,0,f24,f23,f22,f21,f20,0,0);

	const __m256i c0_sh3=_mm256_set_epi8(f04,f03,f02,f01,f00,0,f04,f03,f02,f01,f00,0,f04,f03,f02,f01,f00,0,f04,f03,f02,f01,f00,0,f04,f03,f02,f01,f00,0,0,0);
	const __m256i c1_sh3=_mm256_set_epi8(f14,f13,f12,f11,f10,0,f14,f13,f12,f11,f10,0,f14,f13,f12,f11,f10,0,f14,f13,f12,f11,f10,0,f14,f13,f12,f11,f10,0,0,0);
	const __m256i c2_sh3=_mm256_set_epi8(f24,f23,f22,f21,f20,0,f24,f23,f22,f21,f20,0,f24,f23,f22,f21,f20,0,f24,f23,f22,f21,f20,0,f24,f23,f22,f21,f20,0,0,0);

	const __m256i c0_sh4=_mm256_set_epi8(0,0,0,0,0,f04,f03,f02,f01,f00,0,f04,f03,f02,f01,f00,0,f04,f03,f02,f01,f00,0,f04,f03,f02,f01,f00,0,0,0,0);
	const __m256i c1_sh4=_mm256_set_epi8(0,0,0,0,0,f14,f13,f12,f11,f10,0,f14,f13,f12,f11,f10,0,f14,f13,f12,f11,f10,0,f14,f13,f12,f11,f10,0,0,0,0);
	const __m256i c2_sh4=_mm256_set_epi8(0,0,0,0,0,f24,f23,f22,f21,f20,0,f24,f23,f22,f21,f20,0,f24,f23,f22,f21,f20,0,f24,f23,f22,f21,f20,0,0,0,0);

	const __m256i c0_sh5=_mm256_set_epi8(0,0,0,0,f04,f03,f02,f01,f00,0,f04,f03,f02,f01,f00,0,f04,f03,f02,f01,f00,0,f04,f03,f02,f01,f00,0,0,0,0,0);
	const __m256i c1_sh5=_mm256_set_epi8(0,0,0,0,f14,f13,f12,f11,f10,0,f14,f13,f12,f11,f10,0,f14,f13,f12,f11,f10,0,f14,f13,f12,f11,f10,0,0,0,0,0);
	const __m256i c2_sh5=_mm256_set_epi8(0,0,0,0,f24,f23,f22,f21,f20,0,f24,f23,f22,f21,f20,0,f24,f23,f22,f21,f20,0,f24,f23,f22,f21,f20,0,0,0,0,0);



	//const __m256i f  = _mm256_set_epi16(52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759);
	//const __m256i mask  = _mm256_set_epi8(0,0,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255);
	//const __m256i mask2  = _mm256_set_epi8(0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0);
	//const __m256i mask6  = _mm256_set_epi8(0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0);

	const __m256i mask3  = _mm256_set_epi16(0,0,0,0,0,0,0,65535,0,0,0,0,0,0,0,0);
	const __m256i mask_prelude  = _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0);

	const __m256i output_mask      = _mm256_set_epi16(0,0,0,65535,0,0,65535,0,0,65535,0,0,65535,0,0,65535);
	const __m256i output_mask_sh1  = _mm256_set_epi16(0,0,65535,0,0,65535,0,0,65535,0,0,65535,0,0,65535,0);
	const __m256i output_mask_sh2  = _mm256_set_epi16(0,0,0,    0,65535,0,0,65535,0,0,65535,0,0,65535,0,0);



	const unsigned int REMINDER_ITERATIONS =  (M-((((M-32)/28)*28)+28)); //M-(last_col_value+28)
	//printf("\n%d",REMINDER_ITERATIONS);


	const unsigned int division_case=prepare_for_division(divisor); //determine which is the division case (A, B or C)
	const __m256i f = _mm256_load_si256( (__m256i *) &f_vector[0]);// initialize the division vector

//printf("\n%d %d",division_case,b);


#pragma omp parallel
{

unsigned int row,col;
register __m256i r0,r1,r2,r3,r4,m0,m1,m2,m3,m4;
__m256i output_row0_even,output_row1_even,output_row0_odd,output_row1_odd; //these are for processing row #0 and #1 only.
__m256i output_even,output_odd;
__m256i m0_prelude_row0,m0_prelude_row1;


/*---------------------- Gaussian Blur ---------------------------------*/

    #pragma omp for schedule(static)
	for (row = 2; row < N-2; row++) {

		if (row==2){//in this case I calculate filt[0][:] and filt[1][:] too. No extra loads or multiplications are required for this
			  for (col = 0; col <= M-32; col+=28){

				  if (col==0){//this is a special case as the mask gets outside of the array (it is like adding two zeros in the beginning of frame[][]

						//load the 5 rows
						r0=_mm256_loadu_si256( (__m256i *) &frame1[row-2][0]);
						r1=_mm256_loadu_si256( (__m256i *) &frame1[row-1][0]);
						r2=_mm256_loadu_si256( (__m256i *) &frame1[row][0]);
						r3=_mm256_loadu_si256( (__m256i *) &frame1[row+1][0]);
						r4=_mm256_loadu_si256( (__m256i *) &frame1[row+2][0]);

				  			//START - extra code needed for prelude
				  					m0=_mm256_slli_si256(r0,2);//shift 2 elements left - equivalent to filling with two zeros inthe beginning
				  					m1=_mm256_slli_si256(r1,2);//shift 2 elements left - equivalent to filling with two zeros inthe beginning
				  					m2=_mm256_slli_si256(r2,2);//shift 2 elements left - equivalent to filling with two zeros inthe beginning
				  					m3=_mm256_slli_si256(r3,2);//shift 2 elements left - equivalent to filling with two zeros inthe beginning
				  					m4=_mm256_slli_si256(r4,2);//shift 2 elements left - equivalent to filling with two zeros inthe beginning

				  					r0=_mm256_and_si256(r0,mask_prelude);
				  					r0=_mm256_permute2f128_si256(r0,r0,1);
				  					r0=_mm256_srli_si256(r0,14);//shift 14 elements
				  					r0=_mm256_add_epi16(m0,r0);

				  					r1=_mm256_and_si256(r1,mask_prelude);
				  					r1=_mm256_permute2f128_si256(r1,r1,1);
				  					r1=_mm256_srli_si256(r1,14);//shift 14 elements
				  					r1=_mm256_add_epi16(m1,r1);

				  					r2=_mm256_and_si256(r2,mask_prelude);
				  					r2=_mm256_permute2f128_si256(r2,r2,1);
				  					r2=_mm256_srli_si256(r2,14);//shift 14 elements
				  					r2=_mm256_add_epi16(m2,r2);

				  					r3=_mm256_and_si256(r3,mask_prelude);
				  					r3=_mm256_permute2f128_si256(r3,r3,1);
				  					r3=_mm256_srli_si256(r3,14);//shift 14 elements
				  					r3=_mm256_add_epi16(m3,r3);

				  					r4=_mm256_and_si256(r4,mask_prelude);
				  					r4=_mm256_permute2f128_si256(r4,r4,1);
				  					r4=_mm256_srli_si256(r4,14);//shift 14 elements
				  					r4=_mm256_add_epi16(m4,r4);

				  			//END - extra code needed for prelude
				  		}
				  else {
						//load the 5 rows
						r0=_mm256_loadu_si256( (__m256i *) &frame1[row-2][col-2]);
						r1=_mm256_loadu_si256( (__m256i *) &frame1[row-1][col-2]);
						r2=_mm256_loadu_si256( (__m256i *) &frame1[row][col-2]);
						r3=_mm256_loadu_si256( (__m256i *) &frame1[row+1][col-2]);
						r4=_mm256_loadu_si256( (__m256i *) &frame1[row+2][col-2]);
				  }

				//col iteration computes output pixels of 2,8,14,20,26
				// col+1 iteration computes output pixels of 3,9,15,21,27
				// col+2 iteration computes output pixels of 4,10,16,22,28
				// col+3 iteration computes output pixels of 5,11,17,23,29
				// col+4 iteration computes output pixels of 6,12,18,24,30
				// col+5 iteration computes output pixels of 7,13,19,25,31
				//afterwards, col2 becomes 32 and repeat the above process

				//1st col iteration

				  //--------------row=2
				//multiply with the mask
				m0=_mm256_maddubs_epi16(r0,c0);
				m1=_mm256_maddubs_epi16(r1,c1);
				m2=_mm256_maddubs_epi16(r2,c2);
				m3=_mm256_maddubs_epi16(r3,c1);
				m4=_mm256_maddubs_epi16(r4,c0);

				//vertical add
				m0=_mm256_add_epi16(m0,m1);
				m0=_mm256_add_epi16(m0,m2);
				m0=_mm256_add_epi16(m0,m3);
				m0=_mm256_add_epi16(m0,m4);


				m1=_mm256_srli_si256(m0,2);
				m2=_mm256_add_epi16(m1,m0);

				m1=_mm256_srli_si256(m0,4);
				m2=_mm256_add_epi16(m1,m2);

				//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
				//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
				m4=_mm256_and_si256(m0,mask3);
				m4=_mm256_permute2f128_si256(m4,m4,1);
				m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions
				m2=_mm256_add_epi16(m2,m4);

				//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
				output_even=_mm256_and_si256(m2,output_mask);

				//--------------row=0
				//multiply with the mask
				m0=_mm256_maddubs_epi16(r0,c2);
				m1=_mm256_maddubs_epi16(r1,c1);
				m2=_mm256_maddubs_epi16(r2,c0);

				//vertical add
				m0=_mm256_add_epi16(m0,m1);
				m0=_mm256_add_epi16(m0,m2);


				m1=_mm256_srli_si256(m0,2);
				m2=_mm256_add_epi16(m1,m0);

				m1=_mm256_srli_si256(m0,4);
				m2=_mm256_add_epi16(m1,m2);

				//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
				//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
				m4=_mm256_and_si256(m0,mask3);
				m4=_mm256_permute2f128_si256(m4,m4,1);
				m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions
				m2=_mm256_add_epi16(m2,m4);

				//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
				output_row0_even=_mm256_and_si256(m2,output_mask);

				//--------------row=1
				//multiply with the mask
				m0=_mm256_maddubs_epi16(r0,c1);
				m1=_mm256_maddubs_epi16(r1,c2);
				m2=_mm256_maddubs_epi16(r2,c1);
				m3=_mm256_maddubs_epi16(r3,c0);

				//vertical add
				m0=_mm256_add_epi16(m0,m1);
				m0=_mm256_add_epi16(m0,m2);
				m0=_mm256_add_epi16(m0,m3);

				m1=_mm256_srli_si256(m0,2);
				m2=_mm256_add_epi16(m1,m0);

				m1=_mm256_srli_si256(m0,4);
				m2=_mm256_add_epi16(m1,m2);

				//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
				//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
				m4=_mm256_and_si256(m0,mask3);
				m4=_mm256_permute2f128_si256(m4,m4,1);
				m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions
				m2=_mm256_add_epi16(m2,m4);

				//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
				output_row1_even=_mm256_and_si256(m2,output_mask);

				//2nd col iteration
				//multiply with the mask

				//----row=2
				m0=_mm256_maddubs_epi16(r0,c0_sh1);
				m1=_mm256_maddubs_epi16(r1,c1_sh1);
				m2=_mm256_maddubs_epi16(r2,c2_sh1);
				m3=_mm256_maddubs_epi16(r3,c1_sh1);
				m4=_mm256_maddubs_epi16(r4,c0_sh1);

				//vertical add
				m0=_mm256_add_epi16(m0,m1);
				m0=_mm256_add_epi16(m0,m2);
				m0=_mm256_add_epi16(m0,m3);
				m0=_mm256_add_epi16(m0,m4);


				m1=_mm256_srli_si256(m0,2);
				m2=_mm256_add_epi16(m1,m0);

				m1=_mm256_srli_si256(m0,4);
				m2=_mm256_add_epi16(m1,m2);

				//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
				//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
				m4=_mm256_and_si256(m0,mask3);
				m4=_mm256_permute2f128_si256(m4,m4,1);
				m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions
				m2=_mm256_add_epi16(m2,m4);

				//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
				output_odd = _mm256_and_si256(m2,output_mask);

				//----row=0
				m0=_mm256_maddubs_epi16(r0,c2_sh1);
				m1=_mm256_maddubs_epi16(r1,c1_sh1);
				m2=_mm256_maddubs_epi16(r2,c0_sh1);

				//vertical add
				m0=_mm256_add_epi16(m0,m1);
				m0=_mm256_add_epi16(m0,m2);


				m1=_mm256_srli_si256(m0,2);
				m2=_mm256_add_epi16(m1,m0);

				m1=_mm256_srli_si256(m0,4);
				m2=_mm256_add_epi16(m1,m2);

				//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
				//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
				m4=_mm256_and_si256(m0,mask3);
				m4=_mm256_permute2f128_si256(m4,m4,1);
				m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions
				m2=_mm256_add_epi16(m2,m4);

				//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
				output_row0_odd = _mm256_and_si256(m2,output_mask);

				//----row=1
				m0=_mm256_maddubs_epi16(r0,c1_sh1);
				m1=_mm256_maddubs_epi16(r1,c2_sh1);
				m2=_mm256_maddubs_epi16(r2,c1_sh1);
				m3=_mm256_maddubs_epi16(r3,c0_sh1);

				//vertical add
				m0=_mm256_add_epi16(m0,m1);
				m0=_mm256_add_epi16(m0,m2);
				m0=_mm256_add_epi16(m0,m3);


				m1=_mm256_srli_si256(m0,2);
				m2=_mm256_add_epi16(m1,m0);

				m1=_mm256_srli_si256(m0,4);
				m2=_mm256_add_epi16(m1,m2);

				//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
				//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
				m4=_mm256_and_si256(m0,mask3);
				m4=_mm256_permute2f128_si256(m4,m4,1);
				m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions
				m2=_mm256_add_epi16(m2,m4);

				//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
				output_row1_odd = _mm256_and_si256(m2,output_mask);

		                 //3rd col iteration
				//---row=2
				//multiply with the mask
				m0=_mm256_maddubs_epi16(r0,c0_sh2);
				m1=_mm256_maddubs_epi16(r1,c1_sh2);
				m2=_mm256_maddubs_epi16(r2,c2_sh2);
				m3=_mm256_maddubs_epi16(r3,c1_sh2);
				m4=_mm256_maddubs_epi16(r4,c0_sh2);

				//vertical add
				m0=_mm256_add_epi16(m0,m1);
				m0=_mm256_add_epi16(m0,m2);
				m0=_mm256_add_epi16(m0,m3);
				m0=_mm256_add_epi16(m0,m4);


				m1=_mm256_srli_si256(m0,2);
				m2=_mm256_add_epi16(m1,m0);

				//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,_mm256_extract_epi16(m2,8),0,0,0,0,0,0,0);
				m4=_mm256_and_si256(m2,mask3);
				m4=_mm256_permute2f128_si256(m4,m4,1);
				m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions

				m1=_mm256_srli_si256(m0,4);
				m2=_mm256_add_epi16(m1,m2);
				m2=_mm256_add_epi16(m2,m4);

				//m2 has 16 16bit values now. the results I need are in positions 1,4,7,10,13. keep only those, discard others
				m2 = _mm256_and_si256(m2,output_mask_sh1);
				output_even = _mm256_add_epi16(output_even,m2);

				//---row=0
				//multiply with the mask

				m0=_mm256_maddubs_epi16(r0,c2_sh2);
				m1=_mm256_maddubs_epi16(r1,c1_sh2);
				m2=_mm256_maddubs_epi16(r2,c0_sh2);

				//vertical add
				m0=_mm256_add_epi16(m0,m1);
				m0=_mm256_add_epi16(m0,m2);


				m1=_mm256_srli_si256(m0,2);
				m2=_mm256_add_epi16(m1,m0);

				//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,_mm256_extract_epi16(m2,8),0,0,0,0,0,0,0);
				m4=_mm256_and_si256(m2,mask3);
				m4=_mm256_permute2f128_si256(m4,m4,1);
				m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions

				m1=_mm256_srli_si256(m0,4);
				m2=_mm256_add_epi16(m1,m2);

				//prepei na prosthesw to pixel pu thelw sto r2 kai meta na kanw diairesi me ola mazi

				m2=_mm256_add_epi16(m2,m4);

				//m2 has 16 16bit values now. the results I need are in positions 1,4,7,10,13. keep only those, discard others
				m2 = _mm256_and_si256(m2,output_mask_sh1);
				output_row0_even = _mm256_add_epi16(output_row0_even,m2);


				//---row=1
				//multiply with the mask

				m0=_mm256_maddubs_epi16(r0,c1_sh2);
				m1=_mm256_maddubs_epi16(r1,c2_sh2);
				m2=_mm256_maddubs_epi16(r2,c1_sh2);
				m3=_mm256_maddubs_epi16(r3,c0_sh2);

				//vertical add
				m0=_mm256_add_epi16(m0,m1);
				m0=_mm256_add_epi16(m0,m2);
				m0=_mm256_add_epi16(m0,m3);

				m1=_mm256_srli_si256(m0,2);
				m2=_mm256_add_epi16(m1,m0);

				//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,_mm256_extract_epi16(m2,8),0,0,0,0,0,0,0);
				m4=_mm256_and_si256(m2,mask3);
				m4=_mm256_permute2f128_si256(m4,m4,1);
				m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions

				m1=_mm256_srli_si256(m0,4);
				m2=_mm256_add_epi16(m1,m2);

				//prepei na prosthesw to pixel pu thelw sto r2 kai meta na kanw diairesi me ola mazi

				m2=_mm256_add_epi16(m2,m4);

				//m2 has 16 16bit values now. the results I need are in positions 1,4,7,10,13. keep only those, discard others
				m2 = _mm256_and_si256(m2,output_mask_sh1);
				output_row1_even = _mm256_add_epi16(output_row1_even,m2);

				//4th col iteration

				//4th col iteration

				//multiply with the mask
				m0=_mm256_maddubs_epi16(r0,c0_sh3);
				m1=_mm256_maddubs_epi16(r1,c1_sh3);
				m2=_mm256_maddubs_epi16(r2,c2_sh3);
				m3=_mm256_maddubs_epi16(r3,c1_sh3);
				m4=_mm256_maddubs_epi16(r4,c0_sh3);

				//vertical add
				m0=_mm256_add_epi16(m0,m1);
				m2=_mm256_add_epi16(m2,m3);
				m0=_mm256_add_epi16(m0,m2);
				m0=_mm256_add_epi16(m0,m4);

				//hozizontal additions
				//hadd(3:8)
				//hadd(9:14)
				//hadd(15:20)
				//hadd(21:26)
				//hadd(27:31)
				//result after division will be in 2,8,14,20,26

				m1=_mm256_srli_si256(m0,2);
				m4=_mm256_add_epi16(m1,m0);

				m1=_mm256_srli_si256(m0,4);
				m2=_mm256_add_epi16(m1,m4);

				//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
				//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
				m4=_mm256_and_si256(m4,mask3);
				m4=_mm256_permute2f128_si256(m4,m4,1);
				m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions

				m2=_mm256_add_epi16(m2,m4);

				//m2 has 16 16bit values now. the results I need are in positions 1,4,7,10,13. keep only those, discard others
				m2 = _mm256_and_si256(m2,output_mask_sh1);
				output_odd = _mm256_add_epi16(output_odd,m2);

				//row0
				//multiply with the mask
								m0=_mm256_maddubs_epi16(r0,c2_sh3);
								m1=_mm256_maddubs_epi16(r1,c1_sh3);
								m2=_mm256_maddubs_epi16(r2,c0_sh3);


								//vertical add
								m0=_mm256_add_epi16(m0,m1);
								m0=_mm256_add_epi16(m0,m2);


								m1=_mm256_srli_si256(m0,2);
								m4=_mm256_add_epi16(m1,m0);

								m1=_mm256_srli_si256(m0,4);
								m2=_mm256_add_epi16(m1,m4);

								//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
								//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
								m4=_mm256_and_si256(m4,mask3);
								m4=_mm256_permute2f128_si256(m4,m4,1);
								m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions

								m2=_mm256_add_epi16(m2,m4);

								//m2 has 16 16bit values now. the results I need are in positions 1,4,7,10,13. keep only those, discard others
								m2 = _mm256_and_si256(m2,output_mask_sh1);
								output_row0_odd = _mm256_add_epi16(output_row0_odd,m2);


								//-----------row1
								//multiply with the mask
												m0=_mm256_maddubs_epi16(r0,c1_sh3);
												m1=_mm256_maddubs_epi16(r1,c2_sh3);
												m2=_mm256_maddubs_epi16(r2,c1_sh3);
												m3=_mm256_maddubs_epi16(r3,c0_sh3);

												//vertical add
												m0=_mm256_add_epi16(m0,m1);
												m0=_mm256_add_epi16(m0,m2);
												m0=_mm256_add_epi16(m0,m3);

												m1=_mm256_srli_si256(m0,2);
												m4=_mm256_add_epi16(m1,m0);

												m1=_mm256_srli_si256(m0,4);
												m2=_mm256_add_epi16(m1,m4);

												//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
												//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
												m4=_mm256_and_si256(m4,mask3);
												m4=_mm256_permute2f128_si256(m4,m4,1);
												m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions

												m2=_mm256_add_epi16(m2,m4);

												//m2 has 16 16bit values now. the results I need are in positions 1,4,7,10,13. keep only those, discard others
												m2 = _mm256_and_si256(m2,output_mask_sh1);
												output_row1_odd = _mm256_add_epi16(output_row1_odd,m2);



				//5th col iteration
				//multiply with the mask
				m0=_mm256_maddubs_epi16(r0,c0_sh4);
				m1=_mm256_maddubs_epi16(r1,c1_sh4);
				m2=_mm256_maddubs_epi16(r2,c2_sh4);
				m3=_mm256_maddubs_epi16(r3,c1_sh4);
				m4=_mm256_maddubs_epi16(r4,c0_sh4);

				//vertical add
				m0=_mm256_add_epi16(m0,m1);
				m2=_mm256_add_epi16(m2,m3);
				m0=_mm256_add_epi16(m0,m2);
				m0=_mm256_add_epi16(m0,m4);

				//hozizontal additions
				//hadd(4:9)
				//hadd(10:15)
				//hadd(16:21)
				//hadd(22:27)
				//result after division will be in 4,10,16,22

				m1=_mm256_srli_si256(m0,2);
				m2=_mm256_add_epi16(m1,m0);

				m1=_mm256_srli_si256(m0,4);
				m2=_mm256_add_epi16(m1,m2);


				//m2 has 16 16bit values now. the results I need are in positions 2,5,8,11. keep only those, discard others
				m2 = _mm256_and_si256(m2,output_mask_sh2);
				output_even = _mm256_add_epi16(output_even,m2);

				//row0
				//multiply with the mask
				m0=_mm256_maddubs_epi16(r0,c2_sh4);
				m1=_mm256_maddubs_epi16(r1,c1_sh4);
				m2=_mm256_maddubs_epi16(r2,c0_sh4);


				//vertical add
				m0=_mm256_add_epi16(m0,m1);
				m0=_mm256_add_epi16(m0,m2);


				//hozizontal additions
				//hadd(4:9)
				//hadd(10:15)
				//hadd(16:21)
				//hadd(22:27)
				//result after division will be in 4,10,16,22

				m1=_mm256_srli_si256(m0,2);
				m2=_mm256_add_epi16(m1,m0);

				m1=_mm256_srli_si256(m0,4);
				m2=_mm256_add_epi16(m1,m2);


				//m2 has 16 16bit values now. the results I need are in positions 2,5,8,11. keep only those, discard others
				m2 = _mm256_and_si256(m2,output_mask_sh2);
				output_row0_even = _mm256_add_epi16(output_row0_even,m2);

				//row1
				//multiply with the mask
				m0=_mm256_maddubs_epi16(r0,c1_sh4);
				m1=_mm256_maddubs_epi16(r1,c2_sh4);
				m2=_mm256_maddubs_epi16(r2,c1_sh4);
				m3=_mm256_maddubs_epi16(r3,c0_sh4);


				//vertical add
				m0=_mm256_add_epi16(m0,m1);
				m0=_mm256_add_epi16(m0,m2);
				m0=_mm256_add_epi16(m0,m3);


				m1=_mm256_srli_si256(m0,2);
				m2=_mm256_add_epi16(m1,m0);

				m1=_mm256_srli_si256(m0,4);
				m2=_mm256_add_epi16(m1,m2);


				//m2 has 16 16bit values now. the results I need are in positions 2,5,8,11. keep only those, discard others
				m2 = _mm256_and_si256(m2,output_mask_sh2);
				output_row1_even = _mm256_add_epi16(output_row1_even,m2);


		                 //6th col iteration
				//multiply with the mask
				m0=_mm256_maddubs_epi16(r0,c0_sh5);
				m1=_mm256_maddubs_epi16(r1,c1_sh5);
				m2=_mm256_maddubs_epi16(r2,c2_sh5);
				m3=_mm256_maddubs_epi16(r3,c1_sh5);
				m4=_mm256_maddubs_epi16(r4,c0_sh5);

				//vertical add
				m0=_mm256_add_epi16(m0,m1);
				m2=_mm256_add_epi16(m2,m3);
				m0=_mm256_add_epi16(m0,m2);
				m0=_mm256_add_epi16(m0,m4);

				//hozizontal additions
				//hadd(5:10)
				//hadd(11:16)
				//hadd(17:22)
				//hadd(23:28)
				//result after division will be in 4,10,16,22

				m1=_mm256_srli_si256(m0,2);
				m2=_mm256_add_epi16(m1,m0);

				m1=_mm256_srli_si256(m0,4);
				m2=_mm256_add_epi16(m1,m2);

				//m2 has 16 16bit values now. the results I need are in positions 2,5,8,11. keep only those, discard others
				m2 = _mm256_and_si256(m2,output_mask_sh2);
				output_odd = _mm256_add_epi16(output_odd,m2);

				//now division follows
				output_even=division(division_case,output_even,f);
				output_odd=division(division_case,output_odd,f);

				//shift odd 1 position and add to even
				output_odd = _mm256_slli_si256(output_odd,1);
				output_even = _mm256_add_epi8(output_even,output_odd);

				_mm256_storeu_si256( (__m256i *) &filt[row][col],output_even );

				//row0
				//multiply with the mask
				m0=_mm256_maddubs_epi16(r0,c2_sh5);
				m1=_mm256_maddubs_epi16(r1,c1_sh5);
				m2=_mm256_maddubs_epi16(r2,c0_sh5);

				//vertical add
				m0=_mm256_add_epi16(m0,m1);
				m0=_mm256_add_epi16(m0,m2);


				m1=_mm256_srli_si256(m0,2);
				m2=_mm256_add_epi16(m1,m0);

				m1=_mm256_srli_si256(m0,4);
				m2=_mm256_add_epi16(m1,m2);

				//m2 has 16 16bit values now. the results I need are in positions 2,5,8,11. keep only those, discard others
				m2 = _mm256_and_si256(m2,output_mask_sh2);
				output_row0_odd = _mm256_add_epi16(output_row0_odd,m2);

				//now division follows
				output_row0_even=division(division_case,output_row0_even,f);
				output_row0_odd=division(division_case,output_row0_odd,f);

				//shift odd 1 position and add to even
				output_row0_odd = _mm256_slli_si256(output_row0_odd,1);
				output_row0_even = _mm256_add_epi8(output_row0_even,output_row0_odd);


				_mm256_storeu_si256( (__m256i *) &filt[0][col],output_row0_even );

				//row1

				//multiply with the mask
				m0=_mm256_maddubs_epi16(r0,c1_sh5);
				m1=_mm256_maddubs_epi16(r1,c2_sh5);
				m2=_mm256_maddubs_epi16(r2,c1_sh5);
				m3=_mm256_maddubs_epi16(r3,c0_sh5);

				//vertical add
				m0=_mm256_add_epi16(m0,m1);
				m0=_mm256_add_epi16(m0,m2);
				m0=_mm256_add_epi16(m0,m3);


				m1=_mm256_srli_si256(m0,2);
				m2=_mm256_add_epi16(m1,m0);

				m1=_mm256_srli_si256(m0,4);
				m2=_mm256_add_epi16(m1,m2);

				//m2 has 16 16bit values now. the results I need are in positions 2,5,8,11. keep only those, discard others
				m2 = _mm256_and_si256(m2,output_mask_sh2);
				output_row1_odd = _mm256_add_epi16(output_row1_odd,m2);

				//now division follows
				output_row1_even=division(division_case,output_row1_even,f);
				output_row1_odd=division(division_case,output_row1_odd,f);

				//shift odd 1 position and add to even
				output_row1_odd = _mm256_slli_si256(output_row1_odd,1);
				output_row1_even = _mm256_add_epi8(output_row1_even,output_row1_odd);


				_mm256_storeu_si256( (__m256i *) &filt[1][col],output_row1_even );


				}


		    loop_reminder_first_less_div(frame1,filt,M,N,col,REMINDER_ITERATIONS,division_case,c0,c1,c2,c0_sh1,c1_sh1,c2_sh1,c0_sh2,c1_sh2,c2_sh2,f,divisor,filter5x5);
		}

		else if (row==N-3){//in this case I calculate filt[N-2][:] and filt[N-1][:] too. Below row0 refers to row=N-2 and row1 refers to row=N-1
			for (col = 0; col <= M-32; col+=28){
						 //last col value that does not read outside of the array bounds is (col<M-29)

							  if (col==0){

									//load the 5 rows
									r0=_mm256_loadu_si256( (__m256i *) &frame1[row-2][0]);
									r1=_mm256_loadu_si256( (__m256i *) &frame1[row-1][0]);
									r2=_mm256_loadu_si256( (__m256i *) &frame1[row][0]);
									r3=_mm256_loadu_si256( (__m256i *) &frame1[row+1][0]);
									r4=_mm256_loadu_si256( (__m256i *) &frame1[row+2][0]);

							  			//START - extra code needed for prelude
							  					m0=_mm256_slli_si256(r0,2);//shift 2 elements left - equivalent to filling with two zeros inthe beginning
							  					m1=_mm256_slli_si256(r1,2);//shift 2 elements left - equivalent to filling with two zeros inthe beginning
							  					m2=_mm256_slli_si256(r2,2);//shift 2 elements left - equivalent to filling with two zeros inthe beginning
							  					m3=_mm256_slli_si256(r3,2);//shift 2 elements left - equivalent to filling with two zeros inthe beginning
							  					m4=_mm256_slli_si256(r4,2);//shift 2 elements left - equivalent to filling with two zeros inthe beginning

							  					r0=_mm256_and_si256(r0,mask_prelude);
							  					r0=_mm256_permute2f128_si256(r0,r0,1);
							  					r0=_mm256_srli_si256(r0,14);//shift 14 elements
							  					r0=_mm256_add_epi16(m0,r0);

							  					r1=_mm256_and_si256(r1,mask_prelude);
							  					r1=_mm256_permute2f128_si256(r1,r1,1);
							  					r1=_mm256_srli_si256(r1,14);//shift 14 elements
							  					r1=_mm256_add_epi16(m1,r1);

							  					r2=_mm256_and_si256(r2,mask_prelude);
							  					r2=_mm256_permute2f128_si256(r2,r2,1);
							  					r2=_mm256_srli_si256(r2,14);//shift 14 elements
							  					r2=_mm256_add_epi16(m2,r2);

							  					r3=_mm256_and_si256(r3,mask_prelude);
							  					r3=_mm256_permute2f128_si256(r3,r3,1);
							  					r3=_mm256_srli_si256(r3,14);//shift 14 elements
							  					r3=_mm256_add_epi16(m3,r3);

							  					r4=_mm256_and_si256(r4,mask_prelude);
							  					r4=_mm256_permute2f128_si256(r4,r4,1);
							  					r4=_mm256_srli_si256(r4,14);//shift 14 elements
							  					r4=_mm256_add_epi16(m4,r4);

							  			//END - extra code needed for prelude
							  		}
							  else {
									//load the 5 rows
									r0=_mm256_loadu_si256( (__m256i *) &frame1[row-2][col-2]);
									r1=_mm256_loadu_si256( (__m256i *) &frame1[row-1][col-2]);
									r2=_mm256_loadu_si256( (__m256i *) &frame1[row][col-2]);
									r3=_mm256_loadu_si256( (__m256i *) &frame1[row+1][col-2]);
									r4=_mm256_loadu_si256( (__m256i *) &frame1[row+2][col-2]);
							  }

							//col iteration computes output pixels of 2,8,14,20,26
							// col+1 iteration computes output pixels of 3,9,15,21,27
							// col+2 iteration computes output pixels of 4,10,16,22,28
							// col+3 iteration computes output pixels of 5,11,17,23,29
							// col+4 iteration computes output pixels of 6,12,18,24,30
							// col+5 iteration computes output pixels of 7,13,19,25,31
							//afterwards, col2 becomes 32 and repeat the above process

							//1st col iteration
							 //--------------row=2

							//multiply with the mask
							m0=_mm256_maddubs_epi16(r0,c0);
							m1=_mm256_maddubs_epi16(r1,c1);
							m2=_mm256_maddubs_epi16(r2,c2);
							m3=_mm256_maddubs_epi16(r3,c1);
							m4=_mm256_maddubs_epi16(r4,c0);

							//vertical add
							m4=_mm256_add_epi16(m4,m3);
							m4=_mm256_add_epi16(m4,m2);
							m4=_mm256_add_epi16(m4,m1);
							m0=_mm256_add_epi16(m4,m0);

							m1=_mm256_srli_si256(m0,2);
							m2=_mm256_add_epi16(m1,m0);

							m1=_mm256_srli_si256(m0,4);
							m2=_mm256_add_epi16(m1,m2);

							//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
							//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
							m4=_mm256_and_si256(m0,mask3);
							m4=_mm256_permute2f128_si256(m4,m4,1);
							m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions

							m2=_mm256_add_epi16(m2,m4);

							//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
							output_even=_mm256_and_si256(m2,output_mask);

							//--------------row=0
							//multiply with the mask
							m1=_mm256_maddubs_epi16(r1,c0);
							m2=_mm256_maddubs_epi16(r2,c1);
							m3=_mm256_maddubs_epi16(r3,c2);
							m4=_mm256_maddubs_epi16(r4,c1);

							//vertical add
							m4=_mm256_add_epi16(m4,m3);
							m4=_mm256_add_epi16(m4,m2);
							m4=_mm256_add_epi16(m4,m1); m0_prelude_row0=m4;

							m1=_mm256_srli_si256(m0_prelude_row0,2);
							m2=_mm256_add_epi16(m1,m0_prelude_row0);

							m1=_mm256_srli_si256(m0_prelude_row0,4);
							m2=_mm256_add_epi16(m1,m2);

							//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
							//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
							m4=_mm256_and_si256(m0_prelude_row0,mask3);
							m4=_mm256_permute2f128_si256(m4,m4,1);
							m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions

							m2=_mm256_add_epi16(m2,m4);

							//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
							output_row0_even=_mm256_and_si256(m2,output_mask);

							//--------------row=1
							//multiply with the mask
							m2=_mm256_maddubs_epi16(r2,c0);
							m3=_mm256_maddubs_epi16(r3,c1);
							m4=_mm256_maddubs_epi16(r4,c2);


							//vertical add
							m4=_mm256_add_epi16(m4,m3);
							m4=_mm256_add_epi16(m4,m2);m0_prelude_row1=m4;



							m1=_mm256_srli_si256(m0_prelude_row1,2);
							m2=_mm256_add_epi16(m1,m0_prelude_row1);

							m1=_mm256_srli_si256(m0_prelude_row1,4);
							m2=_mm256_add_epi16(m1,m2);

							//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
							//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
							m4=_mm256_and_si256(m0_prelude_row1,mask3);
							m4=_mm256_permute2f128_si256(m4,m4,1);
							m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions

							m2=_mm256_add_epi16(m2,m4);

							//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
							output_row1_even=_mm256_and_si256(m2,output_mask);

							//2nd col iteration
							//----row=2

							//multiply with the mask
							m0=_mm256_maddubs_epi16(r0,c0_sh1);
							m1=_mm256_maddubs_epi16(r1,c1_sh1);
							m2=_mm256_maddubs_epi16(r2,c2_sh1);
							m3=_mm256_maddubs_epi16(r3,c1_sh1);
							m4=_mm256_maddubs_epi16(r4,c0_sh1);

							//vertical add
							m4=_mm256_add_epi16(m4,m3);
							m4=_mm256_add_epi16(m4,m2);
							m4=_mm256_add_epi16(m4,m1);
							m0=_mm256_add_epi16(m4,m0);


							m1=_mm256_srli_si256(m0,2);
							m2=_mm256_add_epi16(m1,m0);

							m1=_mm256_srli_si256(m0,4);
							m2=_mm256_add_epi16(m1,m2);

							//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
							//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
							m4=_mm256_and_si256(m0,mask3);
							m4=_mm256_permute2f128_si256(m4,m4,1);
							m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions
							m2=_mm256_add_epi16(m2,m4);

							//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
							output_odd = _mm256_and_si256(m2,output_mask);

							//----row=0
							//multiply with the mask
							m1=_mm256_maddubs_epi16(r1,c0_sh1);
							m2=_mm256_maddubs_epi16(r2,c1_sh1);
							m3=_mm256_maddubs_epi16(r3,c2_sh1);
							m4=_mm256_maddubs_epi16(r4,c1_sh1);


							//vertical add
							m4=_mm256_add_epi16(m4,m3);
							m4=_mm256_add_epi16(m4,m2);
							m4=_mm256_add_epi16(m4,m1);m0_prelude_row0=m4;


							m1=_mm256_srli_si256(m0_prelude_row0,2);
							m2=_mm256_add_epi16(m1,m0_prelude_row0);

							m1=_mm256_srli_si256(m0_prelude_row0,4);
							m2=_mm256_add_epi16(m1,m2);

							//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
							//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
							m4=_mm256_and_si256(m0_prelude_row0,mask3);
							m4=_mm256_permute2f128_si256(m4,m4,1);
							m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions
							m2=_mm256_add_epi16(m2,m4);

							//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
							output_row0_odd = _mm256_and_si256(m2,output_mask);

							//----row=1
							//multiply with the mask
							m2=_mm256_maddubs_epi16(r2,c0_sh1);
							m3=_mm256_maddubs_epi16(r3,c1_sh1);
							m4=_mm256_maddubs_epi16(r4,c2_sh1);


							//vertical add
							m4=_mm256_add_epi16(m4,m3);
							m4=_mm256_add_epi16(m4,m2);m0_prelude_row1=m4;


							m1=_mm256_srli_si256(m0_prelude_row1,2);
							m2=_mm256_add_epi16(m1,m0_prelude_row1);

							m1=_mm256_srli_si256(m0_prelude_row1,4);
							m2=_mm256_add_epi16(m1,m2);

							//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
							//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
							m4=_mm256_and_si256(m0_prelude_row1,mask3);
							m4=_mm256_permute2f128_si256(m4,m4,1);
							m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions
							m2=_mm256_add_epi16(m2,m4);

							//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
							output_row1_odd = _mm256_and_si256(m2,output_mask);

					                 //3rd col iteration
							//---row=2
							//multiply with the mask
							m0=_mm256_maddubs_epi16(r0,c0_sh2);
							m1=_mm256_maddubs_epi16(r1,c1_sh2);
							m2=_mm256_maddubs_epi16(r2,c2_sh2);
							m3=_mm256_maddubs_epi16(r3,c1_sh2);
							m4=_mm256_maddubs_epi16(r4,c0_sh2);

							//vertical add
							m4=_mm256_add_epi16(m4,m3);
							m4=_mm256_add_epi16(m4,m2);
							m4=_mm256_add_epi16(m4,m1);
							m0=_mm256_add_epi16(m4,m0);


							m1=_mm256_srli_si256(m0,2);
							m2=_mm256_add_epi16(m1,m0);

							//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,_mm256_extract_epi16(m2,8),0,0,0,0,0,0,0);
							m4=_mm256_and_si256(m2,mask3);
							m4=_mm256_permute2f128_si256(m4,m4,1);
							m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions

							m1=_mm256_srli_si256(m0,4);
							m2=_mm256_add_epi16(m1,m2);

							//prepei na prosthesw to pixel pu thelw sto r2 kai meta na kanw diairesi me ola mazi

							m2=_mm256_add_epi16(m2,m4);

							//m2 has 16 16bit values now. the results I need are in positions 1,4,7,10,13. keep only those, discard others
							m2 = _mm256_and_si256(m2,output_mask_sh1);
							output_even = _mm256_add_epi16(output_even,m2);

							//---row=0
							//multiply with the mask
							m1=_mm256_maddubs_epi16(r1,c0_sh2);
							m2=_mm256_maddubs_epi16(r2,c1_sh2);
							m3=_mm256_maddubs_epi16(r3,c2_sh2);
							m4=_mm256_maddubs_epi16(r4,c1_sh2);


							//vertical add
							m4=_mm256_add_epi16(m4,m3);
							m4=_mm256_add_epi16(m4,m2);
							m4=_mm256_add_epi16(m4,m1);m0_prelude_row0=m4;

							m1=_mm256_srli_si256(m0_prelude_row0,2);
							m2=_mm256_add_epi16(m1,m0_prelude_row0);

							//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,_mm256_extract_epi16(m2,8),0,0,0,0,0,0,0);
							m4=_mm256_and_si256(m2,mask3);
							m4=_mm256_permute2f128_si256(m4,m4,1);
							m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions

							m1=_mm256_srli_si256(m0_prelude_row0,4);
							m2=_mm256_add_epi16(m1,m2);

							//prepei na prosthesw to pixel pu thelw sto r2 kai meta na kanw diairesi me ola mazi

							m2=_mm256_add_epi16(m2,m4);

							//m2 has 16 16bit values now. the results I need are in positions 1,4,7,10,13. keep only those, discard others
							m2 = _mm256_and_si256(m2,output_mask_sh1);
							output_row0_even = _mm256_add_epi16(output_row0_even,m2);


							//---row=1
							//multiply with the mask
							m2=_mm256_maddubs_epi16(r2,c0_sh2);
							m3=_mm256_maddubs_epi16(r3,c1_sh2);
							m4=_mm256_maddubs_epi16(r4,c2_sh2);


							//vertical add
							m4=_mm256_add_epi16(m4,m3);
							m4=_mm256_add_epi16(m4,m2);m0_prelude_row1=m4;


							m1=_mm256_srli_si256(m0_prelude_row1,2);
							m2=_mm256_add_epi16(m1,m0_prelude_row1);

							//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,_mm256_extract_epi16(m2,8),0,0,0,0,0,0,0);
							m4=_mm256_and_si256(m2,mask3);
							m4=_mm256_permute2f128_si256(m4,m4,1);
							m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions

							m1=_mm256_srli_si256(m0_prelude_row1,4);
							m2=_mm256_add_epi16(m1,m2);

							//prepei na prosthesw to pixel pu thelw sto r2 kai meta na kanw diairesi me ola mazi

							m2=_mm256_add_epi16(m2,m4);

							//m2 has 16 16bit values now. the results I need are in positions 1,4,7,10,13. keep only those, discard others
							m2 = _mm256_and_si256(m2,output_mask_sh1);
							output_row1_even = _mm256_add_epi16(output_row1_even,m2);

							//4th col iteration
							//multiply with the mask
							m0=_mm256_maddubs_epi16(r0,c0_sh3);
							m1=_mm256_maddubs_epi16(r1,c1_sh3);
							m2=_mm256_maddubs_epi16(r2,c2_sh3);
							m3=_mm256_maddubs_epi16(r3,c1_sh3);
							m4=_mm256_maddubs_epi16(r4,c0_sh3);

							//vertical add
							m0=_mm256_add_epi16(m0,m1);
							m2=_mm256_add_epi16(m2,m3);
							m0=_mm256_add_epi16(m0,m2);
							m0=_mm256_add_epi16(m0,m4);

							//hozizontal additions
							//hadd(3:8)
							//hadd(9:14)
							//hadd(15:20)
							//hadd(21:26)
							//hadd(27:31)
							//result after division will be in 2,8,14,20,26

							m1=_mm256_srli_si256(m0,2);
							m4=_mm256_add_epi16(m1,m0);

							m1=_mm256_srli_si256(m0,4);
							m2=_mm256_add_epi16(m1,m4);

							//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
							//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
							m4=_mm256_and_si256(m4,mask3);
							m4=_mm256_permute2f128_si256(m4,m4,1);
							m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions

							m2=_mm256_add_epi16(m2,m4);

							//m2 has 16 16bit values now. the results I need are in positions 1,4,7,10,13. keep only those, discard others
							m2 = _mm256_and_si256(m2,output_mask_sh1);
							output_odd = _mm256_add_epi16(output_odd,m2);

							//row0
							//multiply with the mask
								m1=_mm256_maddubs_epi16(r1,c0_sh3);
								m2=_mm256_maddubs_epi16(r2,c1_sh3);
								m3=_mm256_maddubs_epi16(r3,c2_sh3);
								m4=_mm256_maddubs_epi16(r4,c1_sh3);

								//vertical add
								m0=_mm256_add_epi16(m1,m2);
								m0=_mm256_add_epi16(m0,m3);
								m0=_mm256_add_epi16(m0,m4);


								m1=_mm256_srli_si256(m0,2);
								m4=_mm256_add_epi16(m1,m0);

								m1=_mm256_srli_si256(m0,4);
								m2=_mm256_add_epi16(m1,m4);

								//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
								//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
								m4=_mm256_and_si256(m4,mask3);
								m4=_mm256_permute2f128_si256(m4,m4,1);
								m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions

								m2=_mm256_add_epi16(m2,m4);

								//m2 has 16 16bit values now. the results I need are in positions 1,4,7,10,13. keep only those, discard others
								m2 = _mm256_and_si256(m2,output_mask_sh1);
								output_row0_odd = _mm256_add_epi16(output_row0_odd,m2);

								//row1
								//multiply with the mask
									m2=_mm256_maddubs_epi16(r2,c0_sh3);
									m3=_mm256_maddubs_epi16(r3,c1_sh3);
									m4=_mm256_maddubs_epi16(r4,c2_sh3);

									//vertical add
									m0=_mm256_add_epi16(m2,m3);
									m0=_mm256_add_epi16(m0,m4);


									m1=_mm256_srli_si256(m0,2);
									m4=_mm256_add_epi16(m1,m0);

									m1=_mm256_srli_si256(m0,4);
									m2=_mm256_add_epi16(m1,m4);

									//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
									//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
									m4=_mm256_and_si256(m4,mask3);
									m4=_mm256_permute2f128_si256(m4,m4,1);
									m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions

									m2=_mm256_add_epi16(m2,m4);

									//m2 has 16 16bit values now. the results I need are in positions 1,4,7,10,13. keep only those, discard others
									m2 = _mm256_and_si256(m2,output_mask_sh1);
									output_row1_odd = _mm256_add_epi16(output_row1_odd,m2);


							//5th col iteration
							//multiply with the mask
							m0=_mm256_maddubs_epi16(r0,c0_sh4);
							m1=_mm256_maddubs_epi16(r1,c1_sh4);
							m2=_mm256_maddubs_epi16(r2,c2_sh4);
							m3=_mm256_maddubs_epi16(r3,c1_sh4);
							m4=_mm256_maddubs_epi16(r4,c0_sh4);

							//vertical add
							m0=_mm256_add_epi16(m0,m1);
							m2=_mm256_add_epi16(m2,m3);
							m0=_mm256_add_epi16(m0,m2);
							m0=_mm256_add_epi16(m0,m4);

							//hozizontal additions
							//hadd(4:9)
							//hadd(10:15)
							//hadd(16:21)
							//hadd(22:27)
							//result after division will be in 4,10,16,22

							m1=_mm256_srli_si256(m0,2);
							m2=_mm256_add_epi16(m1,m0);

							m1=_mm256_srli_si256(m0,4);
							m2=_mm256_add_epi16(m1,m2);


							//m2 has 16 16bit values now. the results I need are in positions 2,5,8,11. keep only those, discard others
							m2 = _mm256_and_si256(m2,output_mask_sh2);
							output_even = _mm256_add_epi16(output_even,m2);

							//row0
							//multiply with the mask
							m1=_mm256_maddubs_epi16(r1,c0_sh4);
							m2=_mm256_maddubs_epi16(r2,c1_sh4);
							m3=_mm256_maddubs_epi16(r3,c2_sh4);
							m4=_mm256_maddubs_epi16(r4,c1_sh4);

							//vertical add
							m0=_mm256_add_epi16(m1,m2);
							m0=_mm256_add_epi16(m0,m3);
							m0=_mm256_add_epi16(m0,m4);


							m1=_mm256_srli_si256(m0,2);
							m2=_mm256_add_epi16(m1,m0);

							m1=_mm256_srli_si256(m0,4);
							m2=_mm256_add_epi16(m1,m2);


							//m2 has 16 16bit values now. the results I need are in positions 2,5,8,11. keep only those, discard others
							m2 = _mm256_and_si256(m2,output_mask_sh2);
							output_row0_even = _mm256_add_epi16(output_row0_even,m2);

							//row1
							//multiply with the mask
							m2=_mm256_maddubs_epi16(r2,c0_sh4);
							m3=_mm256_maddubs_epi16(r3,c1_sh4);
							m4=_mm256_maddubs_epi16(r4,c2_sh4);

							//vertical add
							m0=_mm256_add_epi16(m2,m3);
							m0=_mm256_add_epi16(m0,m4);


							m1=_mm256_srli_si256(m0,2);
							m2=_mm256_add_epi16(m1,m0);

							m1=_mm256_srli_si256(m0,4);
							m2=_mm256_add_epi16(m1,m2);


							//m2 has 16 16bit values now. the results I need are in positions 2,5,8,11. keep only those, discard others
							m2 = _mm256_and_si256(m2,output_mask_sh2);
							output_row1_even = _mm256_add_epi16(output_row1_even,m2);

					                 //6th col iteration
							//multiply with the mask
							m0=_mm256_maddubs_epi16(r0,c0_sh5);
							m1=_mm256_maddubs_epi16(r1,c1_sh5);
							m2=_mm256_maddubs_epi16(r2,c2_sh5);
							m3=_mm256_maddubs_epi16(r3,c1_sh5);
							m4=_mm256_maddubs_epi16(r4,c0_sh5);

							//vertical add
							m0=_mm256_add_epi16(m0,m1);
							m2=_mm256_add_epi16(m2,m3);
							m0=_mm256_add_epi16(m0,m2);
							m0=_mm256_add_epi16(m0,m4);

							//hozizontal additions
							//hadd(5:10)
							//hadd(11:16)
							//hadd(17:22)
							//hadd(23:28)
							//result after division will be in 4,10,16,22

							m1=_mm256_srli_si256(m0,2);
							m2=_mm256_add_epi16(m1,m0);

							m1=_mm256_srli_si256(m0,4);
							m2=_mm256_add_epi16(m1,m2);

							//m2 has 16 16bit values now. the results I need are in positions 2,5,8,11. keep only those, discard others
							m2 = _mm256_and_si256(m2,output_mask_sh2);
							output_odd = _mm256_add_epi16(output_odd,m2);

							//now division follows
							output_even=division(division_case,output_even,f);
							output_odd=division(division_case,output_odd,f);

							//shift odd 1 position and add to even
							output_odd = _mm256_slli_si256(output_odd,1);
							output_even = _mm256_add_epi8(output_even,output_odd);


							_mm256_storeu_si256( (__m256i *) &filt[row][col],output_even );

							//row0
							//multiply with the mask
							m1=_mm256_maddubs_epi16(r1,c0_sh5);
							m2=_mm256_maddubs_epi16(r2,c1_sh5);
							m3=_mm256_maddubs_epi16(r3,c2_sh5);
							m4=_mm256_maddubs_epi16(r4,c1_sh5);

							//vertical add
							m0=_mm256_add_epi16(m1,m2);
							m0=_mm256_add_epi16(m0,m3);
							m0=_mm256_add_epi16(m0,m4);


							m1=_mm256_srli_si256(m0,2);
							m2=_mm256_add_epi16(m1,m0);

							m1=_mm256_srli_si256(m0,4);
							m2=_mm256_add_epi16(m1,m2);

							//m2 has 16 16bit values now. the results I need are in positions 2,5,8,11. keep only those, discard others
							m2 = _mm256_and_si256(m2,output_mask_sh2);
							output_row0_odd = _mm256_add_epi16(output_row0_odd,m2);

							//now division follows
							output_row0_even=division(division_case,output_row0_even,f);
							output_row0_odd=division(division_case,output_row0_odd,f);

							//shift odd 1 position and add to even
							output_row0_odd = _mm256_slli_si256(output_row0_odd,1);
							output_row0_even = _mm256_add_epi8(output_row0_even,output_row0_odd);


							_mm256_storeu_si256( (__m256i *) &filt[N-2][col],output_row0_even );

							//row1
							//multiply with the mask
							m2=_mm256_maddubs_epi16(r2,c0_sh5);
							m3=_mm256_maddubs_epi16(r3,c1_sh5);
							m4=_mm256_maddubs_epi16(r4,c2_sh5);

							//vertical add
							m0=_mm256_add_epi16(m2,m3);
							m0=_mm256_add_epi16(m0,m4);

								m1=_mm256_srli_si256(m0,2);
							m2=_mm256_add_epi16(m1,m0);

							m1=_mm256_srli_si256(m0,4);
							m2=_mm256_add_epi16(m1,m2);

							//m2 has 16 16bit values now. the results I need are in positions 2,5,8,11. keep only those, discard others
							m2 = _mm256_and_si256(m2,output_mask_sh2);
							output_row1_odd = _mm256_add_epi16(output_row1_odd,m2);

							//now division follows
							output_row1_even=division(division_case,output_row1_even,f);
							output_row1_odd=division(division_case,output_row1_odd,f);

							//shift odd 1 position and add to even
							output_row1_odd = _mm256_slli_si256(output_row1_odd,1);
							output_row1_even = _mm256_add_epi8(output_row1_even,output_row1_odd);


							_mm256_storeu_si256( (__m256i *) &filt[N-1][col],output_row1_even );


							}

		    loop_reminder_last_less_div(frame1,filt,M,N,col,REMINDER_ITERATIONS,division_case,c0,c1,c2,c0_sh1,c1_sh1,c2_sh1,c0_sh2,c1_sh2,c2_sh2,f,divisor,filter5x5);

					}
	else {


	  for (col = 0; col <= M-32; col+=28){
	 //last col value that does not read outside of the array bounds is (col<M-29)

		  if (col==0){

				//load the 5 rows
				r0=_mm256_loadu_si256( (__m256i *) &frame1[row-2][0]);
				r1=_mm256_loadu_si256( (__m256i *) &frame1[row-1][0]);
				r2=_mm256_loadu_si256( (__m256i *) &frame1[row][0]);
				r3=_mm256_loadu_si256( (__m256i *) &frame1[row+1][0]);
				r4=_mm256_loadu_si256( (__m256i *) &frame1[row+2][0]);

		  			//START - extra code needed for prelude
		  					m0=_mm256_slli_si256(r0,2);//shift 2 elements left - equivalent to filling with two zeros inthe beginning
		  					m1=_mm256_slli_si256(r1,2);//shift 2 elements left - equivalent to filling with two zeros inthe beginning
		  					m2=_mm256_slli_si256(r2,2);//shift 2 elements left - equivalent to filling with two zeros inthe beginning
		  					m3=_mm256_slli_si256(r3,2);//shift 2 elements left - equivalent to filling with two zeros inthe beginning
		  					m4=_mm256_slli_si256(r4,2);//shift 2 elements left - equivalent to filling with two zeros inthe beginning

		  					r0=_mm256_and_si256(r0,mask_prelude);
		  					r0=_mm256_permute2f128_si256(r0,r0,1);
		  					r0=_mm256_srli_si256(r0,14);//shift 14 elements
		  					r0=_mm256_add_epi16(m0,r0);

		  					r1=_mm256_and_si256(r1,mask_prelude);
		  					r1=_mm256_permute2f128_si256(r1,r1,1);
		  					r1=_mm256_srli_si256(r1,14);//shift 14 elements
		  					r1=_mm256_add_epi16(m1,r1);

		  					r2=_mm256_and_si256(r2,mask_prelude);
		  					r2=_mm256_permute2f128_si256(r2,r2,1);
		  					r2=_mm256_srli_si256(r2,14);//shift 14 elements
		  					r2=_mm256_add_epi16(m2,r2);

		  					r3=_mm256_and_si256(r3,mask_prelude);
		  					r3=_mm256_permute2f128_si256(r3,r3,1);
		  					r3=_mm256_srli_si256(r3,14);//shift 14 elements
		  					r3=_mm256_add_epi16(m3,r3);

		  					r4=_mm256_and_si256(r4,mask_prelude);
		  					r4=_mm256_permute2f128_si256(r4,r4,1);
		  					r4=_mm256_srli_si256(r4,14);//shift 14 elements
		  					r4=_mm256_add_epi16(m4,r4);

		  			//END - extra code needed for prelude
		  		}
		  else {
				//load the 5 rows
				r0=_mm256_loadu_si256( (__m256i *) &frame1[row-2][col-2]);
				r1=_mm256_loadu_si256( (__m256i *) &frame1[row-1][col-2]);
				r2=_mm256_loadu_si256( (__m256i *) &frame1[row][col-2]);
				r3=_mm256_loadu_si256( (__m256i *) &frame1[row+1][col-2]);
				r4=_mm256_loadu_si256( (__m256i *) &frame1[row+2][col-2]);
		  }

		//col iteration computes output pixels of 2,8,14,20,26
		// col+1 iteration computes output pixels of 3,9,15,21,27
		// col+2 iteration computes output pixels of 4,10,16,22,28
		// col+3 iteration computes output pixels of 5,11,17,23,29
		// col+4 iteration computes output pixels of 6,12,18,24
		// col+5 iteration computes output pixels of 7,13,19,25
		//afterwards, col becomes 30 and repeat the above process

		//1st col iteration


		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,c0);
		m1=_mm256_maddubs_epi16(r1,c1);
		m2=_mm256_maddubs_epi16(r2,c2);
		m3=_mm256_maddubs_epi16(r3,c1);
		m4=_mm256_maddubs_epi16(r4,c0);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m2=_mm256_add_epi16(m2,m3);
		m0=_mm256_add_epi16(m0,m2);
		m0=_mm256_add_epi16(m0,m4);

		//hozizontal additions
		//hadd(0:5)
		//hadd(6:11)
		//hadd(12:17)
		//hadd(18:23)
		//hadd(24:28)

		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		m1=_mm256_srli_si256(m0,4);
		m2=_mm256_add_epi16(m1,m2);

		//after shifts the 15th element is lost as it cannot be propagated to the 16th position (AVX registers are managed as two seperate SSE registers)
		m4=_mm256_and_si256(m0,mask3);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions
		m2=_mm256_add_epi16(m2,m4);

		//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
		output_even=_mm256_and_si256(m2,output_mask);


		//2nd col iteration
		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,c0_sh1);
		m1=_mm256_maddubs_epi16(r1,c1_sh1);
		m2=_mm256_maddubs_epi16(r2,c2_sh1);
		m3=_mm256_maddubs_epi16(r3,c1_sh1);
		m4=_mm256_maddubs_epi16(r4,c0_sh1);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m2=_mm256_add_epi16(m2,m3);
		m0=_mm256_add_epi16(m0,m2);
		m0=_mm256_add_epi16(m0,m4);

		//hozizontal additions
		//hadd(1:6)
		//hadd(7:12)
		//hadd(13:18)
		//hadd(19:24)
		//hadd(25:29)

		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		m1=_mm256_srli_si256(m0,4);
		m2=_mm256_add_epi16(m1,m2);

		//after shifts the 15th element is lost as it cannot be propagated to the 16th position (AVX registers are managed as two seperate SSE registers)
		m4=_mm256_and_si256(m0,mask3);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions
		m2=_mm256_add_epi16(m2,m4);

		//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
		output_odd = _mm256_and_si256(m2,output_mask);



         //3rd col iteration
		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,c0_sh2);
		m1=_mm256_maddubs_epi16(r1,c1_sh2);
		m2=_mm256_maddubs_epi16(r2,c2_sh2);
		m3=_mm256_maddubs_epi16(r3,c1_sh2);
		m4=_mm256_maddubs_epi16(r4,c0_sh2);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m2=_mm256_add_epi16(m2,m3);
		m0=_mm256_add_epi16(m0,m2);
		m0=_mm256_add_epi16(m0,m4);

		//hozizontal additions
		//hadd(2:7)
		//hadd(8:13)
		//hadd(14:19)
		//hadd(20:25)
		//hadd(26:30)

		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		m4=_mm256_and_si256(m2,mask3);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions

		m1=_mm256_srli_si256(m0,4);
		m2=_mm256_add_epi16(m1,m2);
		m2=_mm256_add_epi16(m2,m4);

		//m2 has 16 16bit values now. the results I need are in positions 1,4,7,10,13. keep only those, discard others
		m2 = _mm256_and_si256(m2,output_mask_sh1);
		output_even = _mm256_add_epi16(output_even,m2);


		//4th col iteration

		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,c0_sh3);
		m1=_mm256_maddubs_epi16(r1,c1_sh3);
		m2=_mm256_maddubs_epi16(r2,c2_sh3);
		m3=_mm256_maddubs_epi16(r3,c1_sh3);
		m4=_mm256_maddubs_epi16(r4,c0_sh3);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m2=_mm256_add_epi16(m2,m3);
		m0=_mm256_add_epi16(m0,m2);
		m0=_mm256_add_epi16(m0,m4);

		//hozizontal additions
		//hadd(3:8)
		//hadd(9:14)
		//hadd(15:20)
		//hadd(21:26)
		//hadd(27:31)
		//result after division will be in 2,8,14,20,26

		m1=_mm256_srli_si256(m0,2);
		m4=_mm256_add_epi16(m1,m0);

		m1=_mm256_srli_si256(m0,4);
		m2=_mm256_add_epi16(m1,m4);


		m4=_mm256_and_si256(m4,mask3);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions

		m2=_mm256_add_epi16(m2,m4);

		//m2 has 16 16bit values now. the results I need are in positions 1,4,7,10,13. keep only those, discard others
		m2 = _mm256_and_si256(m2,output_mask_sh1);
		output_odd = _mm256_add_epi16(output_odd,m2);




		//5th col iteration
		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,c0_sh4);
		m1=_mm256_maddubs_epi16(r1,c1_sh4);
		m2=_mm256_maddubs_epi16(r2,c2_sh4);
		m3=_mm256_maddubs_epi16(r3,c1_sh4);
		m4=_mm256_maddubs_epi16(r4,c0_sh4);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m2=_mm256_add_epi16(m2,m3);
		m0=_mm256_add_epi16(m0,m2);
		m0=_mm256_add_epi16(m0,m4);

		//hozizontal additions
		//hadd(4:9)
		//hadd(10:15)
		//hadd(16:21)
		//hadd(22:27)
		//result after division will be in 4,10,16,22

		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		m1=_mm256_srli_si256(m0,4);
		m2=_mm256_add_epi16(m1,m2);


		//m2 has 16 16bit values now. the results I need are in positions 2,5,8,11. keep only those, discard others
		m2 = _mm256_and_si256(m2,output_mask_sh2);
		output_even = _mm256_add_epi16(output_even,m2);



                 //6th col iteration
		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,c0_sh5);
		m1=_mm256_maddubs_epi16(r1,c1_sh5);
		m2=_mm256_maddubs_epi16(r2,c2_sh5);
		m3=_mm256_maddubs_epi16(r3,c1_sh5);
		m4=_mm256_maddubs_epi16(r4,c0_sh5);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m2=_mm256_add_epi16(m2,m3);
		m0=_mm256_add_epi16(m0,m2);
		m0=_mm256_add_epi16(m0,m4);

		//hozizontal additions
		//hadd(5:10)
		//hadd(11:16)
		//hadd(17:22)
		//hadd(23:28)
		//result after division will be in 4,10,16,22

		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		m1=_mm256_srli_si256(m0,4);
		m2=_mm256_add_epi16(m1,m2);

		//m2 has 16 16bit values now. the results I need are in positions 2,5,8,11. keep only those, discard others
		m2 = _mm256_and_si256(m2,output_mask_sh2);
		output_odd = _mm256_add_epi16(output_odd,m2);

		//now division follows
		output_even=division(division_case,output_even,f);
		output_odd=division(division_case,output_odd,f);

		//shift odd 1 position and add to even
		output_odd = _mm256_slli_si256(output_odd,1);
		output_even = _mm256_add_epi8(output_even,output_odd);


		_mm256_storeu_si256( (__m256i *) &filt[row][col],output_even );


		}

        if (REMINDER_ITERATIONS>=29)
        	loop_reminder_high_reminder_values_less_div(frame1,filt,M,N,row,col,REMINDER_ITERATIONS,division_case,c0,c1,c2,c0_sh1,c1_sh1,c2_sh1,c0_sh2,c1_sh2,c2_sh2,f,divisor,filter5x5);
        else
        	loop_reminder_low_reminder_values_less_div(frame1,filt,M,N,row,col,REMINDER_ITERATIONS,division_case,c0,c1,c2,c0_sh1,c1_sh1,c2_sh1,c0_sh2,c1_sh2,c2_sh2,c0_sh3,c1_sh3,c2_sh3,c0_sh4,c1_sh4,c2_sh4,c0_sh5,c1_sh5,c2_sh5,f);
		//padding code. The last three iterations  ABOVE get outside of the array bounds. The last three col iterations read outside but the garbage values are multiplied by zeros (last three mask zeros). From now on, I must include another mask with extra zeros.
//It is faster to read outside of the array's bounds and then fill the right values. there is an insert command that inserts a value to the vector
//TOMORROW: USE Gaussian_Blur_AVX_ver4_plus_less_load ROUTINE FOR LOOP REMINDER. LOAD OUTSIDE OF THE ARRAY AND THEN ZERO THE VALUES NEEDED.

		}
}

}//end of parallel


}









void Gaussian_Blur_optimized_5x5_16_reg_blocking(unsigned char **frame1,unsigned char **filt,const unsigned int M, const unsigned int N, const unsigned short int divisor, signed char **filter5x5){

const signed char f00=filter5x5[0][0];
const signed char f01=filter5x5[0][1];
const signed char f02=filter5x5[0][2];
const signed char f03=filter5x5[0][3];
const signed char f04=filter5x5[0][4];

const signed char f10=filter5x5[1][0];
const signed char f11=filter5x5[1][1];
const signed char f12=filter5x5[1][2];
const signed char f13=filter5x5[1][3];
const signed char f14=filter5x5[1][4];

const signed char f20=filter5x5[2][0];
const signed char f21=filter5x5[2][1];
const signed char f22=filter5x5[2][2];
const signed char f23=filter5x5[2][3];
const signed char f24=filter5x5[2][4];


	const __m256i c0=_mm256_set_epi8(0,0,0,f04,f03,f02,f01,f00,0,f04,f03,f02,f01,f00,0,f04,f03,f02,f01,f00,0,f04,f03,f02,f01,f00,0,f04,f03,f02,f01,f00);
	const __m256i c1=_mm256_set_epi8(0,0,0,f14,f13,f12,f11,f10,0,f14,f13,f12,f11,f10,0,f14,f13,f12,f11,f10,0,f14,f13,f12,f11,f10,0,f14,f13,f12,f11,f10);
	const __m256i c2=_mm256_set_epi8(0,0,0,f24,f23,f22,f21,f20,0,f24,f23,f22,f21,f20,0,f24,f23,f22,f21,f20,0,f24,f23,f22,f21,f20,0,f24,f23,f22,f21,f20);

	const __m256i c0_sh1=_mm256_set_epi8(0,0,f04,f03,f02,f01,f00,0,f04,f03,f02,f01,f00,0,f04,f03,f02,f01,f00,0,f04,f03,f02,f01,f00,0,f04,f03,f02,f01,f00,0);
	const __m256i c1_sh1=_mm256_set_epi8(0,0,f14,f13,f12,f11,f10,0,f14,f13,f12,f11,f10,0,f14,f13,f12,f11,f10,0,f14,f13,f12,f11,f10,0,f14,f13,f12,f11,f10,0);
	const __m256i c2_sh1=_mm256_set_epi8(0,0,f24,f23,f22,f21,f20,0,f24,f23,f22,f21,f20,0,f24,f23,f22,f21,f20,0,f24,f23,f22,f21,f20,0,f24,f23,f22,f21,f20,0);

	const __m256i c0_sh2=_mm256_set_epi8(0,f04,f03,f02,f01,f00,0,f04,f03,f02,f01,f00,0,f04,f03,f02,f01,f00,0,f04,f03,f02,f01,f00,0,f04,f03,f02,f01,f00,0,0);
	const __m256i c1_sh2=_mm256_set_epi8(0,f14,f13,f12,f11,f10,0,f14,f13,f12,f11,f10,0,f14,f13,f12,f11,f10,0,f14,f13,f12,f11,f10,0,f14,f13,f12,f11,f10,0,0);
	const __m256i c2_sh2=_mm256_set_epi8(0,f24,f23,f22,f21,f20,0,f24,f23,f22,f21,f20,0,f24,f23,f22,f21,f20,0,f24,f23,f22,f21,f20,0,f24,f23,f22,f21,f20,0,0);

	const __m256i c0_sh3=_mm256_set_epi8(f04,f03,f02,f01,f00,0,f04,f03,f02,f01,f00,0,f04,f03,f02,f01,f00,0,f04,f03,f02,f01,f00,0,f04,f03,f02,f01,f00,0,0,0);
	const __m256i c1_sh3=_mm256_set_epi8(f14,f13,f12,f11,f10,0,f14,f13,f12,f11,f10,0,f14,f13,f12,f11,f10,0,f14,f13,f12,f11,f10,0,f14,f13,f12,f11,f10,0,0,0);
	const __m256i c2_sh3=_mm256_set_epi8(f24,f23,f22,f21,f20,0,f24,f23,f22,f21,f20,0,f24,f23,f22,f21,f20,0,f24,f23,f22,f21,f20,0,f24,f23,f22,f21,f20,0,0,0);

	const __m256i c0_sh4=_mm256_set_epi8(0,0,0,0,0,f04,f03,f02,f01,f00,0,f04,f03,f02,f01,f00,0,f04,f03,f02,f01,f00,0,f04,f03,f02,f01,f00,0,0,0,0);
	const __m256i c1_sh4=_mm256_set_epi8(0,0,0,0,0,f14,f13,f12,f11,f10,0,f14,f13,f12,f11,f10,0,f14,f13,f12,f11,f10,0,f14,f13,f12,f11,f10,0,0,0,0);
	const __m256i c2_sh4=_mm256_set_epi8(0,0,0,0,0,f24,f23,f22,f21,f20,0,f24,f23,f22,f21,f20,0,f24,f23,f22,f21,f20,0,f24,f23,f22,f21,f20,0,0,0,0);

	const __m256i c0_sh5=_mm256_set_epi8(0,0,0,0,f04,f03,f02,f01,f00,0,f04,f03,f02,f01,f00,0,f04,f03,f02,f01,f00,0,f04,f03,f02,f01,f00,0,0,0,0,0);
	const __m256i c1_sh5=_mm256_set_epi8(0,0,0,0,f14,f13,f12,f11,f10,0,f14,f13,f12,f11,f10,0,f14,f13,f12,f11,f10,0,f14,f13,f12,f11,f10,0,0,0,0,0);
	const __m256i c2_sh5=_mm256_set_epi8(0,0,0,0,f24,f23,f22,f21,f20,0,f24,f23,f22,f21,f20,0,f24,f23,f22,f21,f20,0,f24,f23,f22,f21,f20,0,0,0,0,0);



	//const __m256i f  = _mm256_set_epi16(52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759);
	//const __m256i mask  = _mm256_set_epi8(0,0,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255);
	//const __m256i mask2  = _mm256_set_epi8(0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0);
	//const __m256i mask6  = _mm256_set_epi8(0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0);

	const __m256i mask3  = _mm256_set_epi16(0,0,0,0,0,0,0,65535,0,0,0,0,0,0,0,0);
	const __m256i mask_prelude  = _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0);

	const __m256i output_mask      = _mm256_set_epi16(0,0,0,65535,0,0,65535,0,0,65535,0,0,65535,0,0,65535);
	const __m256i output_mask_sh1  = _mm256_set_epi16(0,0,65535,0,0,65535,0,0,65535,0,0,65535,0,0,65535,0);
	const __m256i output_mask_sh2  = _mm256_set_epi16(0,0,0,    0,65535,0,0,65535,0,0,65535,0,0,65535,0,0);



	const unsigned int REMINDER_ITERATIONS =  (M-((((M-32)/28)*28)+28)); //M-(last_col_value+28)
	//printf("\n%d",REMINDER_ITERATIONS);


	const unsigned int division_case=prepare_for_division(divisor); //determine which is the division case (A, B or C)
	const __m256i f = _mm256_load_si256( (__m256i *) &f_vector[0]);// initialize the division vector

//printf("\n%d %d",division_case,b);


#pragma omp parallel
{

unsigned int row,col;
register __m256i r0,r1,r2,r3,r4,r5,m0,m1,m2,m3,m4,m5;
__m256i output_row0_even,output_row1_even,output_row0_odd,output_row1_odd; //these are for processing row #0 and #1 only.
__m256i output_even,output_odd;
__m256i m0_prelude_row0,m0_prelude_row1;


/*---------------------- Gaussian Blur ---------------------------------*/

    #pragma omp for schedule(static)
	for (row = 1; row <= N-2; row+=2) {

		if (row==1){//in this case I calculate filt[0][:] and filt[1][:] too. No extra loads or multiplications are required for this
			  for (col = 0; col <= M-32; col+=28){

				  if (col==0){//this is a special case as the mask gets outside of the array (it is like adding two zeros in the beginning of frame[][]

						//load the 5 rows
						r0=_mm256_loadu_si256( (__m256i *) &frame1[0][0]);
						r1=_mm256_loadu_si256( (__m256i *) &frame1[1][0]);
						r2=_mm256_loadu_si256( (__m256i *) &frame1[2][0]);
						r3=_mm256_loadu_si256( (__m256i *) &frame1[3][0]);
						r4=_mm256_loadu_si256( (__m256i *) &frame1[4][0]);

				  			//START - extra code needed for prelude
				  					m0=_mm256_slli_si256(r0,2);//shift 2 elements left - equivalent to filling with two zeros inthe beginning
				  					m1=_mm256_slli_si256(r1,2);//shift 2 elements left - equivalent to filling with two zeros inthe beginning
				  					m2=_mm256_slli_si256(r2,2);//shift 2 elements left - equivalent to filling with two zeros inthe beginning
				  					m3=_mm256_slli_si256(r3,2);//shift 2 elements left - equivalent to filling with two zeros inthe beginning
				  					m4=_mm256_slli_si256(r4,2);//shift 2 elements left - equivalent to filling with two zeros inthe beginning

				  					r0=_mm256_and_si256(r0,mask_prelude);
				  					r0=_mm256_permute2f128_si256(r0,r0,1);
				  					r0=_mm256_srli_si256(r0,14);//shift 14 elements
				  					r0=_mm256_add_epi16(m0,r0);

				  					r1=_mm256_and_si256(r1,mask_prelude);
				  					r1=_mm256_permute2f128_si256(r1,r1,1);
				  					r1=_mm256_srli_si256(r1,14);//shift 14 elements
				  					r1=_mm256_add_epi16(m1,r1);

				  					r2=_mm256_and_si256(r2,mask_prelude);
				  					r2=_mm256_permute2f128_si256(r2,r2,1);
				  					r2=_mm256_srli_si256(r2,14);//shift 14 elements
				  					r2=_mm256_add_epi16(m2,r2);

				  					r3=_mm256_and_si256(r3,mask_prelude);
				  					r3=_mm256_permute2f128_si256(r3,r3,1);
				  					r3=_mm256_srli_si256(r3,14);//shift 14 elements
				  					r3=_mm256_add_epi16(m3,r3);

				  					r4=_mm256_and_si256(r4,mask_prelude);
				  					r4=_mm256_permute2f128_si256(r4,r4,1);
				  					r4=_mm256_srli_si256(r4,14);//shift 14 elements
				  					r4=_mm256_add_epi16(m4,r4);

				  			//END - extra code needed for prelude
				  		}
				  else {
						//load the 5 rows
						r0=_mm256_loadu_si256( (__m256i *) &frame1[0][col-2]);
						r1=_mm256_loadu_si256( (__m256i *) &frame1[1][col-2]);
						r2=_mm256_loadu_si256( (__m256i *) &frame1[2][col-2]);
						r3=_mm256_loadu_si256( (__m256i *) &frame1[3][col-2]);
						r4=_mm256_loadu_si256( (__m256i *) &frame1[4][col-2]);
				  }

				//col iteration computes output pixels of 2,8,14,20,26
				// col+1 iteration computes output pixels of 3,9,15,21,27
				// col+2 iteration computes output pixels of 4,10,16,22,28
				// col+3 iteration computes output pixels of 5,11,17,23,29
				// col+4 iteration computes output pixels of 6,12,18,24,30
				// col+5 iteration computes output pixels of 7,13,19,25,31
				//afterwards, col2 becomes 32 and repeat the above process

				//1st col iteration

				  //--------------row=2
				//multiply with the mask
				m0=_mm256_maddubs_epi16(r0,c0);
				m1=_mm256_maddubs_epi16(r1,c1);
				m2=_mm256_maddubs_epi16(r2,c2);
				m3=_mm256_maddubs_epi16(r3,c1);
				m4=_mm256_maddubs_epi16(r4,c0);

				//vertical add
				m0=_mm256_add_epi16(m0,m1);
				m0=_mm256_add_epi16(m0,m2);
				m0=_mm256_add_epi16(m0,m3);
				m0=_mm256_add_epi16(m0,m4);


				m1=_mm256_srli_si256(m0,2);
				m2=_mm256_add_epi16(m1,m0);

				m1=_mm256_srli_si256(m0,4);
				m2=_mm256_add_epi16(m1,m2);

				//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
				//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
				m4=_mm256_and_si256(m0,mask3);
				m4=_mm256_permute2f128_si256(m4,m4,1);
				m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions
				m2=_mm256_add_epi16(m2,m4);

				//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
				output_even=_mm256_and_si256(m2,output_mask);

				//--------------row=0
				//multiply with the mask
				m0=_mm256_maddubs_epi16(r0,c2);
				m1=_mm256_maddubs_epi16(r1,c1);
				m2=_mm256_maddubs_epi16(r2,c0);

				//vertical add
				m0=_mm256_add_epi16(m0,m1);
				m0=_mm256_add_epi16(m0,m2);


				m1=_mm256_srli_si256(m0,2);
				m2=_mm256_add_epi16(m1,m0);

				m1=_mm256_srli_si256(m0,4);
				m2=_mm256_add_epi16(m1,m2);

				//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
				//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
				m4=_mm256_and_si256(m0,mask3);
				m4=_mm256_permute2f128_si256(m4,m4,1);
				m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions
				m2=_mm256_add_epi16(m2,m4);

				//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
				output_row0_even=_mm256_and_si256(m2,output_mask);

				//--------------row=1
				//multiply with the mask
				m0=_mm256_maddubs_epi16(r0,c1);
				m1=_mm256_maddubs_epi16(r1,c2);
				m2=_mm256_maddubs_epi16(r2,c1);
				m3=_mm256_maddubs_epi16(r3,c0);

				//vertical add
				m0=_mm256_add_epi16(m0,m1);
				m0=_mm256_add_epi16(m0,m2);
				m0=_mm256_add_epi16(m0,m3);

				m1=_mm256_srli_si256(m0,2);
				m2=_mm256_add_epi16(m1,m0);

				m1=_mm256_srli_si256(m0,4);
				m2=_mm256_add_epi16(m1,m2);

				//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
				//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
				m4=_mm256_and_si256(m0,mask3);
				m4=_mm256_permute2f128_si256(m4,m4,1);
				m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions
				m2=_mm256_add_epi16(m2,m4);

				//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
				output_row1_even=_mm256_and_si256(m2,output_mask);

				//2nd col iteration
				//multiply with the mask

				//----row=2
				m0=_mm256_maddubs_epi16(r0,c0_sh1);
				m1=_mm256_maddubs_epi16(r1,c1_sh1);
				m2=_mm256_maddubs_epi16(r2,c2_sh1);
				m3=_mm256_maddubs_epi16(r3,c1_sh1);
				m4=_mm256_maddubs_epi16(r4,c0_sh1);

				//vertical add
				m0=_mm256_add_epi16(m0,m1);
				m0=_mm256_add_epi16(m0,m2);
				m0=_mm256_add_epi16(m0,m3);
				m0=_mm256_add_epi16(m0,m4);


				m1=_mm256_srli_si256(m0,2);
				m2=_mm256_add_epi16(m1,m0);

				m1=_mm256_srli_si256(m0,4);
				m2=_mm256_add_epi16(m1,m2);

				//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
				//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
				m4=_mm256_and_si256(m0,mask3);
				m4=_mm256_permute2f128_si256(m4,m4,1);
				m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions
				m2=_mm256_add_epi16(m2,m4);

				//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
				output_odd = _mm256_and_si256(m2,output_mask);

				//----row=0
				m0=_mm256_maddubs_epi16(r0,c2_sh1);
				m1=_mm256_maddubs_epi16(r1,c1_sh1);
				m2=_mm256_maddubs_epi16(r2,c0_sh1);

				//vertical add
				m0=_mm256_add_epi16(m0,m1);
				m0=_mm256_add_epi16(m0,m2);


				m1=_mm256_srli_si256(m0,2);
				m2=_mm256_add_epi16(m1,m0);

				m1=_mm256_srli_si256(m0,4);
				m2=_mm256_add_epi16(m1,m2);

				//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
				//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
				m4=_mm256_and_si256(m0,mask3);
				m4=_mm256_permute2f128_si256(m4,m4,1);
				m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions
				m2=_mm256_add_epi16(m2,m4);

				//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
				output_row0_odd = _mm256_and_si256(m2,output_mask);

				//----row=1
				m0=_mm256_maddubs_epi16(r0,c1_sh1);
				m1=_mm256_maddubs_epi16(r1,c2_sh1);
				m2=_mm256_maddubs_epi16(r2,c1_sh1);
				m3=_mm256_maddubs_epi16(r3,c0_sh1);

				//vertical add
				m0=_mm256_add_epi16(m0,m1);
				m0=_mm256_add_epi16(m0,m2);
				m0=_mm256_add_epi16(m0,m3);


				m1=_mm256_srli_si256(m0,2);
				m2=_mm256_add_epi16(m1,m0);

				m1=_mm256_srli_si256(m0,4);
				m2=_mm256_add_epi16(m1,m2);

				//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
				//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
				m4=_mm256_and_si256(m0,mask3);
				m4=_mm256_permute2f128_si256(m4,m4,1);
				m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions
				m2=_mm256_add_epi16(m2,m4);

				//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
				output_row1_odd = _mm256_and_si256(m2,output_mask);

		                 //3rd col iteration
				//---row=2
				//multiply with the mask
				m0=_mm256_maddubs_epi16(r0,c0_sh2);
				m1=_mm256_maddubs_epi16(r1,c1_sh2);
				m2=_mm256_maddubs_epi16(r2,c2_sh2);
				m3=_mm256_maddubs_epi16(r3,c1_sh2);
				m4=_mm256_maddubs_epi16(r4,c0_sh2);

				//vertical add
				m0=_mm256_add_epi16(m0,m1);
				m0=_mm256_add_epi16(m0,m2);
				m0=_mm256_add_epi16(m0,m3);
				m0=_mm256_add_epi16(m0,m4);


				m1=_mm256_srli_si256(m0,2);
				m2=_mm256_add_epi16(m1,m0);

				//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,_mm256_extract_epi16(m2,8),0,0,0,0,0,0,0);
				m4=_mm256_and_si256(m2,mask3);
				m4=_mm256_permute2f128_si256(m4,m4,1);
				m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions

				m1=_mm256_srli_si256(m0,4);
				m2=_mm256_add_epi16(m1,m2);
				m2=_mm256_add_epi16(m2,m4);

				//m2 has 16 16bit values now. the results I need are in positions 1,4,7,10,13. keep only those, discard others
				m2 = _mm256_and_si256(m2,output_mask_sh1);
				output_even = _mm256_add_epi16(output_even,m2);

				//---row=0
				//multiply with the mask

				m0=_mm256_maddubs_epi16(r0,c2_sh2);
				m1=_mm256_maddubs_epi16(r1,c1_sh2);
				m2=_mm256_maddubs_epi16(r2,c0_sh2);

				//vertical add
				m0=_mm256_add_epi16(m0,m1);
				m0=_mm256_add_epi16(m0,m2);


				m1=_mm256_srli_si256(m0,2);
				m2=_mm256_add_epi16(m1,m0);

				//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,_mm256_extract_epi16(m2,8),0,0,0,0,0,0,0);
				m4=_mm256_and_si256(m2,mask3);
				m4=_mm256_permute2f128_si256(m4,m4,1);
				m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions

				m1=_mm256_srli_si256(m0,4);
				m2=_mm256_add_epi16(m1,m2);

				//prepei na prosthesw to pixel pu thelw sto r2 kai meta na kanw diairesi me ola mazi

				m2=_mm256_add_epi16(m2,m4);

				//m2 has 16 16bit values now. the results I need are in positions 1,4,7,10,13. keep only those, discard others
				m2 = _mm256_and_si256(m2,output_mask_sh1);
				output_row0_even = _mm256_add_epi16(output_row0_even,m2);


				//---row=1
				//multiply with the mask

				m0=_mm256_maddubs_epi16(r0,c1_sh2);
				m1=_mm256_maddubs_epi16(r1,c2_sh2);
				m2=_mm256_maddubs_epi16(r2,c1_sh2);
				m3=_mm256_maddubs_epi16(r3,c0_sh2);

				//vertical add
				m0=_mm256_add_epi16(m0,m1);
				m0=_mm256_add_epi16(m0,m2);
				m0=_mm256_add_epi16(m0,m3);

				m1=_mm256_srli_si256(m0,2);
				m2=_mm256_add_epi16(m1,m0);

				//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,_mm256_extract_epi16(m2,8),0,0,0,0,0,0,0);
				m4=_mm256_and_si256(m2,mask3);
				m4=_mm256_permute2f128_si256(m4,m4,1);
				m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions

				m1=_mm256_srli_si256(m0,4);
				m2=_mm256_add_epi16(m1,m2);

				//prepei na prosthesw to pixel pu thelw sto r2 kai meta na kanw diairesi me ola mazi

				m2=_mm256_add_epi16(m2,m4);

				//m2 has 16 16bit values now. the results I need are in positions 1,4,7,10,13. keep only those, discard others
				m2 = _mm256_and_si256(m2,output_mask_sh1);
				output_row1_even = _mm256_add_epi16(output_row1_even,m2);

				//4th col iteration

				//4th col iteration

				//multiply with the mask
				m0=_mm256_maddubs_epi16(r0,c0_sh3);
				m1=_mm256_maddubs_epi16(r1,c1_sh3);
				m2=_mm256_maddubs_epi16(r2,c2_sh3);
				m3=_mm256_maddubs_epi16(r3,c1_sh3);
				m4=_mm256_maddubs_epi16(r4,c0_sh3);

				//vertical add
				m0=_mm256_add_epi16(m0,m1);
				m2=_mm256_add_epi16(m2,m3);
				m0=_mm256_add_epi16(m0,m2);
				m0=_mm256_add_epi16(m0,m4);

				//hozizontal additions
				//hadd(3:8)
				//hadd(9:14)
				//hadd(15:20)
				//hadd(21:26)
				//hadd(27:31)
				//result after division will be in 2,8,14,20,26

				m1=_mm256_srli_si256(m0,2);
				m4=_mm256_add_epi16(m1,m0);

				m1=_mm256_srli_si256(m0,4);
				m2=_mm256_add_epi16(m1,m4);

				//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
				//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
				m4=_mm256_and_si256(m4,mask3);
				m4=_mm256_permute2f128_si256(m4,m4,1);
				m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions

				m2=_mm256_add_epi16(m2,m4);

				//m2 has 16 16bit values now. the results I need are in positions 1,4,7,10,13. keep only those, discard others
				m2 = _mm256_and_si256(m2,output_mask_sh1);
				output_odd = _mm256_add_epi16(output_odd,m2);

				//row0
				//multiply with the mask
								m0=_mm256_maddubs_epi16(r0,c2_sh3);
								m1=_mm256_maddubs_epi16(r1,c1_sh3);
								m2=_mm256_maddubs_epi16(r2,c0_sh3);


								//vertical add
								m0=_mm256_add_epi16(m0,m1);
								m0=_mm256_add_epi16(m0,m2);


								m1=_mm256_srli_si256(m0,2);
								m4=_mm256_add_epi16(m1,m0);

								m1=_mm256_srli_si256(m0,4);
								m2=_mm256_add_epi16(m1,m4);

								//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
								//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
								m4=_mm256_and_si256(m4,mask3);
								m4=_mm256_permute2f128_si256(m4,m4,1);
								m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions

								m2=_mm256_add_epi16(m2,m4);

								//m2 has 16 16bit values now. the results I need are in positions 1,4,7,10,13. keep only those, discard others
								m2 = _mm256_and_si256(m2,output_mask_sh1);
								output_row0_odd = _mm256_add_epi16(output_row0_odd,m2);


								//-----------row1
								//multiply with the mask
												m0=_mm256_maddubs_epi16(r0,c1_sh3);
												m1=_mm256_maddubs_epi16(r1,c2_sh3);
												m2=_mm256_maddubs_epi16(r2,c1_sh3);
												m3=_mm256_maddubs_epi16(r3,c0_sh3);

												//vertical add
												m0=_mm256_add_epi16(m0,m1);
												m0=_mm256_add_epi16(m0,m2);
												m0=_mm256_add_epi16(m0,m3);

												m1=_mm256_srli_si256(m0,2);
												m4=_mm256_add_epi16(m1,m0);

												m1=_mm256_srli_si256(m0,4);
												m2=_mm256_add_epi16(m1,m4);

												//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
												//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
												m4=_mm256_and_si256(m4,mask3);
												m4=_mm256_permute2f128_si256(m4,m4,1);
												m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions

												m2=_mm256_add_epi16(m2,m4);

												//m2 has 16 16bit values now. the results I need are in positions 1,4,7,10,13. keep only those, discard others
												m2 = _mm256_and_si256(m2,output_mask_sh1);
												output_row1_odd = _mm256_add_epi16(output_row1_odd,m2);



				//5th col iteration
				//multiply with the mask
				m0=_mm256_maddubs_epi16(r0,c0_sh4);
				m1=_mm256_maddubs_epi16(r1,c1_sh4);
				m2=_mm256_maddubs_epi16(r2,c2_sh4);
				m3=_mm256_maddubs_epi16(r3,c1_sh4);
				m4=_mm256_maddubs_epi16(r4,c0_sh4);

				//vertical add
				m0=_mm256_add_epi16(m0,m1);
				m2=_mm256_add_epi16(m2,m3);
				m0=_mm256_add_epi16(m0,m2);
				m0=_mm256_add_epi16(m0,m4);

				//hozizontal additions
				//hadd(4:9)
				//hadd(10:15)
				//hadd(16:21)
				//hadd(22:27)
				//result after division will be in 4,10,16,22

				m1=_mm256_srli_si256(m0,2);
				m2=_mm256_add_epi16(m1,m0);

				m1=_mm256_srli_si256(m0,4);
				m2=_mm256_add_epi16(m1,m2);


				//m2 has 16 16bit values now. the results I need are in positions 2,5,8,11. keep only those, discard others
				m2 = _mm256_and_si256(m2,output_mask_sh2);
				output_even = _mm256_add_epi16(output_even,m2);

				//row0
				//multiply with the mask
				m0=_mm256_maddubs_epi16(r0,c2_sh4);
				m1=_mm256_maddubs_epi16(r1,c1_sh4);
				m2=_mm256_maddubs_epi16(r2,c0_sh4);


				//vertical add
				m0=_mm256_add_epi16(m0,m1);
				m0=_mm256_add_epi16(m0,m2);


				//hozizontal additions
				//hadd(4:9)
				//hadd(10:15)
				//hadd(16:21)
				//hadd(22:27)
				//result after division will be in 4,10,16,22

				m1=_mm256_srli_si256(m0,2);
				m2=_mm256_add_epi16(m1,m0);

				m1=_mm256_srli_si256(m0,4);
				m2=_mm256_add_epi16(m1,m2);


				//m2 has 16 16bit values now. the results I need are in positions 2,5,8,11. keep only those, discard others
				m2 = _mm256_and_si256(m2,output_mask_sh2);
				output_row0_even = _mm256_add_epi16(output_row0_even,m2);

				//row1
				//multiply with the mask
				m0=_mm256_maddubs_epi16(r0,c1_sh4);
				m1=_mm256_maddubs_epi16(r1,c2_sh4);
				m2=_mm256_maddubs_epi16(r2,c1_sh4);
				m3=_mm256_maddubs_epi16(r3,c0_sh4);


				//vertical add
				m0=_mm256_add_epi16(m0,m1);
				m0=_mm256_add_epi16(m0,m2);
				m0=_mm256_add_epi16(m0,m3);


				m1=_mm256_srli_si256(m0,2);
				m2=_mm256_add_epi16(m1,m0);

				m1=_mm256_srli_si256(m0,4);
				m2=_mm256_add_epi16(m1,m2);


				//m2 has 16 16bit values now. the results I need are in positions 2,5,8,11. keep only those, discard others
				m2 = _mm256_and_si256(m2,output_mask_sh2);
				output_row1_even = _mm256_add_epi16(output_row1_even,m2);


		                 //6th col iteration
				//multiply with the mask
				m0=_mm256_maddubs_epi16(r0,c0_sh5);
				m1=_mm256_maddubs_epi16(r1,c1_sh5);
				m2=_mm256_maddubs_epi16(r2,c2_sh5);
				m3=_mm256_maddubs_epi16(r3,c1_sh5);
				m4=_mm256_maddubs_epi16(r4,c0_sh5);

				//vertical add
				m0=_mm256_add_epi16(m0,m1);
				m2=_mm256_add_epi16(m2,m3);
				m0=_mm256_add_epi16(m0,m2);
				m0=_mm256_add_epi16(m0,m4);

				//hozizontal additions
				//hadd(5:10)
				//hadd(11:16)
				//hadd(17:22)
				//hadd(23:28)
				//result after division will be in 4,10,16,22

				m1=_mm256_srli_si256(m0,2);
				m2=_mm256_add_epi16(m1,m0);

				m1=_mm256_srli_si256(m0,4);
				m2=_mm256_add_epi16(m1,m2);

				//m2 has 16 16bit values now. the results I need are in positions 2,5,8,11. keep only those, discard others
				m2 = _mm256_and_si256(m2,output_mask_sh2);
				output_odd = _mm256_add_epi16(output_odd,m2);

				//now division follows
				output_even=division(division_case,output_even,f);
				output_odd=division(division_case,output_odd,f);

				//shift odd 1 position and add to even
				output_odd = _mm256_slli_si256(output_odd,1);
				output_even = _mm256_add_epi8(output_even,output_odd);

				_mm256_storeu_si256( (__m256i *) &filt[2][col],output_even );

				//row0
				//multiply with the mask
				m0=_mm256_maddubs_epi16(r0,c2_sh5);
				m1=_mm256_maddubs_epi16(r1,c1_sh5);
				m2=_mm256_maddubs_epi16(r2,c0_sh5);

				//vertical add
				m0=_mm256_add_epi16(m0,m1);
				m0=_mm256_add_epi16(m0,m2);


				m1=_mm256_srli_si256(m0,2);
				m2=_mm256_add_epi16(m1,m0);

				m1=_mm256_srli_si256(m0,4);
				m2=_mm256_add_epi16(m1,m2);

				//m2 has 16 16bit values now. the results I need are in positions 2,5,8,11. keep only those, discard others
				m2 = _mm256_and_si256(m2,output_mask_sh2);
				output_row0_odd = _mm256_add_epi16(output_row0_odd,m2);

				//now division follows
				output_row0_even=division(division_case,output_row0_even,f);
				output_row0_odd=division(division_case,output_row0_odd,f);

				//shift odd 1 position and add to even
				output_row0_odd = _mm256_slli_si256(output_row0_odd,1);
				output_row0_even = _mm256_add_epi8(output_row0_even,output_row0_odd);


				_mm256_storeu_si256( (__m256i *) &filt[0][col],output_row0_even );

				//row1

				//multiply with the mask
				m0=_mm256_maddubs_epi16(r0,c1_sh5);
				m1=_mm256_maddubs_epi16(r1,c2_sh5);
				m2=_mm256_maddubs_epi16(r2,c1_sh5);
				m3=_mm256_maddubs_epi16(r3,c0_sh5);

				//vertical add
				m0=_mm256_add_epi16(m0,m1);
				m0=_mm256_add_epi16(m0,m2);
				m0=_mm256_add_epi16(m0,m3);


				m1=_mm256_srli_si256(m0,2);
				m2=_mm256_add_epi16(m1,m0);

				m1=_mm256_srli_si256(m0,4);
				m2=_mm256_add_epi16(m1,m2);

				//m2 has 16 16bit values now. the results I need are in positions 2,5,8,11. keep only those, discard others
				m2 = _mm256_and_si256(m2,output_mask_sh2);
				output_row1_odd = _mm256_add_epi16(output_row1_odd,m2);

				//now division follows
				output_row1_even=division(division_case,output_row1_even,f);
				output_row1_odd=division(division_case,output_row1_odd,f);

				//shift odd 1 position and add to even
				output_row1_odd = _mm256_slli_si256(output_row1_odd,1);
				output_row1_even = _mm256_add_epi8(output_row1_even,output_row1_odd);


				_mm256_storeu_si256( (__m256i *) &filt[1][col],output_row1_even );


				}


		    loop_reminder_first_less_div(frame1,filt,M,N,col,REMINDER_ITERATIONS,division_case,c0,c1,c2,c0_sh1,c1_sh1,c2_sh1,c0_sh2,c1_sh2,c2_sh2,f,divisor,filter5x5);
		}

		else if ((row==N-3) ){//in this case I calculate filt[N-3][:], filt[N-2][:] and filt[N-1][:]. Below row0 refers to row=N-2 and row1 refers to row=N-1
			//printf("\nN=%d row=%d N-2=%d",N,row,N-2);
			for (col = 0; col <= M-32; col+=28){
						 //last col value that does not read outside of the array bounds is (col<M-29)

							  if (col==0){

									//load the 5 rows
									r0=_mm256_loadu_si256( (__m256i *) &frame1[N-5][0]);
									r1=_mm256_loadu_si256( (__m256i *) &frame1[N-4][0]);
									r2=_mm256_loadu_si256( (__m256i *) &frame1[N-3][0]);
									r3=_mm256_loadu_si256( (__m256i *) &frame1[N-2][0]);
									r4=_mm256_loadu_si256( (__m256i *) &frame1[N-1][0]);

							  			//START - extra code needed for prelude
							  					m0=_mm256_slli_si256(r0,2);//shift 2 elements left - equivalent to filling with two zeros inthe beginning
							  					m1=_mm256_slli_si256(r1,2);//shift 2 elements left - equivalent to filling with two zeros inthe beginning
							  					m2=_mm256_slli_si256(r2,2);//shift 2 elements left - equivalent to filling with two zeros inthe beginning
							  					m3=_mm256_slli_si256(r3,2);//shift 2 elements left - equivalent to filling with two zeros inthe beginning
							  					m4=_mm256_slli_si256(r4,2);//shift 2 elements left - equivalent to filling with two zeros inthe beginning

							  					r0=_mm256_and_si256(r0,mask_prelude);
							  					r0=_mm256_permute2f128_si256(r0,r0,1);
							  					r0=_mm256_srli_si256(r0,14);//shift 14 elements
							  					r0=_mm256_add_epi16(m0,r0);

							  					r1=_mm256_and_si256(r1,mask_prelude);
							  					r1=_mm256_permute2f128_si256(r1,r1,1);
							  					r1=_mm256_srli_si256(r1,14);//shift 14 elements
							  					r1=_mm256_add_epi16(m1,r1);

							  					r2=_mm256_and_si256(r2,mask_prelude);
							  					r2=_mm256_permute2f128_si256(r2,r2,1);
							  					r2=_mm256_srli_si256(r2,14);//shift 14 elements
							  					r2=_mm256_add_epi16(m2,r2);

							  					r3=_mm256_and_si256(r3,mask_prelude);
							  					r3=_mm256_permute2f128_si256(r3,r3,1);
							  					r3=_mm256_srli_si256(r3,14);//shift 14 elements
							  					r3=_mm256_add_epi16(m3,r3);

							  					r4=_mm256_and_si256(r4,mask_prelude);
							  					r4=_mm256_permute2f128_si256(r4,r4,1);
							  					r4=_mm256_srli_si256(r4,14);//shift 14 elements
							  					r4=_mm256_add_epi16(m4,r4);

							  			//END - extra code needed for prelude
							  		}
							  else {
									//load the 5 rows
									r0=_mm256_loadu_si256( (__m256i *) &frame1[N-5][col-2]);
									r1=_mm256_loadu_si256( (__m256i *) &frame1[N-4][col-2]);
									r2=_mm256_loadu_si256( (__m256i *) &frame1[N-3][col-2]);
									r3=_mm256_loadu_si256( (__m256i *) &frame1[N-2][col-2]);
									r4=_mm256_loadu_si256( (__m256i *) &frame1[N-1][col-2]);
							  }

							//col iteration computes output pixels of 2,8,14,20,26
							// col+1 iteration computes output pixels of 3,9,15,21,27
							// col+2 iteration computes output pixels of 4,10,16,22,28
							// col+3 iteration computes output pixels of 5,11,17,23,29
							// col+4 iteration computes output pixels of 6,12,18,24,30
							// col+5 iteration computes output pixels of 7,13,19,25,31
							//afterwards, col2 becomes 32 and repeat the above process

							//1st col iteration
							 //--------------row=2

							//multiply with the mask
							m0=_mm256_maddubs_epi16(r0,c0);
							m1=_mm256_maddubs_epi16(r1,c1);
							m2=_mm256_maddubs_epi16(r2,c2);
							m3=_mm256_maddubs_epi16(r3,c1);
							m4=_mm256_maddubs_epi16(r4,c0);

							//vertical add
							m4=_mm256_add_epi16(m4,m3);
							m4=_mm256_add_epi16(m4,m2);
							m4=_mm256_add_epi16(m4,m1);
							m0=_mm256_add_epi16(m4,m0);

							m1=_mm256_srli_si256(m0,2);
							m2=_mm256_add_epi16(m1,m0);

							m1=_mm256_srli_si256(m0,4);
							m2=_mm256_add_epi16(m1,m2);

							//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
							//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
							m4=_mm256_and_si256(m0,mask3);
							m4=_mm256_permute2f128_si256(m4,m4,1);
							m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions

							m2=_mm256_add_epi16(m2,m4);

							//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
							output_even=_mm256_and_si256(m2,output_mask);

							//--------------row=0
							//multiply with the mask
							m1=_mm256_maddubs_epi16(r1,c0);
							m2=_mm256_maddubs_epi16(r2,c1);
							m3=_mm256_maddubs_epi16(r3,c2);
							m4=_mm256_maddubs_epi16(r4,c1);

							//vertical add
							m4=_mm256_add_epi16(m4,m3);
							m4=_mm256_add_epi16(m4,m2);
							m4=_mm256_add_epi16(m4,m1); m0_prelude_row0=m4;

							m1=_mm256_srli_si256(m0_prelude_row0,2);
							m2=_mm256_add_epi16(m1,m0_prelude_row0);

							m1=_mm256_srli_si256(m0_prelude_row0,4);
							m2=_mm256_add_epi16(m1,m2);

							//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
							//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
							m4=_mm256_and_si256(m0_prelude_row0,mask3);
							m4=_mm256_permute2f128_si256(m4,m4,1);
							m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions

							m2=_mm256_add_epi16(m2,m4);

							//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
							output_row0_even=_mm256_and_si256(m2,output_mask);

							//--------------row=1
							//multiply with the mask
							m2=_mm256_maddubs_epi16(r2,c0);
							m3=_mm256_maddubs_epi16(r3,c1);
							m4=_mm256_maddubs_epi16(r4,c2);


							//vertical add
							m4=_mm256_add_epi16(m4,m3);
							m4=_mm256_add_epi16(m4,m2);m0_prelude_row1=m4;



							m1=_mm256_srli_si256(m0_prelude_row1,2);
							m2=_mm256_add_epi16(m1,m0_prelude_row1);

							m1=_mm256_srli_si256(m0_prelude_row1,4);
							m2=_mm256_add_epi16(m1,m2);

							//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
							//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
							m4=_mm256_and_si256(m0_prelude_row1,mask3);
							m4=_mm256_permute2f128_si256(m4,m4,1);
							m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions

							m2=_mm256_add_epi16(m2,m4);

							//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
							output_row1_even=_mm256_and_si256(m2,output_mask);

							//2nd col iteration
							//----row=2

							//multiply with the mask
							m0=_mm256_maddubs_epi16(r0,c0_sh1);
							m1=_mm256_maddubs_epi16(r1,c1_sh1);
							m2=_mm256_maddubs_epi16(r2,c2_sh1);
							m3=_mm256_maddubs_epi16(r3,c1_sh1);
							m4=_mm256_maddubs_epi16(r4,c0_sh1);

							//vertical add
							m4=_mm256_add_epi16(m4,m3);
							m4=_mm256_add_epi16(m4,m2);
							m4=_mm256_add_epi16(m4,m1);
							m0=_mm256_add_epi16(m4,m0);


							m1=_mm256_srli_si256(m0,2);
							m2=_mm256_add_epi16(m1,m0);

							m1=_mm256_srli_si256(m0,4);
							m2=_mm256_add_epi16(m1,m2);

							//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
							//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
							m4=_mm256_and_si256(m0,mask3);
							m4=_mm256_permute2f128_si256(m4,m4,1);
							m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions
							m2=_mm256_add_epi16(m2,m4);

							//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
							output_odd = _mm256_and_si256(m2,output_mask);

							//----row=0
							//multiply with the mask
							m1=_mm256_maddubs_epi16(r1,c0_sh1);
							m2=_mm256_maddubs_epi16(r2,c1_sh1);
							m3=_mm256_maddubs_epi16(r3,c2_sh1);
							m4=_mm256_maddubs_epi16(r4,c1_sh1);


							//vertical add
							m4=_mm256_add_epi16(m4,m3);
							m4=_mm256_add_epi16(m4,m2);
							m4=_mm256_add_epi16(m4,m1);m0_prelude_row0=m4;


							m1=_mm256_srli_si256(m0_prelude_row0,2);
							m2=_mm256_add_epi16(m1,m0_prelude_row0);

							m1=_mm256_srli_si256(m0_prelude_row0,4);
							m2=_mm256_add_epi16(m1,m2);

							//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
							//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
							m4=_mm256_and_si256(m0_prelude_row0,mask3);
							m4=_mm256_permute2f128_si256(m4,m4,1);
							m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions
							m2=_mm256_add_epi16(m2,m4);

							//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
							output_row0_odd = _mm256_and_si256(m2,output_mask);

							//----row=1
							//multiply with the mask
							m2=_mm256_maddubs_epi16(r2,c0_sh1);
							m3=_mm256_maddubs_epi16(r3,c1_sh1);
							m4=_mm256_maddubs_epi16(r4,c2_sh1);


							//vertical add
							m4=_mm256_add_epi16(m4,m3);
							m4=_mm256_add_epi16(m4,m2);m0_prelude_row1=m4;


							m1=_mm256_srli_si256(m0_prelude_row1,2);
							m2=_mm256_add_epi16(m1,m0_prelude_row1);

							m1=_mm256_srli_si256(m0_prelude_row1,4);
							m2=_mm256_add_epi16(m1,m2);

							//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
							//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
							m4=_mm256_and_si256(m0_prelude_row1,mask3);
							m4=_mm256_permute2f128_si256(m4,m4,1);
							m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions
							m2=_mm256_add_epi16(m2,m4);

							//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
							output_row1_odd = _mm256_and_si256(m2,output_mask);

					                 //3rd col iteration
							//---row=2
							//multiply with the mask
							m0=_mm256_maddubs_epi16(r0,c0_sh2);
							m1=_mm256_maddubs_epi16(r1,c1_sh2);
							m2=_mm256_maddubs_epi16(r2,c2_sh2);
							m3=_mm256_maddubs_epi16(r3,c1_sh2);
							m4=_mm256_maddubs_epi16(r4,c0_sh2);

							//vertical add
							m4=_mm256_add_epi16(m4,m3);
							m4=_mm256_add_epi16(m4,m2);
							m4=_mm256_add_epi16(m4,m1);
							m0=_mm256_add_epi16(m4,m0);


							m1=_mm256_srli_si256(m0,2);
							m2=_mm256_add_epi16(m1,m0);

							//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,_mm256_extract_epi16(m2,8),0,0,0,0,0,0,0);
							m4=_mm256_and_si256(m2,mask3);
							m4=_mm256_permute2f128_si256(m4,m4,1);
							m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions

							m1=_mm256_srli_si256(m0,4);
							m2=_mm256_add_epi16(m1,m2);

							//prepei na prosthesw to pixel pu thelw sto r2 kai meta na kanw diairesi me ola mazi

							m2=_mm256_add_epi16(m2,m4);

							//m2 has 16 16bit values now. the results I need are in positions 1,4,7,10,13. keep only those, discard others
							m2 = _mm256_and_si256(m2,output_mask_sh1);
							output_even = _mm256_add_epi16(output_even,m2);

							//---row=0
							//multiply with the mask
							m1=_mm256_maddubs_epi16(r1,c0_sh2);
							m2=_mm256_maddubs_epi16(r2,c1_sh2);
							m3=_mm256_maddubs_epi16(r3,c2_sh2);
							m4=_mm256_maddubs_epi16(r4,c1_sh2);


							//vertical add
							m4=_mm256_add_epi16(m4,m3);
							m4=_mm256_add_epi16(m4,m2);
							m4=_mm256_add_epi16(m4,m1);m0_prelude_row0=m4;

							m1=_mm256_srli_si256(m0_prelude_row0,2);
							m2=_mm256_add_epi16(m1,m0_prelude_row0);

							//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,_mm256_extract_epi16(m2,8),0,0,0,0,0,0,0);
							m4=_mm256_and_si256(m2,mask3);
							m4=_mm256_permute2f128_si256(m4,m4,1);
							m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions

							m1=_mm256_srli_si256(m0_prelude_row0,4);
							m2=_mm256_add_epi16(m1,m2);

							//prepei na prosthesw to pixel pu thelw sto r2 kai meta na kanw diairesi me ola mazi

							m2=_mm256_add_epi16(m2,m4);

							//m2 has 16 16bit values now. the results I need are in positions 1,4,7,10,13. keep only those, discard others
							m2 = _mm256_and_si256(m2,output_mask_sh1);
							output_row0_even = _mm256_add_epi16(output_row0_even,m2);


							//---row=1
							//multiply with the mask
							m2=_mm256_maddubs_epi16(r2,c0_sh2);
							m3=_mm256_maddubs_epi16(r3,c1_sh2);
							m4=_mm256_maddubs_epi16(r4,c2_sh2);


							//vertical add
							m4=_mm256_add_epi16(m4,m3);
							m4=_mm256_add_epi16(m4,m2);m0_prelude_row1=m4;


							m1=_mm256_srli_si256(m0_prelude_row1,2);
							m2=_mm256_add_epi16(m1,m0_prelude_row1);

							//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,_mm256_extract_epi16(m2,8),0,0,0,0,0,0,0);
							m4=_mm256_and_si256(m2,mask3);
							m4=_mm256_permute2f128_si256(m4,m4,1);
							m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions

							m1=_mm256_srli_si256(m0_prelude_row1,4);
							m2=_mm256_add_epi16(m1,m2);

							//prepei na prosthesw to pixel pu thelw sto r2 kai meta na kanw diairesi me ola mazi

							m2=_mm256_add_epi16(m2,m4);

							//m2 has 16 16bit values now. the results I need are in positions 1,4,7,10,13. keep only those, discard others
							m2 = _mm256_and_si256(m2,output_mask_sh1);
							output_row1_even = _mm256_add_epi16(output_row1_even,m2);

							//4th col iteration
							//multiply with the mask
							m0=_mm256_maddubs_epi16(r0,c0_sh3);
							m1=_mm256_maddubs_epi16(r1,c1_sh3);
							m2=_mm256_maddubs_epi16(r2,c2_sh3);
							m3=_mm256_maddubs_epi16(r3,c1_sh3);
							m4=_mm256_maddubs_epi16(r4,c0_sh3);

							//vertical add
							m0=_mm256_add_epi16(m0,m1);
							m2=_mm256_add_epi16(m2,m3);
							m0=_mm256_add_epi16(m0,m2);
							m0=_mm256_add_epi16(m0,m4);

							//hozizontal additions
							//hadd(3:8)
							//hadd(9:14)
							//hadd(15:20)
							//hadd(21:26)
							//hadd(27:31)
							//result after division will be in 2,8,14,20,26

							m1=_mm256_srli_si256(m0,2);
							m4=_mm256_add_epi16(m1,m0);

							m1=_mm256_srli_si256(m0,4);
							m2=_mm256_add_epi16(m1,m4);

							//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
							//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
							m4=_mm256_and_si256(m4,mask3);
							m4=_mm256_permute2f128_si256(m4,m4,1);
							m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions

							m2=_mm256_add_epi16(m2,m4);

							//m2 has 16 16bit values now. the results I need are in positions 1,4,7,10,13. keep only those, discard others
							m2 = _mm256_and_si256(m2,output_mask_sh1);
							output_odd = _mm256_add_epi16(output_odd,m2);

							//row0
							//multiply with the mask
								m1=_mm256_maddubs_epi16(r1,c0_sh3);
								m2=_mm256_maddubs_epi16(r2,c1_sh3);
								m3=_mm256_maddubs_epi16(r3,c2_sh3);
								m4=_mm256_maddubs_epi16(r4,c1_sh3);

								//vertical add
								m0=_mm256_add_epi16(m1,m2);
								m0=_mm256_add_epi16(m0,m3);
								m0=_mm256_add_epi16(m0,m4);


								m1=_mm256_srli_si256(m0,2);
								m4=_mm256_add_epi16(m1,m0);

								m1=_mm256_srli_si256(m0,4);
								m2=_mm256_add_epi16(m1,m4);

								//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
								//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
								m4=_mm256_and_si256(m4,mask3);
								m4=_mm256_permute2f128_si256(m4,m4,1);
								m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions

								m2=_mm256_add_epi16(m2,m4);

								//m2 has 16 16bit values now. the results I need are in positions 1,4,7,10,13. keep only those, discard others
								m2 = _mm256_and_si256(m2,output_mask_sh1);
								output_row0_odd = _mm256_add_epi16(output_row0_odd,m2);

								//row1
								//multiply with the mask
									m2=_mm256_maddubs_epi16(r2,c0_sh3);
									m3=_mm256_maddubs_epi16(r3,c1_sh3);
									m4=_mm256_maddubs_epi16(r4,c2_sh3);

									//vertical add
									m0=_mm256_add_epi16(m2,m3);
									m0=_mm256_add_epi16(m0,m4);


									m1=_mm256_srli_si256(m0,2);
									m4=_mm256_add_epi16(m1,m0);

									m1=_mm256_srli_si256(m0,4);
									m2=_mm256_add_epi16(m1,m4);

									//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
									//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
									m4=_mm256_and_si256(m4,mask3);
									m4=_mm256_permute2f128_si256(m4,m4,1);
									m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions

									m2=_mm256_add_epi16(m2,m4);

									//m2 has 16 16bit values now. the results I need are in positions 1,4,7,10,13. keep only those, discard others
									m2 = _mm256_and_si256(m2,output_mask_sh1);
									output_row1_odd = _mm256_add_epi16(output_row1_odd,m2);


							//5th col iteration
							//multiply with the mask
							m0=_mm256_maddubs_epi16(r0,c0_sh4);
							m1=_mm256_maddubs_epi16(r1,c1_sh4);
							m2=_mm256_maddubs_epi16(r2,c2_sh4);
							m3=_mm256_maddubs_epi16(r3,c1_sh4);
							m4=_mm256_maddubs_epi16(r4,c0_sh4);

							//vertical add
							m0=_mm256_add_epi16(m0,m1);
							m2=_mm256_add_epi16(m2,m3);
							m0=_mm256_add_epi16(m0,m2);
							m0=_mm256_add_epi16(m0,m4);

							//hozizontal additions
							//hadd(4:9)
							//hadd(10:15)
							//hadd(16:21)
							//hadd(22:27)
							//result after division will be in 4,10,16,22

							m1=_mm256_srli_si256(m0,2);
							m2=_mm256_add_epi16(m1,m0);

							m1=_mm256_srli_si256(m0,4);
							m2=_mm256_add_epi16(m1,m2);


							//m2 has 16 16bit values now. the results I need are in positions 2,5,8,11. keep only those, discard others
							m2 = _mm256_and_si256(m2,output_mask_sh2);
							output_even = _mm256_add_epi16(output_even,m2);

							//row0
							//multiply with the mask
							m1=_mm256_maddubs_epi16(r1,c0_sh4);
							m2=_mm256_maddubs_epi16(r2,c1_sh4);
							m3=_mm256_maddubs_epi16(r3,c2_sh4);
							m4=_mm256_maddubs_epi16(r4,c1_sh4);

							//vertical add
							m0=_mm256_add_epi16(m1,m2);
							m0=_mm256_add_epi16(m0,m3);
							m0=_mm256_add_epi16(m0,m4);


							m1=_mm256_srli_si256(m0,2);
							m2=_mm256_add_epi16(m1,m0);

							m1=_mm256_srli_si256(m0,4);
							m2=_mm256_add_epi16(m1,m2);


							//m2 has 16 16bit values now. the results I need are in positions 2,5,8,11. keep only those, discard others
							m2 = _mm256_and_si256(m2,output_mask_sh2);
							output_row0_even = _mm256_add_epi16(output_row0_even,m2);

							//row1
							//multiply with the mask
							m2=_mm256_maddubs_epi16(r2,c0_sh4);
							m3=_mm256_maddubs_epi16(r3,c1_sh4);
							m4=_mm256_maddubs_epi16(r4,c2_sh4);

							//vertical add
							m0=_mm256_add_epi16(m2,m3);
							m0=_mm256_add_epi16(m0,m4);


							m1=_mm256_srli_si256(m0,2);
							m2=_mm256_add_epi16(m1,m0);

							m1=_mm256_srli_si256(m0,4);
							m2=_mm256_add_epi16(m1,m2);


							//m2 has 16 16bit values now. the results I need are in positions 2,5,8,11. keep only those, discard others
							m2 = _mm256_and_si256(m2,output_mask_sh2);
							output_row1_even = _mm256_add_epi16(output_row1_even,m2);

					                 //6th col iteration
							//multiply with the mask
							m0=_mm256_maddubs_epi16(r0,c0_sh5);
							m1=_mm256_maddubs_epi16(r1,c1_sh5);
							m2=_mm256_maddubs_epi16(r2,c2_sh5);
							m3=_mm256_maddubs_epi16(r3,c1_sh5);
							m4=_mm256_maddubs_epi16(r4,c0_sh5);

							//vertical add
							m0=_mm256_add_epi16(m0,m1);
							m2=_mm256_add_epi16(m2,m3);
							m0=_mm256_add_epi16(m0,m2);
							m0=_mm256_add_epi16(m0,m4);

							//hozizontal additions
							//hadd(5:10)
							//hadd(11:16)
							//hadd(17:22)
							//hadd(23:28)
							//result after division will be in 4,10,16,22

							m1=_mm256_srli_si256(m0,2);
							m2=_mm256_add_epi16(m1,m0);

							m1=_mm256_srli_si256(m0,4);
							m2=_mm256_add_epi16(m1,m2);

							//m2 has 16 16bit values now. the results I need are in positions 2,5,8,11. keep only those, discard others
							m2 = _mm256_and_si256(m2,output_mask_sh2);
							output_odd = _mm256_add_epi16(output_odd,m2);

							//now division follows
							output_even=division(division_case,output_even,f);
							output_odd=division(division_case,output_odd,f);

							//shift odd 1 position and add to even
							output_odd = _mm256_slli_si256(output_odd,1);
							output_even = _mm256_add_epi8(output_even,output_odd);


							_mm256_storeu_si256( (__m256i *) &filt[N-3][col],output_even );

							//row0
							//multiply with the mask
							m1=_mm256_maddubs_epi16(r1,c0_sh5);
							m2=_mm256_maddubs_epi16(r2,c1_sh5);
							m3=_mm256_maddubs_epi16(r3,c2_sh5);
							m4=_mm256_maddubs_epi16(r4,c1_sh5);

							//vertical add
							m0=_mm256_add_epi16(m1,m2);
							m0=_mm256_add_epi16(m0,m3);
							m0=_mm256_add_epi16(m0,m4);


							m1=_mm256_srli_si256(m0,2);
							m2=_mm256_add_epi16(m1,m0);

							m1=_mm256_srli_si256(m0,4);
							m2=_mm256_add_epi16(m1,m2);

							//m2 has 16 16bit values now. the results I need are in positions 2,5,8,11. keep only those, discard others
							m2 = _mm256_and_si256(m2,output_mask_sh2);
							output_row0_odd = _mm256_add_epi16(output_row0_odd,m2);

							//now division follows
							output_row0_even=division(division_case,output_row0_even,f);
							output_row0_odd=division(division_case,output_row0_odd,f);

							//shift odd 1 position and add to even
							output_row0_odd = _mm256_slli_si256(output_row0_odd,1);
							output_row0_even = _mm256_add_epi8(output_row0_even,output_row0_odd);


							_mm256_storeu_si256( (__m256i *) &filt[N-2][col],output_row0_even );

							//row1
							//multiply with the mask
							m2=_mm256_maddubs_epi16(r2,c0_sh5);
							m3=_mm256_maddubs_epi16(r3,c1_sh5);
							m4=_mm256_maddubs_epi16(r4,c2_sh5);

							//vertical add
							m0=_mm256_add_epi16(m2,m3);
							m0=_mm256_add_epi16(m0,m4);

								m1=_mm256_srli_si256(m0,2);
							m2=_mm256_add_epi16(m1,m0);

							m1=_mm256_srli_si256(m0,4);
							m2=_mm256_add_epi16(m1,m2);

							//m2 has 16 16bit values now. the results I need are in positions 2,5,8,11. keep only those, discard others
							m2 = _mm256_and_si256(m2,output_mask_sh2);
							output_row1_odd = _mm256_add_epi16(output_row1_odd,m2);

							//now division follows
							output_row1_even=division(division_case,output_row1_even,f);
							output_row1_odd=division(division_case,output_row1_odd,f);

							//shift odd 1 position and add to even
							output_row1_odd = _mm256_slli_si256(output_row1_odd,1);
							output_row1_even = _mm256_add_epi8(output_row1_even,output_row1_odd);


							_mm256_storeu_si256( (__m256i *) &filt[N-1][col],output_row1_even );


							}

		    loop_reminder_last_less_div(frame1,filt,M,N,col,REMINDER_ITERATIONS,division_case,c0,c1,c2,c0_sh1,c1_sh1,c2_sh1,c0_sh2,c1_sh2,c2_sh2,f,divisor,filter5x5);

					}
		//an extra condition is needed because of register blocking
		else if ((row==N-2) ){//in this case I calculate just filt[N-2][:] and filt[N-1][:]. Below row0 refers to row=N-2 and row1 refers to row=N-1
					//printf("\nN=%d row=%d N-2=%d",N,row,N-2);
					for (col = 0; col <= M-32; col+=28){
								 //last col value that does not read outside of the array bounds is (col<M-29)

									  if (col==0){

											r1=_mm256_loadu_si256( (__m256i *) &frame1[N-4][0]);
											r2=_mm256_loadu_si256( (__m256i *) &frame1[N-3][0]);
											r3=_mm256_loadu_si256( (__m256i *) &frame1[N-2][0]);
											r4=_mm256_loadu_si256( (__m256i *) &frame1[N-1][0]);

									  			//START - extra code needed for prelude
									  					m1=_mm256_slli_si256(r1,2);//shift 2 elements left - equivalent to filling with two zeros inthe beginning
									  					m2=_mm256_slli_si256(r2,2);//shift 2 elements left - equivalent to filling with two zeros inthe beginning
									  					m3=_mm256_slli_si256(r3,2);//shift 2 elements left - equivalent to filling with two zeros inthe beginning
									  					m4=_mm256_slli_si256(r4,2);//shift 2 elements left - equivalent to filling with two zeros inthe beginning

									  					r1=_mm256_and_si256(r1,mask_prelude);
									  					r1=_mm256_permute2f128_si256(r1,r1,1);
									  					r1=_mm256_srli_si256(r1,14);//shift 14 elements
									  					r1=_mm256_add_epi16(m1,r1);

									  					r2=_mm256_and_si256(r2,mask_prelude);
									  					r2=_mm256_permute2f128_si256(r2,r2,1);
									  					r2=_mm256_srli_si256(r2,14);//shift 14 elements
									  					r2=_mm256_add_epi16(m2,r2);

									  					r3=_mm256_and_si256(r3,mask_prelude);
									  					r3=_mm256_permute2f128_si256(r3,r3,1);
									  					r3=_mm256_srli_si256(r3,14);//shift 14 elements
									  					r3=_mm256_add_epi16(m3,r3);

									  					r4=_mm256_and_si256(r4,mask_prelude);
									  					r4=_mm256_permute2f128_si256(r4,r4,1);
									  					r4=_mm256_srli_si256(r4,14);//shift 14 elements
									  					r4=_mm256_add_epi16(m4,r4);

									  			//END - extra code needed for prelude
									  		}
									  else {

											r1=_mm256_loadu_si256( (__m256i *) &frame1[N-4][col-2]);
											r2=_mm256_loadu_si256( (__m256i *) &frame1[N-3][col-2]);
											r3=_mm256_loadu_si256( (__m256i *) &frame1[N-2][col-2]);
											r4=_mm256_loadu_si256( (__m256i *) &frame1[N-1][col-2]);
									  }

									//col iteration computes output pixels of 2,8,14,20,26
									// col+1 iteration computes output pixels of 3,9,15,21,27
									// col+2 iteration computes output pixels of 4,10,16,22,28
									// col+3 iteration computes output pixels of 5,11,17,23,29
									// col+4 iteration computes output pixels of 6,12,18,24,30
									// col+5 iteration computes output pixels of 7,13,19,25,31
									//afterwards, col2 becomes 32 and repeat the above process

									//1st col iteration

									//--------------row=0
									//multiply with the mask
									m1=_mm256_maddubs_epi16(r1,c0);
									m2=_mm256_maddubs_epi16(r2,c1);
									m3=_mm256_maddubs_epi16(r3,c2);
									m4=_mm256_maddubs_epi16(r4,c1);

									//vertical add
									m4=_mm256_add_epi16(m4,m3);
									m4=_mm256_add_epi16(m4,m2);
									m4=_mm256_add_epi16(m4,m1); m0_prelude_row0=m4;

									m1=_mm256_srli_si256(m0_prelude_row0,2);
									m2=_mm256_add_epi16(m1,m0_prelude_row0);

									m1=_mm256_srli_si256(m0_prelude_row0,4);
									m2=_mm256_add_epi16(m1,m2);

									//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
									//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
									m4=_mm256_and_si256(m0_prelude_row0,mask3);
									m4=_mm256_permute2f128_si256(m4,m4,1);
									m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions

									m2=_mm256_add_epi16(m2,m4);

									//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
									output_row0_even=_mm256_and_si256(m2,output_mask);

									//--------------row=1
									//multiply with the mask
									m2=_mm256_maddubs_epi16(r2,c0);
									m3=_mm256_maddubs_epi16(r3,c1);
									m4=_mm256_maddubs_epi16(r4,c2);


									//vertical add
									m4=_mm256_add_epi16(m4,m3);
									m4=_mm256_add_epi16(m4,m2);m0_prelude_row1=m4;



									m1=_mm256_srli_si256(m0_prelude_row1,2);
									m2=_mm256_add_epi16(m1,m0_prelude_row1);

									m1=_mm256_srli_si256(m0_prelude_row1,4);
									m2=_mm256_add_epi16(m1,m2);

									//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
									//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
									m4=_mm256_and_si256(m0_prelude_row1,mask3);
									m4=_mm256_permute2f128_si256(m4,m4,1);
									m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions

									m2=_mm256_add_epi16(m2,m4);

									//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
									output_row1_even=_mm256_and_si256(m2,output_mask);

									//2nd col iteration

									//----row=0
									//multiply with the mask
									m1=_mm256_maddubs_epi16(r1,c0_sh1);
									m2=_mm256_maddubs_epi16(r2,c1_sh1);
									m3=_mm256_maddubs_epi16(r3,c2_sh1);
									m4=_mm256_maddubs_epi16(r4,c1_sh1);


									//vertical add
									m4=_mm256_add_epi16(m4,m3);
									m4=_mm256_add_epi16(m4,m2);
									m4=_mm256_add_epi16(m4,m1);m0_prelude_row0=m4;


									m1=_mm256_srli_si256(m0_prelude_row0,2);
									m2=_mm256_add_epi16(m1,m0_prelude_row0);

									m1=_mm256_srli_si256(m0_prelude_row0,4);
									m2=_mm256_add_epi16(m1,m2);

									//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
									//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
									m4=_mm256_and_si256(m0_prelude_row0,mask3);
									m4=_mm256_permute2f128_si256(m4,m4,1);
									m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions
									m2=_mm256_add_epi16(m2,m4);

									//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
									output_row0_odd = _mm256_and_si256(m2,output_mask);

									//----row=1
									//multiply with the mask
									m2=_mm256_maddubs_epi16(r2,c0_sh1);
									m3=_mm256_maddubs_epi16(r3,c1_sh1);
									m4=_mm256_maddubs_epi16(r4,c2_sh1);


									//vertical add
									m4=_mm256_add_epi16(m4,m3);
									m4=_mm256_add_epi16(m4,m2);m0_prelude_row1=m4;


									m1=_mm256_srli_si256(m0_prelude_row1,2);
									m2=_mm256_add_epi16(m1,m0_prelude_row1);

									m1=_mm256_srli_si256(m0_prelude_row1,4);
									m2=_mm256_add_epi16(m1,m2);

									//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
									//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
									m4=_mm256_and_si256(m0_prelude_row1,mask3);
									m4=_mm256_permute2f128_si256(m4,m4,1);
									m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions
									m2=_mm256_add_epi16(m2,m4);

									//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
									output_row1_odd = _mm256_and_si256(m2,output_mask);

							                 //3rd col iteration


									//---row=0
									//multiply with the mask
									m1=_mm256_maddubs_epi16(r1,c0_sh2);
									m2=_mm256_maddubs_epi16(r2,c1_sh2);
									m3=_mm256_maddubs_epi16(r3,c2_sh2);
									m4=_mm256_maddubs_epi16(r4,c1_sh2);


									//vertical add
									m4=_mm256_add_epi16(m4,m3);
									m4=_mm256_add_epi16(m4,m2);
									m4=_mm256_add_epi16(m4,m1);m0_prelude_row0=m4;

									m1=_mm256_srli_si256(m0_prelude_row0,2);
									m2=_mm256_add_epi16(m1,m0_prelude_row0);

									//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,_mm256_extract_epi16(m2,8),0,0,0,0,0,0,0);
									m4=_mm256_and_si256(m2,mask3);
									m4=_mm256_permute2f128_si256(m4,m4,1);
									m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions

									m1=_mm256_srli_si256(m0_prelude_row0,4);
									m2=_mm256_add_epi16(m1,m2);

									//prepei na prosthesw to pixel pu thelw sto r2 kai meta na kanw diairesi me ola mazi

									m2=_mm256_add_epi16(m2,m4);

									//m2 has 16 16bit values now. the results I need are in positions 1,4,7,10,13. keep only those, discard others
									m2 = _mm256_and_si256(m2,output_mask_sh1);
									output_row0_even = _mm256_add_epi16(output_row0_even,m2);


									//---row=1
									//multiply with the mask
									m2=_mm256_maddubs_epi16(r2,c0_sh2);
									m3=_mm256_maddubs_epi16(r3,c1_sh2);
									m4=_mm256_maddubs_epi16(r4,c2_sh2);


									//vertical add
									m4=_mm256_add_epi16(m4,m3);
									m4=_mm256_add_epi16(m4,m2);m0_prelude_row1=m4;


									m1=_mm256_srli_si256(m0_prelude_row1,2);
									m2=_mm256_add_epi16(m1,m0_prelude_row1);

									//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,_mm256_extract_epi16(m2,8),0,0,0,0,0,0,0);
									m4=_mm256_and_si256(m2,mask3);
									m4=_mm256_permute2f128_si256(m4,m4,1);
									m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions

									m1=_mm256_srli_si256(m0_prelude_row1,4);
									m2=_mm256_add_epi16(m1,m2);

									//prepei na prosthesw to pixel pu thelw sto r2 kai meta na kanw diairesi me ola mazi

									m2=_mm256_add_epi16(m2,m4);

									//m2 has 16 16bit values now. the results I need are in positions 1,4,7,10,13. keep only those, discard others
									m2 = _mm256_and_si256(m2,output_mask_sh1);
									output_row1_even = _mm256_add_epi16(output_row1_even,m2);

									//4th col iteration

									//row0
									//multiply with the mask
										m1=_mm256_maddubs_epi16(r1,c0_sh3);
										m2=_mm256_maddubs_epi16(r2,c1_sh3);
										m3=_mm256_maddubs_epi16(r3,c2_sh3);
										m4=_mm256_maddubs_epi16(r4,c1_sh3);

										//vertical add
										m0=_mm256_add_epi16(m1,m2);
										m0=_mm256_add_epi16(m0,m3);
										m0=_mm256_add_epi16(m0,m4);


										m1=_mm256_srli_si256(m0,2);
										m4=_mm256_add_epi16(m1,m0);

										m1=_mm256_srli_si256(m0,4);
										m2=_mm256_add_epi16(m1,m4);

										//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
										//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
										m4=_mm256_and_si256(m4,mask3);
										m4=_mm256_permute2f128_si256(m4,m4,1);
										m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions

										m2=_mm256_add_epi16(m2,m4);

										//m2 has 16 16bit values now. the results I need are in positions 1,4,7,10,13. keep only those, discard others
										m2 = _mm256_and_si256(m2,output_mask_sh1);
										output_row0_odd = _mm256_add_epi16(output_row0_odd,m2);

										//row1
										//multiply with the mask
											m2=_mm256_maddubs_epi16(r2,c0_sh3);
											m3=_mm256_maddubs_epi16(r3,c1_sh3);
											m4=_mm256_maddubs_epi16(r4,c2_sh3);

											//vertical add
											m0=_mm256_add_epi16(m2,m3);
											m0=_mm256_add_epi16(m0,m4);


											m1=_mm256_srli_si256(m0,2);
											m4=_mm256_add_epi16(m1,m0);

											m1=_mm256_srli_si256(m0,4);
											m2=_mm256_add_epi16(m1,m4);

											//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
											//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
											m4=_mm256_and_si256(m4,mask3);
											m4=_mm256_permute2f128_si256(m4,m4,1);
											m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions

											m2=_mm256_add_epi16(m2,m4);

											//m2 has 16 16bit values now. the results I need are in positions 1,4,7,10,13. keep only those, discard others
											m2 = _mm256_and_si256(m2,output_mask_sh1);
											output_row1_odd = _mm256_add_epi16(output_row1_odd,m2);


									//5th col iteration


									//row0
									//multiply with the mask
									m1=_mm256_maddubs_epi16(r1,c0_sh4);
									m2=_mm256_maddubs_epi16(r2,c1_sh4);
									m3=_mm256_maddubs_epi16(r3,c2_sh4);
									m4=_mm256_maddubs_epi16(r4,c1_sh4);

									//vertical add
									m0=_mm256_add_epi16(m1,m2);
									m0=_mm256_add_epi16(m0,m3);
									m0=_mm256_add_epi16(m0,m4);


									m1=_mm256_srli_si256(m0,2);
									m2=_mm256_add_epi16(m1,m0);

									m1=_mm256_srli_si256(m0,4);
									m2=_mm256_add_epi16(m1,m2);


									//m2 has 16 16bit values now. the results I need are in positions 2,5,8,11. keep only those, discard others
									m2 = _mm256_and_si256(m2,output_mask_sh2);
									output_row0_even = _mm256_add_epi16(output_row0_even,m2);

									//row1
									//multiply with the mask
									m2=_mm256_maddubs_epi16(r2,c0_sh4);
									m3=_mm256_maddubs_epi16(r3,c1_sh4);
									m4=_mm256_maddubs_epi16(r4,c2_sh4);

									//vertical add
									m0=_mm256_add_epi16(m2,m3);
									m0=_mm256_add_epi16(m0,m4);


									m1=_mm256_srli_si256(m0,2);
									m2=_mm256_add_epi16(m1,m0);

									m1=_mm256_srli_si256(m0,4);
									m2=_mm256_add_epi16(m1,m2);


									//m2 has 16 16bit values now. the results I need are in positions 2,5,8,11. keep only those, discard others
									m2 = _mm256_and_si256(m2,output_mask_sh2);
									output_row1_even = _mm256_add_epi16(output_row1_even,m2);

							                 //6th col iteration

									//row0
									//multiply with the mask
									m1=_mm256_maddubs_epi16(r1,c0_sh5);
									m2=_mm256_maddubs_epi16(r2,c1_sh5);
									m3=_mm256_maddubs_epi16(r3,c2_sh5);
									m4=_mm256_maddubs_epi16(r4,c1_sh5);

									//vertical add
									m0=_mm256_add_epi16(m1,m2);
									m0=_mm256_add_epi16(m0,m3);
									m0=_mm256_add_epi16(m0,m4);


									m1=_mm256_srli_si256(m0,2);
									m2=_mm256_add_epi16(m1,m0);

									m1=_mm256_srli_si256(m0,4);
									m2=_mm256_add_epi16(m1,m2);

									//m2 has 16 16bit values now. the results I need are in positions 2,5,8,11. keep only those, discard others
									m2 = _mm256_and_si256(m2,output_mask_sh2);
									output_row0_odd = _mm256_add_epi16(output_row0_odd,m2);

									//now division follows
									output_row0_even=division(division_case,output_row0_even,f);
									output_row0_odd=division(division_case,output_row0_odd,f);

									//shift odd 1 position and add to even
									output_row0_odd = _mm256_slli_si256(output_row0_odd,1);
									output_row0_even = _mm256_add_epi8(output_row0_even,output_row0_odd);


									_mm256_storeu_si256( (__m256i *) &filt[N-2][col],output_row0_even );

									//row1
									//multiply with the mask
									m2=_mm256_maddubs_epi16(r2,c0_sh5);
									m3=_mm256_maddubs_epi16(r3,c1_sh5);
									m4=_mm256_maddubs_epi16(r4,c2_sh5);

									//vertical add
									m0=_mm256_add_epi16(m2,m3);
									m0=_mm256_add_epi16(m0,m4);

										m1=_mm256_srli_si256(m0,2);
									m2=_mm256_add_epi16(m1,m0);

									m1=_mm256_srli_si256(m0,4);
									m2=_mm256_add_epi16(m1,m2);

									//m2 has 16 16bit values now. the results I need are in positions 2,5,8,11. keep only those, discard others
									m2 = _mm256_and_si256(m2,output_mask_sh2);
									output_row1_odd = _mm256_add_epi16(output_row1_odd,m2);

									//now division follows
									output_row1_even=division(division_case,output_row1_even,f);
									output_row1_odd=division(division_case,output_row1_odd,f);

									//shift odd 1 position and add to even
									output_row1_odd = _mm256_slli_si256(output_row1_odd,1);
									output_row1_even = _mm256_add_epi8(output_row1_even,output_row1_odd);


									_mm256_storeu_si256( (__m256i *) &filt[N-1][col],output_row1_even );


									}

				    loop_reminder_last_less_div_special_case(frame1,filt,M,N,col,REMINDER_ITERATIONS,division_case,c0,c1,c2,c0_sh1,c1_sh1,c2_sh1,c0_sh2,c1_sh2,c2_sh2,f,divisor,filter5x5);

							}

	else {


	  for (col = 0; col <= M-32; col+=28){
	 //last col value that does not read outside of the array bounds is (col<M-29)

		  if (col==0){

				//load the 5 rows
				r0=_mm256_loadu_si256( (__m256i *) &frame1[row-2][0]);
				r1=_mm256_loadu_si256( (__m256i *) &frame1[row-1][0]);
				r2=_mm256_loadu_si256( (__m256i *) &frame1[row][0]);
				r3=_mm256_loadu_si256( (__m256i *) &frame1[row+1][0]);
				r4=_mm256_loadu_si256( (__m256i *) &frame1[row+2][0]);
				r5=_mm256_loadu_si256( (__m256i *) &frame1[row+3][0]);


		  			//START - extra code needed for prelude
		  					m0=_mm256_slli_si256(r0,2);//shift 2 elements left - equivalent to filling with two zeros inthe beginning
		  					m1=_mm256_slli_si256(r1,2);//shift 2 elements left - equivalent to filling with two zeros inthe beginning
		  					m2=_mm256_slli_si256(r2,2);//shift 2 elements left - equivalent to filling with two zeros inthe beginning
		  					m3=_mm256_slli_si256(r3,2);//shift 2 elements left - equivalent to filling with two zeros inthe beginning
		  					m4=_mm256_slli_si256(r4,2);//shift 2 elements left - equivalent to filling with two zeros inthe beginning
		  					m5=_mm256_slli_si256(r5,2);//shift 2 elements left - equivalent to filling with two zeros inthe beginning


		  					r0=_mm256_and_si256(r0,mask_prelude);
		  					r0=_mm256_permute2f128_si256(r0,r0,1);
		  					r0=_mm256_srli_si256(r0,14);//shift 14 elements
		  					r0=_mm256_add_epi16(m0,r0);

		  					r1=_mm256_and_si256(r1,mask_prelude);
		  					r1=_mm256_permute2f128_si256(r1,r1,1);
		  					r1=_mm256_srli_si256(r1,14);//shift 14 elements
		  					r1=_mm256_add_epi16(m1,r1);

		  					r2=_mm256_and_si256(r2,mask_prelude);
		  					r2=_mm256_permute2f128_si256(r2,r2,1);
		  					r2=_mm256_srli_si256(r2,14);//shift 14 elements
		  					r2=_mm256_add_epi16(m2,r2);

		  					r3=_mm256_and_si256(r3,mask_prelude);
		  					r3=_mm256_permute2f128_si256(r3,r3,1);
		  					r3=_mm256_srli_si256(r3,14);//shift 14 elements
		  					r3=_mm256_add_epi16(m3,r3);

		  					r4=_mm256_and_si256(r4,mask_prelude);
		  					r4=_mm256_permute2f128_si256(r4,r4,1);
		  					r4=_mm256_srli_si256(r4,14);//shift 14 elements
		  					r4=_mm256_add_epi16(m4,r4);

		  					r5=_mm256_and_si256(r5,mask_prelude);
		  					r5=_mm256_permute2f128_si256(r5,r5,1);
		  					r5=_mm256_srli_si256(r5,14);//shift 14 elements
		  					r5=_mm256_add_epi16(m5,r5);

		  			//END - extra code needed for prelude
		  		}
		  else {
				//load the 5 rows
				r0=_mm256_loadu_si256( (__m256i *) &frame1[row-2][col-2]);
				r1=_mm256_loadu_si256( (__m256i *) &frame1[row-1][col-2]);
				r2=_mm256_loadu_si256( (__m256i *) &frame1[row][col-2]);
				r3=_mm256_loadu_si256( (__m256i *) &frame1[row+1][col-2]);
				r4=_mm256_loadu_si256( (__m256i *) &frame1[row+2][col-2]);
				r5=_mm256_loadu_si256( (__m256i *) &frame1[row+3][col-2]);

		  }

		//col iteration computes output pixels of 2,8,14,20,26
		// col+1 iteration computes output pixels of 3,9,15,21,27
		// col+2 iteration computes output pixels of 4,10,16,22,28
		// col+3 iteration computes output pixels of 5,11,17,23,29
		// col+4 iteration computes output pixels of 6,12,18,24
		// col+5 iteration computes output pixels of 7,13,19,25
		//afterwards, col becomes 30 and repeat the above process

		//1st col iteration


		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,c0);
		m1=_mm256_maddubs_epi16(r1,c1);
		m2=_mm256_maddubs_epi16(r2,c2);
		m3=_mm256_maddubs_epi16(r3,c1);
		m4=_mm256_maddubs_epi16(r4,c0);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m2=_mm256_add_epi16(m2,m3);
		m0=_mm256_add_epi16(m0,m2);
		m0=_mm256_add_epi16(m0,m4);

		//hozizontal additions
		//hadd(0:5)
		//hadd(6:11)
		//hadd(12:17)
		//hadd(18:23)
		//hadd(24:28)

		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		m1=_mm256_srli_si256(m0,4);
		m2=_mm256_add_epi16(m1,m2);

		//after shifts the 15th element is lost as it cannot be propagated to the 16th position (AVX registers are managed as two seperate SSE registers)
		m4=_mm256_and_si256(m0,mask3);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions
		m2=_mm256_add_epi16(m2,m4);

		//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
		output_even=_mm256_and_si256(m2,output_mask);


		//2nd col iteration
		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,c0_sh1);
		m1=_mm256_maddubs_epi16(r1,c1_sh1);
		m2=_mm256_maddubs_epi16(r2,c2_sh1);
		m3=_mm256_maddubs_epi16(r3,c1_sh1);
		m4=_mm256_maddubs_epi16(r4,c0_sh1);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m2=_mm256_add_epi16(m2,m3);
		m0=_mm256_add_epi16(m0,m2);
		m0=_mm256_add_epi16(m0,m4);

		//hozizontal additions
		//hadd(1:6)
		//hadd(7:12)
		//hadd(13:18)
		//hadd(19:24)
		//hadd(25:29)

		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		m1=_mm256_srli_si256(m0,4);
		m2=_mm256_add_epi16(m1,m2);

		//after shifts the 15th element is lost as it cannot be propagated to the 16th position (AVX registers are managed as two seperate SSE registers)
		m4=_mm256_and_si256(m0,mask3);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions
		m2=_mm256_add_epi16(m2,m4);

		//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
		output_odd = _mm256_and_si256(m2,output_mask);



         //3rd col iteration
		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,c0_sh2);
		m1=_mm256_maddubs_epi16(r1,c1_sh2);
		m2=_mm256_maddubs_epi16(r2,c2_sh2);
		m3=_mm256_maddubs_epi16(r3,c1_sh2);
		m4=_mm256_maddubs_epi16(r4,c0_sh2);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m2=_mm256_add_epi16(m2,m3);
		m0=_mm256_add_epi16(m0,m2);
		m0=_mm256_add_epi16(m0,m4);

		//hozizontal additions
		//hadd(2:7)
		//hadd(8:13)
		//hadd(14:19)
		//hadd(20:25)
		//hadd(26:30)

		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		m4=_mm256_and_si256(m2,mask3);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions

		m1=_mm256_srli_si256(m0,4);
		m2=_mm256_add_epi16(m1,m2);
		m2=_mm256_add_epi16(m2,m4);

		//m2 has 16 16bit values now. the results I need are in positions 1,4,7,10,13. keep only those, discard others
		m2 = _mm256_and_si256(m2,output_mask_sh1);
		output_even = _mm256_add_epi16(output_even,m2);


		//4th col iteration

		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,c0_sh3);
		m1=_mm256_maddubs_epi16(r1,c1_sh3);
		m2=_mm256_maddubs_epi16(r2,c2_sh3);
		m3=_mm256_maddubs_epi16(r3,c1_sh3);
		m4=_mm256_maddubs_epi16(r4,c0_sh3);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m2=_mm256_add_epi16(m2,m3);
		m0=_mm256_add_epi16(m0,m2);
		m0=_mm256_add_epi16(m0,m4);

		//hozizontal additions
		//hadd(3:8)
		//hadd(9:14)
		//hadd(15:20)
		//hadd(21:26)
		//hadd(27:31)
		//result after division will be in 2,8,14,20,26

		m1=_mm256_srli_si256(m0,2);
		m4=_mm256_add_epi16(m1,m0);

		m1=_mm256_srli_si256(m0,4);
		m2=_mm256_add_epi16(m1,m4);


		m4=_mm256_and_si256(m4,mask3);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions

		m2=_mm256_add_epi16(m2,m4);

		//m2 has 16 16bit values now. the results I need are in positions 1,4,7,10,13. keep only those, discard others
		m2 = _mm256_and_si256(m2,output_mask_sh1);
		output_odd = _mm256_add_epi16(output_odd,m2);




		//5th col iteration
		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,c0_sh4);
		m1=_mm256_maddubs_epi16(r1,c1_sh4);
		m2=_mm256_maddubs_epi16(r2,c2_sh4);
		m3=_mm256_maddubs_epi16(r3,c1_sh4);
		m4=_mm256_maddubs_epi16(r4,c0_sh4);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m2=_mm256_add_epi16(m2,m3);
		m0=_mm256_add_epi16(m0,m2);
		m0=_mm256_add_epi16(m0,m4);

		//hozizontal additions
		//hadd(4:9)
		//hadd(10:15)
		//hadd(16:21)
		//hadd(22:27)
		//result after division will be in 4,10,16,22

		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		m1=_mm256_srli_si256(m0,4);
		m2=_mm256_add_epi16(m1,m2);


		//m2 has 16 16bit values now. the results I need are in positions 2,5,8,11. keep only those, discard others
		m2 = _mm256_and_si256(m2,output_mask_sh2);
		output_even = _mm256_add_epi16(output_even,m2);



                 //6th col iteration
		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,c0_sh5);
		m1=_mm256_maddubs_epi16(r1,c1_sh5);
		m2=_mm256_maddubs_epi16(r2,c2_sh5);
		m3=_mm256_maddubs_epi16(r3,c1_sh5);
		m4=_mm256_maddubs_epi16(r4,c0_sh5);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m2=_mm256_add_epi16(m2,m3);
		m0=_mm256_add_epi16(m0,m2);
		m0=_mm256_add_epi16(m0,m4);

		//hozizontal additions
		//hadd(5:10)
		//hadd(11:16)
		//hadd(17:22)
		//hadd(23:28)
		//result after division will be in 4,10,16,22

		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		m1=_mm256_srli_si256(m0,4);
		m2=_mm256_add_epi16(m1,m2);

		//m2 has 16 16bit values now. the results I need are in positions 2,5,8,11. keep only those, discard others
		m2 = _mm256_and_si256(m2,output_mask_sh2);
		output_odd = _mm256_add_epi16(output_odd,m2);

		//now division follows
		output_even=division(division_case,output_even,f);
		output_odd=division(division_case,output_odd,f);

		//shift odd 1 position and add to even
		output_odd = _mm256_slli_si256(output_odd,1);
		output_even = _mm256_add_epi8(output_even,output_odd);


		_mm256_storeu_si256( (__m256i *) &filt[row][col],output_even );


		//row+1


		//multiply with the mask
		m0=_mm256_maddubs_epi16(r1,c0);
		m1=_mm256_maddubs_epi16(r2,c1);
		m2=_mm256_maddubs_epi16(r3,c2);
		m3=_mm256_maddubs_epi16(r4,c1);
		m4=_mm256_maddubs_epi16(r5,c0);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m2=_mm256_add_epi16(m2,m3);
		m0=_mm256_add_epi16(m0,m2);
		m0=_mm256_add_epi16(m0,m4);

		//hozizontal additions
		//hadd(0:5)
		//hadd(6:11)
		//hadd(12:17)
		//hadd(18:23)
		//hadd(24:28)

		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		m1=_mm256_srli_si256(m0,4);
		m2=_mm256_add_epi16(m1,m2);

		//after shifts the 15th element is lost as it cannot be propagated to the 16th position (AVX registers are managed as two seperate SSE registers)
		m4=_mm256_and_si256(m0,mask3);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions
		m2=_mm256_add_epi16(m2,m4);

		//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
		output_even=_mm256_and_si256(m2,output_mask);


		//2nd col iteration
		//multiply with the mask
		m0=_mm256_maddubs_epi16(r1,c0_sh1);
		m1=_mm256_maddubs_epi16(r2,c1_sh1);
		m2=_mm256_maddubs_epi16(r3,c2_sh1);
		m3=_mm256_maddubs_epi16(r4,c1_sh1);
		m4=_mm256_maddubs_epi16(r5,c0_sh1);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m2=_mm256_add_epi16(m2,m3);
		m0=_mm256_add_epi16(m0,m2);
		m0=_mm256_add_epi16(m0,m4);

		//hozizontal additions
		//hadd(1:6)
		//hadd(7:12)
		//hadd(13:18)
		//hadd(19:24)
		//hadd(25:29)

		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		m1=_mm256_srli_si256(m0,4);
		m2=_mm256_add_epi16(m1,m2);

		//after shifts the 15th element is lost as it cannot be propagated to the 16th position (AVX registers are managed as two seperate SSE registers)
		m4=_mm256_and_si256(m0,mask3);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions
		m2=_mm256_add_epi16(m2,m4);

		//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
		output_odd = _mm256_and_si256(m2,output_mask);



         //3rd col iteration
		//multiply with the mask
		m0=_mm256_maddubs_epi16(r1,c0_sh2);
		m1=_mm256_maddubs_epi16(r2,c1_sh2);
		m2=_mm256_maddubs_epi16(r3,c2_sh2);
		m3=_mm256_maddubs_epi16(r4,c1_sh2);
		m4=_mm256_maddubs_epi16(r5,c0_sh2);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m2=_mm256_add_epi16(m2,m3);
		m0=_mm256_add_epi16(m0,m2);
		m0=_mm256_add_epi16(m0,m4);

		//hozizontal additions
		//hadd(2:7)
		//hadd(8:13)
		//hadd(14:19)
		//hadd(20:25)
		//hadd(26:30)

		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		m4=_mm256_and_si256(m2,mask3);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions

		m1=_mm256_srli_si256(m0,4);
		m2=_mm256_add_epi16(m1,m2);
		m2=_mm256_add_epi16(m2,m4);

		//m2 has 16 16bit values now. the results I need are in positions 1,4,7,10,13. keep only those, discard others
		m2 = _mm256_and_si256(m2,output_mask_sh1);
		output_even = _mm256_add_epi16(output_even,m2);


		//4th col iteration

		//multiply with the mask
		m0=_mm256_maddubs_epi16(r1,c0_sh3);
		m1=_mm256_maddubs_epi16(r2,c1_sh3);
		m2=_mm256_maddubs_epi16(r3,c2_sh3);
		m3=_mm256_maddubs_epi16(r4,c1_sh3);
		m4=_mm256_maddubs_epi16(r5,c0_sh3);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m2=_mm256_add_epi16(m2,m3);
		m0=_mm256_add_epi16(m0,m2);
		m0=_mm256_add_epi16(m0,m4);

		//hozizontal additions
		//hadd(3:8)
		//hadd(9:14)
		//hadd(15:20)
		//hadd(21:26)
		//hadd(27:31)
		//result after division will be in 2,8,14,20,26

		m1=_mm256_srli_si256(m0,2);
		m4=_mm256_add_epi16(m1,m0);

		m1=_mm256_srli_si256(m0,4);
		m2=_mm256_add_epi16(m1,m4);


		m4=_mm256_and_si256(m4,mask3);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions

		m2=_mm256_add_epi16(m2,m4);

		//m2 has 16 16bit values now. the results I need are in positions 1,4,7,10,13. keep only those, discard others
		m2 = _mm256_and_si256(m2,output_mask_sh1);
		output_odd = _mm256_add_epi16(output_odd,m2);




		//5th col iteration
		//multiply with the mask
		m0=_mm256_maddubs_epi16(r1,c0_sh4);
		m1=_mm256_maddubs_epi16(r2,c1_sh4);
		m2=_mm256_maddubs_epi16(r3,c2_sh4);
		m3=_mm256_maddubs_epi16(r4,c1_sh4);
		m4=_mm256_maddubs_epi16(r5,c0_sh4);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m2=_mm256_add_epi16(m2,m3);
		m0=_mm256_add_epi16(m0,m2);
		m0=_mm256_add_epi16(m0,m4);

		//hozizontal additions
		//hadd(4:9)
		//hadd(10:15)
		//hadd(16:21)
		//hadd(22:27)
		//result after division will be in 4,10,16,22

		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		m1=_mm256_srli_si256(m0,4);
		m2=_mm256_add_epi16(m1,m2);


		//m2 has 16 16bit values now. the results I need are in positions 2,5,8,11. keep only those, discard others
		m2 = _mm256_and_si256(m2,output_mask_sh2);
		output_even = _mm256_add_epi16(output_even,m2);



                 //6th col iteration
		//multiply with the mask
		m0=_mm256_maddubs_epi16(r1,c0_sh5);
		m1=_mm256_maddubs_epi16(r2,c1_sh5);
		m2=_mm256_maddubs_epi16(r3,c2_sh5);
		m3=_mm256_maddubs_epi16(r4,c1_sh5);
		m4=_mm256_maddubs_epi16(r5,c0_sh5);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m2=_mm256_add_epi16(m2,m3);
		m0=_mm256_add_epi16(m0,m2);
		m0=_mm256_add_epi16(m0,m4);

		//hozizontal additions
		//hadd(5:10)
		//hadd(11:16)
		//hadd(17:22)
		//hadd(23:28)
		//result after division will be in 4,10,16,22

		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		m1=_mm256_srli_si256(m0,4);
		m2=_mm256_add_epi16(m1,m2);

		//m2 has 16 16bit values now. the results I need are in positions 2,5,8,11. keep only those, discard others
		m2 = _mm256_and_si256(m2,output_mask_sh2);
		output_odd = _mm256_add_epi16(output_odd,m2);


		//now division follows
		output_even=division(division_case,output_even,f);
		output_odd=division(division_case,output_odd,f);

		//shift odd 1 position and add to even
		output_odd = _mm256_slli_si256(output_odd,1);
		output_even = _mm256_add_epi8(output_even,output_odd);


		_mm256_storeu_si256( (__m256i *) &filt[row+1][col],output_even );


		}

        if (REMINDER_ITERATIONS>=29){
        	loop_reminder_high_reminder_values_less_div(frame1,filt,M,N,row,col,REMINDER_ITERATIONS,division_case,c0,c1,c2,c0_sh1,c1_sh1,c2_sh1,c0_sh2,c1_sh2,c2_sh2,f,divisor,filter5x5);
        	loop_reminder_high_reminder_values_less_div(frame1,filt,M,N,row+1,col,REMINDER_ITERATIONS,division_case,c0,c1,c2,c0_sh1,c1_sh1,c2_sh1,c0_sh2,c1_sh2,c2_sh2,f,divisor,filter5x5);

        }
        else{
        	loop_reminder_low_reminder_values_less_div(frame1,filt,M,N,row,col,REMINDER_ITERATIONS,division_case,c0,c1,c2,c0_sh1,c1_sh1,c2_sh1,c0_sh2,c1_sh2,c2_sh2,c0_sh3,c1_sh3,c2_sh3,c0_sh4,c1_sh4,c2_sh4,c0_sh5,c1_sh5,c2_sh5,f);
        	loop_reminder_low_reminder_values_less_div(frame1,filt,M,N,row+1,col,REMINDER_ITERATIONS,division_case,c0,c1,c2,c0_sh1,c1_sh1,c2_sh1,c0_sh2,c1_sh2,c2_sh2,c0_sh3,c1_sh3,c2_sh3,c0_sh4,c1_sh4,c2_sh4,c0_sh5,c1_sh5,c2_sh5,f);

        }
        //padding code. The last three iterations  ABOVE get outside of the array bounds. The last three col iterations read outside but the garbage values are multiplied by zeros (last three mask zeros). From now on, I must include another mask with extra zeros.
//It is faster to read outside of the array's bounds and then fill the right values. there is an insert command that inserts a value to the vector
//TOMORROW: USE Gaussian_Blur_AVX_ver4_plus_less_load ROUTINE FOR LOOP REMINDER. LOAD OUTSIDE OF THE ARRAY AND THEN ZERO THE VALUES NEEDED.
	}

}

}//end of parallel


}




void Gaussian_Blur_optimized_5x5_seperable_filter_less_div(unsigned char **frame1,unsigned char **filt,const unsigned int M, const unsigned int N, const unsigned int divisor, const signed char filter_5x5[5][5]){

	//-----------load coefficients for the padding routines. The padding routines use 2d 5x5 filter. They need to be updated.
	/*const __m256i c0=_mm256_set_epi8(0,0,0,filter_5x5[0][4],filter_5x5[0][3],filter_5x5[0][2],filter_5x5[0][1],filter_5x5[0][0],0,filter_5x5[0][4],filter_5x5[0][3],filter_5x5[0][2],filter_5x5[0][1],filter_5x5[0][0],0,filter_5x5[0][4],filter_5x5[0][3],filter_5x5[0][2],filter_5x5[0][1],filter_5x5[0][0],0,filter_5x5[0][4],filter_5x5[0][3],filter_5x5[0][2],filter_5x5[0][1],filter_5x5[0][0],0,filter_5x5[0][4],filter_5x5[0][3],filter_5x5[0][2],filter_5x5[0][1],filter_5x5[0][0]);
	const __m256i c1=_mm256_set_epi8(0,0,0,filter_5x5[1][4],filter_5x5[1][3],filter_5x5[1][2],filter_5x5[1][1],filter_5x5[1][0],0,filter_5x5[1][4],filter_5x5[1][3],filter_5x5[1][2],filter_5x5[1][1],filter_5x5[1][0],0,filter_5x5[1][4],filter_5x5[1][3],filter_5x5[1][2],filter_5x5[1][1],filter_5x5[1][0],0,filter_5x5[1][4],filter_5x5[1][3],filter_5x5[1][2],filter_5x5[1][1],filter_5x5[1][0],0,filter_5x5[1][4],filter_5x5[1][3],filter_5x5[1][2],filter_5x5[1][1],filter_5x5[1][0]);
	const __m256i c2=_mm256_set_epi8(0,0,0,filter_5x5[2][4],filter_5x5[2][3],filter_5x5[2][2],filter_5x5[2][1],filter_5x5[2][0],0,filter_5x5[2][4],filter_5x5[2][3],filter_5x5[2][2],filter_5x5[2][1],filter_5x5[2][0],0,filter_5x5[2][4],filter_5x5[2][3],filter_5x5[2][2],filter_5x5[2][1],filter_5x5[2][0],0,filter_5x5[2][4],filter_5x5[2][3],filter_5x5[2][2],filter_5x5[2][1],filter_5x5[2][0],0,filter_5x5[2][4],filter_5x5[2][3],filter_5x5[2][2],filter_5x5[2][1],filter_5x5[2][0]);

	const __m256i c0_sh1=_mm256_slli_si256(c0,1);
	const __m256i c1_sh1=_mm256_slli_si256(c1,1);
	const __m256i c2_sh1=_mm256_slli_si256(c2,1);

	const __m256i c0_sh2=_mm256_slli_si256(c0,2);
	const __m256i c1_sh2=_mm256_slli_si256(c1,2);
	const __m256i c2_sh2=_mm256_slli_si256(c2,2);*/


	//load coefficients
	const __m256i     cx=_mm256_load_si256( (__m256i *) &gaussian_filter_5x5_seperable_x[0][0]);
	const __m256i cx_sh1=_mm256_load_si256( (__m256i *) &gaussian_filter_5x5_seperable_x[1][0]);
	const __m256i cx_sh2=_mm256_load_si256( (__m256i *) &gaussian_filter_5x5_seperable_x[2][0]);
	const __m256i cx_sh3=_mm256_load_si256( (__m256i *) &gaussian_filter_5x5_seperable_x[3][0]);
	const __m256i cx_sh4=_mm256_load_si256( (__m256i *) &gaussian_filter_5x5_seperable_x[4][0]);
	const __m256i cx_sh5=_mm256_load_si256( (__m256i *) &gaussian_filter_5x5_seperable_x[5][0]);

	const __m256i     cy0=_mm256_load_si256( (__m256i *) &gaussian_filter_5x5_seperable_y[0][0]);
	const __m256i     cy1=_mm256_load_si256( (__m256i *) &gaussian_filter_5x5_seperable_y[1][0]);
	const __m256i     cy2=_mm256_load_si256( (__m256i *) &gaussian_filter_5x5_seperable_y[2][0]);
	const __m256i cy0_sh1=_mm256_load_si256( (__m256i *) &gaussian_filter_5x5_seperable_y[3][0]);
	const __m256i cy1_sh1=_mm256_load_si256( (__m256i *) &gaussian_filter_5x5_seperable_y[4][0]);
	const __m256i cy2_sh1=_mm256_load_si256( (__m256i *) &gaussian_filter_5x5_seperable_y[5][0]);

	//const __m256i f  = _mm256_set_epi16(52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759);
	//const __m256i mask  = _mm256_set_epi8(0,0,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255);
	//const __m256i mask2  = _mm256_set_epi8(0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0);
	//const __m256i mask6  = _mm256_set_epi8(0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0);

	const __m256i mask3  = _mm256_set_epi16(0,0,0,0,0,0,0,65535,0,0,0,0,0,0,0,0);

	const __m256i mask_prelude  = _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0);

	const unsigned int REMINDER_ITERATIONS =  (M-((((M-32)/28)*28)+28)); //M-(last_col_value+28)
	//printf("\n%d",REMINDER_ITERATIONS);

	const __m256i output_mask      = _mm256_set_epi16(0,0,0,65535,0,0,65535,0,0,65535,0,0,65535,0,0,65535);
	const __m256i output_mask_sh1  = _mm256_set_epi16(0,0,65535,0,0,65535,0,0,65535,0,0,65535,0,0,65535,0);
	const __m256i output_mask_sh2  = _mm256_set_epi16(0,0,0,    0,65535,0,0,65535,0,0,65535,0,0,65535,0,0);

	const unsigned int division_case=prepare_for_division(divisor); //determine which is the division case (A, B or C)
	const __m256i f = _mm256_load_si256( (__m256i *) &f_vector[0]);// initialize the division vector



#pragma omp parallel
{

unsigned int row,col;
register __m256i r0,r1,r2,r3,r4,m0,m1,m2,m3,m4,output_even,output_odd,output_row0_even,output_row0_odd,output_row1_even,output_row1_odd;
//__m256i output_row0,output_row1;//these are for processing row #0 and #1 only.
__m256i even,odd,row_0,row_1,row_2;

//MORE EFFICIENT WAY TO DEAL WITH MANY CONSTANTS?

/*---------------------- Gaussian Blur ---------------------------------*/

    #pragma omp for schedule(static)
	for (row = 2; row < N-2; row++) {

		if (row==2){//in this case I calculate filt[0][:] and filt[1][:] too. No extra loads or multiplications are required for this
			  for (col = 0; col <= M-32; col+=28){

				  if (col==0){//this is a special case as the mask gets outside of the array (it is like adding two zeros in the beginning of frame[][]

						//load the 5 rows
						r0=_mm256_loadu_si256( (__m256i *) &frame1[row-2][0]);
						r1=_mm256_loadu_si256( (__m256i *) &frame1[row-1][0]);
						r2=_mm256_loadu_si256( (__m256i *) &frame1[row][0]);
						r3=_mm256_loadu_si256( (__m256i *) &frame1[row+1][0]);
						r4=_mm256_loadu_si256( (__m256i *) &frame1[row+2][0]);

				  			//START - extra code needed for prelude
				  					m0=_mm256_slli_si256(r0,2);//shift 2 elements left - equivalent to filling with two zeros inthe beginning
				  					m1=_mm256_slli_si256(r1,2);//shift 2 elements left - equivalent to filling with two zeros inthe beginning
				  					m2=_mm256_slli_si256(r2,2);//shift 2 elements left - equivalent to filling with two zeros inthe beginning
				  					m3=_mm256_slli_si256(r3,2);//shift 2 elements left - equivalent to filling with two zeros inthe beginning
				  					m4=_mm256_slli_si256(r4,2);//shift 2 elements left - equivalent to filling with two zeros inthe beginning

				  					r0=_mm256_and_si256(r0,mask_prelude);
				  					r0=_mm256_permute2f128_si256(r0,r0,1);
				  					r0=_mm256_srli_si256(r0,14);//shift 14 elements
				  					r0=_mm256_add_epi16(m0,r0);

				  					r1=_mm256_and_si256(r1,mask_prelude);
				  					r1=_mm256_permute2f128_si256(r1,r1,1);
				  					r1=_mm256_srli_si256(r1,14);//shift 14 elements
				  					r1=_mm256_add_epi16(m1,r1);

				  					r2=_mm256_and_si256(r2,mask_prelude);
				  					r2=_mm256_permute2f128_si256(r2,r2,1);
				  					r2=_mm256_srli_si256(r2,14);//shift 14 elements
				  					r2=_mm256_add_epi16(m2,r2);

				  					r3=_mm256_and_si256(r3,mask_prelude);
				  					r3=_mm256_permute2f128_si256(r3,r3,1);
				  					r3=_mm256_srli_si256(r3,14);//shift 14 elements
				  					r3=_mm256_add_epi16(m3,r3);

				  					r4=_mm256_and_si256(r4,mask_prelude);
				  					r4=_mm256_permute2f128_si256(r4,r4,1);
				  					r4=_mm256_srli_si256(r4,14);//shift 14 elements
				  					r4=_mm256_add_epi16(m4,r4);

				  			//END - extra code needed for prelude
				  		}
				  else {
						//load the 5 rows
						r0=_mm256_loadu_si256( (__m256i *) &frame1[row-2][col-2]);
						r1=_mm256_loadu_si256( (__m256i *) &frame1[row-1][col-2]);
						r2=_mm256_loadu_si256( (__m256i *) &frame1[row][col-2]);
						r3=_mm256_loadu_si256( (__m256i *) &frame1[row+1][col-2]);
						r4=_mm256_loadu_si256( (__m256i *) &frame1[row+2][col-2]);
				  }

				//col iteration computes output pixels of 2,8,14,20,26
				// col+1 iteration computes output pixels of 3,9,15,21,27
				// col+2 iteration computes output pixels of 4,10,16,22,28
				// col+3 iteration computes output pixels of 5,11,17,23,29
				// col+4 iteration computes output pixels of 6,12,18,24,30
				// col+5 iteration computes output pixels of 7,13,19,25,31
				//afterwards, col2 becomes 32 and repeat the above process

				  //y filter begins

					//multiply with the mask
					m0=_mm256_maddubs_epi16(r0,cy0);
					m1=_mm256_maddubs_epi16(r1,cy1);
					m2=_mm256_maddubs_epi16(r2,cy2);
					m3=_mm256_maddubs_epi16(r3,cy1);
					m4=_mm256_maddubs_epi16(r4,cy0);

					//vertical add
					m0=_mm256_add_epi16(m0,m1);
					m2=_mm256_add_epi16(m2,m3);
					m0=_mm256_add_epi16(m0,m2);
					m0=_mm256_add_epi16(m0,m4);

					even=division(division_case,m0,f);//even results

					//multiply with the mask
					m0=_mm256_maddubs_epi16(r0,cy0_sh1);
					m1=_mm256_maddubs_epi16(r1,cy1_sh1);
					m2=_mm256_maddubs_epi16(r2,cy2_sh1);
					m3=_mm256_maddubs_epi16(r3,cy1_sh1);
					m4=_mm256_maddubs_epi16(r4,cy0_sh1);

					//vertical add
					m0=_mm256_add_epi16(m0,m1);
					m2=_mm256_add_epi16(m2,m3);
					m0=_mm256_add_epi16(m0,m2);
					m0=_mm256_add_epi16(m0,m4);

					odd=division(division_case,m0,f);//odd results

					//pack to one register
					odd=_mm256_slli_si256(odd,1); //shift one position right
					row_2=_mm256_add_epi8(even,odd); //add the odd with the even

					//y filter ends - row_2 has the data to be processed by x filter

				//1st col iteration

				  //--------------row=2
				//multiply with the mask
				m0=_mm256_maddubs_epi16(row_2,cx);


				m1=_mm256_srli_si256(m0,2);
				m2=_mm256_add_epi16(m1,m0);

				m1=_mm256_srli_si256(m0,4);
				m2=_mm256_add_epi16(m1,m2);

				//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
				//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
				m4=_mm256_and_si256(m0,mask3);
				m4=_mm256_permute2f128_si256(m4,m4,1);
				m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions

				m2=_mm256_add_epi16(m2,m4);

				//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
				output_even=_mm256_and_si256(m2,output_mask);

				//--------------row=0
				  //y filter begins

					//multiply with the mask
					m0=_mm256_maddubs_epi16(r0,cy2);
					m1=_mm256_maddubs_epi16(r1,cy1);
					m2=_mm256_maddubs_epi16(r2,cy0);

					//vertical add
					m0=_mm256_add_epi16(m0,m1);
					m0=_mm256_add_epi16(m0,m2);


					even=division(division_case,m0,f);//even results

					//multiply with the mask
					m0=_mm256_maddubs_epi16(r0,cy2_sh1);
					m1=_mm256_maddubs_epi16(r1,cy1_sh1);
					m2=_mm256_maddubs_epi16(r2,cy0_sh1);

					//vertical add
					m0=_mm256_add_epi16(m0,m1);
					m0=_mm256_add_epi16(m0,m2);

					odd=division(division_case,m0,f);//odd results

					//pack to one register
					odd=_mm256_slli_si256(odd,1); //shift one position right
					row_0=_mm256_add_epi8(even,odd); //add the odd with the even

					//y filter ends - row_0 has the data to be processed by x filter

				//multiply with the mask
				m0=_mm256_maddubs_epi16(row_0,cx);


				m1=_mm256_srli_si256(m0,2);
				m2=_mm256_add_epi16(m1,m0);

				m1=_mm256_srli_si256(m0,4);
				m2=_mm256_add_epi16(m1,m2);

				//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
				//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
				m4=_mm256_and_si256(m0,mask3);
				m4=_mm256_permute2f128_si256(m4,m4,1);
				m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions

				m2=_mm256_add_epi16(m2,m4);

				//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
				output_row0_even=_mm256_and_si256(m2,output_mask);

				//--------------row=1
				  //y filter begins

					//multiply with the mask
					m0=_mm256_maddubs_epi16(r0,cy1);
					m1=_mm256_maddubs_epi16(r1,cy2);
					m2=_mm256_maddubs_epi16(r2,cy1);
					m3=_mm256_maddubs_epi16(r3,cy0);


					//vertical add
					m0=_mm256_add_epi16(m0,m1);
					m2=_mm256_add_epi16(m2,m3);
					m0=_mm256_add_epi16(m0,m2);


					even=division(division_case,m0,f);//even results

					//multiply with the mask
					m0=_mm256_maddubs_epi16(r0,cy1_sh1);
					m1=_mm256_maddubs_epi16(r1,cy2_sh1);
					m2=_mm256_maddubs_epi16(r2,cy1_sh1);
					m3=_mm256_maddubs_epi16(r3,cy0_sh1);

					//vertical add
					m0=_mm256_add_epi16(m0,m1);
					m2=_mm256_add_epi16(m2,m3);
					m0=_mm256_add_epi16(m0,m2);

					odd=division(division_case,m0,f);//odd results

					//pack to one register
					odd=_mm256_slli_si256(odd,1); //shift one position right
					row_1=_mm256_add_epi8(even,odd); //add the odd with the even

					//y filter ends - row_1 has the data to be processed by x filter

				//multiply with the mask
				m0=_mm256_maddubs_epi16(row_1,cx);

				m1=_mm256_srli_si256(m0,2);
				m2=_mm256_add_epi16(m1,m0);

				m1=_mm256_srli_si256(m0,4);
				m2=_mm256_add_epi16(m1,m2);

				//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
				//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
				m4=_mm256_and_si256(m0,mask3);
				m4=_mm256_permute2f128_si256(m4,m4,1);
				m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions

				m2=_mm256_add_epi16(m2,m4);

				//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
				output_row1_even=_mm256_and_si256(m2,output_mask);

				//2nd col iteration

				//multiply with the mask

				//----row=2
				m0=_mm256_maddubs_epi16(row_2,cx_sh1);



				m1=_mm256_srli_si256(m0,2);
				m2=_mm256_add_epi16(m1,m0);

				m1=_mm256_srli_si256(m0,4);
				m2=_mm256_add_epi16(m1,m2);

				//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
				//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
				m4=_mm256_and_si256(m0,mask3);
				m4=_mm256_permute2f128_si256(m4,m4,1);
				m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions
				m2=_mm256_add_epi16(m2,m4);

				//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
				output_odd = _mm256_and_si256(m2,output_mask);

				//----row=0
				m0=_mm256_maddubs_epi16(row_0,cx_sh1);


				m1=_mm256_srli_si256(m0,2);
				m2=_mm256_add_epi16(m1,m0);

				m1=_mm256_srli_si256(m0,4);
				m2=_mm256_add_epi16(m1,m2);

				//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
				//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
				m4=_mm256_and_si256(m0,mask3);
				m4=_mm256_permute2f128_si256(m4,m4,1);
				m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions
				m2=_mm256_add_epi16(m2,m4);

				//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
				output_row0_odd = _mm256_and_si256(m2,output_mask);

				//----row=1
				m0=_mm256_maddubs_epi16(row_1,cx_sh1);


				m1=_mm256_srli_si256(m0,2);
				m2=_mm256_add_epi16(m1,m0);

				m1=_mm256_srli_si256(m0,4);
				m2=_mm256_add_epi16(m1,m2);

				//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
				//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
				m4=_mm256_and_si256(m0,mask3);
				m4=_mm256_permute2f128_si256(m4,m4,1);
				m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions
				m2=_mm256_add_epi16(m2,m4);

				//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
				output_row1_odd = _mm256_and_si256(m2,output_mask);

		                 //3rd col iteration
				//---row=2
				//multiply with the mask
				m0=_mm256_maddubs_epi16(row_2,cx_sh2);


				m1=_mm256_srli_si256(m0,2);
				m2=_mm256_add_epi16(m1,m0);

				//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,_mm256_extract_epi16(m2,8),0,0,0,0,0,0,0);
				m4=_mm256_and_si256(m2,mask3);
				m4=_mm256_permute2f128_si256(m4,m4,1);
				m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions

				m1=_mm256_srli_si256(m0,4);
				m2=_mm256_add_epi16(m1,m2);

				//prepei na prosthesw to pixel pu thelw sto r2 kai meta na kanw diairesi me ola mazi

				m2=_mm256_add_epi16(m2,m4);

				//m2 has 16 16bit values now. the results I need are in positions 1,4,7,10,13. keep only those, discard others
				m2 = _mm256_and_si256(m2,output_mask_sh1);
				output_even = _mm256_add_epi16(output_even,m2);

				//---row=0
				//multiply with the mask
				m0=_mm256_maddubs_epi16(row_0,cx_sh2);


				m1=_mm256_srli_si256(m0,2);
				m2=_mm256_add_epi16(m1,m0);

				//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,_mm256_extract_epi16(m2,8),0,0,0,0,0,0,0);
				m4=_mm256_and_si256(m2,mask3);
				m4=_mm256_permute2f128_si256(m4,m4,1);
				m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions

				m1=_mm256_srli_si256(m0,4);
				m2=_mm256_add_epi16(m1,m2);

				//prepei na prosthesw to pixel pu thelw sto r2 kai meta na kanw diairesi me ola mazi

				m2=_mm256_add_epi16(m2,m4);

				//m2 has 16 16bit values now. the results I need are in positions 1,4,7,10,13. keep only those, discard others
				m2 = _mm256_and_si256(m2,output_mask_sh1);
				output_row0_even = _mm256_add_epi16(output_row0_even,m2);


				//---row=1
				//multiply with the mask
				m0=_mm256_maddubs_epi16(row_1,cx_sh2);

				m1=_mm256_srli_si256(m0,2);
				m2=_mm256_add_epi16(m1,m0);

				//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,_mm256_extract_epi16(m2,8),0,0,0,0,0,0,0);
				m4=_mm256_and_si256(m2,mask3);
				m4=_mm256_permute2f128_si256(m4,m4,1);
				m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions

				m1=_mm256_srli_si256(m0,4);
				m2=_mm256_add_epi16(m1,m2);

				//prepei na prosthesw to pixel pu thelw sto r2 kai meta na kanw diairesi me ola mazi

				m2=_mm256_add_epi16(m2,m4);

				//m2 has 16 16bit values now. the results I need are in positions 1,4,7,10,13. keep only those, discard others
				m2 = _mm256_and_si256(m2,output_mask_sh1);
				output_row1_even = _mm256_add_epi16(output_row1_even,m2);

				//4th col iteration

				//4th col iteration

				//multiply with the mask
				m0=_mm256_maddubs_epi16(row_2,cx_sh3);

				//hozizontal additions
				//hadd(3:8)
				//hadd(9:14)
				//hadd(15:20)
				//hadd(21:26)
				//hadd(27:31)
				//result after division will be in 2,8,14,20,26

				m1=_mm256_srli_si256(m0,2);
				m4=_mm256_add_epi16(m1,m0);

				m1=_mm256_srli_si256(m0,4);
				m2=_mm256_add_epi16(m1,m4);

				//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
				//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
				m4=_mm256_and_si256(m4,mask3);
				m4=_mm256_permute2f128_si256(m4,m4,1);
				m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions

				m2=_mm256_add_epi16(m2,m4);

				//m2 has 16 16bit values now. the results I need are in positions 1,4,7,10,13. keep only those, discard others
				m2 = _mm256_and_si256(m2,output_mask_sh1);
				output_odd = _mm256_add_epi16(output_odd,m2);

				//row0
				//multiply with the mask
				m0=_mm256_maddubs_epi16(row_0,cx_sh3);


								m1=_mm256_srli_si256(m0,2);
								m4=_mm256_add_epi16(m1,m0);

								m1=_mm256_srli_si256(m0,4);
								m2=_mm256_add_epi16(m1,m4);

								//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
								//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
								m4=_mm256_and_si256(m4,mask3);
								m4=_mm256_permute2f128_si256(m4,m4,1);
								m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions

								m2=_mm256_add_epi16(m2,m4);

								//m2 has 16 16bit values now. the results I need are in positions 1,4,7,10,13. keep only those, discard others
								m2 = _mm256_and_si256(m2,output_mask_sh1);
								output_row0_odd = _mm256_add_epi16(output_row0_odd,m2);


								//-----------row1
								//multiply with the mask
								m0=_mm256_maddubs_epi16(row_1,cx_sh3);

												m1=_mm256_srli_si256(m0,2);
												m4=_mm256_add_epi16(m1,m0);

												m1=_mm256_srli_si256(m0,4);
												m2=_mm256_add_epi16(m1,m4);

												//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
												//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
												m4=_mm256_and_si256(m4,mask3);
												m4=_mm256_permute2f128_si256(m4,m4,1);
												m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions

												m2=_mm256_add_epi16(m2,m4);

												//m2 has 16 16bit values now. the results I need are in positions 1,4,7,10,13. keep only those, discard others
												m2 = _mm256_and_si256(m2,output_mask_sh1);
												output_row1_odd = _mm256_add_epi16(output_row1_odd,m2);



				//5th col iteration
				//multiply with the mask
				m0=_mm256_maddubs_epi16(row_2,cx_sh4);

				//hozizontal additions
				//hadd(4:9)
				//hadd(10:15)
				//hadd(16:21)
				//hadd(22:27)
				//result after division will be in 4,10,16,22

				m1=_mm256_srli_si256(m0,2);
				m2=_mm256_add_epi16(m1,m0);

				m1=_mm256_srli_si256(m0,4);
				m2=_mm256_add_epi16(m1,m2);


				//m2 has 16 16bit values now. the results I need are in positions 2,5,8,11. keep only those, discard others
				m2 = _mm256_and_si256(m2,output_mask_sh2);
				output_even = _mm256_add_epi16(output_even,m2);

				//row0
				//multiply with the mask
				m0=_mm256_maddubs_epi16(row_0,cx_sh4);


				//hozizontal additions
				//hadd(4:9)
				//hadd(10:15)
				//hadd(16:21)
				//hadd(22:27)
				//result after division will be in 4,10,16,22

				m1=_mm256_srli_si256(m0,2);
				m2=_mm256_add_epi16(m1,m0);

				m1=_mm256_srli_si256(m0,4);
				m2=_mm256_add_epi16(m1,m2);


				//m2 has 16 16bit values now. the results I need are in positions 2,5,8,11. keep only those, discard others
				m2 = _mm256_and_si256(m2,output_mask_sh2);
				output_row0_even = _mm256_add_epi16(output_row0_even,m2);

				//row1
				//multiply with the mask
				m0=_mm256_maddubs_epi16(row_1,cx_sh4);


				m1=_mm256_srli_si256(m0,2);
				m2=_mm256_add_epi16(m1,m0);

				m1=_mm256_srli_si256(m0,4);
				m2=_mm256_add_epi16(m1,m2);


				//m2 has 16 16bit values now. the results I need are in positions 2,5,8,11. keep only those, discard others
				m2 = _mm256_and_si256(m2,output_mask_sh2);
				output_row1_even = _mm256_add_epi16(output_row1_even,m2);


		                 //6th col iteration
				//multiply with the mask
				m0=_mm256_maddubs_epi16(row_2,cx_sh5);

				//hozizontal additions
				//hadd(5:10)
				//hadd(11:16)
				//hadd(17:22)
				//hadd(23:28)
				//result after division will be in 4,10,16,22

				m1=_mm256_srli_si256(m0,2);
				m2=_mm256_add_epi16(m1,m0);

				m1=_mm256_srli_si256(m0,4);
				m2=_mm256_add_epi16(m1,m2);

				//m2 has 16 16bit values now. the results I need are in positions 2,5,8,11. keep only those, discard others
				m2 = _mm256_and_si256(m2,output_mask_sh2);
				output_odd = _mm256_add_epi16(output_odd,m2);

				//now division follows
				output_even=division(division_case,output_even,f);
				output_odd=division(division_case,output_odd,f);

				//shift odd 1 position and add to even
				output_odd = _mm256_slli_si256(output_odd,1);
				output_even = _mm256_add_epi8(output_even,output_odd);

				_mm256_storeu_si256( (__m256i *) &filt[row][col],output_even );

				//row0
				//multiply with the mask
				m0=_mm256_maddubs_epi16(row_0,cx_sh5);


				m1=_mm256_srli_si256(m0,2);
				m2=_mm256_add_epi16(m1,m0);

				m1=_mm256_srli_si256(m0,4);
				m2=_mm256_add_epi16(m1,m2);

				//m2 has 16 16bit values now. the results I need are in positions 2,5,8,11. keep only those, discard others
				m2 = _mm256_and_si256(m2,output_mask_sh2);
				output_row0_odd = _mm256_add_epi16(output_row0_odd,m2);

				//now division follows
				output_row0_even=division(division_case,output_row0_even,f);
				output_row0_odd=division(division_case,output_row0_odd,f);

				//shift odd 1 position and add to even
				output_row0_odd = _mm256_slli_si256(output_row0_odd,1);
				output_row0_even = _mm256_add_epi8(output_row0_even,output_row0_odd);


				_mm256_storeu_si256( (__m256i *) &filt[0][col],output_row0_even );

				//row1

				//multiply with the mask
				m0=_mm256_maddubs_epi16(row_1,cx_sh5);


				m1=_mm256_srli_si256(m0,2);
				m2=_mm256_add_epi16(m1,m0);

				m1=_mm256_srli_si256(m0,4);
				m2=_mm256_add_epi16(m1,m2);

				//m2 has 16 16bit values now. the results I need are in positions 2,5,8,11. keep only those, discard others
				m2 = _mm256_and_si256(m2,output_mask_sh2);
				output_row1_odd = _mm256_add_epi16(output_row1_odd,m2);

				//now division follows
				output_row1_even=division(division_case,output_row1_even,f);
				output_row1_odd=division(division_case,output_row1_odd,f);

				//shift odd 1 position and add to even
				output_row1_odd = _mm256_slli_si256(output_row1_odd,1);
				output_row1_even = _mm256_add_epi8(output_row1_even,output_row1_odd);


				_mm256_storeu_si256( (__m256i *) &filt[1][col],output_row1_even );


				}
/*
			  if (REMINDER_ITERATIONS>=16)
				loop_reminder_first_rows_high_reminder_values(frame1,filt,M,N,col,REMINDER_ITERATIONS,division_case,c0,c1,c2,c0_sh1,c1_sh1,c2_sh1,c0_sh2,c1_sh2,c2_sh2,f,divisor);
			  else
			    loop_reminder_first_rows_low_reminder_values(frame1,filt,M,N,col,REMINDER_ITERATIONS,division_case,c0,c1,c2,c0_sh1,c1_sh1,c2_sh1,c0_sh2,c1_sh2,c2_sh2,f);
*/
		}

		else if (row==N-3){//in this case I calculate filt[N-2][:] and filt[N-1][:] too. Below row0 refers to row=N-2 and row1 refers to row=N-1
			for (col = 0; col <= M-32; col+=28){
						 //last col value that does not read outside of the array bounds is (col<M-29)

							  if (col==0){

									//load the 5 rows
									r0=_mm256_loadu_si256( (__m256i *) &frame1[row-2][0]);
									r1=_mm256_loadu_si256( (__m256i *) &frame1[row-1][0]);
									r2=_mm256_loadu_si256( (__m256i *) &frame1[row][0]);
									r3=_mm256_loadu_si256( (__m256i *) &frame1[row+1][0]);
									r4=_mm256_loadu_si256( (__m256i *) &frame1[row+2][0]);

							  			//START - extra code needed for prelude
							  					m0=_mm256_slli_si256(r0,2);//shift 2 elements left - equivalent to filling with two zeros inthe beginning
							  					m1=_mm256_slli_si256(r1,2);//shift 2 elements left - equivalent to filling with two zeros inthe beginning
							  					m2=_mm256_slli_si256(r2,2);//shift 2 elements left - equivalent to filling with two zeros inthe beginning
							  					m3=_mm256_slli_si256(r3,2);//shift 2 elements left - equivalent to filling with two zeros inthe beginning
							  					m4=_mm256_slli_si256(r4,2);//shift 2 elements left - equivalent to filling with two zeros inthe beginning

							  					r0=_mm256_and_si256(r0,mask_prelude);
							  					r0=_mm256_permute2f128_si256(r0,r0,1);
							  					r0=_mm256_srli_si256(r0,14);//shift 14 elements
							  					r0=_mm256_add_epi16(m0,r0);

							  					r1=_mm256_and_si256(r1,mask_prelude);
							  					r1=_mm256_permute2f128_si256(r1,r1,1);
							  					r1=_mm256_srli_si256(r1,14);//shift 14 elements
							  					r1=_mm256_add_epi16(m1,r1);

							  					r2=_mm256_and_si256(r2,mask_prelude);
							  					r2=_mm256_permute2f128_si256(r2,r2,1);
							  					r2=_mm256_srli_si256(r2,14);//shift 14 elements
							  					r2=_mm256_add_epi16(m2,r2);

							  					r3=_mm256_and_si256(r3,mask_prelude);
							  					r3=_mm256_permute2f128_si256(r3,r3,1);
							  					r3=_mm256_srli_si256(r3,14);//shift 14 elements
							  					r3=_mm256_add_epi16(m3,r3);

							  					r4=_mm256_and_si256(r4,mask_prelude);
							  					r4=_mm256_permute2f128_si256(r4,r4,1);
							  					r4=_mm256_srli_si256(r4,14);//shift 14 elements
							  					r4=_mm256_add_epi16(m4,r4);

							  			//END - extra code needed for prelude
							  		}
							  else {
									//load the 5 rows
									r0=_mm256_loadu_si256( (__m256i *) &frame1[row-2][col-2]);
									r1=_mm256_loadu_si256( (__m256i *) &frame1[row-1][col-2]);
									r2=_mm256_loadu_si256( (__m256i *) &frame1[row][col-2]);
									r3=_mm256_loadu_si256( (__m256i *) &frame1[row+1][col-2]);
									r4=_mm256_loadu_si256( (__m256i *) &frame1[row+2][col-2]);
							  }

							//col iteration computes output pixels of 2,8,14,20,26
							// col+1 iteration computes output pixels of 3,9,15,21,27
							// col+2 iteration computes output pixels of 4,10,16,22,28
							// col+3 iteration computes output pixels of 5,11,17,23,29
							// col+4 iteration computes output pixels of 6,12,18,24,30
							// col+5 iteration computes output pixels of 7,13,19,25,31
							//afterwards, col2 becomes 32 and repeat the above process

							//1st col iteration
							  //y filter begins

								//multiply with the mask
								m0=_mm256_maddubs_epi16(r0,cy0);
								m1=_mm256_maddubs_epi16(r1,cy1);
								m2=_mm256_maddubs_epi16(r2,cy2);
								m3=_mm256_maddubs_epi16(r3,cy1);
								m4=_mm256_maddubs_epi16(r4,cy0);

								//vertical add
								m0=_mm256_add_epi16(m0,m1);
								m2=_mm256_add_epi16(m2,m3);
								m0=_mm256_add_epi16(m0,m2);
								m0=_mm256_add_epi16(m0,m4);

								even=division(division_case,m0,f);//even results

								//multiply with the mask
								m0=_mm256_maddubs_epi16(r0,cy0_sh1);
								m1=_mm256_maddubs_epi16(r1,cy1_sh1);
								m2=_mm256_maddubs_epi16(r2,cy2_sh1);
								m3=_mm256_maddubs_epi16(r3,cy1_sh1);
								m4=_mm256_maddubs_epi16(r4,cy0_sh1);

								//vertical add
								m0=_mm256_add_epi16(m0,m1);
								m2=_mm256_add_epi16(m2,m3);
								m0=_mm256_add_epi16(m0,m2);
								m0=_mm256_add_epi16(m0,m4);

								odd=division(division_case,m0,f);//odd results

								//pack to one register
								odd=_mm256_slli_si256(odd,1); //shift one position right
								row_2=_mm256_add_epi8(even,odd); //add the odd with the even

								//y filter ends - row_2 has the data to be processed by x filter


							 //--------------row=2

							//multiply with the mask
							m0=_mm256_maddubs_epi16(row_2,cx);


							m1=_mm256_srli_si256(m0,2);
							m2=_mm256_add_epi16(m1,m0);

							m1=_mm256_srli_si256(m0,4);
							m2=_mm256_add_epi16(m1,m2);

							//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
							//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
							m4=_mm256_and_si256(m0,mask3);
							m4=_mm256_permute2f128_si256(m4,m4,1);
							m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions

							m2=_mm256_add_epi16(m2,m4);

							//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
							output_even=_mm256_and_si256(m2,output_mask);

							//--------------row=0
							  //y filter begins

								//multiply with the mask
								m1=_mm256_maddubs_epi16(r1,cy0);
								m2=_mm256_maddubs_epi16(r2,cy1);
								m3=_mm256_maddubs_epi16(r3,cy2);
								m4=_mm256_maddubs_epi16(r4,cy1);

								//vertical add
								m0=_mm256_add_epi16(m1,m2);
								m0=_mm256_add_epi16(m0,m3);
								m0=_mm256_add_epi16(m0,m4);

								even=division(division_case,m0,f);//even results

								//multiply with the mask
								m1=_mm256_maddubs_epi16(r1,cy0_sh1);
								m2=_mm256_maddubs_epi16(r2,cy1_sh1);
								m3=_mm256_maddubs_epi16(r3,cy2_sh1);
								m4=_mm256_maddubs_epi16(r4,cy1_sh1);

								//vertical add
								m0=_mm256_add_epi16(m1,m2);
								m0=_mm256_add_epi16(m0,m3);
								m0=_mm256_add_epi16(m0,m4);

								odd=division(division_case,m0,f);//odd results

								//pack to one register
								odd=_mm256_slli_si256(odd,1); //shift one position right
								row_0=_mm256_add_epi8(even,odd); //add the odd with the even

								//y filter ends - row_0 has the data to be processed by x filter

							//multiply with the mask
							m0=_mm256_maddubs_epi16(row_0,cx);


							m1=_mm256_srli_si256(m0,2);
							m2=_mm256_add_epi16(m1,m0);

							m1=_mm256_srli_si256(m0,4);
							m2=_mm256_add_epi16(m1,m2);

							//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
							//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
							m4=_mm256_and_si256(m0,mask3);
							m4=_mm256_permute2f128_si256(m4,m4,1);
							m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions

							m2=_mm256_add_epi16(m2,m4);

							//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
							output_row0_even=_mm256_and_si256(m2,output_mask);

							//--------------row=1
							  //y filter begins

								//multiply with the mask
								m2=_mm256_maddubs_epi16(r2,cy0);
								m3=_mm256_maddubs_epi16(r3,cy1);
								m4=_mm256_maddubs_epi16(r4,cy2);

								//vertical add
								m0=_mm256_add_epi16(m2,m3);
								m0=_mm256_add_epi16(m0,m4);

								even=division(division_case,m0,f);//even results

								//multiply with the mask
								m2=_mm256_maddubs_epi16(r2,cy0_sh1);
								m3=_mm256_maddubs_epi16(r3,cy1_sh1);
								m4=_mm256_maddubs_epi16(r4,cy2_sh1);

								//vertical add
								m0=_mm256_add_epi16(m2,m3);
								m0=_mm256_add_epi16(m0,m4);

								odd=division(division_case,m0,f);//odd results

								//pack to one register
								odd=_mm256_slli_si256(odd,1); //shift one position right
								row_1=_mm256_add_epi8(even,odd); //add the odd with the even

								//y filter ends - row_1 has the data to be processed by x filter

							//multiply with the mask
							m0=_mm256_maddubs_epi16(row_1,cx);

							m1=_mm256_srli_si256(m0,2);
							m2=_mm256_add_epi16(m1,m0);

							m1=_mm256_srli_si256(m0,4);
							m2=_mm256_add_epi16(m1,m2);

							//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
							//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
							m4=_mm256_and_si256(m0,mask3);
							m4=_mm256_permute2f128_si256(m4,m4,1);
							m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions

							m2=_mm256_add_epi16(m2,m4);

							//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
							output_row1_even=_mm256_and_si256(m2,output_mask);

							//2nd col iteration
							//----row=2

							//multiply with the mask
							m0=_mm256_maddubs_epi16(row_2,cx_sh1);


							m1=_mm256_srli_si256(m0,2);
							m2=_mm256_add_epi16(m1,m0);

							m1=_mm256_srli_si256(m0,4);
							m2=_mm256_add_epi16(m1,m2);

							//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
							//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
							m4=_mm256_and_si256(m0,mask3);
							m4=_mm256_permute2f128_si256(m4,m4,1);
							m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions
							m2=_mm256_add_epi16(m2,m4);

							//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
							output_odd = _mm256_and_si256(m2,output_mask);

							//----row=0
							//multiply with the mask
							m0=_mm256_maddubs_epi16(row_0,cx_sh1);


							m1=_mm256_srli_si256(m0,2);
							m2=_mm256_add_epi16(m1,m0);

							m1=_mm256_srli_si256(m0,4);
							m2=_mm256_add_epi16(m1,m2);

							//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
							//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
							m4=_mm256_and_si256(m0,mask3);
							m4=_mm256_permute2f128_si256(m4,m4,1);
							m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions
							m2=_mm256_add_epi16(m2,m4);

							//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
							output_row0_odd = _mm256_and_si256(m2,output_mask);

							//----row=1
							//multiply with the mask
							m0=_mm256_maddubs_epi16(row_1,cx_sh1);


							m1=_mm256_srli_si256(m0,2);
							m2=_mm256_add_epi16(m1,m0);

							m1=_mm256_srli_si256(m0,4);
							m2=_mm256_add_epi16(m1,m2);

							//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
							//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
							m4=_mm256_and_si256(m0,mask3);
							m4=_mm256_permute2f128_si256(m4,m4,1);
							m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions
							m2=_mm256_add_epi16(m2,m4);

							//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
							output_row1_odd = _mm256_and_si256(m2,output_mask);

					                 //3rd col iteration
							//---row=2
							//multiply with the mask
							m0=_mm256_maddubs_epi16(row_2,cx_sh2);


							m1=_mm256_srli_si256(m0,2);
							m2=_mm256_add_epi16(m1,m0);

							//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,_mm256_extract_epi16(m2,8),0,0,0,0,0,0,0);
							m4=_mm256_and_si256(m2,mask3);
							m4=_mm256_permute2f128_si256(m4,m4,1);
							m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions

							m1=_mm256_srli_si256(m0,4);
							m2=_mm256_add_epi16(m1,m2);

							//prepei na prosthesw to pixel pu thelw sto r2 kai meta na kanw diairesi me ola mazi

							m2=_mm256_add_epi16(m2,m4);

							//m2 has 16 16bit values now. the results I need are in positions 1,4,7,10,13. keep only those, discard others
							m2 = _mm256_and_si256(m2,output_mask_sh1);
							output_even = _mm256_add_epi16(output_even,m2);

							//---row=0
							//multiply with the mask
							m0=_mm256_maddubs_epi16(row_0,cx_sh2);

							m1=_mm256_srli_si256(m0,2);
							m2=_mm256_add_epi16(m1,m0);

							//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,_mm256_extract_epi16(m2,8),0,0,0,0,0,0,0);
							m4=_mm256_and_si256(m2,mask3);
							m4=_mm256_permute2f128_si256(m4,m4,1);
							m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions

							m1=_mm256_srli_si256(m0,4);
							m2=_mm256_add_epi16(m1,m2);

							//prepei na prosthesw to pixel pu thelw sto r2 kai meta na kanw diairesi me ola mazi

							m2=_mm256_add_epi16(m2,m4);

							//m2 has 16 16bit values now. the results I need are in positions 1,4,7,10,13. keep only those, discard others
							m2 = _mm256_and_si256(m2,output_mask_sh1);
							output_row0_even = _mm256_add_epi16(output_row0_even,m2);


							//---row=1
							//multiply with the mask
							m0=_mm256_maddubs_epi16(row_1,cx_sh2);


							m1=_mm256_srli_si256(m0,2);
							m2=_mm256_add_epi16(m1,m0);

							//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,_mm256_extract_epi16(m2,8),0,0,0,0,0,0,0);
							m4=_mm256_and_si256(m2,mask3);
							m4=_mm256_permute2f128_si256(m4,m4,1);
							m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions

							m1=_mm256_srli_si256(m0,4);
							m2=_mm256_add_epi16(m1,m2);

							//prepei na prosthesw to pixel pu thelw sto r2 kai meta na kanw diairesi me ola mazi

							m2=_mm256_add_epi16(m2,m4);

							//m2 has 16 16bit values now. the results I need are in positions 1,4,7,10,13. keep only those, discard others
							m2 = _mm256_and_si256(m2,output_mask_sh1);
							output_row1_even = _mm256_add_epi16(output_row1_even,m2);

							//4th col iteration
							//multiply with the mask
							m0=_mm256_maddubs_epi16(row_2,cx_sh3);

							//hozizontal additions
							//hadd(3:8)
							//hadd(9:14)
							//hadd(15:20)
							//hadd(21:26)
							//hadd(27:31)
							//result after division will be in 2,8,14,20,26

							m1=_mm256_srli_si256(m0,2);
							m4=_mm256_add_epi16(m1,m0);

							m1=_mm256_srli_si256(m0,4);
							m2=_mm256_add_epi16(m1,m4);

							//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
							//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
							m4=_mm256_and_si256(m4,mask3);
							m4=_mm256_permute2f128_si256(m4,m4,1);
							m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions

							m2=_mm256_add_epi16(m2,m4);

							//m2 has 16 16bit values now. the results I need are in positions 1,4,7,10,13. keep only those, discard others
							m2 = _mm256_and_si256(m2,output_mask_sh1);
							output_odd = _mm256_add_epi16(output_odd,m2);

							//row0
							//multiply with the mask
							m0=_mm256_maddubs_epi16(row_0,cx_sh3);


								m1=_mm256_srli_si256(m0,2);
								m4=_mm256_add_epi16(m1,m0);

								m1=_mm256_srli_si256(m0,4);
								m2=_mm256_add_epi16(m1,m4);

								//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
								//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
								m4=_mm256_and_si256(m4,mask3);
								m4=_mm256_permute2f128_si256(m4,m4,1);
								m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions

								m2=_mm256_add_epi16(m2,m4);

								//m2 has 16 16bit values now. the results I need are in positions 1,4,7,10,13. keep only those, discard others
								m2 = _mm256_and_si256(m2,output_mask_sh1);
								output_row0_odd = _mm256_add_epi16(output_row0_odd,m2);

								//row1
								//multiply with the mask
								m0=_mm256_maddubs_epi16(row_1,cx_sh3);

									m1=_mm256_srli_si256(m0,2);
									m4=_mm256_add_epi16(m1,m0);

									m1=_mm256_srli_si256(m0,4);
									m2=_mm256_add_epi16(m1,m4);

									//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
									//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
									m4=_mm256_and_si256(m4,mask3);
									m4=_mm256_permute2f128_si256(m4,m4,1);
									m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions

									m2=_mm256_add_epi16(m2,m4);

									//m2 has 16 16bit values now. the results I need are in positions 1,4,7,10,13. keep only those, discard others
									m2 = _mm256_and_si256(m2,output_mask_sh1);
									output_row1_odd = _mm256_add_epi16(output_row1_odd,m2);


							//5th col iteration
							//multiply with the mask
							m0=_mm256_maddubs_epi16(row_2,cx_sh4);

							//hozizontal additions
							//hadd(4:9)
							//hadd(10:15)
							//hadd(16:21)
							//hadd(22:27)
							//result after division will be in 4,10,16,22

							m1=_mm256_srli_si256(m0,2);
							m2=_mm256_add_epi16(m1,m0);

							m1=_mm256_srli_si256(m0,4);
							m2=_mm256_add_epi16(m1,m2);


							//m2 has 16 16bit values now. the results I need are in positions 2,5,8,11. keep only those, discard others
							m2 = _mm256_and_si256(m2,output_mask_sh2);
							output_even = _mm256_add_epi16(output_even,m2);

							//row0
							//multiply with the mask
							m0=_mm256_maddubs_epi16(row_0,cx_sh4);


							m1=_mm256_srli_si256(m0,2);
							m2=_mm256_add_epi16(m1,m0);

							m1=_mm256_srli_si256(m0,4);
							m2=_mm256_add_epi16(m1,m2);


							//m2 has 16 16bit values now. the results I need are in positions 2,5,8,11. keep only those, discard others
							m2 = _mm256_and_si256(m2,output_mask_sh2);
							output_row0_even = _mm256_add_epi16(output_row0_even,m2);

							//row1
							//multiply with the mask
							m0=_mm256_maddubs_epi16(row_1,cx_sh4);


							m1=_mm256_srli_si256(m0,2);
							m2=_mm256_add_epi16(m1,m0);

							m1=_mm256_srli_si256(m0,4);
							m2=_mm256_add_epi16(m1,m2);


							//m2 has 16 16bit values now. the results I need are in positions 2,5,8,11. keep only those, discard others
							m2 = _mm256_and_si256(m2,output_mask_sh2);
							output_row1_even = _mm256_add_epi16(output_row1_even,m2);

					                 //6th col iteration
							//multiply with the mask
							m0=_mm256_maddubs_epi16(row_2,cx_sh5);

							//hozizontal additions
							//hadd(5:10)
							//hadd(11:16)
							//hadd(17:22)
							//hadd(23:28)
							//result after division will be in 4,10,16,22

							m1=_mm256_srli_si256(m0,2);
							m2=_mm256_add_epi16(m1,m0);

							m1=_mm256_srli_si256(m0,4);
							m2=_mm256_add_epi16(m1,m2);

							//m2 has 16 16bit values now. the results I need are in positions 2,5,8,11. keep only those, discard others
							m2 = _mm256_and_si256(m2,output_mask_sh2);
							output_odd = _mm256_add_epi16(output_odd,m2);

							//now division follows
							output_even=division(division_case,output_even,f);
							output_odd=division(division_case,output_odd,f);

							//shift odd 1 position and add to even
							output_odd = _mm256_slli_si256(output_odd,1);
							output_even = _mm256_add_epi8(output_even,output_odd);


							_mm256_storeu_si256( (__m256i *) &filt[row][col],output_even );


							//row0
							//multiply with the mask
							m0=_mm256_maddubs_epi16(row_0,cx_sh5);


							m1=_mm256_srli_si256(m0,2);
							m2=_mm256_add_epi16(m1,m0);

							m1=_mm256_srli_si256(m0,4);
							m2=_mm256_add_epi16(m1,m2);

							//m2 has 16 16bit values now. the results I need are in positions 2,5,8,11. keep only those, discard others
							m2 = _mm256_and_si256(m2,output_mask_sh2);
							output_row0_odd = _mm256_add_epi16(output_row0_odd,m2);

							//now division follows
							output_row0_even=division(division_case,output_row0_even,f);
							output_row0_odd=division(division_case,output_row0_odd,f);

							//shift odd 1 position and add to even
							output_row0_odd = _mm256_slli_si256(output_row0_odd,1);
							output_row0_even = _mm256_add_epi8(output_row0_even,output_row0_odd);


							_mm256_storeu_si256( (__m256i *) &filt[N-2][col],output_row0_even );

							//row1
							//multiply with the mask
							m0=_mm256_maddubs_epi16(row_1,cx_sh5);

								m1=_mm256_srli_si256(m0,2);
							m2=_mm256_add_epi16(m1,m0);

							m1=_mm256_srli_si256(m0,4);
							m2=_mm256_add_epi16(m1,m2);

							//m2 has 16 16bit values now. the results I need are in positions 2,5,8,11. keep only those, discard others
							m2 = _mm256_and_si256(m2,output_mask_sh2);
							output_row1_odd = _mm256_add_epi16(output_row1_odd,m2);

							//now division follows
							output_row1_even=division(division_case,output_row1_even,f);
							output_row1_odd=division(division_case,output_row1_odd,f);

							//shift odd 1 position and add to even
							output_row1_odd = _mm256_slli_si256(output_row1_odd,1);
							output_row1_even = _mm256_add_epi8(output_row1_even,output_row1_odd);


							_mm256_storeu_si256( (__m256i *) &filt[N-1][col],output_row1_even );


							}
/*
			if (REMINDER_ITERATIONS>=16)
				loop_reminder_last_rows_high_reminder_values(frame1,filt,M,N,col,REMINDER_ITERATIONS,division_case,c0,c1,c2,c0_sh1,c1_sh1,c2_sh1,c0_sh2,c1_sh2,c2_sh2,f,divisor);
			else
				loop_reminder_last_rows_low_reminder_values(frame1,filt,M,N,col,REMINDER_ITERATIONS,division_case,c0,c1,c2,c0_sh1,c1_sh1,c2_sh1,c0_sh2,c1_sh2,c2_sh2,f);
*/
					}
	else {


	  for (col = 0; col <= M-32; col+=28){
	 //last col value that does not read outside of the array bounds is (col<M-29)

		  if (col==0){

				//load the 5 rows
				r0=_mm256_loadu_si256( (__m256i *) &frame1[row-2][0]);
				r1=_mm256_loadu_si256( (__m256i *) &frame1[row-1][0]);
				r2=_mm256_loadu_si256( (__m256i *) &frame1[row][0]);
				r3=_mm256_loadu_si256( (__m256i *) &frame1[row+1][0]);
				r4=_mm256_loadu_si256( (__m256i *) &frame1[row+2][0]);

		  			//START - extra code needed for prelude
		  					m0=_mm256_slli_si256(r0,2);//shift 2 elements left - equivalent to filling with two zeros inthe beginning
		  					m1=_mm256_slli_si256(r1,2);//shift 2 elements left - equivalent to filling with two zeros inthe beginning
		  					m2=_mm256_slli_si256(r2,2);//shift 2 elements left - equivalent to filling with two zeros inthe beginning
		  					m3=_mm256_slli_si256(r3,2);//shift 2 elements left - equivalent to filling with two zeros inthe beginning
		  					m4=_mm256_slli_si256(r4,2);//shift 2 elements left - equivalent to filling with two zeros inthe beginning

		  					r0=_mm256_and_si256(r0,mask_prelude);
		  					r0=_mm256_permute2f128_si256(r0,r0,1);
		  					r0=_mm256_srli_si256(r0,14);//shift 14 elements
		  					r0=_mm256_add_epi16(m0,r0);

		  					r1=_mm256_and_si256(r1,mask_prelude);
		  					r1=_mm256_permute2f128_si256(r1,r1,1);
		  					r1=_mm256_srli_si256(r1,14);//shift 14 elements
		  					r1=_mm256_add_epi16(m1,r1);

		  					r2=_mm256_and_si256(r2,mask_prelude);
		  					r2=_mm256_permute2f128_si256(r2,r2,1);
		  					r2=_mm256_srli_si256(r2,14);//shift 14 elements
		  					r2=_mm256_add_epi16(m2,r2);

		  					r3=_mm256_and_si256(r3,mask_prelude);
		  					r3=_mm256_permute2f128_si256(r3,r3,1);
		  					r3=_mm256_srli_si256(r3,14);//shift 14 elements
		  					r3=_mm256_add_epi16(m3,r3);

		  					r4=_mm256_and_si256(r4,mask_prelude);
		  					r4=_mm256_permute2f128_si256(r4,r4,1);
		  					r4=_mm256_srli_si256(r4,14);//shift 14 elements
		  					r4=_mm256_add_epi16(m4,r4);

		  			//END - extra code needed for prelude
		  		}
		  else {
				//load the 5 rows
				r0=_mm256_loadu_si256( (__m256i *) &frame1[row-2][col-2]);
				r1=_mm256_loadu_si256( (__m256i *) &frame1[row-1][col-2]);
				r2=_mm256_loadu_si256( (__m256i *) &frame1[row][col-2]);
				r3=_mm256_loadu_si256( (__m256i *) &frame1[row+1][col-2]);
				r4=_mm256_loadu_si256( (__m256i *) &frame1[row+2][col-2]);
		  }

		  //y filter begins

			//multiply with the mask
			m0=_mm256_maddubs_epi16(r0,cy0);
			m1=_mm256_maddubs_epi16(r1,cy1);
			m2=_mm256_maddubs_epi16(r2,cy2);
			m3=_mm256_maddubs_epi16(r3,cy1);
			m4=_mm256_maddubs_epi16(r4,cy0);

			//vertical add
			m0=_mm256_add_epi16(m0,m1);
			m2=_mm256_add_epi16(m2,m3);
			m0=_mm256_add_epi16(m0,m2);
			m0=_mm256_add_epi16(m0,m4);

			even=division(division_case,m0,f);//even results

			//multiply with the mask
			m0=_mm256_maddubs_epi16(r0,cy0_sh1);
			m1=_mm256_maddubs_epi16(r1,cy1_sh1);
			m2=_mm256_maddubs_epi16(r2,cy2_sh1);
			m3=_mm256_maddubs_epi16(r3,cy1_sh1);
			m4=_mm256_maddubs_epi16(r4,cy0_sh1);

			//vertical add
			m0=_mm256_add_epi16(m0,m1);
			m2=_mm256_add_epi16(m2,m3);
			m0=_mm256_add_epi16(m0,m2);
			m0=_mm256_add_epi16(m0,m4);

			odd=division(division_case,m0,f);//odd results

			//pack to one register
			odd=_mm256_slli_si256(odd,1); //shift one position right
			r0=_mm256_add_epi8(even,odd); //add the odd with the even

			//y filter ends - r0 has the data to be processed by x filter

		//col iteration computes output pixels of 2,8,14,20,26
		// col+1 iteration computes output pixels of 3,9,15,21,27
		// col+2 iteration computes output pixels of 4,10,16,22,28
		// col+3 iteration computes output pixels of 5,11,17,23,29
		// col+4 iteration computes output pixels of 6,12,18,24
		// col+5 iteration computes output pixels of 7,13,19,25
		//afterwards, col becomes 30 and repeat the above process

		//1st col iteration


		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,cx);


		//hozizontal additions
		//hadd(0:5)
		//hadd(6:11)
		//hadd(12:17)
		//hadd(18:23)
		//hadd(24:28)

		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		m1=_mm256_srli_si256(m0,4);
		m2=_mm256_add_epi16(m1,m2);

		//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
		//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
		m4=_mm256_and_si256(m0,mask3);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions

		m2=_mm256_add_epi16(m2,m4);

		//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
		output_even=_mm256_and_si256(m2,output_mask);


		//2nd col iteration
		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,cx_sh1);

		//hozizontal additions
		//hadd(1:6)
		//hadd(7:12)
		//hadd(13:18)
		//hadd(19:24)
		//hadd(25:29)

		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		m1=_mm256_srli_si256(m0,4);
		m2=_mm256_add_epi16(m1,m2);

		//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
		//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
		m4=_mm256_and_si256(m0,mask3);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions
		m2=_mm256_add_epi16(m2,m4);

		//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
		output_odd = _mm256_and_si256(m2,output_mask);


         //3rd col iteration
		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,cx_sh2);

		//hozizontal additions
		//hadd(2:7)
		//hadd(8:13)
		//hadd(14:19)
		//hadd(20:25)
		//hadd(26:30)


		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,_mm256_extract_epi16(m2,8),0,0,0,0,0,0,0);
		m4=_mm256_and_si256(m2,mask3);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions

		m1=_mm256_srli_si256(m0,4);
		m2=_mm256_add_epi16(m1,m2);

		//prepei na prosthesw to pixel pu thelw sto r2 kai meta na kanw diairesi me ola mazi

		m2=_mm256_add_epi16(m2,m4);

		//m2 has 16 16bit values now. the results I need are in positions 1,4,7,10,13. keep only those, discard others
		m2 = _mm256_and_si256(m2,output_mask_sh1);
		output_even = _mm256_add_epi16(output_even,m2);


		//4th col iteration

		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,cx_sh3);

		//hozizontal additions
		//hadd(3:8)
		//hadd(9:14)
		//hadd(15:20)
		//hadd(21:26)
		//hadd(27:31)
		//result after division will be in 2,8,14,20,26

		m1=_mm256_srli_si256(m0,2);
		m4=_mm256_add_epi16(m1,m0);

		m1=_mm256_srli_si256(m0,4);
		m2=_mm256_add_epi16(m1,m4);

		//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
		//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
		m4=_mm256_and_si256(m4,mask3);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions

		m2=_mm256_add_epi16(m2,m4);

		//m2 has 16 16bit values now. the results I need are in positions 1,4,7,10,13. keep only those, discard others
		m2 = _mm256_and_si256(m2,output_mask_sh1);
		output_odd = _mm256_add_epi16(output_odd,m2);


		//5th col iteration
		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,cx_sh4);

		//hozizontal additions
		//hadd(4:9)
		//hadd(10:15)
		//hadd(16:21)
		//hadd(22:27)
		//result after division will be in 4,10,16,22

		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		m1=_mm256_srli_si256(m0,4);
		m2=_mm256_add_epi16(m1,m2);


		//m2 has 16 16bit values now. the results I need are in positions 2,5,8,11. keep only those, discard others
		m2 = _mm256_and_si256(m2,output_mask_sh2);
		output_even = _mm256_add_epi16(output_even,m2);


                 //6th col iteration
		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,cx_sh5);

		//hozizontal additions
		//hadd(5:10)
		//hadd(11:16)
		//hadd(17:22)
		//hadd(23:28)
		//result after division will be in 4,10,16,22

		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		m1=_mm256_srli_si256(m0,4);
		m2=_mm256_add_epi16(m1,m2);

		//m2 has 16 16bit values now. the results I need are in positions 2,5,8,11. keep only those, discard others
		m2 = _mm256_and_si256(m2,output_mask_sh2);
		output_odd = _mm256_add_epi16(output_odd,m2);

		//now division follows
		output_even=division(division_case,output_even,f);
		output_odd=division(division_case,output_odd,f);

		//shift odd 1 position and add to even
		output_odd = _mm256_slli_si256(output_odd,1);
		output_even = _mm256_add_epi8(output_even,output_odd);


		_mm256_storeu_si256( (__m256i *) &filt[row][col],output_even );


		}

        if (REMINDER_ITERATIONS>=16)
        	loop_reminder_high_reminder_values_seperable(frame1,filt,M,N,row,col,REMINDER_ITERATIONS,division_case, cx, cx_sh1, cx_sh2,cx_sh3,cx_sh4,cx_sh5,cy0, cy1,cy2, cy0_sh1, cy1_sh1, cy2_sh1,f,divisor);
        else
        	loop_reminder_low_reminder_values_seperable(frame1,filt,M,N,row,col,REMINDER_ITERATIONS,division_case, cx, cx_sh1, cx_sh2,cx_sh3,cx_sh4,cx_sh5,cy0, cy1,cy2, cy0_sh1, cy1_sh1, cy2_sh1,f);
		//padding code. The last three iterations  ABOVE get outside of the array bounds. The last three col iterations read outside but the garbage values are multiplied by zeros (last three mask zeros). From now on, I must include another mask with extra zeros.
//It is faster to read outside of the array's bounds and then fill the right values. there is an insert command that inserts a value to the vector
//TOMORROW: USE Gaussian_Blur_AVX_ver4_plus_less_load ROUTINE FOR LOOP REMINDER. LOAD OUTSIDE OF THE ARRAY AND THEN ZERO THE VALUES NEEDED.

		}
}

}//end of parallel


}




//for REMINDER_ITERATIONS >=29 only
int loop_reminder_high_reminder_values_less_div(unsigned char **frame1,unsigned char **filt,const unsigned int M, const unsigned int N,const unsigned int row, const unsigned int col, const unsigned int REMINDER_ITERATIONS,const unsigned int division_case,const __m256i c0,const __m256i c1,const __m256i c2,const __m256i c0_sh1,const __m256i c1_sh1,const __m256i c2_sh1,const __m256i c0_sh2,const __m256i c1_sh2,const __m256i c2_sh2, const __m256i f, const unsigned short int divisor, signed char **filter ){

__m256i r0,r1,r2,r3,r4,m0,m1,m2,m3,m4;
__m256i output_even,output_odd;


		//const __m256i f  = _mm256_set_epi16(52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759);
		//const __m256i mask  = _mm256_set_epi8(0,0,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255);
		//const __m256i mask2  = _mm256_set_epi8(0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0);
		const __m256i mask3  = _mm256_set_epi16(0,0,0,0,0,0,0,65535,0,0,0,0,0,0,0,0);
		//const __m256i mask4  = _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255,0,0,0,0,0,0,0,0,0,0,0,0);
		//const __m256i mask5  = _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0);

		const __m256i output_mask      = _mm256_set_epi16(0,0,0,65535,0,0,65535,0,0,65535,0,0,65535,0,0,65535);
		const __m256i output_mask_sh1  = _mm256_set_epi16(0,0,65535,0,0,65535,0,0,65535,0,0,65535,0,0,65535,0);
		//const __m256i output_mask_sh2  = _mm256_set_epi16(0,0,0,    0,65535,0,0,65535,0,0,65535,0,0,65535,0,0);
		const __m256i mask_new      = _mm256_set_epi16(0,0,0,0,0,0,0,0,0,65535,0,0,0,0,0,0);
		const __m256i mask_new2      = _mm256_set_epi16(0,0,0,0,0,0,0,0,65535,0,0,0,0,0,0,0);



 __m256i reminder_mask1,reminder_mask2;

	reminder_mask1=_mm256_load_si256( (__m256i *) &reminder_msk1[REMINDER_ITERATIONS-1][0]);
	reminder_mask2=_mm256_load_si256( (__m256i *) &reminder_msk2[REMINDER_ITERATIONS-1][0]);




	 //1st col iteration
	//load the 5 rows
	r0=_mm256_loadu_si256( (__m256i *) &frame1[row-2][col-2]);
	r1=_mm256_loadu_si256( (__m256i *) &frame1[row-1][col-2]);
	r2=_mm256_loadu_si256( (__m256i *) &frame1[row][col-2]);
	r3=_mm256_loadu_si256( (__m256i *) &frame1[row+1][col-2]);
	r4=_mm256_loadu_si256( (__m256i *) &frame1[row+2][col-2]);

	/*
	 * The above load operations load outside of the array bounds; these elements are filled with zeros just after
	 * Furthermore, I add two extra zeros in the end to compute col=N-2 and col=N-1. The last value of col is N-1, not N
	 */

	//AND r0-r4 with reminder_mask
	r0=_mm256_and_si256(r0,reminder_mask1);
	r1=_mm256_and_si256(r1,reminder_mask1);
	r2=_mm256_and_si256(r2,reminder_mask1);
	r3=_mm256_and_si256(r3,reminder_mask1);
	r4=_mm256_and_si256(r4,reminder_mask1);

	//multiply with the mask
	m0=_mm256_maddubs_epi16(r0,c0);
	m1=_mm256_maddubs_epi16(r1,c1);
	m2=_mm256_maddubs_epi16(r2,c2);
	m3=_mm256_maddubs_epi16(r3,c1);
	m4=_mm256_maddubs_epi16(r4,c0);

	//vertical add
	m0=_mm256_add_epi16(m0,m1);
	m2=_mm256_add_epi16(m2,m3);
	m0=_mm256_add_epi16(m0,m2);
	m0=_mm256_add_epi16(m0,m4);


	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);

	m1=_mm256_srli_si256(m0,4);
	m2=_mm256_add_epi16(m1,m2);

	//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
	//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
	m4=_mm256_and_si256(m0,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions

	m2=_mm256_add_epi16(m2,m4);

	//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
	output_even=_mm256_and_si256(m2,output_mask);

	//2nd col iteration
	//multiply with the mask
	m0=_mm256_maddubs_epi16(r0,c0_sh1);
	m1=_mm256_maddubs_epi16(r1,c1_sh1);
	m2=_mm256_maddubs_epi16(r2,c2_sh1);
	m3=_mm256_maddubs_epi16(r3,c1_sh1);
	m4=_mm256_maddubs_epi16(r4,c0_sh1);

	//vertical add
	m0=_mm256_add_epi16(m0,m1);
	m2=_mm256_add_epi16(m2,m3);
	m0=_mm256_add_epi16(m0,m2);
	m0=_mm256_add_epi16(m0,m4);

	//hozizontal additions
	//hadd(0:5)   and store filt[row[col]
	//hadd(6:11)   and store filt[row[col+8]
	//hadd(12:17) and store filt[row[col+14]
	//hadd(18:23) and store filt[row[col+20]
	//hadd(24:30) and store filt[row[col+26]

	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);

	m1=_mm256_srli_si256(m0,4);
	m2=_mm256_add_epi16(m1,m2);

	//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
	//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
	m4=_mm256_and_si256(m0,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions
	m2=_mm256_add_epi16(m2,m4);

	//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
	output_odd = _mm256_and_si256(m2,output_mask);

             //3rd col iteration
	//multiply with the mask
	m0=_mm256_maddubs_epi16(r0,c0_sh2);
	m1=_mm256_maddubs_epi16(r1,c1_sh2);
	m2=_mm256_maddubs_epi16(r2,c2_sh2);
	m3=_mm256_maddubs_epi16(r3,c1_sh2);
	m4=_mm256_maddubs_epi16(r4,c0_sh2);

	//vertical add
	m0=_mm256_add_epi16(m0,m1);
	m2=_mm256_add_epi16(m2,m3);
	m0=_mm256_add_epi16(m0,m2);
	m0=_mm256_add_epi16(m0,m4);

	//hozizontal additions
	//hadd(0:5)   and store filt[row[col]
	//hadd(6:11)   and store filt[row[col+8]
	//hadd(12:17) and store filt[row[col+14]
	//hadd(18:23) and store filt[row[col+20]
	//hadd(24:30) and store filt[row[col+26]

	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);

	//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,_mm256_extract_epi16(m2,8),0,0,0,0,0,0,0);
	m4=_mm256_and_si256(m2,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions

	m1=_mm256_srli_si256(m0,4);
	m2=_mm256_add_epi16(m1,m2);

	//prepei na prosthesw to pixel pu thelw sto r2 kai meta na kanw diairesi me ola mazi

	m2=_mm256_add_epi16(m2,m4);

	//m2 has 16 16bit values now. the results I need are in positions 1,4,7,10,13. keep only those, discard others
	m2 = _mm256_and_si256(m2,output_mask_sh1);
	output_even = _mm256_add_epi16(output_even,m2);


	//4th col iteration
	//load the 5 rows
	r0=_mm256_loadu_si256( (__m256i *) &frame1[row-2][col+3-2]);
	r1=_mm256_loadu_si256( (__m256i *) &frame1[row-1][col+3-2]);
	r2=_mm256_loadu_si256( (__m256i *) &frame1[row][col+3-2]);
	r3=_mm256_loadu_si256( (__m256i *) &frame1[row+1][col+3-2]);
	r4=_mm256_loadu_si256( (__m256i *) &frame1[row+2][col+3-2]);

	//AND r0-r4 with reminder_mask
	r0=_mm256_and_si256(r0,reminder_mask2);
	r1=_mm256_and_si256(r1,reminder_mask2);
	r2=_mm256_and_si256(r2,reminder_mask2);
	r3=_mm256_and_si256(r3,reminder_mask2);
	r4=_mm256_and_si256(r4,reminder_mask2);

	//multiply with the mask
	m0=_mm256_maddubs_epi16(r0,c0);
	m1=_mm256_maddubs_epi16(r1,c1);
	m2=_mm256_maddubs_epi16(r2,c2);
	m3=_mm256_maddubs_epi16(r3,c1);
	m4=_mm256_maddubs_epi16(r4,c0);

	//vertical add
	m0=_mm256_add_epi16(m0,m1);
	m2=_mm256_add_epi16(m2,m3);
	m0=_mm256_add_epi16(m0,m2);
	m0=_mm256_add_epi16(m0,m4);


	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);

	m1=_mm256_srli_si256(m0,4);
	m2=_mm256_add_epi16(m1,m2);

	//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
	//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
	m4=_mm256_and_si256(m0,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions

	m2=_mm256_add_epi16(m2,m4);

	//m2 has 16 16bit values now. the results I need are in positions 1,4,7,10,13. keep only those, discard others
	m2 = _mm256_and_si256(m2,output_mask);
	m2 = _mm256_slli_si256(m2,2);
	output_odd = _mm256_add_epi16(output_odd,m2);



	//5th col iteration
	//multiply with the mask
	m0=_mm256_maddubs_epi16(r0,c0_sh1);
	m1=_mm256_maddubs_epi16(r1,c1_sh1);
	m2=_mm256_maddubs_epi16(r2,c2_sh1);
	m3=_mm256_maddubs_epi16(r3,c1_sh1);
	m4=_mm256_maddubs_epi16(r4,c0_sh1);

	//vertical add
	m0=_mm256_add_epi16(m0,m1);
	m2=_mm256_add_epi16(m2,m3);
	m0=_mm256_add_epi16(m0,m2);
	m0=_mm256_add_epi16(m0,m4);


	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);

	m1=_mm256_srli_si256(m0,4);
	m2=_mm256_add_epi16(m1,m2);

	//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
	//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
	m4=_mm256_and_si256(m0,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions

	m2=_mm256_add_epi16(m2,m4);

	//m2 has 16 16bit values now.
	m2 = _mm256_and_si256(m2,output_mask); //holds results in positions 0,3,6,9,12
	m1 = _mm256_slli_si256(m2,4); //put results in positions 2,5,8,11,14 , but 8 is not written
	m2=_mm256_and_si256(m2,mask_new);//extract only the 6th
	m2=_mm256_permute2f128_si256(m2,m2,1);
	m2=_mm256_srli_si256(m2,12);
	m2 = _mm256_add_epi16(m1,m2);
	output_even = _mm256_add_epi16(output_even,m2);



             //6th col iteration
	//multiply with the mask
	m0=_mm256_maddubs_epi16(r0,c0_sh2);
	m1=_mm256_maddubs_epi16(r1,c1_sh2);
	m2=_mm256_maddubs_epi16(r2,c2_sh2);
	m3=_mm256_maddubs_epi16(r3,c1_sh2);
	m4=_mm256_maddubs_epi16(r4,c0_sh2);

	//vertical add
	m0=_mm256_add_epi16(m0,m1);
	m2=_mm256_add_epi16(m2,m3);
	m0=_mm256_add_epi16(m0,m2);
	m0=_mm256_add_epi16(m0,m4);


	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);
	//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,_mm256_extract_epi16(m2,8),0,0,0,0,0,0,0);
	m4=_mm256_and_si256(m2,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions

	m1=_mm256_srli_si256(m0,4);
	m2=_mm256_add_epi16(m1,m2);
	m2=_mm256_add_epi16(m2,m4);

	//m2 has 16 16bit values now.
	m2 = _mm256_and_si256(m2,output_mask_sh1);//holds results in positions 1,4,7,10,13
	m1 = _mm256_slli_si256(m2,2); //put results in positions 2,5,8,11,14 , but 8 is not written
	m2=_mm256_and_si256(m2,mask_new2);//extract only the 7th
	m2=_mm256_permute2f128_si256(m2,m2,1);
	m2=_mm256_srli_si256(m2,14);
	m2 = _mm256_add_epi16(m1,m2);
	output_odd = _mm256_add_epi16(output_odd,m2);


	//now division follows
	output_even=division(division_case,output_even,f);
	output_odd=division(division_case,output_odd,f);

	//shift odd 1 position and add to even
	output_odd = _mm256_slli_si256(output_odd,1);
	output_even = _mm256_add_epi8(output_even,output_odd);


	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels

	switch (REMINDER_ITERATIONS){
	case 29:
		filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
		filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
		filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
		filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output_even,19);
		filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output_even,20);
		filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output_even,21);
		filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output_even,22);
		filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output_even,23);
		filt[row][col+24] = (unsigned char) _mm256_extract_epi8(output_even,24);
		filt[row][col+25] = (unsigned char) _mm256_extract_epi8(output_even,25);
		filt[row][col+26] = (unsigned char) _mm256_extract_epi8(output_even,26);
		filt[row][col+27] = (unsigned char) _mm256_extract_epi8(output_even,27);
		filt[row][col+28] = (unsigned char) _mm256_extract_epi8(output_even,28);
		  break;
	case 30:
		filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
		filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
		filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
		filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output_even,19);
		filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output_even,20);
		filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output_even,21);
		filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output_even,22);
		filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output_even,23);
		filt[row][col+24] = (unsigned char) _mm256_extract_epi8(output_even,24);
		filt[row][col+25] = (unsigned char) _mm256_extract_epi8(output_even,25);
		filt[row][col+26] = (unsigned char) _mm256_extract_epi8(output_even,26);
		filt[row][col+27] = (unsigned char) _mm256_extract_epi8(output_even,27);
		filt[row][col+28] = (unsigned char) _mm256_extract_epi8(output_even,28);
		filt[row][col+29] = (unsigned char) _mm256_extract_epi8(output_even,29);
		  break;
	case 31:
		filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
		filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
		filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
		filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output_even,19);
		filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output_even,20);
		filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output_even,21);
		filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output_even,22);
		filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output_even,23);
		filt[row][col+24] = (unsigned char) _mm256_extract_epi8(output_even,24);
		filt[row][col+25] = (unsigned char) _mm256_extract_epi8(output_even,25);
		filt[row][col+26] = (unsigned char) _mm256_extract_epi8(output_even,26);
		filt[row][col+27] = (unsigned char) _mm256_extract_epi8(output_even,27);
		filt[row][col+28] = (unsigned char) _mm256_extract_epi8(output_even,28);
		filt[row][col+29] = (unsigned char) _mm256_extract_epi8(output_even,29);

		//the filt[row][M-1] pixel will be computed without vectorization
		int newPixel = 0;
		newPixel += frame1[row-2][M-1-2] * filter[0][0];
		newPixel += frame1[row-2][M-1-1] * filter[0][1];
		newPixel += frame1[row-2][M-1-0] * filter[0][2];

		newPixel += frame1[row-1][M-1-2] * filter[1][0];
		newPixel += frame1[row-1][M-1-1] * filter[1][1];
		newPixel += frame1[row-1][M-1-0] * filter[1][2];

		newPixel += frame1[row-0][M-1-2] * filter[2][0];
		newPixel += frame1[row-0][M-1-1] * filter[2][1];
		newPixel += frame1[row-0][M-1-0] * filter[2][2];

		newPixel += frame1[row+1][M-1-2] * filter[3][0];
		newPixel += frame1[row+1][M-1-1] * filter[3][1];
		newPixel += frame1[row+1][M-1-0] * filter[3][2];

		newPixel += frame1[row+2][M-1-2] * filter[4][0];
		newPixel += frame1[row+2][M-1-1] * filter[4][1];
		newPixel += frame1[row+2][M-1-0] * filter[4][2];

    	filt[row][M-1] = (unsigned char) (newPixel / divisor);

		  break;
	default:
		printf("\n something went wrong");
	}

return 0;

}




//for loop reminder<29 only
int loop_reminder_low_reminder_values_less_div(unsigned char **frame1,unsigned char **filt,const unsigned int M, const unsigned int N,const unsigned int row, const unsigned int col,const unsigned int REMINDER_ITERATIONS,const unsigned int division_case,const __m256i c0,const __m256i c1,const __m256i c2,const __m256i c0_sh1,const __m256i c1_sh1,const __m256i c2_sh1,const __m256i c0_sh2,const __m256i c1_sh2,const __m256i c2_sh2, const __m256i c0_sh3,const __m256i c1_sh3,const __m256i c2_sh3,const __m256i c0_sh4,const __m256i c1_sh4,const __m256i c2_sh4,const __m256i c0_sh5,const __m256i c1_sh5,const __m256i c2_sh5,const __m256i f){

register	__m256i r0,r1,r2,r3,r4,m0,m1,m2,m3,m4,output_even,output_odd;


	//const __m256i f  = _mm256_set_epi16(52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759);
const __m256i output_mask      = _mm256_set_epi16(0,0,0,65535,0,0,65535,0,0,65535,0,0,65535,0,0,65535);
const __m256i output_mask_sh1  = _mm256_set_epi16(0,0,65535,0,0,65535,0,0,65535,0,0,65535,0,0,65535,0);
const __m256i output_mask_sh2  = _mm256_set_epi16(0,0,0,    0,65535,0,0,65535,0,0,65535,0,0,65535,0,0);
const __m256i mask3  = _mm256_set_epi16(0,0,0,0,0,0,0,65535,0,0,0,0,0,0,0,0);

 __m256i reminder_mask1;

if (REMINDER_ITERATIONS == 0){
	return 0; //no loop reminder is needed
}

	reminder_mask1=_mm256_load_si256( (__m256i *) &reminder_msk1[REMINDER_ITERATIONS-1][0]);
	//reminder_mask2=_mm256_load_si256( (__m256i *) &reminder_msk2[REMINDER_ITERATIONS-1][0]);



	 //1st col iteration
	//load the 5 rows
	r0=_mm256_loadu_si256( (__m256i *) &frame1[row-2][col-2]);
	r1=_mm256_loadu_si256( (__m256i *) &frame1[row-1][col-2]);
	r2=_mm256_loadu_si256( (__m256i *) &frame1[row][col-2]);
	r3=_mm256_loadu_si256( (__m256i *) &frame1[row+1][col-2]);
	r4=_mm256_loadu_si256( (__m256i *) &frame1[row+2][col-2]);

	/*
	 * The above load operations load outside of the array bounds; these elements are filled with zeros just after
	 * Furthermore, I add two extra zeros in the end to compute col=N-2 and col=N-1. The last value of col is N-1, not N
	 */

	//AND r0-r4 with reminder_mask
	r0=_mm256_and_si256(r0,reminder_mask1);
	r1=_mm256_and_si256(r1,reminder_mask1);
	r2=_mm256_and_si256(r2,reminder_mask1);
	r3=_mm256_and_si256(r3,reminder_mask1);
	r4=_mm256_and_si256(r4,reminder_mask1);

	//multiply with the mask
	m0=_mm256_maddubs_epi16(r0,c0);
	m1=_mm256_maddubs_epi16(r1,c1);
	m2=_mm256_maddubs_epi16(r2,c2);
	m3=_mm256_maddubs_epi16(r3,c1);
	m4=_mm256_maddubs_epi16(r4,c0);

	//vertical add
	m0=_mm256_add_epi16(m0,m1);
	m2=_mm256_add_epi16(m2,m3);
	m0=_mm256_add_epi16(m0,m2);
	m0=_mm256_add_epi16(m0,m4);

	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);

	m1=_mm256_srli_si256(m0,4);
	m2=_mm256_add_epi16(m1,m2);

	//after shifts the 15th element is lost as it cannot be propagated to the 16th position (AVX registers are managed as two seperate SSE registers)
	m4=_mm256_and_si256(m0,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions
	m2=_mm256_add_epi16(m2,m4);

	//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
	output_even=_mm256_and_si256(m2,output_mask);



    //2nd col iteration

	//multiply with the mask
	m0=_mm256_maddubs_epi16(r0,c0_sh1);
	m1=_mm256_maddubs_epi16(r1,c1_sh1);
	m2=_mm256_maddubs_epi16(r2,c2_sh1);
	m3=_mm256_maddubs_epi16(r3,c1_sh1);
	m4=_mm256_maddubs_epi16(r4,c0_sh1);

	//vertical add
	m0=_mm256_add_epi16(m0,m1);
	m2=_mm256_add_epi16(m2,m3);
	m0=_mm256_add_epi16(m0,m2);
	m0=_mm256_add_epi16(m0,m4);


	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);

	m1=_mm256_srli_si256(m0,4);
	m2=_mm256_add_epi16(m1,m2);

	//after shifts the 15th element is lost as it cannot be propagated to the 16th position (AVX registers are managed as two seperate SSE registers)
	m4=_mm256_and_si256(m0,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions
	m2=_mm256_add_epi16(m2,m4);

	//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
	output_odd = _mm256_and_si256(m2,output_mask);




     //3rd col iteration

	//multiply with the mask
	m0=_mm256_maddubs_epi16(r0,c0_sh2);
	m1=_mm256_maddubs_epi16(r1,c1_sh2);
	m2=_mm256_maddubs_epi16(r2,c2_sh2);
	m3=_mm256_maddubs_epi16(r3,c1_sh2);
	m4=_mm256_maddubs_epi16(r4,c0_sh2);

	//vertical add
	m0=_mm256_add_epi16(m0,m1);
	m2=_mm256_add_epi16(m2,m3);
	m0=_mm256_add_epi16(m0,m2);
	m0=_mm256_add_epi16(m0,m4);

	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);

	m4=_mm256_and_si256(m2,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions

	m1=_mm256_srli_si256(m0,4);
	m2=_mm256_add_epi16(m1,m2);
	m2=_mm256_add_epi16(m2,m4);

	//m2 has 16 16bit values now. the results I need are in positions 1,4,7,10,13. keep only those, discard others
	m2 = _mm256_and_si256(m2,output_mask_sh1);
	output_even = _mm256_add_epi16(output_even,m2);



	 //4th col iteration

	//multiply with the mask
	m0=_mm256_maddubs_epi16(r0,c0_sh3);
	m1=_mm256_maddubs_epi16(r1,c1_sh3);
	m2=_mm256_maddubs_epi16(r2,c2_sh3);
	m3=_mm256_maddubs_epi16(r3,c1_sh3);
	m4=_mm256_maddubs_epi16(r4,c0_sh3);

	//vertical add
	m0=_mm256_add_epi16(m0,m1);
	m2=_mm256_add_epi16(m2,m3);
	m0=_mm256_add_epi16(m0,m2);
	m0=_mm256_add_epi16(m0,m4);


	m1=_mm256_srli_si256(m0,2);
	m4=_mm256_add_epi16(m1,m0);

	m1=_mm256_srli_si256(m0,4);
	m2=_mm256_add_epi16(m1,m4);


	m4=_mm256_and_si256(m4,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions

	m2=_mm256_add_epi16(m2,m4);

	//m2 has 16 16bit values now. the results I need are in positions 1,4,7,10,13. keep only those, discard others
	m2 = _mm256_and_si256(m2,output_mask_sh1);
	output_odd = _mm256_add_epi16(output_odd,m2);


   //5th col iteration

	m0=_mm256_maddubs_epi16(r0,c0_sh4);
			m1=_mm256_maddubs_epi16(r1,c1_sh4);
			m2=_mm256_maddubs_epi16(r2,c2_sh4);
			m3=_mm256_maddubs_epi16(r3,c1_sh4);
			m4=_mm256_maddubs_epi16(r4,c0_sh4);

			//vertical add
			m0=_mm256_add_epi16(m0,m1);
			m2=_mm256_add_epi16(m2,m3);
			m0=_mm256_add_epi16(m0,m2);
			m0=_mm256_add_epi16(m0,m4);

			//hozizontal additions
			//hadd(4:9)
			//hadd(10:15)
			//hadd(16:21)
			//hadd(22:27)
			//result after division will be in 4,10,16,22

			m1=_mm256_srli_si256(m0,2);
			m2=_mm256_add_epi16(m1,m0);

			m1=_mm256_srli_si256(m0,4);
			m2=_mm256_add_epi16(m1,m2);


			//m2 has 16 16bit values now. the results I need are in positions 2,5,8,11. keep only those, discard others
			m2 = _mm256_and_si256(m2,output_mask_sh2);
			output_even = _mm256_add_epi16(output_even,m2);

    //6th col iteration

			//multiply with the mask
			m0=_mm256_maddubs_epi16(r0,c0_sh5);
			m1=_mm256_maddubs_epi16(r1,c1_sh5);
			m2=_mm256_maddubs_epi16(r2,c2_sh5);
			m3=_mm256_maddubs_epi16(r3,c1_sh5);
			m4=_mm256_maddubs_epi16(r4,c0_sh5);


	//vertical add
	m0=_mm256_add_epi16(m0,m1);
	m2=_mm256_add_epi16(m2,m3);
	m0=_mm256_add_epi16(m0,m2);
	m0=_mm256_add_epi16(m0,m4);

	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);

	m1=_mm256_srli_si256(m0,4);
	m2=_mm256_add_epi16(m1,m2);

	//m2 has 16 16bit values now. the results I need are in positions 2,5,8,11. keep only those, discard others
	m2 = _mm256_and_si256(m2,output_mask_sh2);
	output_odd = _mm256_add_epi16(output_odd,m2);

	//now division follows
	output_even=division(division_case,output_even,f);
	output_odd=division(division_case,output_odd,f);

	//shift odd 1 position and add to even
	output_odd = _mm256_slli_si256(output_odd,1);
	output_even = _mm256_add_epi8(output_even,output_odd);

 	switch (REMINDER_ITERATIONS){
	case 1:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
	  break;
	case 2:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		  break;
	case 3:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		  break;
	case 4:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output_even,3);
		  break;
	case 5:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output_even,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output_even,4);
		  break;
	case 6:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output_even,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output_even,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output_even,5);
		  break;
	case 7:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output_even,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output_even,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output_even,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output_even,6);
		  break;
	case 8:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output_even,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output_even,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output_even,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output_even,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output_even,7);
		  break;
	case 9:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output_even,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output_even,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output_even,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output_even,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output_even,7);
		filt[row][col+8] = (unsigned char) _mm256_extract_epi8(output_even,8);
		  break;
	case 10:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output_even,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output_even,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output_even,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output_even,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output_even,7);
		filt[row][col+8] = (unsigned char) _mm256_extract_epi8(output_even,8);
		filt[row][col+9] = (unsigned char) _mm256_extract_epi8(output_even,9);
		  break;
	case 11:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output_even,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output_even,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output_even,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output_even,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output_even,7);
		filt[row][col+8] = (unsigned char) _mm256_extract_epi8(output_even,8);
		filt[row][col+9] = (unsigned char) _mm256_extract_epi8(output_even,9);
		filt[row][col+10] = (unsigned char) _mm256_extract_epi8(output_even,10);
		  break;
	case 12:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output_even,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output_even,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output_even,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output_even,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output_even,7);
		filt[row][col+8] = (unsigned char) _mm256_extract_epi8(output_even,8);
		filt[row][col+9] = (unsigned char) _mm256_extract_epi8(output_even,9);
		filt[row][col+10] = (unsigned char) _mm256_extract_epi8(output_even,10);
		filt[row][col+11] = (unsigned char) _mm256_extract_epi8(output_even,11);
		  break;
	case 13:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output_even,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output_even,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output_even,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output_even,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output_even,7);
		filt[row][col+8] = (unsigned char) _mm256_extract_epi8(output_even,8);
		filt[row][col+9] = (unsigned char) _mm256_extract_epi8(output_even,9);
		filt[row][col+10] = (unsigned char) _mm256_extract_epi8(output_even,10);
		filt[row][col+11] = (unsigned char) _mm256_extract_epi8(output_even,11);
		filt[row][col+12] = (unsigned char) _mm256_extract_epi8(output_even,12);
		  break;
	case 14:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output_even,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output_even,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output_even,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output_even,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output_even,7);
		filt[row][col+8] = (unsigned char) _mm256_extract_epi8(output_even,8);
		filt[row][col+9] = (unsigned char) _mm256_extract_epi8(output_even,9);
		filt[row][col+10] = (unsigned char) _mm256_extract_epi8(output_even,10);
		filt[row][col+11] = (unsigned char) _mm256_extract_epi8(output_even,11);
		filt[row][col+12] = (unsigned char) _mm256_extract_epi8(output_even,12);
		filt[row][col+13] = (unsigned char) _mm256_extract_epi8(output_even,13);
		  break;
	case 15:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output_even,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output_even,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output_even,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output_even,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output_even,7);
		filt[row][col+8] = (unsigned char) _mm256_extract_epi8(output_even,8);
		filt[row][col+9] = (unsigned char) _mm256_extract_epi8(output_even,9);
		filt[row][col+10] = (unsigned char) _mm256_extract_epi8(output_even,10);
		filt[row][col+11] = (unsigned char) _mm256_extract_epi8(output_even,11);
		filt[row][col+12] = (unsigned char) _mm256_extract_epi8(output_even,12);
		filt[row][col+13] = (unsigned char) _mm256_extract_epi8(output_even,13);
		filt[row][col+14] = (unsigned char) _mm256_extract_epi8(output_even,14);
		  break;
	case 16:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
		  break;
	case 17:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
		  break;
	case 18:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
		  break;
	case 19:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
		  break;
	case 20:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output_even,19);
		  break;
	case 21:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output_even,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output_even,20);
		  break;
	case 22:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output_even,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output_even,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output_even,21);
		break;
	case 23:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output_even,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output_even,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output_even,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output_even,22);
		  break;
	case 24:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output_even,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output_even,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output_even,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output_even,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output_even,23);
		  break;
	case 25:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output_even,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output_even,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output_even,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output_even,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output_even,23);
	filt[row][col+24] = (unsigned char) _mm256_extract_epi8(output_even,24);
		  break;
	case 26:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output_even,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output_even,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output_even,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output_even,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output_even,23);
	filt[row][col+24] = (unsigned char) _mm256_extract_epi8(output_even,24);
	filt[row][col+25] = (unsigned char) _mm256_extract_epi8(output_even,25);
		  break;
	case 27:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output_even,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output_even,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output_even,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output_even,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output_even,23);
	filt[row][col+24] = (unsigned char) _mm256_extract_epi8(output_even,24);
	filt[row][col+25] = (unsigned char) _mm256_extract_epi8(output_even,25);
	filt[row][col+26] = (unsigned char) _mm256_extract_epi8(output_even,26);
		  break;
	case 28:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output_even,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output_even,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output_even,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output_even,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output_even,23);
	filt[row][col+24] = (unsigned char) _mm256_extract_epi8(output_even,24);
	filt[row][col+25] = (unsigned char) _mm256_extract_epi8(output_even,25);
	filt[row][col+26] = (unsigned char) _mm256_extract_epi8(output_even,26);
	filt[row][col+27] = (unsigned char) _mm256_extract_epi8(output_even,27);
		  break;
	default:
		printf("\nsomething went wrong");
		return -1;
 	}

return 0;

}



//for row=0,1,2 only
int loop_reminder_first_less_div(unsigned char **frame1,unsigned char **filt,const unsigned int M, const unsigned int N, const unsigned int col, const unsigned int REMINDER_ITERATIONS,const unsigned int division_case,const __m256i c0,const __m256i c1,const __m256i c2,const __m256i c0_sh1,const __m256i c1_sh1,const __m256i c2_sh1,const __m256i c0_sh2,const __m256i c1_sh2,const __m256i c2_sh2, const __m256i f, const unsigned short int divisor, signed char **filter ){

__m256i r0,r1,r2,r3,r4,m0,m1,m2,m3,m4;
__m256i output_even,output_odd;
__m256i output_row0_even,output_row1_even,output_row0_odd,output_row1_odd;

		//const __m256i f  = _mm256_set_epi16(52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759);
		//const __m256i mask  = _mm256_set_epi8(0,0,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255);
		//const __m256i mask2  = _mm256_set_epi8(0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0);
		const __m256i mask3  = _mm256_set_epi16(0,0,0,0,0,0,0,65535,0,0,0,0,0,0,0,0);
		//const __m256i mask4  = _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255,0,0,0,0,0,0,0,0,0,0,0,0);
		//const __m256i mask5  = _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0);

		const __m256i output_mask      = _mm256_set_epi16(0,0,0,65535,0,0,65535,0,0,65535,0,0,65535,0,0,65535);
		const __m256i output_mask_sh1  = _mm256_set_epi16(0,0,65535,0,0,65535,0,0,65535,0,0,65535,0,0,65535,0);
		//const __m256i output_mask_sh2  = _mm256_set_epi16(0,0,0,    0,65535,0,0,65535,0,0,65535,0,0,65535,0,0);
		const __m256i mask_new      = _mm256_set_epi16(0,0,0,0,0,0,0,0,0,65535,0,0,0,0,0,0);
		const __m256i mask_new2      = _mm256_set_epi16(0,0,0,0,0,0,0,0,65535,0,0,0,0,0,0,0);



 __m256i reminder_mask1,reminder_mask2;

	reminder_mask1=_mm256_load_si256( (__m256i *) &reminder_msk1[REMINDER_ITERATIONS-1][0]);
	reminder_mask2=_mm256_load_si256( (__m256i *) &reminder_msk2[REMINDER_ITERATIONS-1][0]);


	 //1st col iteration
	//load the 5 rows
	r0=_mm256_loadu_si256( (__m256i *) &frame1[0][col-2]);
	r1=_mm256_loadu_si256( (__m256i *) &frame1[1][col-2]);
	r2=_mm256_loadu_si256( (__m256i *) &frame1[2][col-2]);
	r3=_mm256_loadu_si256( (__m256i *) &frame1[3][col-2]);
	r4=_mm256_loadu_si256( (__m256i *) &frame1[4][col-2]);

	/*
	 * The above load operations load outside of the array bounds; these elements are filled with zeros just after
	 * Furthermore, I add two extra zeros in the end to compute col=N-2 and col=N-1. The last value of col is N-1, not N
	 */

	//AND r0-r4 with reminder_mask
	r0=_mm256_and_si256(r0,reminder_mask1);
	r1=_mm256_and_si256(r1,reminder_mask1);
	r2=_mm256_and_si256(r2,reminder_mask1);
	r3=_mm256_and_si256(r3,reminder_mask1);
	r4=_mm256_and_si256(r4,reminder_mask1);

	//row=2
	//multiply with the mask
	m0=_mm256_maddubs_epi16(r0,c0);
	m1=_mm256_maddubs_epi16(r1,c1);
	m2=_mm256_maddubs_epi16(r2,c2);
	m3=_mm256_maddubs_epi16(r3,c1);
	m4=_mm256_maddubs_epi16(r4,c0);

	//vertical add
	m0=_mm256_add_epi16(m0,m1);
	m2=_mm256_add_epi16(m2,m3);
	m0=_mm256_add_epi16(m0,m2);
	m0=_mm256_add_epi16(m0,m4);


	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);

	m1=_mm256_srli_si256(m0,4);
	m2=_mm256_add_epi16(m1,m2);

	//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
	//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
	m4=_mm256_and_si256(m0,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions

	m2=_mm256_add_epi16(m2,m4);

	//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
	output_even=_mm256_and_si256(m2,output_mask);

	//row=0
	//multiply with the mask
	m0=_mm256_maddubs_epi16(r0,c2);
	m1=_mm256_maddubs_epi16(r1,c1);
	m2=_mm256_maddubs_epi16(r2,c0);


	//vertical add
	m0=_mm256_add_epi16(m0,m1);
	m0=_mm256_add_epi16(m0,m2);


	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);

	m1=_mm256_srli_si256(m0,4);
	m2=_mm256_add_epi16(m1,m2);

	//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
	//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
	m4=_mm256_and_si256(m0,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions

	m2=_mm256_add_epi16(m2,m4);

	//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
	output_row0_even=_mm256_and_si256(m2,output_mask);


	//row=1
	//multiply with the mask
	m0=_mm256_maddubs_epi16(r0,c1);
	m1=_mm256_maddubs_epi16(r1,c2);
	m2=_mm256_maddubs_epi16(r2,c1);
	m3=_mm256_maddubs_epi16(r3,c0);


	//vertical add
	m0=_mm256_add_epi16(m0,m1);
	m0=_mm256_add_epi16(m0,m2);
	m0=_mm256_add_epi16(m0,m3);

	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);

	m1=_mm256_srli_si256(m0,4);
	m2=_mm256_add_epi16(m1,m2);

	//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
	//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
	m4=_mm256_and_si256(m0,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions

	m2=_mm256_add_epi16(m2,m4);

	//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
	output_row1_even=_mm256_and_si256(m2,output_mask);

	//2nd col iteration

	//row=2
	//multiply with the mask
	m0=_mm256_maddubs_epi16(r0,c0_sh1);
	m1=_mm256_maddubs_epi16(r1,c1_sh1);
	m2=_mm256_maddubs_epi16(r2,c2_sh1);
	m3=_mm256_maddubs_epi16(r3,c1_sh1);
	m4=_mm256_maddubs_epi16(r4,c0_sh1);

	//vertical add
	m0=_mm256_add_epi16(m0,m1);
	m2=_mm256_add_epi16(m2,m3);
	m0=_mm256_add_epi16(m0,m2);
	m0=_mm256_add_epi16(m0,m4);

	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);

	m1=_mm256_srli_si256(m0,4);
	m2=_mm256_add_epi16(m1,m2);

	//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
	//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
	m4=_mm256_and_si256(m0,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions
	m2=_mm256_add_epi16(m2,m4);

	//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
	output_odd = _mm256_and_si256(m2,output_mask);

	//row=0
	//multiply with the mask
	m0=_mm256_maddubs_epi16(r0,c2_sh1);
	m1=_mm256_maddubs_epi16(r1,c1_sh1);
	m2=_mm256_maddubs_epi16(r2,c0_sh1);


	//vertical add
	m0=_mm256_add_epi16(m0,m1);
	m0=_mm256_add_epi16(m0,m2);


	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);

	m1=_mm256_srli_si256(m0,4);
	m2=_mm256_add_epi16(m1,m2);

	//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
	//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
	m4=_mm256_and_si256(m0,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions
	m2=_mm256_add_epi16(m2,m4);

	//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
	output_row0_odd = _mm256_and_si256(m2,output_mask);

	//row=1
	//multiply with the mask
	m0=_mm256_maddubs_epi16(r0,c1_sh1);
	m1=_mm256_maddubs_epi16(r1,c2_sh1);
	m2=_mm256_maddubs_epi16(r2,c1_sh1);
	m3=_mm256_maddubs_epi16(r3,c0_sh1);

	//vertical add
	m0=_mm256_add_epi16(m0,m1);
	m0=_mm256_add_epi16(m0,m2);
	m0=_mm256_add_epi16(m0,m3);

	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);

	m1=_mm256_srli_si256(m0,4);
	m2=_mm256_add_epi16(m1,m2);

	//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
	//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
	m4=_mm256_and_si256(m0,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions
	m2=_mm256_add_epi16(m2,m4);

	//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
	output_row1_odd = _mm256_and_si256(m2,output_mask);

             //3rd col iteration

	//row=2
	//multiply with the mask
	m0=_mm256_maddubs_epi16(r0,c0_sh2);
	m1=_mm256_maddubs_epi16(r1,c1_sh2);
	m2=_mm256_maddubs_epi16(r2,c2_sh2);
	m3=_mm256_maddubs_epi16(r3,c1_sh2);
	m4=_mm256_maddubs_epi16(r4,c0_sh2);

	//vertical add
	m0=_mm256_add_epi16(m0,m1);
	m2=_mm256_add_epi16(m2,m3);
	m0=_mm256_add_epi16(m0,m2);
	m0=_mm256_add_epi16(m0,m4);


	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);

	//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,_mm256_extract_epi16(m2,8),0,0,0,0,0,0,0);
	m4=_mm256_and_si256(m2,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions

	m1=_mm256_srli_si256(m0,4);
	m2=_mm256_add_epi16(m1,m2);

	//prepei na prosthesw to pixel pu thelw sto r2 kai meta na kanw diairesi me ola mazi

	m2=_mm256_add_epi16(m2,m4);

	//m2 has 16 16bit values now. the results I need are in positions 1,4,7,10,13. keep only those, discard others
	m2 = _mm256_and_si256(m2,output_mask_sh1);
	output_even = _mm256_add_epi16(output_even,m2);

	//row=0
	//multiply with the mask
	m0=_mm256_maddubs_epi16(r0,c2_sh2);
	m1=_mm256_maddubs_epi16(r1,c1_sh2);
	m2=_mm256_maddubs_epi16(r2,c0_sh2);

	//vertical add
	m0=_mm256_add_epi16(m0,m1);
	m0=_mm256_add_epi16(m0,m2);


	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);

	//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,_mm256_extract_epi16(m2,8),0,0,0,0,0,0,0);
	m4=_mm256_and_si256(m2,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions

	m1=_mm256_srli_si256(m0,4);
	m2=_mm256_add_epi16(m1,m2);

	//prepei na prosthesw to pixel pu thelw sto r2 kai meta na kanw diairesi me ola mazi

	m2=_mm256_add_epi16(m2,m4);

	//m2 has 16 16bit values now. the results I need are in positions 1,4,7,10,13. keep only those, discard others
	m2 = _mm256_and_si256(m2,output_mask_sh1);
	output_row0_even = _mm256_add_epi16(output_row0_even,m2);

	//row=1
	//multiply with the mask
	m0=_mm256_maddubs_epi16(r0,c1_sh2);
	m1=_mm256_maddubs_epi16(r1,c2_sh2);
	m2=_mm256_maddubs_epi16(r2,c1_sh2);
	m3=_mm256_maddubs_epi16(r3,c0_sh2);

	//vertical add
	m0=_mm256_add_epi16(m0,m1);
	m0=_mm256_add_epi16(m0,m2);
	m0=_mm256_add_epi16(m0,m3);


	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);

	//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,_mm256_extract_epi16(m2,8),0,0,0,0,0,0,0);
	m4=_mm256_and_si256(m2,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions

	m1=_mm256_srli_si256(m0,4);
	m2=_mm256_add_epi16(m1,m2);

	//prepei na prosthesw to pixel pu thelw sto r2 kai meta na kanw diairesi me ola mazi

	m2=_mm256_add_epi16(m2,m4);

	//m2 has 16 16bit values now. the results I need are in positions 1,4,7,10,13. keep only those, discard others
	m2 = _mm256_and_si256(m2,output_mask_sh1);
	output_row1_even = _mm256_add_epi16(output_row1_even,m2);

	//4th col iteration
	//load the 5 rows
	r0=_mm256_loadu_si256( (__m256i *) &frame1[0][col+3-2]);
	r1=_mm256_loadu_si256( (__m256i *) &frame1[1][col+3-2]);
	r2=_mm256_loadu_si256( (__m256i *) &frame1[2][col+3-2]);
	r3=_mm256_loadu_si256( (__m256i *) &frame1[3][col+3-2]);
	r4=_mm256_loadu_si256( (__m256i *) &frame1[4][col+3-2]);

	//AND r0-r4 with reminder_mask
	r0=_mm256_and_si256(r0,reminder_mask2);
	r1=_mm256_and_si256(r1,reminder_mask2);
	r2=_mm256_and_si256(r2,reminder_mask2);
	r3=_mm256_and_si256(r3,reminder_mask2);
	r4=_mm256_and_si256(r4,reminder_mask2);

	//row=2
	//multiply with the mask
	m0=_mm256_maddubs_epi16(r0,c0);
	m1=_mm256_maddubs_epi16(r1,c1);
	m2=_mm256_maddubs_epi16(r2,c2);
	m3=_mm256_maddubs_epi16(r3,c1);
	m4=_mm256_maddubs_epi16(r4,c0);

	//vertical add
	m0=_mm256_add_epi16(m0,m1);
	m2=_mm256_add_epi16(m2,m3);
	m0=_mm256_add_epi16(m0,m2);
	m0=_mm256_add_epi16(m0,m4);


	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);

	m1=_mm256_srli_si256(m0,4);
	m2=_mm256_add_epi16(m1,m2);

	//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
	//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
	m4=_mm256_and_si256(m0,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions

	m2=_mm256_add_epi16(m2,m4);

	//m2 has 16 16bit values now. the results I need are in positions 1,4,7,10,13. keep only those, discard others
	m2 = _mm256_and_si256(m2,output_mask);
	m2 = _mm256_slli_si256(m2,2);
	output_odd = _mm256_add_epi16(output_odd,m2);

	//row=0
	//multiply with the mask
	m0=_mm256_maddubs_epi16(r2,c0);
	m1=_mm256_maddubs_epi16(r1,c1);
	m2=_mm256_maddubs_epi16(r0,c2);


	//vertical add
	m0=_mm256_add_epi16(m0,m1);
	m0=_mm256_add_epi16(m2,m0);


	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);

	m1=_mm256_srli_si256(m0,4);
	m2=_mm256_add_epi16(m1,m2);

	//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
	//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
	m4=_mm256_and_si256(m0,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions

	m2=_mm256_add_epi16(m2,m4);

	//m2 has 16 16bit values now. the results I need are in positions 1,4,7,10,13. keep only those, discard others
	m2 = _mm256_and_si256(m2,output_mask);
	m2 = _mm256_slli_si256(m2,2);
	output_row0_odd = _mm256_add_epi16(output_row0_odd,m2);

	//row=1
	//multiply with the mask
	m0=_mm256_maddubs_epi16(r0,c1);
	m1=_mm256_maddubs_epi16(r1,c2);
	m2=_mm256_maddubs_epi16(r2,c1);
	m3=_mm256_maddubs_epi16(r3,c0);

	//vertical add
	m0=_mm256_add_epi16(m0,m1);
	m0=_mm256_add_epi16(m2,m0);
	m0=_mm256_add_epi16(m0,m3);


	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);

	m1=_mm256_srli_si256(m0,4);
	m2=_mm256_add_epi16(m1,m2);

	//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
	//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
	m4=_mm256_and_si256(m0,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions

	m2=_mm256_add_epi16(m2,m4);

	//m2 has 16 16bit values now. the results I need are in positions 1,4,7,10,13. keep only those, discard others
	m2 = _mm256_and_si256(m2,output_mask);
	m2 = _mm256_slli_si256(m2,2);
	output_row1_odd = _mm256_add_epi16(output_row1_odd,m2);

	//5th col iteration

	//row=2
	//multiply with the mask
	m0=_mm256_maddubs_epi16(r0,c0_sh1);
	m1=_mm256_maddubs_epi16(r1,c1_sh1);
	m2=_mm256_maddubs_epi16(r2,c2_sh1);
	m3=_mm256_maddubs_epi16(r3,c1_sh1);
	m4=_mm256_maddubs_epi16(r4,c0_sh1);

	//vertical add
	m0=_mm256_add_epi16(m0,m1);
	m2=_mm256_add_epi16(m2,m3);
	m0=_mm256_add_epi16(m0,m2);
	m0=_mm256_add_epi16(m0,m4);


	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);

	m1=_mm256_srli_si256(m0,4);
	m2=_mm256_add_epi16(m1,m2);

	//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
	//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
	m4=_mm256_and_si256(m0,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions

	m2=_mm256_add_epi16(m2,m4);

	//m2 has 16 16bit values now.
	m2 = _mm256_and_si256(m2,output_mask); //holds results in positions 0,3,6,9,12
	m1 = _mm256_slli_si256(m2,4); //put results in positions 2,5,8,11,14 , but 8 is not written
	m2=_mm256_and_si256(m2,mask_new);//extract only the 6th
	m2=_mm256_permute2f128_si256(m2,m2,1);
	m2=_mm256_srli_si256(m2,12);
	m2 = _mm256_add_epi16(m1,m2);
	output_even = _mm256_add_epi16(output_even,m2);

	//row=0
	//multiply with the mask
	m0=_mm256_maddubs_epi16(r0,c2_sh1);
	m1=_mm256_maddubs_epi16(r1,c1_sh1);
	m2=_mm256_maddubs_epi16(r2,c0_sh1);


	//vertical add
	m0=_mm256_add_epi16(m0,m1);
	m0=_mm256_add_epi16(m2,m0);



	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);

	m1=_mm256_srli_si256(m0,4);
	m2=_mm256_add_epi16(m1,m2);

	//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
	//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
	m4=_mm256_and_si256(m0,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions

	m2=_mm256_add_epi16(m2,m4);

	//m2 has 16 16bit values now.
	m2 = _mm256_and_si256(m2,output_mask); //holds results in positions 0,3,6,9,12
	m1 = _mm256_slli_si256(m2,4); //put results in positions 2,5,8,11,14 , but 8 is not written
	m2=_mm256_and_si256(m2,mask_new);//extract only the 6th
	m2=_mm256_permute2f128_si256(m2,m2,1);
	m2=_mm256_srli_si256(m2,12);
	m2 = _mm256_add_epi16(m1,m2);
	output_row0_even = _mm256_add_epi16(output_row0_even,m2);

	//row=1
	//multiply with the mask
	m0=_mm256_maddubs_epi16(r0,c1_sh1);
	m1=_mm256_maddubs_epi16(r1,c2_sh1);
	m2=_mm256_maddubs_epi16(r2,c1_sh1);
	m3=_mm256_maddubs_epi16(r3,c0_sh1);

	//vertical add
	m0=_mm256_add_epi16(m0,m1);
	m0=_mm256_add_epi16(m2,m0);
	m0=_mm256_add_epi16(m0,m3);


	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);

	m1=_mm256_srli_si256(m0,4);
	m2=_mm256_add_epi16(m1,m2);

	//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
	//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
	m4=_mm256_and_si256(m0,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions

	m2=_mm256_add_epi16(m2,m4);

	//m2 has 16 16bit values now.
	m2 = _mm256_and_si256(m2,output_mask); //holds results in positions 0,3,6,9,12
	m1 = _mm256_slli_si256(m2,4); //put results in positions 2,5,8,11,14 , but 8 is not written
	m2=_mm256_and_si256(m2,mask_new);//extract only the 6th
	m2=_mm256_permute2f128_si256(m2,m2,1);
	m2=_mm256_srli_si256(m2,12);
	m2 = _mm256_add_epi16(m1,m2);
	output_row1_even = _mm256_add_epi16(output_row1_even,m2);

             //6th col iteration
	//row=2
	//multiply with the mask
	m0=_mm256_maddubs_epi16(r0,c0_sh2);
	m1=_mm256_maddubs_epi16(r1,c1_sh2);
	m2=_mm256_maddubs_epi16(r2,c2_sh2);
	m3=_mm256_maddubs_epi16(r3,c1_sh2);
	m4=_mm256_maddubs_epi16(r4,c0_sh2);

	//vertical add
	m0=_mm256_add_epi16(m0,m1);
	m2=_mm256_add_epi16(m2,m3);
	m0=_mm256_add_epi16(m0,m2);
	m0=_mm256_add_epi16(m0,m4);


	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);
	//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,_mm256_extract_epi16(m2,8),0,0,0,0,0,0,0);
	m4=_mm256_and_si256(m2,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions

	m1=_mm256_srli_si256(m0,4);
	m2=_mm256_add_epi16(m1,m2);
	m2=_mm256_add_epi16(m2,m4);

	//m2 has 16 16bit values now.
	m2 = _mm256_and_si256(m2,output_mask_sh1);//holds results in positions 1,4,7,10,13
	m1 = _mm256_slli_si256(m2,2); //put results in positions 2,5,8,11,14 , but 8 is not written
	m2=_mm256_and_si256(m2,mask_new2);//extract only the 7th
	m2=_mm256_permute2f128_si256(m2,m2,1);
	m2=_mm256_srli_si256(m2,14);
	m2 = _mm256_add_epi16(m1,m2);
	output_odd = _mm256_add_epi16(output_odd,m2);


	//now division follows
	output_even=division(division_case,output_even,f);
	output_odd=division(division_case,output_odd,f);

	//shift odd 1 position and add to even
	output_odd = _mm256_slli_si256(output_odd,1);
	output_even = _mm256_add_epi8(output_even,output_odd);

	//row=0
	//multiply with the mask
	m0=_mm256_maddubs_epi16(r0,c2_sh2);
	m1=_mm256_maddubs_epi16(r1,c1_sh2);
	m2=_mm256_maddubs_epi16(r2,c0_sh2);

	//vertical add
	m0=_mm256_add_epi16(m0,m1);
	m0=_mm256_add_epi16(m2,m0);



	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);
	//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,_mm256_extract_epi16(m2,8),0,0,0,0,0,0,0);
	m4=_mm256_and_si256(m2,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions

	m1=_mm256_srli_si256(m0,4);
	m2=_mm256_add_epi16(m1,m2);
	m2=_mm256_add_epi16(m2,m4);

	//m2 has 16 16bit values now.
	m2 = _mm256_and_si256(m2,output_mask_sh1);//holds results in positions 1,4,7,10,13
	m1 = _mm256_slli_si256(m2,2); //put results in positions 2,5,8,11,14 , but 8 is not written
	m2=_mm256_and_si256(m2,mask_new2);//extract only the 7th
	m2=_mm256_permute2f128_si256(m2,m2,1);
	m2=_mm256_srli_si256(m2,14);
	m2 = _mm256_add_epi16(m1,m2);
	output_row0_odd = _mm256_add_epi16(output_row0_odd,m2);


	//now division follows
	output_row0_even=division(division_case,output_row0_even,f);
	output_row0_odd=division(division_case,output_row0_odd,f);

	//shift odd 1 position and add to even
	output_row0_odd = _mm256_slli_si256(output_row0_odd,1);
	output_row0_even = _mm256_add_epi8(output_row0_even,output_row0_odd);

	//row=1
	//multiply with the mask
	m0=_mm256_maddubs_epi16(r0,c1_sh2);
	m1=_mm256_maddubs_epi16(r1,c2_sh2);
	m2=_mm256_maddubs_epi16(r2,c1_sh2);
	m3=_mm256_maddubs_epi16(r3,c0_sh2);

	//vertical add
	m0=_mm256_add_epi16(m0,m1);
	m0=_mm256_add_epi16(m2,m0);
	m0=_mm256_add_epi16(m0,m3);


	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);
	//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,_mm256_extract_epi16(m2,8),0,0,0,0,0,0,0);
	m4=_mm256_and_si256(m2,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions

	m1=_mm256_srli_si256(m0,4);
	m2=_mm256_add_epi16(m1,m2);
	m2=_mm256_add_epi16(m2,m4);

	//m2 has 16 16bit values now.
	m2 = _mm256_and_si256(m2,output_mask_sh1);//holds results in positions 1,4,7,10,13
	m1 = _mm256_slli_si256(m2,2); //put results in positions 2,5,8,11,14 , but 8 is not written
	m2=_mm256_and_si256(m2,mask_new2);//extract only the 7th
	m2=_mm256_permute2f128_si256(m2,m2,1);
	m2=_mm256_srli_si256(m2,14);
	m2 = _mm256_add_epi16(m1,m2);
	output_row1_odd = _mm256_add_epi16(output_row1_odd,m2);


	//now division follows
	output_row1_even=division(division_case,output_row1_even,f);
	output_row1_odd=division(division_case,output_row1_odd,f);

	//shift odd 1 position and add to even
	output_row1_odd = _mm256_slli_si256(output_row1_odd,1);
	output_row1_even = _mm256_add_epi8(output_row1_even,output_row1_odd);




	switch (REMINDER_ITERATIONS){
	case 1:
		filt[2][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[0][col] = (unsigned char) _mm256_extract_epi8(output_row0_even,0);
		filt[1][col] = (unsigned char) _mm256_extract_epi8(output_row1_even,0);
		 break;
	case 2:
		filt[2][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[2][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);

		filt[0][col] = (unsigned char) _mm256_extract_epi8(output_row0_even,0);
		filt[0][col+1] = (unsigned char) _mm256_extract_epi8(output_row0_even,1);

		filt[1][col] = (unsigned char) _mm256_extract_epi8(output_row1_even,0);
		filt[1][col+1] = (unsigned char) _mm256_extract_epi8(output_row1_even,1);
		 break;
	case 3:
		filt[2][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[2][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[2][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);

		filt[0][col] = (unsigned char) _mm256_extract_epi8(output_row0_even,0);
		filt[0][col+1] = (unsigned char) _mm256_extract_epi8(output_row0_even,1);
		filt[0][col+2] = (unsigned char) _mm256_extract_epi8(output_row0_even,2);

		filt[1][col] = (unsigned char) _mm256_extract_epi8(output_row1_even,0);
		filt[1][col+1] = (unsigned char) _mm256_extract_epi8(output_row1_even,1);
		filt[1][col+2] = (unsigned char) _mm256_extract_epi8(output_row1_even,2);
		  break;
	case 4:
		filt[2][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[2][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[2][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		filt[2][col+3] = (unsigned char) _mm256_extract_epi8(output_even,3);

		filt[0][col] = (unsigned char) _mm256_extract_epi8(output_row0_even,0);
		filt[0][col+1] = (unsigned char) _mm256_extract_epi8(output_row0_even,1);
		filt[0][col+2] = (unsigned char) _mm256_extract_epi8(output_row0_even,2);
		filt[0][col+3] = (unsigned char) _mm256_extract_epi8(output_row0_even,3);

		filt[1][col] = (unsigned char) _mm256_extract_epi8(output_row1_even,0);
		filt[1][col+1] = (unsigned char) _mm256_extract_epi8(output_row1_even,1);
		filt[1][col+2] = (unsigned char) _mm256_extract_epi8(output_row1_even,2);
		filt[1][col+3] = (unsigned char) _mm256_extract_epi8(output_row1_even,3);
		  break;
	case 5:
		filt[2][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[2][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[2][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		filt[2][col+3] = (unsigned char) _mm256_extract_epi8(output_even,3);
		filt[2][col+4] = (unsigned char) _mm256_extract_epi8(output_even,4);

		filt[0][col] = (unsigned char) _mm256_extract_epi8(output_row0_even,0);
		filt[0][col+1] = (unsigned char) _mm256_extract_epi8(output_row0_even,1);
		filt[0][col+2] = (unsigned char) _mm256_extract_epi8(output_row0_even,2);
		filt[0][col+3] = (unsigned char) _mm256_extract_epi8(output_row0_even,3);
		filt[0][col+4] = (unsigned char) _mm256_extract_epi8(output_row0_even,4);

		filt[1][col] = (unsigned char) _mm256_extract_epi8(output_row1_even,0);
		filt[1][col+1] = (unsigned char) _mm256_extract_epi8(output_row1_even,1);
		filt[1][col+2] = (unsigned char) _mm256_extract_epi8(output_row1_even,2);
		filt[1][col+3] = (unsigned char) _mm256_extract_epi8(output_row1_even,3);
		filt[1][col+4] = (unsigned char) _mm256_extract_epi8(output_row1_even,4);
		  break;
	case 6:
		filt[2][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[2][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[2][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		filt[2][col+3] = (unsigned char) _mm256_extract_epi8(output_even,3);
		filt[2][col+4] = (unsigned char) _mm256_extract_epi8(output_even,4);
		filt[2][col+5] = (unsigned char) _mm256_extract_epi8(output_even,5);

		filt[0][col] = (unsigned char) _mm256_extract_epi8(output_row0_even,0);
		filt[0][col+1] = (unsigned char) _mm256_extract_epi8(output_row0_even,1);
		filt[0][col+2] = (unsigned char) _mm256_extract_epi8(output_row0_even,2);
		filt[0][col+3] = (unsigned char) _mm256_extract_epi8(output_row0_even,3);
		filt[0][col+4] = (unsigned char) _mm256_extract_epi8(output_row0_even,4);
		filt[0][col+5] = (unsigned char) _mm256_extract_epi8(output_row0_even,5);

		filt[1][col] = (unsigned char) _mm256_extract_epi8(output_row1_even,0);
		filt[1][col+1] = (unsigned char) _mm256_extract_epi8(output_row1_even,1);
		filt[1][col+2] = (unsigned char) _mm256_extract_epi8(output_row1_even,2);
		filt[1][col+3] = (unsigned char) _mm256_extract_epi8(output_row1_even,3);
		filt[1][col+4] = (unsigned char) _mm256_extract_epi8(output_row1_even,4);
		filt[1][col+5] = (unsigned char) _mm256_extract_epi8(output_row1_even,5);
		  break;
	case 7:
		filt[2][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[2][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[2][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		filt[2][col+3] = (unsigned char) _mm256_extract_epi8(output_even,3);
		filt[2][col+4] = (unsigned char) _mm256_extract_epi8(output_even,4);
		filt[2][col+5] = (unsigned char) _mm256_extract_epi8(output_even,5);
		filt[2][col+6] = (unsigned char) _mm256_extract_epi8(output_even,6);

		filt[0][col] = (unsigned char) _mm256_extract_epi8(output_row0_even,0);
		filt[0][col+1] = (unsigned char) _mm256_extract_epi8(output_row0_even,1);
		filt[0][col+2] = (unsigned char) _mm256_extract_epi8(output_row0_even,2);
		filt[0][col+3] = (unsigned char) _mm256_extract_epi8(output_row0_even,3);
		filt[0][col+4] = (unsigned char) _mm256_extract_epi8(output_row0_even,4);
		filt[0][col+5] = (unsigned char) _mm256_extract_epi8(output_row0_even,5);
		filt[0][col+6] = (unsigned char) _mm256_extract_epi8(output_row0_even,6);

		filt[1][col] = (unsigned char) _mm256_extract_epi8(output_row1_even,0);
		filt[1][col+1] = (unsigned char) _mm256_extract_epi8(output_row1_even,1);
		filt[1][col+2] = (unsigned char) _mm256_extract_epi8(output_row1_even,2);
		filt[1][col+3] = (unsigned char) _mm256_extract_epi8(output_row1_even,3);
		filt[1][col+4] = (unsigned char) _mm256_extract_epi8(output_row1_even,4);
		filt[1][col+5] = (unsigned char) _mm256_extract_epi8(output_row1_even,5);
		filt[1][col+6] = (unsigned char) _mm256_extract_epi8(output_row1_even,6);
		  break;
	case 8:
		filt[2][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[2][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[2][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		filt[2][col+3] = (unsigned char) _mm256_extract_epi8(output_even,3);
		filt[2][col+4] = (unsigned char) _mm256_extract_epi8(output_even,4);
		filt[2][col+5] = (unsigned char) _mm256_extract_epi8(output_even,5);
		filt[2][col+6] = (unsigned char) _mm256_extract_epi8(output_even,6);
		filt[2][col+7] = (unsigned char) _mm256_extract_epi8(output_even,7);

		filt[0][col] = (unsigned char) _mm256_extract_epi8(output_row0_even,0);
		filt[0][col+1] = (unsigned char) _mm256_extract_epi8(output_row0_even,1);
		filt[0][col+2] = (unsigned char) _mm256_extract_epi8(output_row0_even,2);
		filt[0][col+3] = (unsigned char) _mm256_extract_epi8(output_row0_even,3);
		filt[0][col+4] = (unsigned char) _mm256_extract_epi8(output_row0_even,4);
		filt[0][col+5] = (unsigned char) _mm256_extract_epi8(output_row0_even,5);
		filt[0][col+6] = (unsigned char) _mm256_extract_epi8(output_row0_even,6);
		filt[0][col+7] = (unsigned char) _mm256_extract_epi8(output_row0_even,7);

		filt[1][col] = (unsigned char) _mm256_extract_epi8(output_row1_even,0);
		filt[1][col+1] = (unsigned char) _mm256_extract_epi8(output_row1_even,1);
		filt[1][col+2] = (unsigned char) _mm256_extract_epi8(output_row1_even,2);
		filt[1][col+3] = (unsigned char) _mm256_extract_epi8(output_row1_even,3);
		filt[1][col+4] = (unsigned char) _mm256_extract_epi8(output_row1_even,4);
		filt[1][col+5] = (unsigned char) _mm256_extract_epi8(output_row1_even,5);
		filt[1][col+6] = (unsigned char) _mm256_extract_epi8(output_row1_even,6);
		filt[1][col+7] = (unsigned char) _mm256_extract_epi8(output_row1_even,7);
		  break;
	case 9:
		filt[2][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[2][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[2][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		filt[2][col+3] = (unsigned char) _mm256_extract_epi8(output_even,3);
		filt[2][col+4] = (unsigned char) _mm256_extract_epi8(output_even,4);
		filt[2][col+5] = (unsigned char) _mm256_extract_epi8(output_even,5);
		filt[2][col+6] = (unsigned char) _mm256_extract_epi8(output_even,6);
		filt[2][col+7] = (unsigned char) _mm256_extract_epi8(output_even,7);
		filt[2][col+8] = (unsigned char) _mm256_extract_epi8(output_even,8);

		filt[0][col] = (unsigned char) _mm256_extract_epi8(output_row0_even,0);
		filt[0][col+1] = (unsigned char) _mm256_extract_epi8(output_row0_even,1);
		filt[0][col+2] = (unsigned char) _mm256_extract_epi8(output_row0_even,2);
		filt[0][col+3] = (unsigned char) _mm256_extract_epi8(output_row0_even,3);
		filt[0][col+4] = (unsigned char) _mm256_extract_epi8(output_row0_even,4);
		filt[0][col+5] = (unsigned char) _mm256_extract_epi8(output_row0_even,5);
		filt[0][col+6] = (unsigned char) _mm256_extract_epi8(output_row0_even,6);
		filt[0][col+7] = (unsigned char) _mm256_extract_epi8(output_row0_even,7);
		filt[0][col+8] = (unsigned char) _mm256_extract_epi8(output_row0_even,8);

		filt[1][col] = (unsigned char) _mm256_extract_epi8(output_row1_even,0);
		filt[1][col+1] = (unsigned char) _mm256_extract_epi8(output_row1_even,1);
		filt[1][col+2] = (unsigned char) _mm256_extract_epi8(output_row1_even,2);
		filt[1][col+3] = (unsigned char) _mm256_extract_epi8(output_row1_even,3);
		filt[1][col+4] = (unsigned char) _mm256_extract_epi8(output_row1_even,4);
		filt[1][col+5] = (unsigned char) _mm256_extract_epi8(output_row1_even,5);
		filt[1][col+6] = (unsigned char) _mm256_extract_epi8(output_row1_even,6);
		filt[1][col+7] = (unsigned char) _mm256_extract_epi8(output_row1_even,7);
		filt[1][col+8] = (unsigned char) _mm256_extract_epi8(output_row1_even,8);
		  break;
	case 10:
		filt[2][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[2][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[2][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		filt[2][col+3] = (unsigned char) _mm256_extract_epi8(output_even,3);
		filt[2][col+4] = (unsigned char) _mm256_extract_epi8(output_even,4);
		filt[2][col+5] = (unsigned char) _mm256_extract_epi8(output_even,5);
		filt[2][col+6] = (unsigned char) _mm256_extract_epi8(output_even,6);
		filt[2][col+7] = (unsigned char) _mm256_extract_epi8(output_even,7);
		filt[2][col+8] = (unsigned char) _mm256_extract_epi8(output_even,8);
		filt[2][col+9] = (unsigned char) _mm256_extract_epi8(output_even,9);

		filt[0][col] = (unsigned char) _mm256_extract_epi8(output_row0_even,0);
		filt[0][col+1] = (unsigned char) _mm256_extract_epi8(output_row0_even,1);
		filt[0][col+2] = (unsigned char) _mm256_extract_epi8(output_row0_even,2);
		filt[0][col+3] = (unsigned char) _mm256_extract_epi8(output_row0_even,3);
		filt[0][col+4] = (unsigned char) _mm256_extract_epi8(output_row0_even,4);
		filt[0][col+5] = (unsigned char) _mm256_extract_epi8(output_row0_even,5);
		filt[0][col+6] = (unsigned char) _mm256_extract_epi8(output_row0_even,6);
		filt[0][col+7] = (unsigned char) _mm256_extract_epi8(output_row0_even,7);
		filt[0][col+8] = (unsigned char) _mm256_extract_epi8(output_row0_even,8);
		filt[0][col+9] = (unsigned char) _mm256_extract_epi8(output_row0_even,9);

		filt[1][col] = (unsigned char) _mm256_extract_epi8(output_row1_even,0);
		filt[1][col+1] = (unsigned char) _mm256_extract_epi8(output_row1_even,1);
		filt[1][col+2] = (unsigned char) _mm256_extract_epi8(output_row1_even,2);
		filt[1][col+3] = (unsigned char) _mm256_extract_epi8(output_row1_even,3);
		filt[1][col+4] = (unsigned char) _mm256_extract_epi8(output_row1_even,4);
		filt[1][col+5] = (unsigned char) _mm256_extract_epi8(output_row1_even,5);
		filt[1][col+6] = (unsigned char) _mm256_extract_epi8(output_row1_even,6);
		filt[1][col+7] = (unsigned char) _mm256_extract_epi8(output_row1_even,7);
		filt[1][col+8] = (unsigned char) _mm256_extract_epi8(output_row1_even,8);
		filt[1][col+9] = (unsigned char) _mm256_extract_epi8(output_row1_even,9);
		  break;
	case 11:
		filt[2][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[2][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[2][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		filt[2][col+3] = (unsigned char) _mm256_extract_epi8(output_even,3);
		filt[2][col+4] = (unsigned char) _mm256_extract_epi8(output_even,4);
		filt[2][col+5] = (unsigned char) _mm256_extract_epi8(output_even,5);
		filt[2][col+6] = (unsigned char) _mm256_extract_epi8(output_even,6);
		filt[2][col+7] = (unsigned char) _mm256_extract_epi8(output_even,7);
		filt[2][col+8] = (unsigned char) _mm256_extract_epi8(output_even,8);
		filt[2][col+9] = (unsigned char) _mm256_extract_epi8(output_even,9);
		filt[2][col+10] = (unsigned char) _mm256_extract_epi8(output_even,10);

		filt[0][col] = (unsigned char) _mm256_extract_epi8(output_row0_even,0);
		filt[0][col+1] = (unsigned char) _mm256_extract_epi8(output_row0_even,1);
		filt[0][col+2] = (unsigned char) _mm256_extract_epi8(output_row0_even,2);
		filt[0][col+3] = (unsigned char) _mm256_extract_epi8(output_row0_even,3);
		filt[0][col+4] = (unsigned char) _mm256_extract_epi8(output_row0_even,4);
		filt[0][col+5] = (unsigned char) _mm256_extract_epi8(output_row0_even,5);
		filt[0][col+6] = (unsigned char) _mm256_extract_epi8(output_row0_even,6);
		filt[0][col+7] = (unsigned char) _mm256_extract_epi8(output_row0_even,7);
		filt[0][col+8] = (unsigned char) _mm256_extract_epi8(output_row0_even,8);
		filt[0][col+9] = (unsigned char) _mm256_extract_epi8(output_row0_even,9);
		filt[0][col+10] = (unsigned char) _mm256_extract_epi8(output_row0_even,10);

		filt[1][col] = (unsigned char) _mm256_extract_epi8(output_row1_even,0);
		filt[1][col+1] = (unsigned char) _mm256_extract_epi8(output_row1_even,1);
		filt[1][col+2] = (unsigned char) _mm256_extract_epi8(output_row1_even,2);
		filt[1][col+3] = (unsigned char) _mm256_extract_epi8(output_row1_even,3);
		filt[1][col+4] = (unsigned char) _mm256_extract_epi8(output_row1_even,4);
		filt[1][col+5] = (unsigned char) _mm256_extract_epi8(output_row1_even,5);
		filt[1][col+6] = (unsigned char) _mm256_extract_epi8(output_row1_even,6);
		filt[1][col+7] = (unsigned char) _mm256_extract_epi8(output_row1_even,7);
		filt[1][col+8] = (unsigned char) _mm256_extract_epi8(output_row1_even,8);
		filt[1][col+9] = (unsigned char) _mm256_extract_epi8(output_row1_even,9);
		filt[1][col+10] = (unsigned char) _mm256_extract_epi8(output_row1_even,10);
		  break;
	case 12:
		filt[2][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[2][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[2][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		filt[2][col+3] = (unsigned char) _mm256_extract_epi8(output_even,3);
		filt[2][col+4] = (unsigned char) _mm256_extract_epi8(output_even,4);
		filt[2][col+5] = (unsigned char) _mm256_extract_epi8(output_even,5);
		filt[2][col+6] = (unsigned char) _mm256_extract_epi8(output_even,6);
		filt[2][col+7] = (unsigned char) _mm256_extract_epi8(output_even,7);
		filt[2][col+8] = (unsigned char) _mm256_extract_epi8(output_even,8);
		filt[2][col+9] = (unsigned char) _mm256_extract_epi8(output_even,9);
		filt[2][col+10] = (unsigned char) _mm256_extract_epi8(output_even,10);
		filt[2][col+11] = (unsigned char) _mm256_extract_epi8(output_even,11);

		filt[0][col] = (unsigned char) _mm256_extract_epi8(output_row0_even,0);
		filt[0][col+1] = (unsigned char) _mm256_extract_epi8(output_row0_even,1);
		filt[0][col+2] = (unsigned char) _mm256_extract_epi8(output_row0_even,2);
		filt[0][col+3] = (unsigned char) _mm256_extract_epi8(output_row0_even,3);
		filt[0][col+4] = (unsigned char) _mm256_extract_epi8(output_row0_even,4);
		filt[0][col+5] = (unsigned char) _mm256_extract_epi8(output_row0_even,5);
		filt[0][col+6] = (unsigned char) _mm256_extract_epi8(output_row0_even,6);
		filt[0][col+7] = (unsigned char) _mm256_extract_epi8(output_row0_even,7);
		filt[0][col+8] = (unsigned char) _mm256_extract_epi8(output_row0_even,8);
		filt[0][col+9] = (unsigned char) _mm256_extract_epi8(output_row0_even,9);
		filt[0][col+10] = (unsigned char) _mm256_extract_epi8(output_row0_even,10);
		filt[0][col+11] = (unsigned char) _mm256_extract_epi8(output_row0_even,11);

		filt[1][col] = (unsigned char) _mm256_extract_epi8(output_row1_even,0);
		filt[1][col+1] = (unsigned char) _mm256_extract_epi8(output_row1_even,1);
		filt[1][col+2] = (unsigned char) _mm256_extract_epi8(output_row1_even,2);
		filt[1][col+3] = (unsigned char) _mm256_extract_epi8(output_row1_even,3);
		filt[1][col+4] = (unsigned char) _mm256_extract_epi8(output_row1_even,4);
		filt[1][col+5] = (unsigned char) _mm256_extract_epi8(output_row1_even,5);
		filt[1][col+6] = (unsigned char) _mm256_extract_epi8(output_row1_even,6);
		filt[1][col+7] = (unsigned char) _mm256_extract_epi8(output_row1_even,7);
		filt[1][col+8] = (unsigned char) _mm256_extract_epi8(output_row1_even,8);
		filt[1][col+9] = (unsigned char) _mm256_extract_epi8(output_row1_even,9);
		filt[1][col+10] = (unsigned char) _mm256_extract_epi8(output_row1_even,10);
		filt[1][col+11] = (unsigned char) _mm256_extract_epi8(output_row1_even,11);
		  break;
	case 13:
		filt[2][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[2][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[2][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		filt[2][col+3] = (unsigned char) _mm256_extract_epi8(output_even,3);
		filt[2][col+4] = (unsigned char) _mm256_extract_epi8(output_even,4);
		filt[2][col+5] = (unsigned char) _mm256_extract_epi8(output_even,5);
		filt[2][col+6] = (unsigned char) _mm256_extract_epi8(output_even,6);
		filt[2][col+7] = (unsigned char) _mm256_extract_epi8(output_even,7);
		filt[2][col+8] = (unsigned char) _mm256_extract_epi8(output_even,8);
		filt[2][col+9] = (unsigned char) _mm256_extract_epi8(output_even,9);
		filt[2][col+10] = (unsigned char) _mm256_extract_epi8(output_even,10);
		filt[2][col+11] = (unsigned char) _mm256_extract_epi8(output_even,11);
		filt[2][col+12] = (unsigned char) _mm256_extract_epi8(output_even,12);

		filt[0][col] = (unsigned char) _mm256_extract_epi8(output_row0_even,0);
		filt[0][col+1] = (unsigned char) _mm256_extract_epi8(output_row0_even,1);
		filt[0][col+2] = (unsigned char) _mm256_extract_epi8(output_row0_even,2);
		filt[0][col+3] = (unsigned char) _mm256_extract_epi8(output_row0_even,3);
		filt[0][col+4] = (unsigned char) _mm256_extract_epi8(output_row0_even,4);
		filt[0][col+5] = (unsigned char) _mm256_extract_epi8(output_row0_even,5);
		filt[0][col+6] = (unsigned char) _mm256_extract_epi8(output_row0_even,6);
		filt[0][col+7] = (unsigned char) _mm256_extract_epi8(output_row0_even,7);
		filt[0][col+8] = (unsigned char) _mm256_extract_epi8(output_row0_even,8);
		filt[0][col+9] = (unsigned char) _mm256_extract_epi8(output_row0_even,9);
		filt[0][col+10] = (unsigned char) _mm256_extract_epi8(output_row0_even,10);
		filt[0][col+11] = (unsigned char) _mm256_extract_epi8(output_row0_even,11);
		filt[0][col+12] = (unsigned char) _mm256_extract_epi8(output_row0_even,12);

		filt[1][col] = (unsigned char) _mm256_extract_epi8(output_row1_even,0);
		filt[1][col+1] = (unsigned char) _mm256_extract_epi8(output_row1_even,1);
		filt[1][col+2] = (unsigned char) _mm256_extract_epi8(output_row1_even,2);
		filt[1][col+3] = (unsigned char) _mm256_extract_epi8(output_row1_even,3);
		filt[1][col+4] = (unsigned char) _mm256_extract_epi8(output_row1_even,4);
		filt[1][col+5] = (unsigned char) _mm256_extract_epi8(output_row1_even,5);
		filt[1][col+6] = (unsigned char) _mm256_extract_epi8(output_row1_even,6);
		filt[1][col+7] = (unsigned char) _mm256_extract_epi8(output_row1_even,7);
		filt[1][col+8] = (unsigned char) _mm256_extract_epi8(output_row1_even,8);
		filt[1][col+9] = (unsigned char) _mm256_extract_epi8(output_row1_even,9);
		filt[1][col+10] = (unsigned char) _mm256_extract_epi8(output_row1_even,10);
		filt[1][col+11] = (unsigned char) _mm256_extract_epi8(output_row1_even,11);
		filt[1][col+12] = (unsigned char) _mm256_extract_epi8(output_row1_even,12);
		  break;
	case 14:
		filt[2][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[2][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[2][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		filt[2][col+3] = (unsigned char) _mm256_extract_epi8(output_even,3);
		filt[2][col+4] = (unsigned char) _mm256_extract_epi8(output_even,4);
		filt[2][col+5] = (unsigned char) _mm256_extract_epi8(output_even,5);
		filt[2][col+6] = (unsigned char) _mm256_extract_epi8(output_even,6);
		filt[2][col+7] = (unsigned char) _mm256_extract_epi8(output_even,7);
		filt[2][col+8] = (unsigned char) _mm256_extract_epi8(output_even,8);
		filt[2][col+9] = (unsigned char) _mm256_extract_epi8(output_even,9);
		filt[2][col+10] = (unsigned char) _mm256_extract_epi8(output_even,10);
		filt[2][col+11] = (unsigned char) _mm256_extract_epi8(output_even,11);
		filt[2][col+12] = (unsigned char) _mm256_extract_epi8(output_even,12);
		filt[2][col+13] = (unsigned char) _mm256_extract_epi8(output_even,13);

		filt[0][col] = (unsigned char) _mm256_extract_epi8(output_row0_even,0);
		filt[0][col+1] = (unsigned char) _mm256_extract_epi8(output_row0_even,1);
		filt[0][col+2] = (unsigned char) _mm256_extract_epi8(output_row0_even,2);
		filt[0][col+3] = (unsigned char) _mm256_extract_epi8(output_row0_even,3);
		filt[0][col+4] = (unsigned char) _mm256_extract_epi8(output_row0_even,4);
		filt[0][col+5] = (unsigned char) _mm256_extract_epi8(output_row0_even,5);
		filt[0][col+6] = (unsigned char) _mm256_extract_epi8(output_row0_even,6);
		filt[0][col+7] = (unsigned char) _mm256_extract_epi8(output_row0_even,7);
		filt[0][col+8] = (unsigned char) _mm256_extract_epi8(output_row0_even,8);
		filt[0][col+9] = (unsigned char) _mm256_extract_epi8(output_row0_even,9);
		filt[0][col+10] = (unsigned char) _mm256_extract_epi8(output_row0_even,10);
		filt[0][col+11] = (unsigned char) _mm256_extract_epi8(output_row0_even,11);
		filt[0][col+12] = (unsigned char) _mm256_extract_epi8(output_row0_even,12);
		filt[0][col+13] = (unsigned char) _mm256_extract_epi8(output_row0_even,13);

		filt[1][col] = (unsigned char) _mm256_extract_epi8(output_row1_even,0);
		filt[1][col+1] = (unsigned char) _mm256_extract_epi8(output_row1_even,1);
		filt[1][col+2] = (unsigned char) _mm256_extract_epi8(output_row1_even,2);
		filt[1][col+3] = (unsigned char) _mm256_extract_epi8(output_row1_even,3);
		filt[1][col+4] = (unsigned char) _mm256_extract_epi8(output_row1_even,4);
		filt[1][col+5] = (unsigned char) _mm256_extract_epi8(output_row1_even,5);
		filt[1][col+6] = (unsigned char) _mm256_extract_epi8(output_row1_even,6);
		filt[1][col+7] = (unsigned char) _mm256_extract_epi8(output_row1_even,7);
		filt[1][col+8] = (unsigned char) _mm256_extract_epi8(output_row1_even,8);
		filt[1][col+9] = (unsigned char) _mm256_extract_epi8(output_row1_even,9);
		filt[1][col+10] = (unsigned char) _mm256_extract_epi8(output_row1_even,10);
		filt[1][col+11] = (unsigned char) _mm256_extract_epi8(output_row1_even,11);
		filt[1][col+12] = (unsigned char) _mm256_extract_epi8(output_row1_even,12);
		filt[1][col+13] = (unsigned char) _mm256_extract_epi8(output_row1_even,13);
		  break;
	case 15:
		filt[2][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[2][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[2][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		filt[2][col+3] = (unsigned char) _mm256_extract_epi8(output_even,3);
		filt[2][col+4] = (unsigned char) _mm256_extract_epi8(output_even,4);
		filt[2][col+5] = (unsigned char) _mm256_extract_epi8(output_even,5);
		filt[2][col+6] = (unsigned char) _mm256_extract_epi8(output_even,6);
		filt[2][col+7] = (unsigned char) _mm256_extract_epi8(output_even,7);
		filt[2][col+8] = (unsigned char) _mm256_extract_epi8(output_even,8);
		filt[2][col+9] = (unsigned char) _mm256_extract_epi8(output_even,9);
		filt[2][col+10] = (unsigned char) _mm256_extract_epi8(output_even,10);
		filt[2][col+11] = (unsigned char) _mm256_extract_epi8(output_even,11);
		filt[2][col+12] = (unsigned char) _mm256_extract_epi8(output_even,12);
		filt[2][col+13] = (unsigned char) _mm256_extract_epi8(output_even,13);
		filt[2][col+14] = (unsigned char) _mm256_extract_epi8(output_even,14);

		filt[0][col] = (unsigned char) _mm256_extract_epi8(output_row0_even,0);
		filt[0][col+1] = (unsigned char) _mm256_extract_epi8(output_row0_even,1);
		filt[0][col+2] = (unsigned char) _mm256_extract_epi8(output_row0_even,2);
		filt[0][col+3] = (unsigned char) _mm256_extract_epi8(output_row0_even,3);
		filt[0][col+4] = (unsigned char) _mm256_extract_epi8(output_row0_even,4);
		filt[0][col+5] = (unsigned char) _mm256_extract_epi8(output_row0_even,5);
		filt[0][col+6] = (unsigned char) _mm256_extract_epi8(output_row0_even,6);
		filt[0][col+7] = (unsigned char) _mm256_extract_epi8(output_row0_even,7);
		filt[0][col+8] = (unsigned char) _mm256_extract_epi8(output_row0_even,8);
		filt[0][col+9] = (unsigned char) _mm256_extract_epi8(output_row0_even,9);
		filt[0][col+10] = (unsigned char) _mm256_extract_epi8(output_row0_even,10);
		filt[0][col+11] = (unsigned char) _mm256_extract_epi8(output_row0_even,11);
		filt[0][col+12] = (unsigned char) _mm256_extract_epi8(output_row0_even,12);
		filt[0][col+13] = (unsigned char) _mm256_extract_epi8(output_row0_even,13);
		filt[0][col+14] = (unsigned char) _mm256_extract_epi8(output_row0_even,14);

		filt[1][col] = (unsigned char) _mm256_extract_epi8(output_row1_even,0);
		filt[1][col+1] = (unsigned char) _mm256_extract_epi8(output_row1_even,1);
		filt[1][col+2] = (unsigned char) _mm256_extract_epi8(output_row1_even,2);
		filt[1][col+3] = (unsigned char) _mm256_extract_epi8(output_row1_even,3);
		filt[1][col+4] = (unsigned char) _mm256_extract_epi8(output_row1_even,4);
		filt[1][col+5] = (unsigned char) _mm256_extract_epi8(output_row1_even,5);
		filt[1][col+6] = (unsigned char) _mm256_extract_epi8(output_row1_even,6);
		filt[1][col+7] = (unsigned char) _mm256_extract_epi8(output_row1_even,7);
		filt[1][col+8] = (unsigned char) _mm256_extract_epi8(output_row1_even,8);
		filt[1][col+9] = (unsigned char) _mm256_extract_epi8(output_row1_even,9);
		filt[1][col+10] = (unsigned char) _mm256_extract_epi8(output_row1_even,10);
		filt[1][col+11] = (unsigned char) _mm256_extract_epi8(output_row1_even,11);
		filt[1][col+12] = (unsigned char) _mm256_extract_epi8(output_row1_even,12);
		filt[1][col+13] = (unsigned char) _mm256_extract_epi8(output_row1_even,13);
		filt[1][col+14] = (unsigned char) _mm256_extract_epi8(output_row1_even,14);
		  break;
	case 16:
		_mm_storeu_si128( (__m128i *) &filt[2][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
		_mm_storeu_si128( (__m128i *) &filt[0][col],_mm256_extractf128_si256(output_row0_even, 0)); //store low 128bit - 16pixels
		_mm_storeu_si128( (__m128i *) &filt[1][col],_mm256_extractf128_si256(output_row1_even, 0)); //store low 128bit - 16pixels
		  break;
	case 17:
		_mm_storeu_si128( (__m128i *) &filt[2][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
		filt[2][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);

		_mm_storeu_si128( (__m128i *) &filt[0][col],_mm256_extractf128_si256(output_row0_even, 0)); //store low 128bit - 16pixels
		filt[0][col+16] = (unsigned char) _mm256_extract_epi8(output_row0_even,16);

		_mm_storeu_si128( (__m128i *) &filt[1][col],_mm256_extractf128_si256(output_row1_even, 0)); //store low 128bit - 16pixels
		filt[1][col+16] = (unsigned char) _mm256_extract_epi8(output_row1_even,16);
		  break;
	case 18:
		_mm_storeu_si128( (__m128i *) &filt[2][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
		filt[2][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
		filt[2][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);

		_mm_storeu_si128( (__m128i *) &filt[0][col],_mm256_extractf128_si256(output_row0_even, 0)); //store low 128bit - 16pixels
		filt[0][col+16] = (unsigned char) _mm256_extract_epi8(output_row0_even,16);
		filt[0][col+17] = (unsigned char) _mm256_extract_epi8(output_row0_even,17);

		_mm_storeu_si128( (__m128i *) &filt[1][col],_mm256_extractf128_si256(output_row1_even, 0)); //store low 128bit - 16pixels
		filt[1][col+16] = (unsigned char) _mm256_extract_epi8(output_row1_even,16);
		filt[1][col+17] = (unsigned char) _mm256_extract_epi8(output_row1_even,17);
		  break;
	case 19:
		_mm_storeu_si128( (__m128i *) &filt[2][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
		filt[2][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
		filt[2][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
		filt[2][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);

		_mm_storeu_si128( (__m128i *) &filt[0][col],_mm256_extractf128_si256(output_row0_even, 0)); //store low 128bit - 16pixels
		filt[0][col+16] = (unsigned char) _mm256_extract_epi8(output_row0_even,16);
		filt[0][col+17] = (unsigned char) _mm256_extract_epi8(output_row0_even,17);
		filt[0][col+18] = (unsigned char) _mm256_extract_epi8(output_row0_even,18);

		_mm_storeu_si128( (__m128i *) &filt[1][col],_mm256_extractf128_si256(output_row1_even, 0)); //store low 128bit - 16pixels
		filt[1][col+16] = (unsigned char) _mm256_extract_epi8(output_row1_even,16);
		filt[1][col+17] = (unsigned char) _mm256_extract_epi8(output_row1_even,17);
		filt[1][col+18] = (unsigned char) _mm256_extract_epi8(output_row1_even,18);
		  break;
	case 20:
		_mm_storeu_si128( (__m128i *) &filt[2][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
		filt[2][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
		filt[2][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
		filt[2][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
		filt[2][col+19] = (unsigned char) _mm256_extract_epi8(output_even,19);

		_mm_storeu_si128( (__m128i *) &filt[0][col],_mm256_extractf128_si256(output_row0_even, 0)); //store low 128bit - 16pixels
		filt[0][col+16] = (unsigned char) _mm256_extract_epi8(output_row0_even,16);
		filt[0][col+17] = (unsigned char) _mm256_extract_epi8(output_row0_even,17);
		filt[0][col+18] = (unsigned char) _mm256_extract_epi8(output_row0_even,18);
		filt[0][col+19] = (unsigned char) _mm256_extract_epi8(output_row0_even,19);

		_mm_storeu_si128( (__m128i *) &filt[1][col],_mm256_extractf128_si256(output_row1_even, 0)); //store low 128bit - 16pixels
		filt[1][col+16] = (unsigned char) _mm256_extract_epi8(output_row1_even,16);
		filt[1][col+17] = (unsigned char) _mm256_extract_epi8(output_row1_even,17);
		filt[1][col+18] = (unsigned char) _mm256_extract_epi8(output_row1_even,18);
		filt[1][col+19] = (unsigned char) _mm256_extract_epi8(output_row1_even,19);
		  break;
	case 21:
		_mm_storeu_si128( (__m128i *) &filt[2][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
		filt[2][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
		filt[2][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
		filt[2][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
		filt[2][col+19] = (unsigned char) _mm256_extract_epi8(output_even,19);
		filt[2][col+20] = (unsigned char) _mm256_extract_epi8(output_even,20);

		_mm_storeu_si128( (__m128i *) &filt[0][col],_mm256_extractf128_si256(output_row0_even, 0)); //store low 128bit - 16pixels
		filt[0][col+16] = (unsigned char) _mm256_extract_epi8(output_row0_even,16);
		filt[0][col+17] = (unsigned char) _mm256_extract_epi8(output_row0_even,17);
		filt[0][col+18] = (unsigned char) _mm256_extract_epi8(output_row0_even,18);
		filt[0][col+19] = (unsigned char) _mm256_extract_epi8(output_row0_even,19);
		filt[0][col+20] = (unsigned char) _mm256_extract_epi8(output_row0_even,20);

		_mm_storeu_si128( (__m128i *) &filt[1][col],_mm256_extractf128_si256(output_row1_even, 0)); //store low 128bit - 16pixels
		filt[1][col+16] = (unsigned char) _mm256_extract_epi8(output_row1_even,16);
		filt[1][col+17] = (unsigned char) _mm256_extract_epi8(output_row1_even,17);
		filt[1][col+18] = (unsigned char) _mm256_extract_epi8(output_row1_even,18);
		filt[1][col+19] = (unsigned char) _mm256_extract_epi8(output_row1_even,19);
		filt[1][col+20] = (unsigned char) _mm256_extract_epi8(output_row1_even,20);
		  break;
	case 22:
		_mm_storeu_si128( (__m128i *) &filt[2][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
		filt[2][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
		filt[2][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
		filt[2][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
		filt[2][col+19] = (unsigned char) _mm256_extract_epi8(output_even,19);
		filt[2][col+20] = (unsigned char) _mm256_extract_epi8(output_even,20);
		filt[2][col+21] = (unsigned char) _mm256_extract_epi8(output_even,21);

		_mm_storeu_si128( (__m128i *) &filt[0][col],_mm256_extractf128_si256(output_row0_even, 0)); //store low 128bit - 16pixels
		filt[0][col+16] = (unsigned char) _mm256_extract_epi8(output_row0_even,16);
		filt[0][col+17] = (unsigned char) _mm256_extract_epi8(output_row0_even,17);
		filt[0][col+18] = (unsigned char) _mm256_extract_epi8(output_row0_even,18);
		filt[0][col+19] = (unsigned char) _mm256_extract_epi8(output_row0_even,19);
		filt[0][col+20] = (unsigned char) _mm256_extract_epi8(output_row0_even,20);
		filt[0][col+21] = (unsigned char) _mm256_extract_epi8(output_row0_even,21);

		_mm_storeu_si128( (__m128i *) &filt[1][col],_mm256_extractf128_si256(output_row1_even, 0)); //store low 128bit - 16pixels
		filt[1][col+16] = (unsigned char) _mm256_extract_epi8(output_row1_even,16);
		filt[1][col+17] = (unsigned char) _mm256_extract_epi8(output_row1_even,17);
		filt[1][col+18] = (unsigned char) _mm256_extract_epi8(output_row1_even,18);
		filt[1][col+19] = (unsigned char) _mm256_extract_epi8(output_row1_even,19);
		filt[1][col+20] = (unsigned char) _mm256_extract_epi8(output_row1_even,20);
		filt[1][col+21] = (unsigned char) _mm256_extract_epi8(output_row1_even,21);
		  break;
	case 23:
		_mm_storeu_si128( (__m128i *) &filt[2][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
		filt[2][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
		filt[2][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
		filt[2][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
		filt[2][col+19] = (unsigned char) _mm256_extract_epi8(output_even,19);
		filt[2][col+20] = (unsigned char) _mm256_extract_epi8(output_even,20);
		filt[2][col+21] = (unsigned char) _mm256_extract_epi8(output_even,21);
		filt[2][col+22] = (unsigned char) _mm256_extract_epi8(output_even,22);


		_mm_storeu_si128( (__m128i *) &filt[0][col],_mm256_extractf128_si256(output_row0_even, 0)); //store low 128bit - 16pixels
		filt[0][col+16] = (unsigned char) _mm256_extract_epi8(output_row0_even,16);
		filt[0][col+17] = (unsigned char) _mm256_extract_epi8(output_row0_even,17);
		filt[0][col+18] = (unsigned char) _mm256_extract_epi8(output_row0_even,18);
		filt[0][col+19] = (unsigned char) _mm256_extract_epi8(output_row0_even,19);
		filt[0][col+20] = (unsigned char) _mm256_extract_epi8(output_row0_even,20);
		filt[0][col+21] = (unsigned char) _mm256_extract_epi8(output_row0_even,21);
		filt[0][col+22] = (unsigned char) _mm256_extract_epi8(output_row0_even,22);

		_mm_storeu_si128( (__m128i *) &filt[1][col],_mm256_extractf128_si256(output_row1_even, 0)); //store low 128bit - 16pixels
		filt[1][col+16] = (unsigned char) _mm256_extract_epi8(output_row1_even,16);
		filt[1][col+17] = (unsigned char) _mm256_extract_epi8(output_row1_even,17);
		filt[1][col+18] = (unsigned char) _mm256_extract_epi8(output_row1_even,18);
		filt[1][col+19] = (unsigned char) _mm256_extract_epi8(output_row1_even,19);
		filt[1][col+20] = (unsigned char) _mm256_extract_epi8(output_row1_even,20);
		filt[1][col+21] = (unsigned char) _mm256_extract_epi8(output_row1_even,21);
		filt[1][col+22] = (unsigned char) _mm256_extract_epi8(output_row1_even,22);
		  break;
	case 24:
		_mm_storeu_si128( (__m128i *) &filt[2][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
		filt[2][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
		filt[2][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
		filt[2][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
		filt[2][col+19] = (unsigned char) _mm256_extract_epi8(output_even,19);
		filt[2][col+20] = (unsigned char) _mm256_extract_epi8(output_even,20);
		filt[2][col+21] = (unsigned char) _mm256_extract_epi8(output_even,21);
		filt[2][col+22] = (unsigned char) _mm256_extract_epi8(output_even,22);
		filt[2][col+23] = (unsigned char) _mm256_extract_epi8(output_even,23);

		_mm_storeu_si128( (__m128i *) &filt[0][col],_mm256_extractf128_si256(output_row0_even, 0)); //store low 128bit - 16pixels
		filt[0][col+16] = (unsigned char) _mm256_extract_epi8(output_row0_even,16);
		filt[0][col+17] = (unsigned char) _mm256_extract_epi8(output_row0_even,17);
		filt[0][col+18] = (unsigned char) _mm256_extract_epi8(output_row0_even,18);
		filt[0][col+19] = (unsigned char) _mm256_extract_epi8(output_row0_even,19);
		filt[0][col+20] = (unsigned char) _mm256_extract_epi8(output_row0_even,20);
		filt[0][col+21] = (unsigned char) _mm256_extract_epi8(output_row0_even,21);
		filt[0][col+22] = (unsigned char) _mm256_extract_epi8(output_row0_even,22);
		filt[0][col+23] = (unsigned char) _mm256_extract_epi8(output_row0_even,23);

		_mm_storeu_si128( (__m128i *) &filt[1][col],_mm256_extractf128_si256(output_row1_even, 0)); //store low 128bit - 16pixels
		filt[1][col+16] = (unsigned char) _mm256_extract_epi8(output_row1_even,16);
		filt[1][col+17] = (unsigned char) _mm256_extract_epi8(output_row1_even,17);
		filt[1][col+18] = (unsigned char) _mm256_extract_epi8(output_row1_even,18);
		filt[1][col+19] = (unsigned char) _mm256_extract_epi8(output_row1_even,19);
		filt[1][col+20] = (unsigned char) _mm256_extract_epi8(output_row1_even,20);
		filt[1][col+21] = (unsigned char) _mm256_extract_epi8(output_row1_even,21);
		filt[1][col+22] = (unsigned char) _mm256_extract_epi8(output_row1_even,22);
		filt[1][col+23] = (unsigned char) _mm256_extract_epi8(output_row1_even,23);
		  break;
	case 25:
		_mm_storeu_si128( (__m128i *) &filt[2][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
		filt[2][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
		filt[2][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
		filt[2][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
		filt[2][col+19] = (unsigned char) _mm256_extract_epi8(output_even,19);
		filt[2][col+20] = (unsigned char) _mm256_extract_epi8(output_even,20);
		filt[2][col+21] = (unsigned char) _mm256_extract_epi8(output_even,21);
		filt[2][col+22] = (unsigned char) _mm256_extract_epi8(output_even,22);
		filt[2][col+23] = (unsigned char) _mm256_extract_epi8(output_even,23);
		filt[2][col+24] = (unsigned char) _mm256_extract_epi8(output_even,24);

		_mm_storeu_si128( (__m128i *) &filt[0][col],_mm256_extractf128_si256(output_row0_even, 0)); //store low 128bit - 16pixels
		filt[0][col+16] = (unsigned char) _mm256_extract_epi8(output_row0_even,16);
		filt[0][col+17] = (unsigned char) _mm256_extract_epi8(output_row0_even,17);
		filt[0][col+18] = (unsigned char) _mm256_extract_epi8(output_row0_even,18);
		filt[0][col+19] = (unsigned char) _mm256_extract_epi8(output_row0_even,19);
		filt[0][col+20] = (unsigned char) _mm256_extract_epi8(output_row0_even,20);
		filt[0][col+21] = (unsigned char) _mm256_extract_epi8(output_row0_even,21);
		filt[0][col+22] = (unsigned char) _mm256_extract_epi8(output_row0_even,22);
		filt[0][col+23] = (unsigned char) _mm256_extract_epi8(output_row0_even,23);
		filt[0][col+24] = (unsigned char) _mm256_extract_epi8(output_row0_even,24);

		_mm_storeu_si128( (__m128i *) &filt[1][col],_mm256_extractf128_si256(output_row1_even, 0)); //store low 128bit - 16pixels
		filt[1][col+16] = (unsigned char) _mm256_extract_epi8(output_row1_even,16);
		filt[1][col+17] = (unsigned char) _mm256_extract_epi8(output_row1_even,17);
		filt[1][col+18] = (unsigned char) _mm256_extract_epi8(output_row1_even,18);
		filt[1][col+19] = (unsigned char) _mm256_extract_epi8(output_row1_even,19);
		filt[1][col+20] = (unsigned char) _mm256_extract_epi8(output_row1_even,20);
		filt[1][col+21] = (unsigned char) _mm256_extract_epi8(output_row1_even,21);
		filt[1][col+22] = (unsigned char) _mm256_extract_epi8(output_row1_even,22);
		filt[1][col+23] = (unsigned char) _mm256_extract_epi8(output_row1_even,23);
		filt[1][col+24] = (unsigned char) _mm256_extract_epi8(output_row1_even,24);
		  break;
	case 26:
		_mm_storeu_si128( (__m128i *) &filt[2][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
		filt[2][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
		filt[2][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
		filt[2][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
		filt[2][col+19] = (unsigned char) _mm256_extract_epi8(output_even,19);
		filt[2][col+20] = (unsigned char) _mm256_extract_epi8(output_even,20);
		filt[2][col+21] = (unsigned char) _mm256_extract_epi8(output_even,21);
		filt[2][col+22] = (unsigned char) _mm256_extract_epi8(output_even,22);
		filt[2][col+23] = (unsigned char) _mm256_extract_epi8(output_even,23);
		filt[2][col+24] = (unsigned char) _mm256_extract_epi8(output_even,24);
		filt[2][col+25] = (unsigned char) _mm256_extract_epi8(output_even,25);

		_mm_storeu_si128( (__m128i *) &filt[0][col],_mm256_extractf128_si256(output_row0_even, 0)); //store low 128bit - 16pixels
		filt[0][col+16] = (unsigned char) _mm256_extract_epi8(output_row0_even,16);
		filt[0][col+17] = (unsigned char) _mm256_extract_epi8(output_row0_even,17);
		filt[0][col+18] = (unsigned char) _mm256_extract_epi8(output_row0_even,18);
		filt[0][col+19] = (unsigned char) _mm256_extract_epi8(output_row0_even,19);
		filt[0][col+20] = (unsigned char) _mm256_extract_epi8(output_row0_even,20);
		filt[0][col+21] = (unsigned char) _mm256_extract_epi8(output_row0_even,21);
		filt[0][col+22] = (unsigned char) _mm256_extract_epi8(output_row0_even,22);
		filt[0][col+23] = (unsigned char) _mm256_extract_epi8(output_row0_even,23);
		filt[0][col+24] = (unsigned char) _mm256_extract_epi8(output_row0_even,24);
		filt[0][col+25] = (unsigned char) _mm256_extract_epi8(output_row0_even,25);

		_mm_storeu_si128( (__m128i *) &filt[1][col],_mm256_extractf128_si256(output_row1_even, 0)); //store low 128bit - 16pixels
		filt[1][col+16] = (unsigned char) _mm256_extract_epi8(output_row1_even,16);
		filt[1][col+17] = (unsigned char) _mm256_extract_epi8(output_row1_even,17);
		filt[1][col+18] = (unsigned char) _mm256_extract_epi8(output_row1_even,18);
		filt[1][col+19] = (unsigned char) _mm256_extract_epi8(output_row1_even,19);
		filt[1][col+20] = (unsigned char) _mm256_extract_epi8(output_row1_even,20);
		filt[1][col+21] = (unsigned char) _mm256_extract_epi8(output_row1_even,21);
		filt[1][col+22] = (unsigned char) _mm256_extract_epi8(output_row1_even,22);
		filt[1][col+23] = (unsigned char) _mm256_extract_epi8(output_row1_even,23);
		filt[1][col+24] = (unsigned char) _mm256_extract_epi8(output_row1_even,24);
		filt[1][col+25] = (unsigned char) _mm256_extract_epi8(output_row1_even,25);
		  break;
	case 27:
		_mm_storeu_si128( (__m128i *) &filt[2][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
		filt[2][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
		filt[2][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
		filt[2][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
		filt[2][col+19] = (unsigned char) _mm256_extract_epi8(output_even,19);
		filt[2][col+20] = (unsigned char) _mm256_extract_epi8(output_even,20);
		filt[2][col+21] = (unsigned char) _mm256_extract_epi8(output_even,21);
		filt[2][col+22] = (unsigned char) _mm256_extract_epi8(output_even,22);
		filt[2][col+23] = (unsigned char) _mm256_extract_epi8(output_even,23);
		filt[2][col+24] = (unsigned char) _mm256_extract_epi8(output_even,24);
		filt[2][col+25] = (unsigned char) _mm256_extract_epi8(output_even,25);
		filt[2][col+26] = (unsigned char) _mm256_extract_epi8(output_even,26);

		_mm_storeu_si128( (__m128i *) &filt[0][col],_mm256_extractf128_si256(output_row0_even, 0)); //store low 128bit - 16pixels
		filt[0][col+16] = (unsigned char) _mm256_extract_epi8(output_row0_even,16);
		filt[0][col+17] = (unsigned char) _mm256_extract_epi8(output_row0_even,17);
		filt[0][col+18] = (unsigned char) _mm256_extract_epi8(output_row0_even,18);
		filt[0][col+19] = (unsigned char) _mm256_extract_epi8(output_row0_even,19);
		filt[0][col+20] = (unsigned char) _mm256_extract_epi8(output_row0_even,20);
		filt[0][col+21] = (unsigned char) _mm256_extract_epi8(output_row0_even,21);
		filt[0][col+22] = (unsigned char) _mm256_extract_epi8(output_row0_even,22);
		filt[0][col+23] = (unsigned char) _mm256_extract_epi8(output_row0_even,23);
		filt[0][col+24] = (unsigned char) _mm256_extract_epi8(output_row0_even,24);
		filt[0][col+25] = (unsigned char) _mm256_extract_epi8(output_row0_even,25);
		filt[0][col+26] = (unsigned char) _mm256_extract_epi8(output_row0_even,26);

		_mm_storeu_si128( (__m128i *) &filt[1][col],_mm256_extractf128_si256(output_row1_even, 0)); //store low 128bit - 16pixels
		filt[1][col+16] = (unsigned char) _mm256_extract_epi8(output_row1_even,16);
		filt[1][col+17] = (unsigned char) _mm256_extract_epi8(output_row1_even,17);
		filt[1][col+18] = (unsigned char) _mm256_extract_epi8(output_row1_even,18);
		filt[1][col+19] = (unsigned char) _mm256_extract_epi8(output_row1_even,19);
		filt[1][col+20] = (unsigned char) _mm256_extract_epi8(output_row1_even,20);
		filt[1][col+21] = (unsigned char) _mm256_extract_epi8(output_row1_even,21);
		filt[1][col+22] = (unsigned char) _mm256_extract_epi8(output_row1_even,22);
		filt[1][col+23] = (unsigned char) _mm256_extract_epi8(output_row1_even,23);
		filt[1][col+24] = (unsigned char) _mm256_extract_epi8(output_row1_even,24);
		filt[1][col+25] = (unsigned char) _mm256_extract_epi8(output_row1_even,25);
		filt[1][col+26] = (unsigned char) _mm256_extract_epi8(output_row1_even,26);
		  break;
	case 28:
		_mm_storeu_si128( (__m128i *) &filt[2][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
		filt[2][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
		filt[2][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
		filt[2][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
		filt[2][col+19] = (unsigned char) _mm256_extract_epi8(output_even,19);
		filt[2][col+20] = (unsigned char) _mm256_extract_epi8(output_even,20);
		filt[2][col+21] = (unsigned char) _mm256_extract_epi8(output_even,21);
		filt[2][col+22] = (unsigned char) _mm256_extract_epi8(output_even,22);
		filt[2][col+23] = (unsigned char) _mm256_extract_epi8(output_even,23);
		filt[2][col+24] = (unsigned char) _mm256_extract_epi8(output_even,24);
		filt[2][col+25] = (unsigned char) _mm256_extract_epi8(output_even,25);
		filt[2][col+26] = (unsigned char) _mm256_extract_epi8(output_even,26);
		filt[2][col+27] = (unsigned char) _mm256_extract_epi8(output_even,27);

		_mm_storeu_si128( (__m128i *) &filt[0][col],_mm256_extractf128_si256(output_row0_even, 0)); //store low 128bit - 16pixels
		filt[0][col+16] = (unsigned char) _mm256_extract_epi8(output_row0_even,16);
		filt[0][col+17] = (unsigned char) _mm256_extract_epi8(output_row0_even,17);
		filt[0][col+18] = (unsigned char) _mm256_extract_epi8(output_row0_even,18);
		filt[0][col+19] = (unsigned char) _mm256_extract_epi8(output_row0_even,19);
		filt[0][col+20] = (unsigned char) _mm256_extract_epi8(output_row0_even,20);
		filt[0][col+21] = (unsigned char) _mm256_extract_epi8(output_row0_even,21);
		filt[0][col+22] = (unsigned char) _mm256_extract_epi8(output_row0_even,22);
		filt[0][col+23] = (unsigned char) _mm256_extract_epi8(output_row0_even,23);
		filt[0][col+24] = (unsigned char) _mm256_extract_epi8(output_row0_even,24);
		filt[0][col+25] = (unsigned char) _mm256_extract_epi8(output_row0_even,25);
		filt[0][col+26] = (unsigned char) _mm256_extract_epi8(output_row0_even,26);
		filt[0][col+27] = (unsigned char) _mm256_extract_epi8(output_row0_even,27);

		_mm_storeu_si128( (__m128i *) &filt[1][col],_mm256_extractf128_si256(output_row1_even, 0)); //store low 128bit - 16pixels
		filt[1][col+16] = (unsigned char) _mm256_extract_epi8(output_row1_even,16);
		filt[1][col+17] = (unsigned char) _mm256_extract_epi8(output_row1_even,17);
		filt[1][col+18] = (unsigned char) _mm256_extract_epi8(output_row1_even,18);
		filt[1][col+19] = (unsigned char) _mm256_extract_epi8(output_row1_even,19);
		filt[1][col+20] = (unsigned char) _mm256_extract_epi8(output_row1_even,20);
		filt[1][col+21] = (unsigned char) _mm256_extract_epi8(output_row1_even,21);
		filt[1][col+22] = (unsigned char) _mm256_extract_epi8(output_row1_even,22);
		filt[1][col+23] = (unsigned char) _mm256_extract_epi8(output_row1_even,23);
		filt[1][col+24] = (unsigned char) _mm256_extract_epi8(output_row1_even,24);
		filt[1][col+25] = (unsigned char) _mm256_extract_epi8(output_row1_even,25);
		filt[1][col+26] = (unsigned char) _mm256_extract_epi8(output_row1_even,26);
		filt[1][col+27] = (unsigned char) _mm256_extract_epi8(output_row1_even,27);
		  break;
	case 29:
		_mm_storeu_si128( (__m128i *) &filt[2][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
		filt[2][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
		filt[2][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
		filt[2][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
		filt[2][col+19] = (unsigned char) _mm256_extract_epi8(output_even,19);
		filt[2][col+20] = (unsigned char) _mm256_extract_epi8(output_even,20);
		filt[2][col+21] = (unsigned char) _mm256_extract_epi8(output_even,21);
		filt[2][col+22] = (unsigned char) _mm256_extract_epi8(output_even,22);
		filt[2][col+23] = (unsigned char) _mm256_extract_epi8(output_even,23);
		filt[2][col+24] = (unsigned char) _mm256_extract_epi8(output_even,24);
		filt[2][col+25] = (unsigned char) _mm256_extract_epi8(output_even,25);
		filt[2][col+26] = (unsigned char) _mm256_extract_epi8(output_even,26);
		filt[2][col+27] = (unsigned char) _mm256_extract_epi8(output_even,27);
		filt[2][col+28] = (unsigned char) _mm256_extract_epi8(output_even,28);

		_mm_storeu_si128( (__m128i *) &filt[0][col],_mm256_extractf128_si256(output_row0_even, 0)); //store low 128bit - 16pixels
		filt[0][col+16] = (unsigned char) _mm256_extract_epi8(output_row0_even,16);
		filt[0][col+17] = (unsigned char) _mm256_extract_epi8(output_row0_even,17);
		filt[0][col+18] = (unsigned char) _mm256_extract_epi8(output_row0_even,18);
		filt[0][col+19] = (unsigned char) _mm256_extract_epi8(output_row0_even,19);
		filt[0][col+20] = (unsigned char) _mm256_extract_epi8(output_row0_even,20);
		filt[0][col+21] = (unsigned char) _mm256_extract_epi8(output_row0_even,21);
		filt[0][col+22] = (unsigned char) _mm256_extract_epi8(output_row0_even,22);
		filt[0][col+23] = (unsigned char) _mm256_extract_epi8(output_row0_even,23);
		filt[0][col+24] = (unsigned char) _mm256_extract_epi8(output_row0_even,24);
		filt[0][col+25] = (unsigned char) _mm256_extract_epi8(output_row0_even,25);
		filt[0][col+26] = (unsigned char) _mm256_extract_epi8(output_row0_even,26);
		filt[0][col+27] = (unsigned char) _mm256_extract_epi8(output_row0_even,27);
		filt[0][col+28] = (unsigned char) _mm256_extract_epi8(output_row0_even,28);

		_mm_storeu_si128( (__m128i *) &filt[1][col],_mm256_extractf128_si256(output_row1_even, 0)); //store low 128bit - 16pixels
		filt[1][col+16] = (unsigned char) _mm256_extract_epi8(output_row1_even,16);
		filt[1][col+17] = (unsigned char) _mm256_extract_epi8(output_row1_even,17);
		filt[1][col+18] = (unsigned char) _mm256_extract_epi8(output_row1_even,18);
		filt[1][col+19] = (unsigned char) _mm256_extract_epi8(output_row1_even,19);
		filt[1][col+20] = (unsigned char) _mm256_extract_epi8(output_row1_even,20);
		filt[1][col+21] = (unsigned char) _mm256_extract_epi8(output_row1_even,21);
		filt[1][col+22] = (unsigned char) _mm256_extract_epi8(output_row1_even,22);
		filt[1][col+23] = (unsigned char) _mm256_extract_epi8(output_row1_even,23);
		filt[1][col+24] = (unsigned char) _mm256_extract_epi8(output_row1_even,24);
		filt[1][col+25] = (unsigned char) _mm256_extract_epi8(output_row1_even,25);
		filt[1][col+26] = (unsigned char) _mm256_extract_epi8(output_row1_even,26);
		filt[1][col+27] = (unsigned char) _mm256_extract_epi8(output_row1_even,27);
		filt[1][col+28] = (unsigned char) _mm256_extract_epi8(output_row1_even,28);
		  break;
	case 30:
		_mm_storeu_si128( (__m128i *) &filt[2][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
		filt[2][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
		filt[2][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
		filt[2][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
		filt[2][col+19] = (unsigned char) _mm256_extract_epi8(output_even,19);
		filt[2][col+20] = (unsigned char) _mm256_extract_epi8(output_even,20);
		filt[2][col+21] = (unsigned char) _mm256_extract_epi8(output_even,21);
		filt[2][col+22] = (unsigned char) _mm256_extract_epi8(output_even,22);
		filt[2][col+23] = (unsigned char) _mm256_extract_epi8(output_even,23);
		filt[2][col+24] = (unsigned char) _mm256_extract_epi8(output_even,24);
		filt[2][col+25] = (unsigned char) _mm256_extract_epi8(output_even,25);
		filt[2][col+26] = (unsigned char) _mm256_extract_epi8(output_even,26);
		filt[2][col+27] = (unsigned char) _mm256_extract_epi8(output_even,27);
		filt[2][col+28] = (unsigned char) _mm256_extract_epi8(output_even,28);
		filt[2][col+29] = (unsigned char) _mm256_extract_epi8(output_even,29);

		_mm_storeu_si128( (__m128i *) &filt[0][col],_mm256_extractf128_si256(output_row0_even, 0)); //store low 128bit - 16pixels
		filt[0][col+16] = (unsigned char) _mm256_extract_epi8(output_row0_even,16);
		filt[0][col+17] = (unsigned char) _mm256_extract_epi8(output_row0_even,17);
		filt[0][col+18] = (unsigned char) _mm256_extract_epi8(output_row0_even,18);
		filt[0][col+19] = (unsigned char) _mm256_extract_epi8(output_row0_even,19);
		filt[0][col+20] = (unsigned char) _mm256_extract_epi8(output_row0_even,20);
		filt[0][col+21] = (unsigned char) _mm256_extract_epi8(output_row0_even,21);
		filt[0][col+22] = (unsigned char) _mm256_extract_epi8(output_row0_even,22);
		filt[0][col+23] = (unsigned char) _mm256_extract_epi8(output_row0_even,23);
		filt[0][col+24] = (unsigned char) _mm256_extract_epi8(output_row0_even,24);
		filt[0][col+25] = (unsigned char) _mm256_extract_epi8(output_row0_even,25);
		filt[0][col+26] = (unsigned char) _mm256_extract_epi8(output_row0_even,26);
		filt[0][col+27] = (unsigned char) _mm256_extract_epi8(output_row0_even,27);
		filt[0][col+28] = (unsigned char) _mm256_extract_epi8(output_row0_even,28);
		filt[0][col+29] = (unsigned char) _mm256_extract_epi8(output_row0_even,29);


		_mm_storeu_si128( (__m128i *) &filt[1][col],_mm256_extractf128_si256(output_row1_even, 0)); //store low 128bit - 16pixels
		filt[1][col+16] = (unsigned char) _mm256_extract_epi8(output_row1_even,16);
		filt[1][col+17] = (unsigned char) _mm256_extract_epi8(output_row1_even,17);
		filt[1][col+18] = (unsigned char) _mm256_extract_epi8(output_row1_even,18);
		filt[1][col+19] = (unsigned char) _mm256_extract_epi8(output_row1_even,19);
		filt[1][col+20] = (unsigned char) _mm256_extract_epi8(output_row1_even,20);
		filt[1][col+21] = (unsigned char) _mm256_extract_epi8(output_row1_even,21);
		filt[1][col+22] = (unsigned char) _mm256_extract_epi8(output_row1_even,22);
		filt[1][col+23] = (unsigned char) _mm256_extract_epi8(output_row1_even,23);
		filt[1][col+24] = (unsigned char) _mm256_extract_epi8(output_row1_even,24);
		filt[1][col+25] = (unsigned char) _mm256_extract_epi8(output_row1_even,25);
		filt[1][col+26] = (unsigned char) _mm256_extract_epi8(output_row1_even,26);
		filt[1][col+27] = (unsigned char) _mm256_extract_epi8(output_row1_even,27);
		filt[1][col+28] = (unsigned char) _mm256_extract_epi8(output_row1_even,28);
		filt[1][col+29] = (unsigned char) _mm256_extract_epi8(output_row1_even,29);
		  break;
	case 31:
		_mm_storeu_si128( (__m128i *) &filt[2][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
		filt[2][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
		filt[2][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
		filt[2][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
		filt[2][col+19] = (unsigned char) _mm256_extract_epi8(output_even,19);
		filt[2][col+20] = (unsigned char) _mm256_extract_epi8(output_even,20);
		filt[2][col+21] = (unsigned char) _mm256_extract_epi8(output_even,21);
		filt[2][col+22] = (unsigned char) _mm256_extract_epi8(output_even,22);
		filt[2][col+23] = (unsigned char) _mm256_extract_epi8(output_even,23);
		filt[2][col+24] = (unsigned char) _mm256_extract_epi8(output_even,24);
		filt[2][col+25] = (unsigned char) _mm256_extract_epi8(output_even,25);
		filt[2][col+26] = (unsigned char) _mm256_extract_epi8(output_even,26);
		filt[2][col+27] = (unsigned char) _mm256_extract_epi8(output_even,27);
		filt[2][col+28] = (unsigned char) _mm256_extract_epi8(output_even,28);
		filt[2][col+29] = (unsigned char) _mm256_extract_epi8(output_even,29);

		_mm_storeu_si128( (__m128i *) &filt[0][col],_mm256_extractf128_si256(output_row0_even, 0)); //store low 128bit - 16pixels
		filt[0][col+16] = (unsigned char) _mm256_extract_epi8(output_row0_even,16);
		filt[0][col+17] = (unsigned char) _mm256_extract_epi8(output_row0_even,17);
		filt[0][col+18] = (unsigned char) _mm256_extract_epi8(output_row0_even,18);
		filt[0][col+19] = (unsigned char) _mm256_extract_epi8(output_row0_even,19);
		filt[0][col+20] = (unsigned char) _mm256_extract_epi8(output_row0_even,20);
		filt[0][col+21] = (unsigned char) _mm256_extract_epi8(output_row0_even,21);
		filt[0][col+22] = (unsigned char) _mm256_extract_epi8(output_row0_even,22);
		filt[0][col+23] = (unsigned char) _mm256_extract_epi8(output_row0_even,23);
		filt[0][col+24] = (unsigned char) _mm256_extract_epi8(output_row0_even,24);
		filt[0][col+25] = (unsigned char) _mm256_extract_epi8(output_row0_even,25);
		filt[0][col+26] = (unsigned char) _mm256_extract_epi8(output_row0_even,26);
		filt[0][col+27] = (unsigned char) _mm256_extract_epi8(output_row0_even,27);
		filt[0][col+28] = (unsigned char) _mm256_extract_epi8(output_row0_even,28);
		filt[0][col+29] = (unsigned char) _mm256_extract_epi8(output_row0_even,29);


		_mm_storeu_si128( (__m128i *) &filt[1][col],_mm256_extractf128_si256(output_row1_even, 0)); //store low 128bit - 16pixels
		filt[1][col+16] = (unsigned char) _mm256_extract_epi8(output_row1_even,16);
		filt[1][col+17] = (unsigned char) _mm256_extract_epi8(output_row1_even,17);
		filt[1][col+18] = (unsigned char) _mm256_extract_epi8(output_row1_even,18);
		filt[1][col+19] = (unsigned char) _mm256_extract_epi8(output_row1_even,19);
		filt[1][col+20] = (unsigned char) _mm256_extract_epi8(output_row1_even,20);
		filt[1][col+21] = (unsigned char) _mm256_extract_epi8(output_row1_even,21);
		filt[1][col+22] = (unsigned char) _mm256_extract_epi8(output_row1_even,22);
		filt[1][col+23] = (unsigned char) _mm256_extract_epi8(output_row1_even,23);
		filt[1][col+24] = (unsigned char) _mm256_extract_epi8(output_row1_even,24);
		filt[1][col+25] = (unsigned char) _mm256_extract_epi8(output_row1_even,25);
		filt[1][col+26] = (unsigned char) _mm256_extract_epi8(output_row1_even,26);
		filt[1][col+27] = (unsigned char) _mm256_extract_epi8(output_row1_even,27);
		filt[1][col+28] = (unsigned char) _mm256_extract_epi8(output_row1_even,28);
		filt[1][col+29] = (unsigned char) _mm256_extract_epi8(output_row1_even,29);

		//the filt[0:2][M-1] pixel will be computed without vectorization
		int newPixel;
		for (int row=0;row<=2;row++){
		 newPixel = 0;
		for (int rowOffset=-2; rowOffset<=2; rowOffset++) {
			for (int colOffset=-2; colOffset<=2; colOffset++) {

			   if ( ((row+rowOffset)<N) && ((M-1+colOffset)<M) && ((row+rowOffset)>=0) && ((M-1+colOffset)>=0) )
	              newPixel += frame1[row+rowOffset][M-1+colOffset] * filter[2 + rowOffset][2 + colOffset];

			      }
		        }

	   filt[row][M-1] = (unsigned char) (newPixel / divisor);
		}

		  break;
	default:
		printf("\n something went wrong");
	}

return 0;

}





//for row=N-3,N-2,N-1 only
//row0 refers to N-2, while row1 refers to N-1
int loop_reminder_last_less_div(unsigned char **frame1,unsigned char **filt,const unsigned int M, const unsigned int N, const unsigned int col, const unsigned int REMINDER_ITERATIONS,const unsigned int division_case,const __m256i c0,const __m256i c1,const __m256i c2,const __m256i c0_sh1,const __m256i c1_sh1,const __m256i c2_sh1,const __m256i c0_sh2,const __m256i c1_sh2,const __m256i c2_sh2, const __m256i f, const unsigned short int divisor,signed char **filter ){

__m256i r0,r1,r2,r3,r4,m0,m1,m2,m3,m4;
__m256i output_even,output_odd;
__m256i output_row0_even,output_row1_even,output_row0_odd,output_row1_odd;

		//const __m256i f  = _mm256_set_epi16(52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759);
		//const __m256i mask  = _mm256_set_epi8(0,0,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255);
		//const __m256i mask2  = _mm256_set_epi8(0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0);
		const __m256i mask3  = _mm256_set_epi16(0,0,0,0,0,0,0,65535,0,0,0,0,0,0,0,0);
		//const __m256i mask4  = _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255,0,0,0,0,0,0,0,0,0,0,0,0);
		//const __m256i mask5  = _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0);

		const __m256i output_mask      = _mm256_set_epi16(0,0,0,65535,0,0,65535,0,0,65535,0,0,65535,0,0,65535);
		const __m256i output_mask_sh1  = _mm256_set_epi16(0,0,65535,0,0,65535,0,0,65535,0,0,65535,0,0,65535,0);
		//const __m256i output_mask_sh2  = _mm256_set_epi16(0,0,0,    0,65535,0,0,65535,0,0,65535,0,0,65535,0,0);
		const __m256i mask_new      = _mm256_set_epi16(0,0,0,0,0,0,0,0,0,65535,0,0,0,0,0,0);
		const __m256i mask_new2      = _mm256_set_epi16(0,0,0,0,0,0,0,0,65535,0,0,0,0,0,0,0);



 __m256i reminder_mask1,reminder_mask2;

	reminder_mask1=_mm256_load_si256( (__m256i *) &reminder_msk1[REMINDER_ITERATIONS-1][0]);
	reminder_mask2=_mm256_load_si256( (__m256i *) &reminder_msk2[REMINDER_ITERATIONS-1][0]);


	 //1st col iteration
	//load the 5 rows
	r0=_mm256_loadu_si256( (__m256i *) &frame1[N-5][col-2]);
	r1=_mm256_loadu_si256( (__m256i *) &frame1[N-4][col-2]);
	r2=_mm256_loadu_si256( (__m256i *) &frame1[N-3][col-2]);
	r3=_mm256_loadu_si256( (__m256i *) &frame1[N-2][col-2]);
	r4=_mm256_loadu_si256( (__m256i *) &frame1[N-1][col-2]);

	/*
	 * The above load operations load outside of the array bounds; these elements are filled with zeros just after
	 * Furthermore, I add two extra zeros in the end to compute col=N-2 and col=N-1. The last value of col is N-1, not N
	 */

	//AND r0-r4 with reminder_mask
	r0=_mm256_and_si256(r0,reminder_mask1);
	r1=_mm256_and_si256(r1,reminder_mask1);
	r2=_mm256_and_si256(r2,reminder_mask1);
	r3=_mm256_and_si256(r3,reminder_mask1);
	r4=_mm256_and_si256(r4,reminder_mask1);

	//row=2
	//multiply with the mask
	m0=_mm256_maddubs_epi16(r0,c0);
	m1=_mm256_maddubs_epi16(r1,c1);
	m2=_mm256_maddubs_epi16(r2,c2);
	m3=_mm256_maddubs_epi16(r3,c1);
	m4=_mm256_maddubs_epi16(r4,c0);

	//vertical add
	m0=_mm256_add_epi16(m0,m1);
	m2=_mm256_add_epi16(m2,m3);
	m0=_mm256_add_epi16(m0,m2);
	m0=_mm256_add_epi16(m0,m4);


	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);

	m1=_mm256_srli_si256(m0,4);
	m2=_mm256_add_epi16(m1,m2);

	//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
	//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
	m4=_mm256_and_si256(m0,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions

	m2=_mm256_add_epi16(m2,m4);

	//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
	output_even=_mm256_and_si256(m2,output_mask);

	//row=0
	//multiply with the mask
	m1=_mm256_maddubs_epi16(r1,c0);
	m2=_mm256_maddubs_epi16(r2,c1);
	m3=_mm256_maddubs_epi16(r3,c2);
	m4=_mm256_maddubs_epi16(r4,c1);

	//vertical add
	m0=_mm256_add_epi16(m1,m2);
	m0=_mm256_add_epi16(m0,m3);
	m0=_mm256_add_epi16(m0,m4);


	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);

	m1=_mm256_srli_si256(m0,4);
	m2=_mm256_add_epi16(m1,m2);

	//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
	//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
	m4=_mm256_and_si256(m0,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions

	m2=_mm256_add_epi16(m2,m4);

	//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
	output_row0_even=_mm256_and_si256(m2,output_mask);


	//row=1
	//multiply with the mask
	m2=_mm256_maddubs_epi16(r2,c0);
	m3=_mm256_maddubs_epi16(r3,c1);
	m4=_mm256_maddubs_epi16(r4,c2);

	//vertical add
	m0=_mm256_add_epi16(m2,m3);
	m0=_mm256_add_epi16(m0,m4);


	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);

	m1=_mm256_srli_si256(m0,4);
	m2=_mm256_add_epi16(m1,m2);

	//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
	//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
	m4=_mm256_and_si256(m0,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions

	m2=_mm256_add_epi16(m2,m4);

	//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
	output_row1_even=_mm256_and_si256(m2,output_mask);

	//2nd col iteration

	//row=2
	//multiply with the mask
	m0=_mm256_maddubs_epi16(r0,c0_sh1);
	m1=_mm256_maddubs_epi16(r1,c1_sh1);
	m2=_mm256_maddubs_epi16(r2,c2_sh1);
	m3=_mm256_maddubs_epi16(r3,c1_sh1);
	m4=_mm256_maddubs_epi16(r4,c0_sh1);

	//vertical add
	m0=_mm256_add_epi16(m0,m1);
	m2=_mm256_add_epi16(m2,m3);
	m0=_mm256_add_epi16(m0,m2);
	m0=_mm256_add_epi16(m0,m4);

	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);

	m1=_mm256_srli_si256(m0,4);
	m2=_mm256_add_epi16(m1,m2);

	//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
	//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
	m4=_mm256_and_si256(m0,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions
	m2=_mm256_add_epi16(m2,m4);

	//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
	output_odd = _mm256_and_si256(m2,output_mask);

	//row=0
	//multiply with the mask
	m1=_mm256_maddubs_epi16(r1,c0_sh1);
	m2=_mm256_maddubs_epi16(r2,c1_sh1);
	m3=_mm256_maddubs_epi16(r3,c2_sh1);
	m4=_mm256_maddubs_epi16(r4,c1_sh1);

	//vertical add
	m0=_mm256_add_epi16(m1,m2);
	m0=_mm256_add_epi16(m0,m3);
	m0=_mm256_add_epi16(m0,m4);


	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);

	m1=_mm256_srli_si256(m0,4);
	m2=_mm256_add_epi16(m1,m2);

	//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
	//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
	m4=_mm256_and_si256(m0,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions
	m2=_mm256_add_epi16(m2,m4);

	//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
	output_row0_odd = _mm256_and_si256(m2,output_mask);

	//row=1
	//multiply with the mask
	m2=_mm256_maddubs_epi16(r2,c0_sh1);
	m3=_mm256_maddubs_epi16(r3,c1_sh1);
	m4=_mm256_maddubs_epi16(r4,c2_sh1);

	//vertical add
	m0=_mm256_add_epi16(m2,m3);
	m0=_mm256_add_epi16(m0,m4);

	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);

	m1=_mm256_srli_si256(m0,4);
	m2=_mm256_add_epi16(m1,m2);

	//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
	//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
	m4=_mm256_and_si256(m0,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions
	m2=_mm256_add_epi16(m2,m4);

	//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
	output_row1_odd = _mm256_and_si256(m2,output_mask);

             //3rd col iteration

	//row=2
	//multiply with the mask
	m0=_mm256_maddubs_epi16(r0,c0_sh2);
	m1=_mm256_maddubs_epi16(r1,c1_sh2);
	m2=_mm256_maddubs_epi16(r2,c2_sh2);
	m3=_mm256_maddubs_epi16(r3,c1_sh2);
	m4=_mm256_maddubs_epi16(r4,c0_sh2);

	//vertical add
	m0=_mm256_add_epi16(m0,m1);
	m2=_mm256_add_epi16(m2,m3);
	m0=_mm256_add_epi16(m0,m2);
	m0=_mm256_add_epi16(m0,m4);


	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);

	//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,_mm256_extract_epi16(m2,8),0,0,0,0,0,0,0);
	m4=_mm256_and_si256(m2,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions

	m1=_mm256_srli_si256(m0,4);
	m2=_mm256_add_epi16(m1,m2);

	//prepei na prosthesw to pixel pu thelw sto r2 kai meta na kanw diairesi me ola mazi

	m2=_mm256_add_epi16(m2,m4);

	//m2 has 16 16bit values now. the results I need are in positions 1,4,7,10,13. keep only those, discard others
	m2 = _mm256_and_si256(m2,output_mask_sh1);
	output_even = _mm256_add_epi16(output_even,m2);

	//row=0
	//multiply with the mask
	m1=_mm256_maddubs_epi16(r1,c0_sh2);
	m2=_mm256_maddubs_epi16(r2,c1_sh2);
	m3=_mm256_maddubs_epi16(r3,c2_sh2);
	m4=_mm256_maddubs_epi16(r4,c1_sh2);

	//vertical add
	m0=_mm256_add_epi16(m1,m2);
	m0=_mm256_add_epi16(m0,m3);
	m0=_mm256_add_epi16(m0,m4);


	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);

	//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,_mm256_extract_epi16(m2,8),0,0,0,0,0,0,0);
	m4=_mm256_and_si256(m2,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions

	m1=_mm256_srli_si256(m0,4);
	m2=_mm256_add_epi16(m1,m2);

	//prepei na prosthesw to pixel pu thelw sto r2 kai meta na kanw diairesi me ola mazi

	m2=_mm256_add_epi16(m2,m4);

	//m2 has 16 16bit values now. the results I need are in positions 1,4,7,10,13. keep only those, discard others
	m2 = _mm256_and_si256(m2,output_mask_sh1);
	output_row0_even = _mm256_add_epi16(output_row0_even,m2);

	//row=1
	//multiply with the mask
	m2=_mm256_maddubs_epi16(r2,c0_sh2);
	m3=_mm256_maddubs_epi16(r3,c1_sh2);
	m4=_mm256_maddubs_epi16(r4,c2_sh2);

	//vertical add
	m0=_mm256_add_epi16(m2,m3);
	m0=_mm256_add_epi16(m0,m4);


	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);

	//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,_mm256_extract_epi16(m2,8),0,0,0,0,0,0,0);
	m4=_mm256_and_si256(m2,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions

	m1=_mm256_srli_si256(m0,4);
	m2=_mm256_add_epi16(m1,m2);

	//prepei na prosthesw to pixel pu thelw sto r2 kai meta na kanw diairesi me ola mazi

	m2=_mm256_add_epi16(m2,m4);

	//m2 has 16 16bit values now. the results I need are in positions 1,4,7,10,13. keep only those, discard others
	m2 = _mm256_and_si256(m2,output_mask_sh1);
	output_row1_even = _mm256_add_epi16(output_row1_even,m2);

	//4th col iteration
	//load the 5 rows
	r0=_mm256_loadu_si256( (__m256i *) &frame1[N-5][col+3-2]);
	r1=_mm256_loadu_si256( (__m256i *) &frame1[N-4][col+3-2]);
	r2=_mm256_loadu_si256( (__m256i *) &frame1[N-3][col+3-2]);
	r3=_mm256_loadu_si256( (__m256i *) &frame1[N-2][col+3-2]);
	r4=_mm256_loadu_si256( (__m256i *) &frame1[N-1][col+3-2]);

	//AND r0-r4 with reminder_mask
	r0=_mm256_and_si256(r0,reminder_mask2);
	r1=_mm256_and_si256(r1,reminder_mask2);
	r2=_mm256_and_si256(r2,reminder_mask2);
	r3=_mm256_and_si256(r3,reminder_mask2);
	r4=_mm256_and_si256(r4,reminder_mask2);

	//row=2
	//multiply with the mask
	m0=_mm256_maddubs_epi16(r0,c0);
	m1=_mm256_maddubs_epi16(r1,c1);
	m2=_mm256_maddubs_epi16(r2,c2);
	m3=_mm256_maddubs_epi16(r3,c1);
	m4=_mm256_maddubs_epi16(r4,c0);

	//vertical add
	m0=_mm256_add_epi16(m0,m1);
	m2=_mm256_add_epi16(m2,m3);
	m0=_mm256_add_epi16(m0,m2);
	m0=_mm256_add_epi16(m0,m4);


	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);

	m1=_mm256_srli_si256(m0,4);
	m2=_mm256_add_epi16(m1,m2);

	//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
	//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
	m4=_mm256_and_si256(m0,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions

	m2=_mm256_add_epi16(m2,m4);

	//m2 has 16 16bit values now. the results I need are in positions 1,4,7,10,13. keep only those, discard others
	m2 = _mm256_and_si256(m2,output_mask);
	m2 = _mm256_slli_si256(m2,2);
	output_odd = _mm256_add_epi16(output_odd,m2);

	//row=0
	//multiply with the mask
	m1=_mm256_maddubs_epi16(r1,c0);
	m2=_mm256_maddubs_epi16(r2,c1);
	m3=_mm256_maddubs_epi16(r3,c2);
	m4=_mm256_maddubs_epi16(r4,c1);

	//vertical add
	m0=_mm256_add_epi16(m1,m2);
	m0=_mm256_add_epi16(m0,m3);
	m0=_mm256_add_epi16(m0,m4);


	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);

	m1=_mm256_srli_si256(m0,4);
	m2=_mm256_add_epi16(m1,m2);

	//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
	//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
	m4=_mm256_and_si256(m0,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions

	m2=_mm256_add_epi16(m2,m4);

	//m2 has 16 16bit values now. the results I need are in positions 1,4,7,10,13. keep only those, discard others
	m2 = _mm256_and_si256(m2,output_mask);
	m2 = _mm256_slli_si256(m2,2);
	output_row0_odd = _mm256_add_epi16(output_row0_odd,m2);

	//row=1
	//multiply with the mask
	m2=_mm256_maddubs_epi16(r2,c0);
	m3=_mm256_maddubs_epi16(r3,c1);
	m4=_mm256_maddubs_epi16(r4,c2);

	//vertical add
	m0=_mm256_add_epi16(m2,m3);
	m0=_mm256_add_epi16(m0,m4);


	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);

	m1=_mm256_srli_si256(m0,4);
	m2=_mm256_add_epi16(m1,m2);

	//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
	//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
	m4=_mm256_and_si256(m0,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions

	m2=_mm256_add_epi16(m2,m4);

	//m2 has 16 16bit values now. the results I need are in positions 1,4,7,10,13. keep only those, discard others
	m2 = _mm256_and_si256(m2,output_mask);
	m2 = _mm256_slli_si256(m2,2);
	output_row1_odd = _mm256_add_epi16(output_row1_odd,m2);

	//5th col iteration

	//row=2
	//multiply with the mask
	m0=_mm256_maddubs_epi16(r0,c0_sh1);
	m1=_mm256_maddubs_epi16(r1,c1_sh1);
	m2=_mm256_maddubs_epi16(r2,c2_sh1);
	m3=_mm256_maddubs_epi16(r3,c1_sh1);
	m4=_mm256_maddubs_epi16(r4,c0_sh1);

	//vertical add
	m0=_mm256_add_epi16(m0,m1);
	m2=_mm256_add_epi16(m2,m3);
	m0=_mm256_add_epi16(m0,m2);
	m0=_mm256_add_epi16(m0,m4);


	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);

	m1=_mm256_srli_si256(m0,4);
	m2=_mm256_add_epi16(m1,m2);

	//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
	//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
	m4=_mm256_and_si256(m0,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions

	m2=_mm256_add_epi16(m2,m4);

	//m2 has 16 16bit values now.
	m2 = _mm256_and_si256(m2,output_mask); //holds results in positions 0,3,6,9,12
	m1 = _mm256_slli_si256(m2,4); //put results in positions 2,5,8,11,14 , but 8 is not written
	m2=_mm256_and_si256(m2,mask_new);//extract only the 6th
	m2=_mm256_permute2f128_si256(m2,m2,1);
	m2=_mm256_srli_si256(m2,12);
	m2 = _mm256_add_epi16(m1,m2);
	output_even = _mm256_add_epi16(output_even,m2);

	//row=0
	//multiply with the mask
	m1=_mm256_maddubs_epi16(r1,c0_sh1);
	m2=_mm256_maddubs_epi16(r2,c1_sh1);
	m3=_mm256_maddubs_epi16(r3,c2_sh1);
	m4=_mm256_maddubs_epi16(r4,c1_sh1);

	//vertical add
	m0=_mm256_add_epi16(m1,m2);
	m0=_mm256_add_epi16(m0,m3);
	m0=_mm256_add_epi16(m0,m4);



	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);

	m1=_mm256_srli_si256(m0,4);
	m2=_mm256_add_epi16(m1,m2);

	//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
	//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
	m4=_mm256_and_si256(m0,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions

	m2=_mm256_add_epi16(m2,m4);

	//m2 has 16 16bit values now.
	m2 = _mm256_and_si256(m2,output_mask); //holds results in positions 0,3,6,9,12
	m1 = _mm256_slli_si256(m2,4); //put results in positions 2,5,8,11,14 , but 8 is not written
	m2=_mm256_and_si256(m2,mask_new);//extract only the 6th
	m2=_mm256_permute2f128_si256(m2,m2,1);
	m2=_mm256_srli_si256(m2,12);
	m2 = _mm256_add_epi16(m1,m2);
	output_row0_even = _mm256_add_epi16(output_row0_even,m2);

	//row=1
	//multiply with the mask
	m2=_mm256_maddubs_epi16(r2,c0_sh1);
	m3=_mm256_maddubs_epi16(r3,c1_sh1);
	m4=_mm256_maddubs_epi16(r4,c2_sh1);

	//vertical add
	m0=_mm256_add_epi16(m2,m3);
	m0=_mm256_add_epi16(m0,m4);


	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);

	m1=_mm256_srli_si256(m0,4);
	m2=_mm256_add_epi16(m1,m2);

	//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
	//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
	m4=_mm256_and_si256(m0,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions

	m2=_mm256_add_epi16(m2,m4);

	//m2 has 16 16bit values now.
	m2 = _mm256_and_si256(m2,output_mask); //holds results in positions 0,3,6,9,12
	m1 = _mm256_slli_si256(m2,4); //put results in positions 2,5,8,11,14 , but 8 is not written
	m2=_mm256_and_si256(m2,mask_new);//extract only the 6th
	m2=_mm256_permute2f128_si256(m2,m2,1);
	m2=_mm256_srli_si256(m2,12);
	m2 = _mm256_add_epi16(m1,m2);
	output_row1_even = _mm256_add_epi16(output_row1_even,m2);

             //6th col iteration
	//row=2
	//multiply with the mask
	m0=_mm256_maddubs_epi16(r0,c0_sh2);
	m1=_mm256_maddubs_epi16(r1,c1_sh2);
	m2=_mm256_maddubs_epi16(r2,c2_sh2);
	m3=_mm256_maddubs_epi16(r3,c1_sh2);
	m4=_mm256_maddubs_epi16(r4,c0_sh2);

	//vertical add
	m0=_mm256_add_epi16(m0,m1);
	m2=_mm256_add_epi16(m2,m3);
	m0=_mm256_add_epi16(m0,m2);
	m0=_mm256_add_epi16(m0,m4);


	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);
	//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,_mm256_extract_epi16(m2,8),0,0,0,0,0,0,0);
	m4=_mm256_and_si256(m2,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions

	m1=_mm256_srli_si256(m0,4);
	m2=_mm256_add_epi16(m1,m2);
	m2=_mm256_add_epi16(m2,m4);

	//m2 has 16 16bit values now.
	m2 = _mm256_and_si256(m2,output_mask_sh1);//holds results in positions 1,4,7,10,13
	m1 = _mm256_slli_si256(m2,2); //put results in positions 2,5,8,11,14 , but 8 is not written
	m2=_mm256_and_si256(m2,mask_new2);//extract only the 7th
	m2=_mm256_permute2f128_si256(m2,m2,1);
	m2=_mm256_srli_si256(m2,14);
	m2 = _mm256_add_epi16(m1,m2);
	output_odd = _mm256_add_epi16(output_odd,m2);


	//now division follows
	output_even=division(division_case,output_even,f);
	output_odd=division(division_case,output_odd,f);

	//shift odd 1 position and add to even
	output_odd = _mm256_slli_si256(output_odd,1);
	output_even = _mm256_add_epi8(output_even,output_odd);

	//row=0
	//multiply with the mask
	m1=_mm256_maddubs_epi16(r1,c0_sh2);
	m2=_mm256_maddubs_epi16(r2,c1_sh2);
	m3=_mm256_maddubs_epi16(r3,c2_sh2);
	m4=_mm256_maddubs_epi16(r4,c1_sh2);

	//vertical add
	m0=_mm256_add_epi16(m1,m2);
	m0=_mm256_add_epi16(m0,m3);
	m0=_mm256_add_epi16(m0,m4);


	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);
	//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,_mm256_extract_epi16(m2,8),0,0,0,0,0,0,0);
	m4=_mm256_and_si256(m2,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions

	m1=_mm256_srli_si256(m0,4);
	m2=_mm256_add_epi16(m1,m2);
	m2=_mm256_add_epi16(m2,m4);

	//m2 has 16 16bit values now.
	m2 = _mm256_and_si256(m2,output_mask_sh1);//holds results in positions 1,4,7,10,13
	m1 = _mm256_slli_si256(m2,2); //put results in positions 2,5,8,11,14 , but 8 is not written
	m2=_mm256_and_si256(m2,mask_new2);//extract only the 7th
	m2=_mm256_permute2f128_si256(m2,m2,1);
	m2=_mm256_srli_si256(m2,14);
	m2 = _mm256_add_epi16(m1,m2);
	output_row0_odd = _mm256_add_epi16(output_row0_odd,m2);


	//now division follows
	output_row0_even=division(division_case,output_row0_even,f);
	output_row0_odd=division(division_case,output_row0_odd,f);

	//shift odd 1 position and add to even
	output_row0_odd = _mm256_slli_si256(output_row0_odd,1);
	output_row0_even = _mm256_add_epi8(output_row0_even,output_row0_odd);

	//row=1
	//multiply with the mask
	m2=_mm256_maddubs_epi16(r2,c0_sh2);
	m3=_mm256_maddubs_epi16(r3,c1_sh2);
	m4=_mm256_maddubs_epi16(r4,c2_sh2);

	//vertical add
	m0=_mm256_add_epi16(m2,m3);
	m0=_mm256_add_epi16(m0,m4);


	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);
	//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,_mm256_extract_epi16(m2,8),0,0,0,0,0,0,0);
	m4=_mm256_and_si256(m2,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions

	m1=_mm256_srli_si256(m0,4);
	m2=_mm256_add_epi16(m1,m2);
	m2=_mm256_add_epi16(m2,m4);

	//m2 has 16 16bit values now.
	m2 = _mm256_and_si256(m2,output_mask_sh1);//holds results in positions 1,4,7,10,13
	m1 = _mm256_slli_si256(m2,2); //put results in positions 2,5,8,11,14 , but 8 is not written
	m2=_mm256_and_si256(m2,mask_new2);//extract only the 7th
	m2=_mm256_permute2f128_si256(m2,m2,1);
	m2=_mm256_srli_si256(m2,14);
	m2 = _mm256_add_epi16(m1,m2);
	output_row1_odd = _mm256_add_epi16(output_row1_odd,m2);


	//now division follows
	output_row1_even=division(division_case,output_row1_even,f);
	output_row1_odd=division(division_case,output_row1_odd,f);

	//shift odd 1 position and add to even
	output_row1_odd = _mm256_slli_si256(output_row1_odd,1);
	output_row1_even = _mm256_add_epi8(output_row1_even,output_row1_odd);




	switch (REMINDER_ITERATIONS){
	case 1:
		filt[N-3][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[N-2][col] = (unsigned char) _mm256_extract_epi8(output_row0_even,0);
		filt[N-1][col] = (unsigned char) _mm256_extract_epi8(output_row1_even,0);
		 break;
	case 2:
		filt[N-3][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[N-3][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);

		filt[N-2][col] = (unsigned char) _mm256_extract_epi8(output_row0_even,0);
		filt[N-2][col+1] = (unsigned char) _mm256_extract_epi8(output_row0_even,1);

		filt[N-1][col] = (unsigned char) _mm256_extract_epi8(output_row1_even,0);
		filt[N-1][col+1] = (unsigned char) _mm256_extract_epi8(output_row1_even,1);
		 break;
	case 3:
		filt[N-3][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[N-3][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[N-3][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);

		filt[N-2][col] = (unsigned char) _mm256_extract_epi8(output_row0_even,0);
		filt[N-2][col+1] = (unsigned char) _mm256_extract_epi8(output_row0_even,1);
		filt[N-2][col+2] = (unsigned char) _mm256_extract_epi8(output_row0_even,2);

		filt[N-1][col] = (unsigned char) _mm256_extract_epi8(output_row1_even,0);
		filt[N-1][col+1] = (unsigned char) _mm256_extract_epi8(output_row1_even,1);
		filt[N-1][col+2] = (unsigned char) _mm256_extract_epi8(output_row1_even,2);
		  break;
	case 4:
		filt[N-3][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[N-3][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[N-3][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		filt[N-3][col+3] = (unsigned char) _mm256_extract_epi8(output_even,3);

		filt[N-2][col] = (unsigned char) _mm256_extract_epi8(output_row0_even,0);
		filt[N-2][col+1] = (unsigned char) _mm256_extract_epi8(output_row0_even,1);
		filt[N-2][col+2] = (unsigned char) _mm256_extract_epi8(output_row0_even,2);
		filt[N-2][col+3] = (unsigned char) _mm256_extract_epi8(output_row0_even,3);

		filt[N-1][col] = (unsigned char) _mm256_extract_epi8(output_row1_even,0);
		filt[N-1][col+1] = (unsigned char) _mm256_extract_epi8(output_row1_even,1);
		filt[N-1][col+2] = (unsigned char) _mm256_extract_epi8(output_row1_even,2);
		filt[N-1][col+3] = (unsigned char) _mm256_extract_epi8(output_row1_even,3);
		  break;
	case 5:
		filt[N-3][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[N-3][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[N-3][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		filt[N-3][col+3] = (unsigned char) _mm256_extract_epi8(output_even,3);
		filt[N-3][col+4] = (unsigned char) _mm256_extract_epi8(output_even,4);

		filt[N-2][col] = (unsigned char) _mm256_extract_epi8(output_row0_even,0);
		filt[N-2][col+1] = (unsigned char) _mm256_extract_epi8(output_row0_even,1);
		filt[N-2][col+2] = (unsigned char) _mm256_extract_epi8(output_row0_even,2);
		filt[N-2][col+3] = (unsigned char) _mm256_extract_epi8(output_row0_even,3);
		filt[N-2][col+4] = (unsigned char) _mm256_extract_epi8(output_row0_even,4);

		filt[N-1][col] = (unsigned char) _mm256_extract_epi8(output_row1_even,0);
		filt[N-1][col+1] = (unsigned char) _mm256_extract_epi8(output_row1_even,1);
		filt[N-1][col+2] = (unsigned char) _mm256_extract_epi8(output_row1_even,2);
		filt[N-1][col+3] = (unsigned char) _mm256_extract_epi8(output_row1_even,3);
		filt[N-1][col+4] = (unsigned char) _mm256_extract_epi8(output_row1_even,4);
		  break;
	case 6:
		filt[N-3][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[N-3][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[N-3][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		filt[N-3][col+3] = (unsigned char) _mm256_extract_epi8(output_even,3);
		filt[N-3][col+4] = (unsigned char) _mm256_extract_epi8(output_even,4);
		filt[N-3][col+5] = (unsigned char) _mm256_extract_epi8(output_even,5);

		filt[N-2][col] = (unsigned char) _mm256_extract_epi8(output_row0_even,0);
		filt[N-2][col+1] = (unsigned char) _mm256_extract_epi8(output_row0_even,1);
		filt[N-2][col+2] = (unsigned char) _mm256_extract_epi8(output_row0_even,2);
		filt[N-2][col+3] = (unsigned char) _mm256_extract_epi8(output_row0_even,3);
		filt[N-2][col+4] = (unsigned char) _mm256_extract_epi8(output_row0_even,4);
		filt[N-2][col+5] = (unsigned char) _mm256_extract_epi8(output_row0_even,5);

		filt[N-1][col] = (unsigned char) _mm256_extract_epi8(output_row1_even,0);
		filt[N-1][col+1] = (unsigned char) _mm256_extract_epi8(output_row1_even,1);
		filt[N-1][col+2] = (unsigned char) _mm256_extract_epi8(output_row1_even,2);
		filt[N-1][col+3] = (unsigned char) _mm256_extract_epi8(output_row1_even,3);
		filt[N-1][col+4] = (unsigned char) _mm256_extract_epi8(output_row1_even,4);
		filt[N-1][col+5] = (unsigned char) _mm256_extract_epi8(output_row1_even,5);
		  break;
	case 7:
		filt[N-3][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[N-3][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[N-3][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		filt[N-3][col+3] = (unsigned char) _mm256_extract_epi8(output_even,3);
		filt[N-3][col+4] = (unsigned char) _mm256_extract_epi8(output_even,4);
		filt[N-3][col+5] = (unsigned char) _mm256_extract_epi8(output_even,5);
		filt[N-3][col+6] = (unsigned char) _mm256_extract_epi8(output_even,6);

		filt[N-2][col] = (unsigned char) _mm256_extract_epi8(output_row0_even,0);
		filt[N-2][col+1] = (unsigned char) _mm256_extract_epi8(output_row0_even,1);
		filt[N-2][col+2] = (unsigned char) _mm256_extract_epi8(output_row0_even,2);
		filt[N-2][col+3] = (unsigned char) _mm256_extract_epi8(output_row0_even,3);
		filt[N-2][col+4] = (unsigned char) _mm256_extract_epi8(output_row0_even,4);
		filt[N-2][col+5] = (unsigned char) _mm256_extract_epi8(output_row0_even,5);
		filt[N-2][col+6] = (unsigned char) _mm256_extract_epi8(output_row0_even,6);

		filt[N-1][col] = (unsigned char) _mm256_extract_epi8(output_row1_even,0);
		filt[N-1][col+1] = (unsigned char) _mm256_extract_epi8(output_row1_even,1);
		filt[N-1][col+2] = (unsigned char) _mm256_extract_epi8(output_row1_even,2);
		filt[N-1][col+3] = (unsigned char) _mm256_extract_epi8(output_row1_even,3);
		filt[N-1][col+4] = (unsigned char) _mm256_extract_epi8(output_row1_even,4);
		filt[N-1][col+5] = (unsigned char) _mm256_extract_epi8(output_row1_even,5);
		filt[N-1][col+6] = (unsigned char) _mm256_extract_epi8(output_row1_even,6);
		  break;
	case 8:
		filt[N-3][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[N-3][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[N-3][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		filt[N-3][col+3] = (unsigned char) _mm256_extract_epi8(output_even,3);
		filt[N-3][col+4] = (unsigned char) _mm256_extract_epi8(output_even,4);
		filt[N-3][col+5] = (unsigned char) _mm256_extract_epi8(output_even,5);
		filt[N-3][col+6] = (unsigned char) _mm256_extract_epi8(output_even,6);
		filt[N-3][col+7] = (unsigned char) _mm256_extract_epi8(output_even,7);

		filt[N-2][col] = (unsigned char) _mm256_extract_epi8(output_row0_even,0);
		filt[N-2][col+1] = (unsigned char) _mm256_extract_epi8(output_row0_even,1);
		filt[N-2][col+2] = (unsigned char) _mm256_extract_epi8(output_row0_even,2);
		filt[N-2][col+3] = (unsigned char) _mm256_extract_epi8(output_row0_even,3);
		filt[N-2][col+4] = (unsigned char) _mm256_extract_epi8(output_row0_even,4);
		filt[N-2][col+5] = (unsigned char) _mm256_extract_epi8(output_row0_even,5);
		filt[N-2][col+6] = (unsigned char) _mm256_extract_epi8(output_row0_even,6);
		filt[N-2][col+7] = (unsigned char) _mm256_extract_epi8(output_row0_even,7);

		filt[N-1][col] = (unsigned char) _mm256_extract_epi8(output_row1_even,0);
		filt[N-1][col+1] = (unsigned char) _mm256_extract_epi8(output_row1_even,1);
		filt[N-1][col+2] = (unsigned char) _mm256_extract_epi8(output_row1_even,2);
		filt[N-1][col+3] = (unsigned char) _mm256_extract_epi8(output_row1_even,3);
		filt[N-1][col+4] = (unsigned char) _mm256_extract_epi8(output_row1_even,4);
		filt[N-1][col+5] = (unsigned char) _mm256_extract_epi8(output_row1_even,5);
		filt[N-1][col+6] = (unsigned char) _mm256_extract_epi8(output_row1_even,6);
		filt[N-1][col+7] = (unsigned char) _mm256_extract_epi8(output_row1_even,7);
		  break;
	case 9:
		filt[N-3][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[N-3][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[N-3][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		filt[N-3][col+3] = (unsigned char) _mm256_extract_epi8(output_even,3);
		filt[N-3][col+4] = (unsigned char) _mm256_extract_epi8(output_even,4);
		filt[N-3][col+5] = (unsigned char) _mm256_extract_epi8(output_even,5);
		filt[N-3][col+6] = (unsigned char) _mm256_extract_epi8(output_even,6);
		filt[N-3][col+7] = (unsigned char) _mm256_extract_epi8(output_even,7);
		filt[N-3][col+8] = (unsigned char) _mm256_extract_epi8(output_even,8);

		filt[N-2][col] = (unsigned char) _mm256_extract_epi8(output_row0_even,0);
		filt[N-2][col+1] = (unsigned char) _mm256_extract_epi8(output_row0_even,1);
		filt[N-2][col+2] = (unsigned char) _mm256_extract_epi8(output_row0_even,2);
		filt[N-2][col+3] = (unsigned char) _mm256_extract_epi8(output_row0_even,3);
		filt[N-2][col+4] = (unsigned char) _mm256_extract_epi8(output_row0_even,4);
		filt[N-2][col+5] = (unsigned char) _mm256_extract_epi8(output_row0_even,5);
		filt[N-2][col+6] = (unsigned char) _mm256_extract_epi8(output_row0_even,6);
		filt[N-2][col+7] = (unsigned char) _mm256_extract_epi8(output_row0_even,7);
		filt[N-2][col+8] = (unsigned char) _mm256_extract_epi8(output_row0_even,8);

		filt[N-1][col] = (unsigned char) _mm256_extract_epi8(output_row1_even,0);
		filt[N-1][col+1] = (unsigned char) _mm256_extract_epi8(output_row1_even,1);
		filt[N-1][col+2] = (unsigned char) _mm256_extract_epi8(output_row1_even,2);
		filt[N-1][col+3] = (unsigned char) _mm256_extract_epi8(output_row1_even,3);
		filt[N-1][col+4] = (unsigned char) _mm256_extract_epi8(output_row1_even,4);
		filt[N-1][col+5] = (unsigned char) _mm256_extract_epi8(output_row1_even,5);
		filt[N-1][col+6] = (unsigned char) _mm256_extract_epi8(output_row1_even,6);
		filt[N-1][col+7] = (unsigned char) _mm256_extract_epi8(output_row1_even,7);
		filt[N-1][col+8] = (unsigned char) _mm256_extract_epi8(output_row1_even,8);
		  break;
	case 10:
		filt[N-3][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[N-3][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[N-3][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		filt[N-3][col+3] = (unsigned char) _mm256_extract_epi8(output_even,3);
		filt[N-3][col+4] = (unsigned char) _mm256_extract_epi8(output_even,4);
		filt[N-3][col+5] = (unsigned char) _mm256_extract_epi8(output_even,5);
		filt[N-3][col+6] = (unsigned char) _mm256_extract_epi8(output_even,6);
		filt[N-3][col+7] = (unsigned char) _mm256_extract_epi8(output_even,7);
		filt[N-3][col+8] = (unsigned char) _mm256_extract_epi8(output_even,8);
		filt[N-3][col+9] = (unsigned char) _mm256_extract_epi8(output_even,9);

		filt[N-2][col] = (unsigned char) _mm256_extract_epi8(output_row0_even,0);
		filt[N-2][col+1] = (unsigned char) _mm256_extract_epi8(output_row0_even,1);
		filt[N-2][col+2] = (unsigned char) _mm256_extract_epi8(output_row0_even,2);
		filt[N-2][col+3] = (unsigned char) _mm256_extract_epi8(output_row0_even,3);
		filt[N-2][col+4] = (unsigned char) _mm256_extract_epi8(output_row0_even,4);
		filt[N-2][col+5] = (unsigned char) _mm256_extract_epi8(output_row0_even,5);
		filt[N-2][col+6] = (unsigned char) _mm256_extract_epi8(output_row0_even,6);
		filt[N-2][col+7] = (unsigned char) _mm256_extract_epi8(output_row0_even,7);
		filt[N-2][col+8] = (unsigned char) _mm256_extract_epi8(output_row0_even,8);
		filt[N-2][col+9] = (unsigned char) _mm256_extract_epi8(output_row0_even,9);

		filt[N-1][col] = (unsigned char) _mm256_extract_epi8(output_row1_even,0);
		filt[N-1][col+1] = (unsigned char) _mm256_extract_epi8(output_row1_even,1);
		filt[N-1][col+2] = (unsigned char) _mm256_extract_epi8(output_row1_even,2);
		filt[N-1][col+3] = (unsigned char) _mm256_extract_epi8(output_row1_even,3);
		filt[N-1][col+4] = (unsigned char) _mm256_extract_epi8(output_row1_even,4);
		filt[N-1][col+5] = (unsigned char) _mm256_extract_epi8(output_row1_even,5);
		filt[N-1][col+6] = (unsigned char) _mm256_extract_epi8(output_row1_even,6);
		filt[N-1][col+7] = (unsigned char) _mm256_extract_epi8(output_row1_even,7);
		filt[N-1][col+8] = (unsigned char) _mm256_extract_epi8(output_row1_even,8);
		filt[N-1][col+9] = (unsigned char) _mm256_extract_epi8(output_row1_even,9);
		  break;
	case 11:
		filt[N-3][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[N-3][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[N-3][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		filt[N-3][col+3] = (unsigned char) _mm256_extract_epi8(output_even,3);
		filt[N-3][col+4] = (unsigned char) _mm256_extract_epi8(output_even,4);
		filt[N-3][col+5] = (unsigned char) _mm256_extract_epi8(output_even,5);
		filt[N-3][col+6] = (unsigned char) _mm256_extract_epi8(output_even,6);
		filt[N-3][col+7] = (unsigned char) _mm256_extract_epi8(output_even,7);
		filt[N-3][col+8] = (unsigned char) _mm256_extract_epi8(output_even,8);
		filt[N-3][col+9] = (unsigned char) _mm256_extract_epi8(output_even,9);
		filt[N-3][col+10] = (unsigned char) _mm256_extract_epi8(output_even,10);

		filt[N-2][col] = (unsigned char) _mm256_extract_epi8(output_row0_even,0);
		filt[N-2][col+1] = (unsigned char) _mm256_extract_epi8(output_row0_even,1);
		filt[N-2][col+2] = (unsigned char) _mm256_extract_epi8(output_row0_even,2);
		filt[N-2][col+3] = (unsigned char) _mm256_extract_epi8(output_row0_even,3);
		filt[N-2][col+4] = (unsigned char) _mm256_extract_epi8(output_row0_even,4);
		filt[N-2][col+5] = (unsigned char) _mm256_extract_epi8(output_row0_even,5);
		filt[N-2][col+6] = (unsigned char) _mm256_extract_epi8(output_row0_even,6);
		filt[N-2][col+7] = (unsigned char) _mm256_extract_epi8(output_row0_even,7);
		filt[N-2][col+8] = (unsigned char) _mm256_extract_epi8(output_row0_even,8);
		filt[N-2][col+9] = (unsigned char) _mm256_extract_epi8(output_row0_even,9);
		filt[N-2][col+10] = (unsigned char) _mm256_extract_epi8(output_row0_even,10);

		filt[N-1][col] = (unsigned char) _mm256_extract_epi8(output_row1_even,0);
		filt[N-1][col+1] = (unsigned char) _mm256_extract_epi8(output_row1_even,1);
		filt[N-1][col+2] = (unsigned char) _mm256_extract_epi8(output_row1_even,2);
		filt[N-1][col+3] = (unsigned char) _mm256_extract_epi8(output_row1_even,3);
		filt[N-1][col+4] = (unsigned char) _mm256_extract_epi8(output_row1_even,4);
		filt[N-1][col+5] = (unsigned char) _mm256_extract_epi8(output_row1_even,5);
		filt[N-1][col+6] = (unsigned char) _mm256_extract_epi8(output_row1_even,6);
		filt[N-1][col+7] = (unsigned char) _mm256_extract_epi8(output_row1_even,7);
		filt[N-1][col+8] = (unsigned char) _mm256_extract_epi8(output_row1_even,8);
		filt[N-1][col+9] = (unsigned char) _mm256_extract_epi8(output_row1_even,9);
		filt[N-1][col+10] = (unsigned char) _mm256_extract_epi8(output_row1_even,10);
		  break;
	case 12:
		filt[N-3][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[N-3][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[N-3][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		filt[N-3][col+3] = (unsigned char) _mm256_extract_epi8(output_even,3);
		filt[N-3][col+4] = (unsigned char) _mm256_extract_epi8(output_even,4);
		filt[N-3][col+5] = (unsigned char) _mm256_extract_epi8(output_even,5);
		filt[N-3][col+6] = (unsigned char) _mm256_extract_epi8(output_even,6);
		filt[N-3][col+7] = (unsigned char) _mm256_extract_epi8(output_even,7);
		filt[N-3][col+8] = (unsigned char) _mm256_extract_epi8(output_even,8);
		filt[N-3][col+9] = (unsigned char) _mm256_extract_epi8(output_even,9);
		filt[N-3][col+10] = (unsigned char) _mm256_extract_epi8(output_even,10);
		filt[N-3][col+11] = (unsigned char) _mm256_extract_epi8(output_even,11);

		filt[N-2][col] = (unsigned char) _mm256_extract_epi8(output_row0_even,0);
		filt[N-2][col+1] = (unsigned char) _mm256_extract_epi8(output_row0_even,1);
		filt[N-2][col+2] = (unsigned char) _mm256_extract_epi8(output_row0_even,2);
		filt[N-2][col+3] = (unsigned char) _mm256_extract_epi8(output_row0_even,3);
		filt[N-2][col+4] = (unsigned char) _mm256_extract_epi8(output_row0_even,4);
		filt[N-2][col+5] = (unsigned char) _mm256_extract_epi8(output_row0_even,5);
		filt[N-2][col+6] = (unsigned char) _mm256_extract_epi8(output_row0_even,6);
		filt[N-2][col+7] = (unsigned char) _mm256_extract_epi8(output_row0_even,7);
		filt[N-2][col+8] = (unsigned char) _mm256_extract_epi8(output_row0_even,8);
		filt[N-2][col+9] = (unsigned char) _mm256_extract_epi8(output_row0_even,9);
		filt[N-2][col+10] = (unsigned char) _mm256_extract_epi8(output_row0_even,10);
		filt[N-2][col+11] = (unsigned char) _mm256_extract_epi8(output_row0_even,11);

		filt[N-1][col] = (unsigned char) _mm256_extract_epi8(output_row1_even,0);
		filt[N-1][col+1] = (unsigned char) _mm256_extract_epi8(output_row1_even,1);
		filt[N-1][col+2] = (unsigned char) _mm256_extract_epi8(output_row1_even,2);
		filt[N-1][col+3] = (unsigned char) _mm256_extract_epi8(output_row1_even,3);
		filt[N-1][col+4] = (unsigned char) _mm256_extract_epi8(output_row1_even,4);
		filt[N-1][col+5] = (unsigned char) _mm256_extract_epi8(output_row1_even,5);
		filt[N-1][col+6] = (unsigned char) _mm256_extract_epi8(output_row1_even,6);
		filt[N-1][col+7] = (unsigned char) _mm256_extract_epi8(output_row1_even,7);
		filt[N-1][col+8] = (unsigned char) _mm256_extract_epi8(output_row1_even,8);
		filt[N-1][col+9] = (unsigned char) _mm256_extract_epi8(output_row1_even,9);
		filt[N-1][col+10] = (unsigned char) _mm256_extract_epi8(output_row1_even,10);
		filt[N-1][col+11] = (unsigned char) _mm256_extract_epi8(output_row1_even,11);
		  break;
	case 13:
		filt[N-3][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[N-3][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[N-3][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		filt[N-3][col+3] = (unsigned char) _mm256_extract_epi8(output_even,3);
		filt[N-3][col+4] = (unsigned char) _mm256_extract_epi8(output_even,4);
		filt[N-3][col+5] = (unsigned char) _mm256_extract_epi8(output_even,5);
		filt[N-3][col+6] = (unsigned char) _mm256_extract_epi8(output_even,6);
		filt[N-3][col+7] = (unsigned char) _mm256_extract_epi8(output_even,7);
		filt[N-3][col+8] = (unsigned char) _mm256_extract_epi8(output_even,8);
		filt[N-3][col+9] = (unsigned char) _mm256_extract_epi8(output_even,9);
		filt[N-3][col+10] = (unsigned char) _mm256_extract_epi8(output_even,10);
		filt[N-3][col+11] = (unsigned char) _mm256_extract_epi8(output_even,11);
		filt[N-3][col+12] = (unsigned char) _mm256_extract_epi8(output_even,12);

		filt[N-2][col] = (unsigned char) _mm256_extract_epi8(output_row0_even,0);
		filt[N-2][col+1] = (unsigned char) _mm256_extract_epi8(output_row0_even,1);
		filt[N-2][col+2] = (unsigned char) _mm256_extract_epi8(output_row0_even,2);
		filt[N-2][col+3] = (unsigned char) _mm256_extract_epi8(output_row0_even,3);
		filt[N-2][col+4] = (unsigned char) _mm256_extract_epi8(output_row0_even,4);
		filt[N-2][col+5] = (unsigned char) _mm256_extract_epi8(output_row0_even,5);
		filt[N-2][col+6] = (unsigned char) _mm256_extract_epi8(output_row0_even,6);
		filt[N-2][col+7] = (unsigned char) _mm256_extract_epi8(output_row0_even,7);
		filt[N-2][col+8] = (unsigned char) _mm256_extract_epi8(output_row0_even,8);
		filt[N-2][col+9] = (unsigned char) _mm256_extract_epi8(output_row0_even,9);
		filt[N-2][col+10] = (unsigned char) _mm256_extract_epi8(output_row0_even,10);
		filt[N-2][col+11] = (unsigned char) _mm256_extract_epi8(output_row0_even,11);
		filt[N-2][col+12] = (unsigned char) _mm256_extract_epi8(output_row0_even,12);

		filt[N-1][col] = (unsigned char) _mm256_extract_epi8(output_row1_even,0);
		filt[N-1][col+1] = (unsigned char) _mm256_extract_epi8(output_row1_even,1);
		filt[N-1][col+2] = (unsigned char) _mm256_extract_epi8(output_row1_even,2);
		filt[N-1][col+3] = (unsigned char) _mm256_extract_epi8(output_row1_even,3);
		filt[N-1][col+4] = (unsigned char) _mm256_extract_epi8(output_row1_even,4);
		filt[N-1][col+5] = (unsigned char) _mm256_extract_epi8(output_row1_even,5);
		filt[N-1][col+6] = (unsigned char) _mm256_extract_epi8(output_row1_even,6);
		filt[N-1][col+7] = (unsigned char) _mm256_extract_epi8(output_row1_even,7);
		filt[N-1][col+8] = (unsigned char) _mm256_extract_epi8(output_row1_even,8);
		filt[N-1][col+9] = (unsigned char) _mm256_extract_epi8(output_row1_even,9);
		filt[N-1][col+10] = (unsigned char) _mm256_extract_epi8(output_row1_even,10);
		filt[N-1][col+11] = (unsigned char) _mm256_extract_epi8(output_row1_even,11);
		filt[N-1][col+12] = (unsigned char) _mm256_extract_epi8(output_row1_even,12);
		  break;
	case 14:
		filt[N-3][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[N-3][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[N-3][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		filt[N-3][col+3] = (unsigned char) _mm256_extract_epi8(output_even,3);
		filt[N-3][col+4] = (unsigned char) _mm256_extract_epi8(output_even,4);
		filt[N-3][col+5] = (unsigned char) _mm256_extract_epi8(output_even,5);
		filt[N-3][col+6] = (unsigned char) _mm256_extract_epi8(output_even,6);
		filt[N-3][col+7] = (unsigned char) _mm256_extract_epi8(output_even,7);
		filt[N-3][col+8] = (unsigned char) _mm256_extract_epi8(output_even,8);
		filt[N-3][col+9] = (unsigned char) _mm256_extract_epi8(output_even,9);
		filt[N-3][col+10] = (unsigned char) _mm256_extract_epi8(output_even,10);
		filt[N-3][col+11] = (unsigned char) _mm256_extract_epi8(output_even,11);
		filt[N-3][col+12] = (unsigned char) _mm256_extract_epi8(output_even,12);
		filt[N-3][col+13] = (unsigned char) _mm256_extract_epi8(output_even,13);

		filt[N-2][col] = (unsigned char) _mm256_extract_epi8(output_row0_even,0);
		filt[N-2][col+1] = (unsigned char) _mm256_extract_epi8(output_row0_even,1);
		filt[N-2][col+2] = (unsigned char) _mm256_extract_epi8(output_row0_even,2);
		filt[N-2][col+3] = (unsigned char) _mm256_extract_epi8(output_row0_even,3);
		filt[N-2][col+4] = (unsigned char) _mm256_extract_epi8(output_row0_even,4);
		filt[N-2][col+5] = (unsigned char) _mm256_extract_epi8(output_row0_even,5);
		filt[N-2][col+6] = (unsigned char) _mm256_extract_epi8(output_row0_even,6);
		filt[N-2][col+7] = (unsigned char) _mm256_extract_epi8(output_row0_even,7);
		filt[N-2][col+8] = (unsigned char) _mm256_extract_epi8(output_row0_even,8);
		filt[N-2][col+9] = (unsigned char) _mm256_extract_epi8(output_row0_even,9);
		filt[N-2][col+10] = (unsigned char) _mm256_extract_epi8(output_row0_even,10);
		filt[N-2][col+11] = (unsigned char) _mm256_extract_epi8(output_row0_even,11);
		filt[N-2][col+12] = (unsigned char) _mm256_extract_epi8(output_row0_even,12);
		filt[N-2][col+13] = (unsigned char) _mm256_extract_epi8(output_row0_even,13);

		filt[N-1][col] = (unsigned char) _mm256_extract_epi8(output_row1_even,0);
		filt[N-1][col+1] = (unsigned char) _mm256_extract_epi8(output_row1_even,1);
		filt[N-1][col+2] = (unsigned char) _mm256_extract_epi8(output_row1_even,2);
		filt[N-1][col+3] = (unsigned char) _mm256_extract_epi8(output_row1_even,3);
		filt[N-1][col+4] = (unsigned char) _mm256_extract_epi8(output_row1_even,4);
		filt[N-1][col+5] = (unsigned char) _mm256_extract_epi8(output_row1_even,5);
		filt[N-1][col+6] = (unsigned char) _mm256_extract_epi8(output_row1_even,6);
		filt[N-1][col+7] = (unsigned char) _mm256_extract_epi8(output_row1_even,7);
		filt[N-1][col+8] = (unsigned char) _mm256_extract_epi8(output_row1_even,8);
		filt[N-1][col+9] = (unsigned char) _mm256_extract_epi8(output_row1_even,9);
		filt[N-1][col+10] = (unsigned char) _mm256_extract_epi8(output_row1_even,10);
		filt[N-1][col+11] = (unsigned char) _mm256_extract_epi8(output_row1_even,11);
		filt[N-1][col+12] = (unsigned char) _mm256_extract_epi8(output_row1_even,12);
		filt[N-1][col+13] = (unsigned char) _mm256_extract_epi8(output_row1_even,13);
		  break;
	case 15:
		filt[N-3][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[N-3][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[N-3][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		filt[N-3][col+3] = (unsigned char) _mm256_extract_epi8(output_even,3);
		filt[N-3][col+4] = (unsigned char) _mm256_extract_epi8(output_even,4);
		filt[N-3][col+5] = (unsigned char) _mm256_extract_epi8(output_even,5);
		filt[N-3][col+6] = (unsigned char) _mm256_extract_epi8(output_even,6);
		filt[N-3][col+7] = (unsigned char) _mm256_extract_epi8(output_even,7);
		filt[N-3][col+8] = (unsigned char) _mm256_extract_epi8(output_even,8);
		filt[N-3][col+9] = (unsigned char) _mm256_extract_epi8(output_even,9);
		filt[N-3][col+10] = (unsigned char) _mm256_extract_epi8(output_even,10);
		filt[N-3][col+11] = (unsigned char) _mm256_extract_epi8(output_even,11);
		filt[N-3][col+12] = (unsigned char) _mm256_extract_epi8(output_even,12);
		filt[N-3][col+13] = (unsigned char) _mm256_extract_epi8(output_even,13);
		filt[N-3][col+14] = (unsigned char) _mm256_extract_epi8(output_even,14);

		filt[N-2][col] = (unsigned char) _mm256_extract_epi8(output_row0_even,0);
		filt[N-2][col+1] = (unsigned char) _mm256_extract_epi8(output_row0_even,1);
		filt[N-2][col+2] = (unsigned char) _mm256_extract_epi8(output_row0_even,2);
		filt[N-2][col+3] = (unsigned char) _mm256_extract_epi8(output_row0_even,3);
		filt[N-2][col+4] = (unsigned char) _mm256_extract_epi8(output_row0_even,4);
		filt[N-2][col+5] = (unsigned char) _mm256_extract_epi8(output_row0_even,5);
		filt[N-2][col+6] = (unsigned char) _mm256_extract_epi8(output_row0_even,6);
		filt[N-2][col+7] = (unsigned char) _mm256_extract_epi8(output_row0_even,7);
		filt[N-2][col+8] = (unsigned char) _mm256_extract_epi8(output_row0_even,8);
		filt[N-2][col+9] = (unsigned char) _mm256_extract_epi8(output_row0_even,9);
		filt[N-2][col+10] = (unsigned char) _mm256_extract_epi8(output_row0_even,10);
		filt[N-2][col+11] = (unsigned char) _mm256_extract_epi8(output_row0_even,11);
		filt[N-2][col+12] = (unsigned char) _mm256_extract_epi8(output_row0_even,12);
		filt[N-2][col+13] = (unsigned char) _mm256_extract_epi8(output_row0_even,13);
		filt[N-2][col+14] = (unsigned char) _mm256_extract_epi8(output_row0_even,14);

		filt[N-1][col] = (unsigned char) _mm256_extract_epi8(output_row1_even,0);
		filt[N-1][col+1] = (unsigned char) _mm256_extract_epi8(output_row1_even,1);
		filt[N-1][col+2] = (unsigned char) _mm256_extract_epi8(output_row1_even,2);
		filt[N-1][col+3] = (unsigned char) _mm256_extract_epi8(output_row1_even,3);
		filt[N-1][col+4] = (unsigned char) _mm256_extract_epi8(output_row1_even,4);
		filt[N-1][col+5] = (unsigned char) _mm256_extract_epi8(output_row1_even,5);
		filt[N-1][col+6] = (unsigned char) _mm256_extract_epi8(output_row1_even,6);
		filt[N-1][col+7] = (unsigned char) _mm256_extract_epi8(output_row1_even,7);
		filt[N-1][col+8] = (unsigned char) _mm256_extract_epi8(output_row1_even,8);
		filt[N-1][col+9] = (unsigned char) _mm256_extract_epi8(output_row1_even,9);
		filt[N-1][col+10] = (unsigned char) _mm256_extract_epi8(output_row1_even,10);
		filt[N-1][col+11] = (unsigned char) _mm256_extract_epi8(output_row1_even,11);
		filt[N-1][col+12] = (unsigned char) _mm256_extract_epi8(output_row1_even,12);
		filt[N-1][col+13] = (unsigned char) _mm256_extract_epi8(output_row1_even,13);
		filt[N-1][col+14] = (unsigned char) _mm256_extract_epi8(output_row1_even,14);
		  break;
	case 16:
		_mm_storeu_si128( (__m128i *) &filt[N-3][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
		_mm_storeu_si128( (__m128i *) &filt[N-2][col],_mm256_extractf128_si256(output_row0_even, 0)); //store low 128bit - 16pixels
		_mm_storeu_si128( (__m128i *) &filt[N-1][col],_mm256_extractf128_si256(output_row1_even, 0)); //store low 128bit - 16pixels
		  break;
	case 17:
		_mm_storeu_si128( (__m128i *) &filt[N-3][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
		filt[N-3][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);

		_mm_storeu_si128( (__m128i *) &filt[N-2][col],_mm256_extractf128_si256(output_row0_even, 0)); //store low 128bit - 16pixels
		filt[N-2][col+16] = (unsigned char) _mm256_extract_epi8(output_row0_even,16);

		_mm_storeu_si128( (__m128i *) &filt[N-1][col],_mm256_extractf128_si256(output_row1_even, 0)); //store low 128bit - 16pixels
		filt[N-1][col+16] = (unsigned char) _mm256_extract_epi8(output_row1_even,16);
		  break;
	case 18:
		_mm_storeu_si128( (__m128i *) &filt[N-3][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
		filt[N-3][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
		filt[N-3][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);

		_mm_storeu_si128( (__m128i *) &filt[N-2][col],_mm256_extractf128_si256(output_row0_even, 0)); //store low 128bit - 16pixels
		filt[N-2][col+16] = (unsigned char) _mm256_extract_epi8(output_row0_even,16);
		filt[N-2][col+17] = (unsigned char) _mm256_extract_epi8(output_row0_even,17);

		_mm_storeu_si128( (__m128i *) &filt[N-1][col],_mm256_extractf128_si256(output_row1_even, 0)); //store low 128bit - 16pixels
		filt[N-1][col+16] = (unsigned char) _mm256_extract_epi8(output_row1_even,16);
		filt[N-1][col+17] = (unsigned char) _mm256_extract_epi8(output_row1_even,17);
		  break;
	case 19:
		_mm_storeu_si128( (__m128i *) &filt[N-3][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
		filt[N-3][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
		filt[N-3][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
		filt[N-3][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);

		_mm_storeu_si128( (__m128i *) &filt[N-2][col],_mm256_extractf128_si256(output_row0_even, 0)); //store low 128bit - 16pixels
		filt[N-2][col+16] = (unsigned char) _mm256_extract_epi8(output_row0_even,16);
		filt[N-2][col+17] = (unsigned char) _mm256_extract_epi8(output_row0_even,17);
		filt[N-2][col+18] = (unsigned char) _mm256_extract_epi8(output_row0_even,18);

		_mm_storeu_si128( (__m128i *) &filt[N-1][col],_mm256_extractf128_si256(output_row1_even, 0)); //store low 128bit - 16pixels
		filt[N-1][col+16] = (unsigned char) _mm256_extract_epi8(output_row1_even,16);
		filt[N-1][col+17] = (unsigned char) _mm256_extract_epi8(output_row1_even,17);
		filt[N-1][col+18] = (unsigned char) _mm256_extract_epi8(output_row1_even,18);
		  break;
	case 20:
		_mm_storeu_si128( (__m128i *) &filt[N-3][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
		filt[N-3][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
		filt[N-3][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
		filt[N-3][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
		filt[N-3][col+19] = (unsigned char) _mm256_extract_epi8(output_even,19);

		_mm_storeu_si128( (__m128i *) &filt[N-2][col],_mm256_extractf128_si256(output_row0_even, 0)); //store low 128bit - 16pixels
		filt[N-2][col+16] = (unsigned char) _mm256_extract_epi8(output_row0_even,16);
		filt[N-2][col+17] = (unsigned char) _mm256_extract_epi8(output_row0_even,17);
		filt[N-2][col+18] = (unsigned char) _mm256_extract_epi8(output_row0_even,18);
		filt[N-2][col+19] = (unsigned char) _mm256_extract_epi8(output_row0_even,19);

		_mm_storeu_si128( (__m128i *) &filt[N-1][col],_mm256_extractf128_si256(output_row1_even, 0)); //store low 128bit - 16pixels
		filt[N-1][col+16] = (unsigned char) _mm256_extract_epi8(output_row1_even,16);
		filt[N-1][col+17] = (unsigned char) _mm256_extract_epi8(output_row1_even,17);
		filt[N-1][col+18] = (unsigned char) _mm256_extract_epi8(output_row1_even,18);
		filt[N-1][col+19] = (unsigned char) _mm256_extract_epi8(output_row1_even,19);
		  break;
	case 21:
		_mm_storeu_si128( (__m128i *) &filt[N-3][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
		filt[N-3][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
		filt[N-3][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
		filt[N-3][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
		filt[N-3][col+19] = (unsigned char) _mm256_extract_epi8(output_even,19);
		filt[N-3][col+20] = (unsigned char) _mm256_extract_epi8(output_even,20);

		_mm_storeu_si128( (__m128i *) &filt[N-2][col],_mm256_extractf128_si256(output_row0_even, 0)); //store low 128bit - 16pixels
		filt[N-2][col+16] = (unsigned char) _mm256_extract_epi8(output_row0_even,16);
		filt[N-2][col+17] = (unsigned char) _mm256_extract_epi8(output_row0_even,17);
		filt[N-2][col+18] = (unsigned char) _mm256_extract_epi8(output_row0_even,18);
		filt[N-2][col+19] = (unsigned char) _mm256_extract_epi8(output_row0_even,19);
		filt[N-2][col+20] = (unsigned char) _mm256_extract_epi8(output_row0_even,20);

		_mm_storeu_si128( (__m128i *) &filt[N-1][col],_mm256_extractf128_si256(output_row1_even, 0)); //store low 128bit - 16pixels
		filt[N-1][col+16] = (unsigned char) _mm256_extract_epi8(output_row1_even,16);
		filt[N-1][col+17] = (unsigned char) _mm256_extract_epi8(output_row1_even,17);
		filt[N-1][col+18] = (unsigned char) _mm256_extract_epi8(output_row1_even,18);
		filt[N-1][col+19] = (unsigned char) _mm256_extract_epi8(output_row1_even,19);
		filt[N-1][col+20] = (unsigned char) _mm256_extract_epi8(output_row1_even,20);
		  break;
	case 22:
		_mm_storeu_si128( (__m128i *) &filt[N-3][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
		filt[N-3][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
		filt[N-3][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
		filt[N-3][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
		filt[N-3][col+19] = (unsigned char) _mm256_extract_epi8(output_even,19);
		filt[N-3][col+20] = (unsigned char) _mm256_extract_epi8(output_even,20);
		filt[N-3][col+21] = (unsigned char) _mm256_extract_epi8(output_even,21);

		_mm_storeu_si128( (__m128i *) &filt[N-2][col],_mm256_extractf128_si256(output_row0_even, 0)); //store low 128bit - 16pixels
		filt[N-2][col+16] = (unsigned char) _mm256_extract_epi8(output_row0_even,16);
		filt[N-2][col+17] = (unsigned char) _mm256_extract_epi8(output_row0_even,17);
		filt[N-2][col+18] = (unsigned char) _mm256_extract_epi8(output_row0_even,18);
		filt[N-2][col+19] = (unsigned char) _mm256_extract_epi8(output_row0_even,19);
		filt[N-2][col+20] = (unsigned char) _mm256_extract_epi8(output_row0_even,20);
		filt[N-2][col+21] = (unsigned char) _mm256_extract_epi8(output_row0_even,21);

		_mm_storeu_si128( (__m128i *) &filt[N-1][col],_mm256_extractf128_si256(output_row1_even, 0)); //store low 128bit - 16pixels
		filt[N-1][col+16] = (unsigned char) _mm256_extract_epi8(output_row1_even,16);
		filt[N-1][col+17] = (unsigned char) _mm256_extract_epi8(output_row1_even,17);
		filt[N-1][col+18] = (unsigned char) _mm256_extract_epi8(output_row1_even,18);
		filt[N-1][col+19] = (unsigned char) _mm256_extract_epi8(output_row1_even,19);
		filt[N-1][col+20] = (unsigned char) _mm256_extract_epi8(output_row1_even,20);
		filt[N-1][col+21] = (unsigned char) _mm256_extract_epi8(output_row1_even,21);
		  break;
	case 23:
		_mm_storeu_si128( (__m128i *) &filt[N-3][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
		filt[N-3][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
		filt[N-3][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
		filt[N-3][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
		filt[N-3][col+19] = (unsigned char) _mm256_extract_epi8(output_even,19);
		filt[N-3][col+20] = (unsigned char) _mm256_extract_epi8(output_even,20);
		filt[N-3][col+21] = (unsigned char) _mm256_extract_epi8(output_even,21);
		filt[N-3][col+22] = (unsigned char) _mm256_extract_epi8(output_even,22);


		_mm_storeu_si128( (__m128i *) &filt[N-2][col],_mm256_extractf128_si256(output_row0_even, 0)); //store low 128bit - 16pixels
		filt[N-2][col+16] = (unsigned char) _mm256_extract_epi8(output_row0_even,16);
		filt[N-2][col+17] = (unsigned char) _mm256_extract_epi8(output_row0_even,17);
		filt[N-2][col+18] = (unsigned char) _mm256_extract_epi8(output_row0_even,18);
		filt[N-2][col+19] = (unsigned char) _mm256_extract_epi8(output_row0_even,19);
		filt[N-2][col+20] = (unsigned char) _mm256_extract_epi8(output_row0_even,20);
		filt[N-2][col+21] = (unsigned char) _mm256_extract_epi8(output_row0_even,21);
		filt[N-2][col+22] = (unsigned char) _mm256_extract_epi8(output_row0_even,22);

		_mm_storeu_si128( (__m128i *) &filt[N-1][col],_mm256_extractf128_si256(output_row1_even, 0)); //store low 128bit - 16pixels
		filt[N-1][col+16] = (unsigned char) _mm256_extract_epi8(output_row1_even,16);
		filt[N-1][col+17] = (unsigned char) _mm256_extract_epi8(output_row1_even,17);
		filt[N-1][col+18] = (unsigned char) _mm256_extract_epi8(output_row1_even,18);
		filt[N-1][col+19] = (unsigned char) _mm256_extract_epi8(output_row1_even,19);
		filt[N-1][col+20] = (unsigned char) _mm256_extract_epi8(output_row1_even,20);
		filt[N-1][col+21] = (unsigned char) _mm256_extract_epi8(output_row1_even,21);
		filt[N-1][col+22] = (unsigned char) _mm256_extract_epi8(output_row1_even,22);
		  break;
	case 24:
		_mm_storeu_si128( (__m128i *) &filt[N-3][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
		filt[N-3][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
		filt[N-3][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
		filt[N-3][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
		filt[N-3][col+19] = (unsigned char) _mm256_extract_epi8(output_even,19);
		filt[N-3][col+20] = (unsigned char) _mm256_extract_epi8(output_even,20);
		filt[N-3][col+21] = (unsigned char) _mm256_extract_epi8(output_even,21);
		filt[N-3][col+22] = (unsigned char) _mm256_extract_epi8(output_even,22);
		filt[N-3][col+23] = (unsigned char) _mm256_extract_epi8(output_even,23);

		_mm_storeu_si128( (__m128i *) &filt[N-2][col],_mm256_extractf128_si256(output_row0_even, 0)); //store low 128bit - 16pixels
		filt[N-2][col+16] = (unsigned char) _mm256_extract_epi8(output_row0_even,16);
		filt[N-2][col+17] = (unsigned char) _mm256_extract_epi8(output_row0_even,17);
		filt[N-2][col+18] = (unsigned char) _mm256_extract_epi8(output_row0_even,18);
		filt[N-2][col+19] = (unsigned char) _mm256_extract_epi8(output_row0_even,19);
		filt[N-2][col+20] = (unsigned char) _mm256_extract_epi8(output_row0_even,20);
		filt[N-2][col+21] = (unsigned char) _mm256_extract_epi8(output_row0_even,21);
		filt[N-2][col+22] = (unsigned char) _mm256_extract_epi8(output_row0_even,22);
		filt[N-2][col+23] = (unsigned char) _mm256_extract_epi8(output_row0_even,23);

		_mm_storeu_si128( (__m128i *) &filt[N-1][col],_mm256_extractf128_si256(output_row1_even, 0)); //store low 128bit - 16pixels
		filt[N-1][col+16] = (unsigned char) _mm256_extract_epi8(output_row1_even,16);
		filt[N-1][col+17] = (unsigned char) _mm256_extract_epi8(output_row1_even,17);
		filt[N-1][col+18] = (unsigned char) _mm256_extract_epi8(output_row1_even,18);
		filt[N-1][col+19] = (unsigned char) _mm256_extract_epi8(output_row1_even,19);
		filt[N-1][col+20] = (unsigned char) _mm256_extract_epi8(output_row1_even,20);
		filt[N-1][col+21] = (unsigned char) _mm256_extract_epi8(output_row1_even,21);
		filt[N-1][col+22] = (unsigned char) _mm256_extract_epi8(output_row1_even,22);
		filt[N-1][col+23] = (unsigned char) _mm256_extract_epi8(output_row1_even,23);
		  break;
	case 25:
		_mm_storeu_si128( (__m128i *) &filt[N-3][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
		filt[N-3][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
		filt[N-3][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
		filt[N-3][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
		filt[N-3][col+19] = (unsigned char) _mm256_extract_epi8(output_even,19);
		filt[N-3][col+20] = (unsigned char) _mm256_extract_epi8(output_even,20);
		filt[N-3][col+21] = (unsigned char) _mm256_extract_epi8(output_even,21);
		filt[N-3][col+22] = (unsigned char) _mm256_extract_epi8(output_even,22);
		filt[N-3][col+23] = (unsigned char) _mm256_extract_epi8(output_even,23);
		filt[N-3][col+24] = (unsigned char) _mm256_extract_epi8(output_even,24);

		_mm_storeu_si128( (__m128i *) &filt[N-2][col],_mm256_extractf128_si256(output_row0_even, 0)); //store low 128bit - 16pixels
		filt[N-2][col+16] = (unsigned char) _mm256_extract_epi8(output_row0_even,16);
		filt[N-2][col+17] = (unsigned char) _mm256_extract_epi8(output_row0_even,17);
		filt[N-2][col+18] = (unsigned char) _mm256_extract_epi8(output_row0_even,18);
		filt[N-2][col+19] = (unsigned char) _mm256_extract_epi8(output_row0_even,19);
		filt[N-2][col+20] = (unsigned char) _mm256_extract_epi8(output_row0_even,20);
		filt[N-2][col+21] = (unsigned char) _mm256_extract_epi8(output_row0_even,21);
		filt[N-2][col+22] = (unsigned char) _mm256_extract_epi8(output_row0_even,22);
		filt[N-2][col+23] = (unsigned char) _mm256_extract_epi8(output_row0_even,23);
		filt[N-2][col+24] = (unsigned char) _mm256_extract_epi8(output_row0_even,24);

		_mm_storeu_si128( (__m128i *) &filt[N-1][col],_mm256_extractf128_si256(output_row1_even, 0)); //store low 128bit - 16pixels
		filt[N-1][col+16] = (unsigned char) _mm256_extract_epi8(output_row1_even,16);
		filt[N-1][col+17] = (unsigned char) _mm256_extract_epi8(output_row1_even,17);
		filt[N-1][col+18] = (unsigned char) _mm256_extract_epi8(output_row1_even,18);
		filt[N-1][col+19] = (unsigned char) _mm256_extract_epi8(output_row1_even,19);
		filt[N-1][col+20] = (unsigned char) _mm256_extract_epi8(output_row1_even,20);
		filt[N-1][col+21] = (unsigned char) _mm256_extract_epi8(output_row1_even,21);
		filt[N-1][col+22] = (unsigned char) _mm256_extract_epi8(output_row1_even,22);
		filt[N-1][col+23] = (unsigned char) _mm256_extract_epi8(output_row1_even,23);
		filt[N-1][col+24] = (unsigned char) _mm256_extract_epi8(output_row1_even,24);
		  break;
	case 26:
		_mm_storeu_si128( (__m128i *) &filt[N-3][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
		filt[N-3][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
		filt[N-3][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
		filt[N-3][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
		filt[N-3][col+19] = (unsigned char) _mm256_extract_epi8(output_even,19);
		filt[N-3][col+20] = (unsigned char) _mm256_extract_epi8(output_even,20);
		filt[N-3][col+21] = (unsigned char) _mm256_extract_epi8(output_even,21);
		filt[N-3][col+22] = (unsigned char) _mm256_extract_epi8(output_even,22);
		filt[N-3][col+23] = (unsigned char) _mm256_extract_epi8(output_even,23);
		filt[N-3][col+24] = (unsigned char) _mm256_extract_epi8(output_even,24);
		filt[N-3][col+25] = (unsigned char) _mm256_extract_epi8(output_even,25);

		_mm_storeu_si128( (__m128i *) &filt[N-2][col],_mm256_extractf128_si256(output_row0_even, 0)); //store low 128bit - 16pixels
		filt[N-2][col+16] = (unsigned char) _mm256_extract_epi8(output_row0_even,16);
		filt[N-2][col+17] = (unsigned char) _mm256_extract_epi8(output_row0_even,17);
		filt[N-2][col+18] = (unsigned char) _mm256_extract_epi8(output_row0_even,18);
		filt[N-2][col+19] = (unsigned char) _mm256_extract_epi8(output_row0_even,19);
		filt[N-2][col+20] = (unsigned char) _mm256_extract_epi8(output_row0_even,20);
		filt[N-2][col+21] = (unsigned char) _mm256_extract_epi8(output_row0_even,21);
		filt[N-2][col+22] = (unsigned char) _mm256_extract_epi8(output_row0_even,22);
		filt[N-2][col+23] = (unsigned char) _mm256_extract_epi8(output_row0_even,23);
		filt[N-2][col+24] = (unsigned char) _mm256_extract_epi8(output_row0_even,24);
		filt[N-2][col+25] = (unsigned char) _mm256_extract_epi8(output_row0_even,25);

		_mm_storeu_si128( (__m128i *) &filt[N-1][col],_mm256_extractf128_si256(output_row1_even, 0)); //store low 128bit - 16pixels
		filt[N-1][col+16] = (unsigned char) _mm256_extract_epi8(output_row1_even,16);
		filt[N-1][col+17] = (unsigned char) _mm256_extract_epi8(output_row1_even,17);
		filt[N-1][col+18] = (unsigned char) _mm256_extract_epi8(output_row1_even,18);
		filt[N-1][col+19] = (unsigned char) _mm256_extract_epi8(output_row1_even,19);
		filt[N-1][col+20] = (unsigned char) _mm256_extract_epi8(output_row1_even,20);
		filt[N-1][col+21] = (unsigned char) _mm256_extract_epi8(output_row1_even,21);
		filt[N-1][col+22] = (unsigned char) _mm256_extract_epi8(output_row1_even,22);
		filt[N-1][col+23] = (unsigned char) _mm256_extract_epi8(output_row1_even,23);
		filt[N-1][col+24] = (unsigned char) _mm256_extract_epi8(output_row1_even,24);
		filt[N-1][col+25] = (unsigned char) _mm256_extract_epi8(output_row1_even,25);
		  break;
	case 27:
		_mm_storeu_si128( (__m128i *) &filt[N-3][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
		filt[N-3][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
		filt[N-3][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
		filt[N-3][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
		filt[N-3][col+19] = (unsigned char) _mm256_extract_epi8(output_even,19);
		filt[N-3][col+20] = (unsigned char) _mm256_extract_epi8(output_even,20);
		filt[N-3][col+21] = (unsigned char) _mm256_extract_epi8(output_even,21);
		filt[N-3][col+22] = (unsigned char) _mm256_extract_epi8(output_even,22);
		filt[N-3][col+23] = (unsigned char) _mm256_extract_epi8(output_even,23);
		filt[N-3][col+24] = (unsigned char) _mm256_extract_epi8(output_even,24);
		filt[N-3][col+25] = (unsigned char) _mm256_extract_epi8(output_even,25);
		filt[N-3][col+26] = (unsigned char) _mm256_extract_epi8(output_even,26);

		_mm_storeu_si128( (__m128i *) &filt[N-2][col],_mm256_extractf128_si256(output_row0_even, 0)); //store low 128bit - 16pixels
		filt[N-2][col+16] = (unsigned char) _mm256_extract_epi8(output_row0_even,16);
		filt[N-2][col+17] = (unsigned char) _mm256_extract_epi8(output_row0_even,17);
		filt[N-2][col+18] = (unsigned char) _mm256_extract_epi8(output_row0_even,18);
		filt[N-2][col+19] = (unsigned char) _mm256_extract_epi8(output_row0_even,19);
		filt[N-2][col+20] = (unsigned char) _mm256_extract_epi8(output_row0_even,20);
		filt[N-2][col+21] = (unsigned char) _mm256_extract_epi8(output_row0_even,21);
		filt[N-2][col+22] = (unsigned char) _mm256_extract_epi8(output_row0_even,22);
		filt[N-2][col+23] = (unsigned char) _mm256_extract_epi8(output_row0_even,23);
		filt[N-2][col+24] = (unsigned char) _mm256_extract_epi8(output_row0_even,24);
		filt[N-2][col+25] = (unsigned char) _mm256_extract_epi8(output_row0_even,25);
		filt[N-2][col+26] = (unsigned char) _mm256_extract_epi8(output_row0_even,26);

		_mm_storeu_si128( (__m128i *) &filt[N-1][col],_mm256_extractf128_si256(output_row1_even, 0)); //store low 128bit - 16pixels
		filt[N-1][col+16] = (unsigned char) _mm256_extract_epi8(output_row1_even,16);
		filt[N-1][col+17] = (unsigned char) _mm256_extract_epi8(output_row1_even,17);
		filt[N-1][col+18] = (unsigned char) _mm256_extract_epi8(output_row1_even,18);
		filt[N-1][col+19] = (unsigned char) _mm256_extract_epi8(output_row1_even,19);
		filt[N-1][col+20] = (unsigned char) _mm256_extract_epi8(output_row1_even,20);
		filt[N-1][col+21] = (unsigned char) _mm256_extract_epi8(output_row1_even,21);
		filt[N-1][col+22] = (unsigned char) _mm256_extract_epi8(output_row1_even,22);
		filt[N-1][col+23] = (unsigned char) _mm256_extract_epi8(output_row1_even,23);
		filt[N-1][col+24] = (unsigned char) _mm256_extract_epi8(output_row1_even,24);
		filt[N-1][col+25] = (unsigned char) _mm256_extract_epi8(output_row1_even,25);
		filt[N-1][col+26] = (unsigned char) _mm256_extract_epi8(output_row1_even,26);
		  break;
	case 28:
		_mm_storeu_si128( (__m128i *) &filt[N-3][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
		filt[N-3][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
		filt[N-3][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
		filt[N-3][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
		filt[N-3][col+19] = (unsigned char) _mm256_extract_epi8(output_even,19);
		filt[N-3][col+20] = (unsigned char) _mm256_extract_epi8(output_even,20);
		filt[N-3][col+21] = (unsigned char) _mm256_extract_epi8(output_even,21);
		filt[N-3][col+22] = (unsigned char) _mm256_extract_epi8(output_even,22);
		filt[N-3][col+23] = (unsigned char) _mm256_extract_epi8(output_even,23);
		filt[N-3][col+24] = (unsigned char) _mm256_extract_epi8(output_even,24);
		filt[N-3][col+25] = (unsigned char) _mm256_extract_epi8(output_even,25);
		filt[N-3][col+26] = (unsigned char) _mm256_extract_epi8(output_even,26);
		filt[N-3][col+27] = (unsigned char) _mm256_extract_epi8(output_even,27);

		_mm_storeu_si128( (__m128i *) &filt[N-2][col],_mm256_extractf128_si256(output_row0_even, 0)); //store low 128bit - 16pixels
		filt[N-2][col+16] = (unsigned char) _mm256_extract_epi8(output_row0_even,16);
		filt[N-2][col+17] = (unsigned char) _mm256_extract_epi8(output_row0_even,17);
		filt[N-2][col+18] = (unsigned char) _mm256_extract_epi8(output_row0_even,18);
		filt[N-2][col+19] = (unsigned char) _mm256_extract_epi8(output_row0_even,19);
		filt[N-2][col+20] = (unsigned char) _mm256_extract_epi8(output_row0_even,20);
		filt[N-2][col+21] = (unsigned char) _mm256_extract_epi8(output_row0_even,21);
		filt[N-2][col+22] = (unsigned char) _mm256_extract_epi8(output_row0_even,22);
		filt[N-2][col+23] = (unsigned char) _mm256_extract_epi8(output_row0_even,23);
		filt[N-2][col+24] = (unsigned char) _mm256_extract_epi8(output_row0_even,24);
		filt[N-2][col+25] = (unsigned char) _mm256_extract_epi8(output_row0_even,25);
		filt[N-2][col+26] = (unsigned char) _mm256_extract_epi8(output_row0_even,26);
		filt[N-2][col+27] = (unsigned char) _mm256_extract_epi8(output_row0_even,27);

		_mm_storeu_si128( (__m128i *) &filt[N-1][col],_mm256_extractf128_si256(output_row1_even, 0)); //store low 128bit - 16pixels
		filt[N-1][col+16] = (unsigned char) _mm256_extract_epi8(output_row1_even,16);
		filt[N-1][col+17] = (unsigned char) _mm256_extract_epi8(output_row1_even,17);
		filt[N-1][col+18] = (unsigned char) _mm256_extract_epi8(output_row1_even,18);
		filt[N-1][col+19] = (unsigned char) _mm256_extract_epi8(output_row1_even,19);
		filt[N-1][col+20] = (unsigned char) _mm256_extract_epi8(output_row1_even,20);
		filt[N-1][col+21] = (unsigned char) _mm256_extract_epi8(output_row1_even,21);
		filt[N-1][col+22] = (unsigned char) _mm256_extract_epi8(output_row1_even,22);
		filt[N-1][col+23] = (unsigned char) _mm256_extract_epi8(output_row1_even,23);
		filt[N-1][col+24] = (unsigned char) _mm256_extract_epi8(output_row1_even,24);
		filt[N-1][col+25] = (unsigned char) _mm256_extract_epi8(output_row1_even,25);
		filt[N-1][col+26] = (unsigned char) _mm256_extract_epi8(output_row1_even,26);
		filt[N-1][col+27] = (unsigned char) _mm256_extract_epi8(output_row1_even,27);
		  break;
	case 29:
		_mm_storeu_si128( (__m128i *) &filt[N-3][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
		filt[N-3][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
		filt[N-3][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
		filt[N-3][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
		filt[N-3][col+19] = (unsigned char) _mm256_extract_epi8(output_even,19);
		filt[N-3][col+20] = (unsigned char) _mm256_extract_epi8(output_even,20);
		filt[N-3][col+21] = (unsigned char) _mm256_extract_epi8(output_even,21);
		filt[N-3][col+22] = (unsigned char) _mm256_extract_epi8(output_even,22);
		filt[N-3][col+23] = (unsigned char) _mm256_extract_epi8(output_even,23);
		filt[N-3][col+24] = (unsigned char) _mm256_extract_epi8(output_even,24);
		filt[N-3][col+25] = (unsigned char) _mm256_extract_epi8(output_even,25);
		filt[N-3][col+26] = (unsigned char) _mm256_extract_epi8(output_even,26);
		filt[N-3][col+27] = (unsigned char) _mm256_extract_epi8(output_even,27);
		filt[N-3][col+28] = (unsigned char) _mm256_extract_epi8(output_even,28);

		_mm_storeu_si128( (__m128i *) &filt[N-2][col],_mm256_extractf128_si256(output_row0_even, 0)); //store low 128bit - 16pixels
		filt[N-2][col+16] = (unsigned char) _mm256_extract_epi8(output_row0_even,16);
		filt[N-2][col+17] = (unsigned char) _mm256_extract_epi8(output_row0_even,17);
		filt[N-2][col+18] = (unsigned char) _mm256_extract_epi8(output_row0_even,18);
		filt[N-2][col+19] = (unsigned char) _mm256_extract_epi8(output_row0_even,19);
		filt[N-2][col+20] = (unsigned char) _mm256_extract_epi8(output_row0_even,20);
		filt[N-2][col+21] = (unsigned char) _mm256_extract_epi8(output_row0_even,21);
		filt[N-2][col+22] = (unsigned char) _mm256_extract_epi8(output_row0_even,22);
		filt[N-2][col+23] = (unsigned char) _mm256_extract_epi8(output_row0_even,23);
		filt[N-2][col+24] = (unsigned char) _mm256_extract_epi8(output_row0_even,24);
		filt[N-2][col+25] = (unsigned char) _mm256_extract_epi8(output_row0_even,25);
		filt[N-2][col+26] = (unsigned char) _mm256_extract_epi8(output_row0_even,26);
		filt[N-2][col+27] = (unsigned char) _mm256_extract_epi8(output_row0_even,27);
		filt[N-2][col+28] = (unsigned char) _mm256_extract_epi8(output_row0_even,28);

		_mm_storeu_si128( (__m128i *) &filt[N-1][col],_mm256_extractf128_si256(output_row1_even, 0)); //store low 128bit - 16pixels
		filt[N-1][col+16] = (unsigned char) _mm256_extract_epi8(output_row1_even,16);
		filt[N-1][col+17] = (unsigned char) _mm256_extract_epi8(output_row1_even,17);
		filt[N-1][col+18] = (unsigned char) _mm256_extract_epi8(output_row1_even,18);
		filt[N-1][col+19] = (unsigned char) _mm256_extract_epi8(output_row1_even,19);
		filt[N-1][col+20] = (unsigned char) _mm256_extract_epi8(output_row1_even,20);
		filt[N-1][col+21] = (unsigned char) _mm256_extract_epi8(output_row1_even,21);
		filt[N-1][col+22] = (unsigned char) _mm256_extract_epi8(output_row1_even,22);
		filt[N-1][col+23] = (unsigned char) _mm256_extract_epi8(output_row1_even,23);
		filt[N-1][col+24] = (unsigned char) _mm256_extract_epi8(output_row1_even,24);
		filt[N-1][col+25] = (unsigned char) _mm256_extract_epi8(output_row1_even,25);
		filt[N-1][col+26] = (unsigned char) _mm256_extract_epi8(output_row1_even,26);
		filt[N-1][col+27] = (unsigned char) _mm256_extract_epi8(output_row1_even,27);
		filt[N-1][col+28] = (unsigned char) _mm256_extract_epi8(output_row1_even,28);
		  break;
	case 30:
		_mm_storeu_si128( (__m128i *) &filt[N-3][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
		filt[N-3][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
		filt[N-3][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
		filt[N-3][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
		filt[N-3][col+19] = (unsigned char) _mm256_extract_epi8(output_even,19);
		filt[N-3][col+20] = (unsigned char) _mm256_extract_epi8(output_even,20);
		filt[N-3][col+21] = (unsigned char) _mm256_extract_epi8(output_even,21);
		filt[N-3][col+22] = (unsigned char) _mm256_extract_epi8(output_even,22);
		filt[N-3][col+23] = (unsigned char) _mm256_extract_epi8(output_even,23);
		filt[N-3][col+24] = (unsigned char) _mm256_extract_epi8(output_even,24);
		filt[N-3][col+25] = (unsigned char) _mm256_extract_epi8(output_even,25);
		filt[N-3][col+26] = (unsigned char) _mm256_extract_epi8(output_even,26);
		filt[N-3][col+27] = (unsigned char) _mm256_extract_epi8(output_even,27);
		filt[N-3][col+28] = (unsigned char) _mm256_extract_epi8(output_even,28);
		filt[N-3][col+29] = (unsigned char) _mm256_extract_epi8(output_even,29);

		_mm_storeu_si128( (__m128i *) &filt[N-2][col],_mm256_extractf128_si256(output_row0_even, 0)); //store low 128bit - 16pixels
		filt[N-2][col+16] = (unsigned char) _mm256_extract_epi8(output_row0_even,16);
		filt[N-2][col+17] = (unsigned char) _mm256_extract_epi8(output_row0_even,17);
		filt[N-2][col+18] = (unsigned char) _mm256_extract_epi8(output_row0_even,18);
		filt[N-2][col+19] = (unsigned char) _mm256_extract_epi8(output_row0_even,19);
		filt[N-2][col+20] = (unsigned char) _mm256_extract_epi8(output_row0_even,20);
		filt[N-2][col+21] = (unsigned char) _mm256_extract_epi8(output_row0_even,21);
		filt[N-2][col+22] = (unsigned char) _mm256_extract_epi8(output_row0_even,22);
		filt[N-2][col+23] = (unsigned char) _mm256_extract_epi8(output_row0_even,23);
		filt[N-2][col+24] = (unsigned char) _mm256_extract_epi8(output_row0_even,24);
		filt[N-2][col+25] = (unsigned char) _mm256_extract_epi8(output_row0_even,25);
		filt[N-2][col+26] = (unsigned char) _mm256_extract_epi8(output_row0_even,26);
		filt[N-2][col+27] = (unsigned char) _mm256_extract_epi8(output_row0_even,27);
		filt[N-2][col+28] = (unsigned char) _mm256_extract_epi8(output_row0_even,28);
		filt[N-2][col+29] = (unsigned char) _mm256_extract_epi8(output_row0_even,29);


		_mm_storeu_si128( (__m128i *) &filt[N-1][col],_mm256_extractf128_si256(output_row1_even, 0)); //store low 128bit - 16pixels
		filt[N-1][col+16] = (unsigned char) _mm256_extract_epi8(output_row1_even,16);
		filt[N-1][col+17] = (unsigned char) _mm256_extract_epi8(output_row1_even,17);
		filt[N-1][col+18] = (unsigned char) _mm256_extract_epi8(output_row1_even,18);
		filt[N-1][col+19] = (unsigned char) _mm256_extract_epi8(output_row1_even,19);
		filt[N-1][col+20] = (unsigned char) _mm256_extract_epi8(output_row1_even,20);
		filt[N-1][col+21] = (unsigned char) _mm256_extract_epi8(output_row1_even,21);
		filt[N-1][col+22] = (unsigned char) _mm256_extract_epi8(output_row1_even,22);
		filt[N-1][col+23] = (unsigned char) _mm256_extract_epi8(output_row1_even,23);
		filt[N-1][col+24] = (unsigned char) _mm256_extract_epi8(output_row1_even,24);
		filt[N-1][col+25] = (unsigned char) _mm256_extract_epi8(output_row1_even,25);
		filt[N-1][col+26] = (unsigned char) _mm256_extract_epi8(output_row1_even,26);
		filt[N-1][col+27] = (unsigned char) _mm256_extract_epi8(output_row1_even,27);
		filt[N-1][col+28] = (unsigned char) _mm256_extract_epi8(output_row1_even,28);
		filt[N-1][col+29] = (unsigned char) _mm256_extract_epi8(output_row1_even,29);
		  break;
	case 31:
		_mm_storeu_si128( (__m128i *) &filt[N-3][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
		filt[N-3][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
		filt[N-3][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
		filt[N-3][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
		filt[N-3][col+19] = (unsigned char) _mm256_extract_epi8(output_even,19);
		filt[N-3][col+20] = (unsigned char) _mm256_extract_epi8(output_even,20);
		filt[N-3][col+21] = (unsigned char) _mm256_extract_epi8(output_even,21);
		filt[N-3][col+22] = (unsigned char) _mm256_extract_epi8(output_even,22);
		filt[N-3][col+23] = (unsigned char) _mm256_extract_epi8(output_even,23);
		filt[N-3][col+24] = (unsigned char) _mm256_extract_epi8(output_even,24);
		filt[N-3][col+25] = (unsigned char) _mm256_extract_epi8(output_even,25);
		filt[N-3][col+26] = (unsigned char) _mm256_extract_epi8(output_even,26);
		filt[N-3][col+27] = (unsigned char) _mm256_extract_epi8(output_even,27);
		filt[N-3][col+28] = (unsigned char) _mm256_extract_epi8(output_even,28);
		filt[N-3][col+29] = (unsigned char) _mm256_extract_epi8(output_even,29);

		_mm_storeu_si128( (__m128i *) &filt[N-2][col],_mm256_extractf128_si256(output_row0_even, 0)); //store low 128bit - 16pixels
		filt[N-2][col+16] = (unsigned char) _mm256_extract_epi8(output_row0_even,16);
		filt[N-2][col+17] = (unsigned char) _mm256_extract_epi8(output_row0_even,17);
		filt[N-2][col+18] = (unsigned char) _mm256_extract_epi8(output_row0_even,18);
		filt[N-2][col+19] = (unsigned char) _mm256_extract_epi8(output_row0_even,19);
		filt[N-2][col+20] = (unsigned char) _mm256_extract_epi8(output_row0_even,20);
		filt[N-2][col+21] = (unsigned char) _mm256_extract_epi8(output_row0_even,21);
		filt[N-2][col+22] = (unsigned char) _mm256_extract_epi8(output_row0_even,22);
		filt[N-2][col+23] = (unsigned char) _mm256_extract_epi8(output_row0_even,23);
		filt[N-2][col+24] = (unsigned char) _mm256_extract_epi8(output_row0_even,24);
		filt[N-2][col+25] = (unsigned char) _mm256_extract_epi8(output_row0_even,25);
		filt[N-2][col+26] = (unsigned char) _mm256_extract_epi8(output_row0_even,26);
		filt[N-2][col+27] = (unsigned char) _mm256_extract_epi8(output_row0_even,27);
		filt[N-2][col+28] = (unsigned char) _mm256_extract_epi8(output_row0_even,28);
		filt[N-2][col+29] = (unsigned char) _mm256_extract_epi8(output_row0_even,29);


		_mm_storeu_si128( (__m128i *) &filt[N-1][col],_mm256_extractf128_si256(output_row1_even, 0)); //store low 128bit - 16pixels
		filt[N-1][col+16] = (unsigned char) _mm256_extract_epi8(output_row1_even,16);
		filt[N-1][col+17] = (unsigned char) _mm256_extract_epi8(output_row1_even,17);
		filt[N-1][col+18] = (unsigned char) _mm256_extract_epi8(output_row1_even,18);
		filt[N-1][col+19] = (unsigned char) _mm256_extract_epi8(output_row1_even,19);
		filt[N-1][col+20] = (unsigned char) _mm256_extract_epi8(output_row1_even,20);
		filt[N-1][col+21] = (unsigned char) _mm256_extract_epi8(output_row1_even,21);
		filt[N-1][col+22] = (unsigned char) _mm256_extract_epi8(output_row1_even,22);
		filt[N-1][col+23] = (unsigned char) _mm256_extract_epi8(output_row1_even,23);
		filt[N-1][col+24] = (unsigned char) _mm256_extract_epi8(output_row1_even,24);
		filt[N-1][col+25] = (unsigned char) _mm256_extract_epi8(output_row1_even,25);
		filt[N-1][col+26] = (unsigned char) _mm256_extract_epi8(output_row1_even,26);
		filt[N-1][col+27] = (unsigned char) _mm256_extract_epi8(output_row1_even,27);
		filt[N-1][col+28] = (unsigned char) _mm256_extract_epi8(output_row1_even,28);
		filt[N-1][col+29] = (unsigned char) _mm256_extract_epi8(output_row1_even,29);

		//the filt[0:2][M-1] pixel will be computed without vectorization
		int newPixel;
		for (int row=N-3;row<=N-1;row++){
		 newPixel = 0;
		for (int rowOffset=-2; rowOffset<=2; rowOffset++) {
			for (int colOffset=-2; colOffset<=2; colOffset++) {

			   if ( ((row+rowOffset)<N) && ((M-1+colOffset)<M) && ((row+rowOffset)>=0) && ((M-1+colOffset)>=0) )
	              newPixel += frame1[row+rowOffset][M-1+colOffset] * filter[2 + rowOffset][2 + colOffset];

			      }
		        }
	   filt[row][M-1] = (unsigned char) (newPixel / divisor);
		}

		  break;
	default:
		printf("\n something went wrong");
	}

return 0;

}







//for row=N-2,N-1 only
//row0 refers to N-2, while row1 refers to N-1
int loop_reminder_last_less_div_special_case(unsigned char **frame1,unsigned char **filt,const unsigned int M, const unsigned int N, const unsigned int col, const unsigned int REMINDER_ITERATIONS,const unsigned int division_case,const __m256i c0,const __m256i c1,const __m256i c2,const __m256i c0_sh1,const __m256i c1_sh1,const __m256i c2_sh1,const __m256i c0_sh2,const __m256i c1_sh2,const __m256i c2_sh2, const __m256i f, const unsigned short int divisor,signed char **filter ){

__m256i r1,r2,r3,r4,m0,m1,m2,m3,m4;
__m256i output_row0_even,output_row1_even,output_row0_odd,output_row1_odd;

		//const __m256i f  = _mm256_set_epi16(52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759);
		//const __m256i mask  = _mm256_set_epi8(0,0,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255);
		//const __m256i mask2  = _mm256_set_epi8(0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0);
		const __m256i mask3  = _mm256_set_epi16(0,0,0,0,0,0,0,65535,0,0,0,0,0,0,0,0);
		//const __m256i mask4  = _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255,0,0,0,0,0,0,0,0,0,0,0,0);
		//const __m256i mask5  = _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0);

		const __m256i output_mask      = _mm256_set_epi16(0,0,0,65535,0,0,65535,0,0,65535,0,0,65535,0,0,65535);
		const __m256i output_mask_sh1  = _mm256_set_epi16(0,0,65535,0,0,65535,0,0,65535,0,0,65535,0,0,65535,0);
		//const __m256i output_mask_sh2  = _mm256_set_epi16(0,0,0,    0,65535,0,0,65535,0,0,65535,0,0,65535,0,0);
		const __m256i mask_new      = _mm256_set_epi16(0,0,0,0,0,0,0,0,0,65535,0,0,0,0,0,0);
		const __m256i mask_new2      = _mm256_set_epi16(0,0,0,0,0,0,0,0,65535,0,0,0,0,0,0,0);



 __m256i reminder_mask1,reminder_mask2;

	reminder_mask1=_mm256_load_si256( (__m256i *) &reminder_msk1[REMINDER_ITERATIONS-1][0]);
	reminder_mask2=_mm256_load_si256( (__m256i *) &reminder_msk2[REMINDER_ITERATIONS-1][0]);


	 //1st col iteration
	//load the 4 rows
	r1=_mm256_loadu_si256( (__m256i *) &frame1[N-4][col-2]);
	r2=_mm256_loadu_si256( (__m256i *) &frame1[N-3][col-2]);
	r3=_mm256_loadu_si256( (__m256i *) &frame1[N-2][col-2]);
	r4=_mm256_loadu_si256( (__m256i *) &frame1[N-1][col-2]);

	/*
	 * The above load operations load outside of the array bounds; these elements are filled with zeros just after
	 * Furthermore, I add two extra zeros in the end to compute col=N-2 and col=N-1. The last value of col is N-1, not N
	 */

	//AND r0-r4 with reminder_mask
	r1=_mm256_and_si256(r1,reminder_mask1);
	r2=_mm256_and_si256(r2,reminder_mask1);
	r3=_mm256_and_si256(r3,reminder_mask1);
	r4=_mm256_and_si256(r4,reminder_mask1);



	//row=0
	//multiply with the mask
	m1=_mm256_maddubs_epi16(r1,c0);
	m2=_mm256_maddubs_epi16(r2,c1);
	m3=_mm256_maddubs_epi16(r3,c2);
	m4=_mm256_maddubs_epi16(r4,c1);

	//vertical add
	m0=_mm256_add_epi16(m1,m2);
	m0=_mm256_add_epi16(m0,m3);
	m0=_mm256_add_epi16(m0,m4);


	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);

	m1=_mm256_srli_si256(m0,4);
	m2=_mm256_add_epi16(m1,m2);

	//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
	//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
	m4=_mm256_and_si256(m0,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions

	m2=_mm256_add_epi16(m2,m4);

	//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
	output_row0_even=_mm256_and_si256(m2,output_mask);


	//row=1
	//multiply with the mask
	m2=_mm256_maddubs_epi16(r2,c0);
	m3=_mm256_maddubs_epi16(r3,c1);
	m4=_mm256_maddubs_epi16(r4,c2);

	//vertical add
	m0=_mm256_add_epi16(m2,m3);
	m0=_mm256_add_epi16(m0,m4);


	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);

	m1=_mm256_srli_si256(m0,4);
	m2=_mm256_add_epi16(m1,m2);

	//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
	//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
	m4=_mm256_and_si256(m0,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions

	m2=_mm256_add_epi16(m2,m4);

	//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
	output_row1_even=_mm256_and_si256(m2,output_mask);

	//2nd col iteration



	//row=0
	//multiply with the mask
	m1=_mm256_maddubs_epi16(r1,c0_sh1);
	m2=_mm256_maddubs_epi16(r2,c1_sh1);
	m3=_mm256_maddubs_epi16(r3,c2_sh1);
	m4=_mm256_maddubs_epi16(r4,c1_sh1);

	//vertical add
	m0=_mm256_add_epi16(m1,m2);
	m0=_mm256_add_epi16(m0,m3);
	m0=_mm256_add_epi16(m0,m4);


	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);

	m1=_mm256_srli_si256(m0,4);
	m2=_mm256_add_epi16(m1,m2);

	//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
	//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
	m4=_mm256_and_si256(m0,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions
	m2=_mm256_add_epi16(m2,m4);

	//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
	output_row0_odd = _mm256_and_si256(m2,output_mask);

	//row=1
	//multiply with the mask
	m2=_mm256_maddubs_epi16(r2,c0_sh1);
	m3=_mm256_maddubs_epi16(r3,c1_sh1);
	m4=_mm256_maddubs_epi16(r4,c2_sh1);

	//vertical add
	m0=_mm256_add_epi16(m2,m3);
	m0=_mm256_add_epi16(m0,m4);

	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);

	m1=_mm256_srli_si256(m0,4);
	m2=_mm256_add_epi16(m1,m2);

	//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
	//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
	m4=_mm256_and_si256(m0,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions
	m2=_mm256_add_epi16(m2,m4);

	//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
	output_row1_odd = _mm256_and_si256(m2,output_mask);

             //3rd col iteration


	//row=0
	//multiply with the mask
	m1=_mm256_maddubs_epi16(r1,c0_sh2);
	m2=_mm256_maddubs_epi16(r2,c1_sh2);
	m3=_mm256_maddubs_epi16(r3,c2_sh2);
	m4=_mm256_maddubs_epi16(r4,c1_sh2);

	//vertical add
	m0=_mm256_add_epi16(m1,m2);
	m0=_mm256_add_epi16(m0,m3);
	m0=_mm256_add_epi16(m0,m4);


	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);

	//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,_mm256_extract_epi16(m2,8),0,0,0,0,0,0,0);
	m4=_mm256_and_si256(m2,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions

	m1=_mm256_srli_si256(m0,4);
	m2=_mm256_add_epi16(m1,m2);

	//prepei na prosthesw to pixel pu thelw sto r2 kai meta na kanw diairesi me ola mazi

	m2=_mm256_add_epi16(m2,m4);

	//m2 has 16 16bit values now. the results I need are in positions 1,4,7,10,13. keep only those, discard others
	m2 = _mm256_and_si256(m2,output_mask_sh1);
	output_row0_even = _mm256_add_epi16(output_row0_even,m2);

	//row=1
	//multiply with the mask
	m2=_mm256_maddubs_epi16(r2,c0_sh2);
	m3=_mm256_maddubs_epi16(r3,c1_sh2);
	m4=_mm256_maddubs_epi16(r4,c2_sh2);

	//vertical add
	m0=_mm256_add_epi16(m2,m3);
	m0=_mm256_add_epi16(m0,m4);


	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);

	//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,_mm256_extract_epi16(m2,8),0,0,0,0,0,0,0);
	m4=_mm256_and_si256(m2,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions

	m1=_mm256_srli_si256(m0,4);
	m2=_mm256_add_epi16(m1,m2);

	//prepei na prosthesw to pixel pu thelw sto r2 kai meta na kanw diairesi me ola mazi

	m2=_mm256_add_epi16(m2,m4);

	//m2 has 16 16bit values now. the results I need are in positions 1,4,7,10,13. keep only those, discard others
	m2 = _mm256_and_si256(m2,output_mask_sh1);
	output_row1_even = _mm256_add_epi16(output_row1_even,m2);

	//4th col iteration
	//load the 5 rows
	r1=_mm256_loadu_si256( (__m256i *) &frame1[N-4][col+3-2]);
	r2=_mm256_loadu_si256( (__m256i *) &frame1[N-3][col+3-2]);
	r3=_mm256_loadu_si256( (__m256i *) &frame1[N-2][col+3-2]);
	r4=_mm256_loadu_si256( (__m256i *) &frame1[N-1][col+3-2]);

	//AND r0-r4 with reminder_mask
	r1=_mm256_and_si256(r1,reminder_mask2);
	r2=_mm256_and_si256(r2,reminder_mask2);
	r3=_mm256_and_si256(r3,reminder_mask2);
	r4=_mm256_and_si256(r4,reminder_mask2);



	//row=0
	//multiply with the mask
	m1=_mm256_maddubs_epi16(r1,c0);
	m2=_mm256_maddubs_epi16(r2,c1);
	m3=_mm256_maddubs_epi16(r3,c2);
	m4=_mm256_maddubs_epi16(r4,c1);

	//vertical add
	m0=_mm256_add_epi16(m1,m2);
	m0=_mm256_add_epi16(m0,m3);
	m0=_mm256_add_epi16(m0,m4);


	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);

	m1=_mm256_srli_si256(m0,4);
	m2=_mm256_add_epi16(m1,m2);

	//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
	//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
	m4=_mm256_and_si256(m0,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions

	m2=_mm256_add_epi16(m2,m4);

	//m2 has 16 16bit values now. the results I need are in positions 1,4,7,10,13. keep only those, discard others
	m2 = _mm256_and_si256(m2,output_mask);
	m2 = _mm256_slli_si256(m2,2);
	output_row0_odd = _mm256_add_epi16(output_row0_odd,m2);

	//row=1
	//multiply with the mask
	m2=_mm256_maddubs_epi16(r2,c0);
	m3=_mm256_maddubs_epi16(r3,c1);
	m4=_mm256_maddubs_epi16(r4,c2);

	//vertical add
	m0=_mm256_add_epi16(m2,m3);
	m0=_mm256_add_epi16(m0,m4);


	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);

	m1=_mm256_srli_si256(m0,4);
	m2=_mm256_add_epi16(m1,m2);

	//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
	//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
	m4=_mm256_and_si256(m0,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions

	m2=_mm256_add_epi16(m2,m4);

	//m2 has 16 16bit values now. the results I need are in positions 1,4,7,10,13. keep only those, discard others
	m2 = _mm256_and_si256(m2,output_mask);
	m2 = _mm256_slli_si256(m2,2);
	output_row1_odd = _mm256_add_epi16(output_row1_odd,m2);

	//5th col iteration



	//row=0
	//multiply with the mask
	m1=_mm256_maddubs_epi16(r1,c0_sh1);
	m2=_mm256_maddubs_epi16(r2,c1_sh1);
	m3=_mm256_maddubs_epi16(r3,c2_sh1);
	m4=_mm256_maddubs_epi16(r4,c1_sh1);

	//vertical add
	m0=_mm256_add_epi16(m1,m2);
	m0=_mm256_add_epi16(m0,m3);
	m0=_mm256_add_epi16(m0,m4);



	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);

	m1=_mm256_srli_si256(m0,4);
	m2=_mm256_add_epi16(m1,m2);

	//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
	//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
	m4=_mm256_and_si256(m0,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions

	m2=_mm256_add_epi16(m2,m4);

	//m2 has 16 16bit values now.
	m2 = _mm256_and_si256(m2,output_mask); //holds results in positions 0,3,6,9,12
	m1 = _mm256_slli_si256(m2,4); //put results in positions 2,5,8,11,14 , but 8 is not written
	m2=_mm256_and_si256(m2,mask_new);//extract only the 6th
	m2=_mm256_permute2f128_si256(m2,m2,1);
	m2=_mm256_srli_si256(m2,12);
	m2 = _mm256_add_epi16(m1,m2);
	output_row0_even = _mm256_add_epi16(output_row0_even,m2);

	//row=1
	//multiply with the mask
	m2=_mm256_maddubs_epi16(r2,c0_sh1);
	m3=_mm256_maddubs_epi16(r3,c1_sh1);
	m4=_mm256_maddubs_epi16(r4,c2_sh1);

	//vertical add
	m0=_mm256_add_epi16(m2,m3);
	m0=_mm256_add_epi16(m0,m4);


	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);

	m1=_mm256_srli_si256(m0,4);
	m2=_mm256_add_epi16(m1,m2);

	//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
	//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
	m4=_mm256_and_si256(m0,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions

	m2=_mm256_add_epi16(m2,m4);

	//m2 has 16 16bit values now.
	m2 = _mm256_and_si256(m2,output_mask); //holds results in positions 0,3,6,9,12
	m1 = _mm256_slli_si256(m2,4); //put results in positions 2,5,8,11,14 , but 8 is not written
	m2=_mm256_and_si256(m2,mask_new);//extract only the 6th
	m2=_mm256_permute2f128_si256(m2,m2,1);
	m2=_mm256_srli_si256(m2,12);
	m2 = _mm256_add_epi16(m1,m2);
	output_row1_even = _mm256_add_epi16(output_row1_even,m2);

             //6th col iteration

	//row=0
	//multiply with the mask
	m1=_mm256_maddubs_epi16(r1,c0_sh2);
	m2=_mm256_maddubs_epi16(r2,c1_sh2);
	m3=_mm256_maddubs_epi16(r3,c2_sh2);
	m4=_mm256_maddubs_epi16(r4,c1_sh2);

	//vertical add
	m0=_mm256_add_epi16(m1,m2);
	m0=_mm256_add_epi16(m0,m3);
	m0=_mm256_add_epi16(m0,m4);


	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);
	//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,_mm256_extract_epi16(m2,8),0,0,0,0,0,0,0);
	m4=_mm256_and_si256(m2,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions

	m1=_mm256_srli_si256(m0,4);
	m2=_mm256_add_epi16(m1,m2);
	m2=_mm256_add_epi16(m2,m4);

	//m2 has 16 16bit values now.
	m2 = _mm256_and_si256(m2,output_mask_sh1);//holds results in positions 1,4,7,10,13
	m1 = _mm256_slli_si256(m2,2); //put results in positions 2,5,8,11,14 , but 8 is not written
	m2=_mm256_and_si256(m2,mask_new2);//extract only the 7th
	m2=_mm256_permute2f128_si256(m2,m2,1);
	m2=_mm256_srli_si256(m2,14);
	m2 = _mm256_add_epi16(m1,m2);
	output_row0_odd = _mm256_add_epi16(output_row0_odd,m2);


	//now division follows
	output_row0_even=division(division_case,output_row0_even,f);
	output_row0_odd=division(division_case,output_row0_odd,f);

	//shift odd 1 position and add to even
	output_row0_odd = _mm256_slli_si256(output_row0_odd,1);
	output_row0_even = _mm256_add_epi8(output_row0_even,output_row0_odd);

	//row=1
	//multiply with the mask
	m2=_mm256_maddubs_epi16(r2,c0_sh2);
	m3=_mm256_maddubs_epi16(r3,c1_sh2);
	m4=_mm256_maddubs_epi16(r4,c2_sh2);

	//vertical add
	m0=_mm256_add_epi16(m2,m3);
	m0=_mm256_add_epi16(m0,m4);


	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);
	//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,_mm256_extract_epi16(m2,8),0,0,0,0,0,0,0);
	m4=_mm256_and_si256(m2,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions

	m1=_mm256_srli_si256(m0,4);
	m2=_mm256_add_epi16(m1,m2);
	m2=_mm256_add_epi16(m2,m4);

	//m2 has 16 16bit values now.
	m2 = _mm256_and_si256(m2,output_mask_sh1);//holds results in positions 1,4,7,10,13
	m1 = _mm256_slli_si256(m2,2); //put results in positions 2,5,8,11,14 , but 8 is not written
	m2=_mm256_and_si256(m2,mask_new2);//extract only the 7th
	m2=_mm256_permute2f128_si256(m2,m2,1);
	m2=_mm256_srli_si256(m2,14);
	m2 = _mm256_add_epi16(m1,m2);
	output_row1_odd = _mm256_add_epi16(output_row1_odd,m2);


	//now division follows
	output_row1_even=division(division_case,output_row1_even,f);
	output_row1_odd=division(division_case,output_row1_odd,f);

	//shift odd 1 position and add to even
	output_row1_odd = _mm256_slli_si256(output_row1_odd,1);
	output_row1_even = _mm256_add_epi8(output_row1_even,output_row1_odd);




	switch (REMINDER_ITERATIONS){
	case 1:
		filt[N-2][col] = (unsigned char) _mm256_extract_epi8(output_row0_even,0);
		filt[N-1][col] = (unsigned char) _mm256_extract_epi8(output_row1_even,0);
		 break;
	case 2:

		filt[N-2][col] = (unsigned char) _mm256_extract_epi8(output_row0_even,0);
		filt[N-2][col+1] = (unsigned char) _mm256_extract_epi8(output_row0_even,1);

		filt[N-1][col] = (unsigned char) _mm256_extract_epi8(output_row1_even,0);
		filt[N-1][col+1] = (unsigned char) _mm256_extract_epi8(output_row1_even,1);
		 break;
	case 3:

		filt[N-2][col] = (unsigned char) _mm256_extract_epi8(output_row0_even,0);
		filt[N-2][col+1] = (unsigned char) _mm256_extract_epi8(output_row0_even,1);
		filt[N-2][col+2] = (unsigned char) _mm256_extract_epi8(output_row0_even,2);

		filt[N-1][col] = (unsigned char) _mm256_extract_epi8(output_row1_even,0);
		filt[N-1][col+1] = (unsigned char) _mm256_extract_epi8(output_row1_even,1);
		filt[N-1][col+2] = (unsigned char) _mm256_extract_epi8(output_row1_even,2);
		  break;
	case 4:

		filt[N-2][col] = (unsigned char) _mm256_extract_epi8(output_row0_even,0);
		filt[N-2][col+1] = (unsigned char) _mm256_extract_epi8(output_row0_even,1);
		filt[N-2][col+2] = (unsigned char) _mm256_extract_epi8(output_row0_even,2);
		filt[N-2][col+3] = (unsigned char) _mm256_extract_epi8(output_row0_even,3);

		filt[N-1][col] = (unsigned char) _mm256_extract_epi8(output_row1_even,0);
		filt[N-1][col+1] = (unsigned char) _mm256_extract_epi8(output_row1_even,1);
		filt[N-1][col+2] = (unsigned char) _mm256_extract_epi8(output_row1_even,2);
		filt[N-1][col+3] = (unsigned char) _mm256_extract_epi8(output_row1_even,3);
		  break;
	case 5:

		filt[N-2][col] = (unsigned char) _mm256_extract_epi8(output_row0_even,0);
		filt[N-2][col+1] = (unsigned char) _mm256_extract_epi8(output_row0_even,1);
		filt[N-2][col+2] = (unsigned char) _mm256_extract_epi8(output_row0_even,2);
		filt[N-2][col+3] = (unsigned char) _mm256_extract_epi8(output_row0_even,3);
		filt[N-2][col+4] = (unsigned char) _mm256_extract_epi8(output_row0_even,4);

		filt[N-1][col] = (unsigned char) _mm256_extract_epi8(output_row1_even,0);
		filt[N-1][col+1] = (unsigned char) _mm256_extract_epi8(output_row1_even,1);
		filt[N-1][col+2] = (unsigned char) _mm256_extract_epi8(output_row1_even,2);
		filt[N-1][col+3] = (unsigned char) _mm256_extract_epi8(output_row1_even,3);
		filt[N-1][col+4] = (unsigned char) _mm256_extract_epi8(output_row1_even,4);
		  break;
	case 6:
		filt[N-2][col] = (unsigned char) _mm256_extract_epi8(output_row0_even,0);
		filt[N-2][col+1] = (unsigned char) _mm256_extract_epi8(output_row0_even,1);
		filt[N-2][col+2] = (unsigned char) _mm256_extract_epi8(output_row0_even,2);
		filt[N-2][col+3] = (unsigned char) _mm256_extract_epi8(output_row0_even,3);
		filt[N-2][col+4] = (unsigned char) _mm256_extract_epi8(output_row0_even,4);
		filt[N-2][col+5] = (unsigned char) _mm256_extract_epi8(output_row0_even,5);

		filt[N-1][col] = (unsigned char) _mm256_extract_epi8(output_row1_even,0);
		filt[N-1][col+1] = (unsigned char) _mm256_extract_epi8(output_row1_even,1);
		filt[N-1][col+2] = (unsigned char) _mm256_extract_epi8(output_row1_even,2);
		filt[N-1][col+3] = (unsigned char) _mm256_extract_epi8(output_row1_even,3);
		filt[N-1][col+4] = (unsigned char) _mm256_extract_epi8(output_row1_even,4);
		filt[N-1][col+5] = (unsigned char) _mm256_extract_epi8(output_row1_even,5);
		  break;
	case 7:
		filt[N-2][col] = (unsigned char) _mm256_extract_epi8(output_row0_even,0);
		filt[N-2][col+1] = (unsigned char) _mm256_extract_epi8(output_row0_even,1);
		filt[N-2][col+2] = (unsigned char) _mm256_extract_epi8(output_row0_even,2);
		filt[N-2][col+3] = (unsigned char) _mm256_extract_epi8(output_row0_even,3);
		filt[N-2][col+4] = (unsigned char) _mm256_extract_epi8(output_row0_even,4);
		filt[N-2][col+5] = (unsigned char) _mm256_extract_epi8(output_row0_even,5);
		filt[N-2][col+6] = (unsigned char) _mm256_extract_epi8(output_row0_even,6);

		filt[N-1][col] = (unsigned char) _mm256_extract_epi8(output_row1_even,0);
		filt[N-1][col+1] = (unsigned char) _mm256_extract_epi8(output_row1_even,1);
		filt[N-1][col+2] = (unsigned char) _mm256_extract_epi8(output_row1_even,2);
		filt[N-1][col+3] = (unsigned char) _mm256_extract_epi8(output_row1_even,3);
		filt[N-1][col+4] = (unsigned char) _mm256_extract_epi8(output_row1_even,4);
		filt[N-1][col+5] = (unsigned char) _mm256_extract_epi8(output_row1_even,5);
		filt[N-1][col+6] = (unsigned char) _mm256_extract_epi8(output_row1_even,6);
		  break;
	case 8:
		filt[N-2][col] = (unsigned char) _mm256_extract_epi8(output_row0_even,0);
		filt[N-2][col+1] = (unsigned char) _mm256_extract_epi8(output_row0_even,1);
		filt[N-2][col+2] = (unsigned char) _mm256_extract_epi8(output_row0_even,2);
		filt[N-2][col+3] = (unsigned char) _mm256_extract_epi8(output_row0_even,3);
		filt[N-2][col+4] = (unsigned char) _mm256_extract_epi8(output_row0_even,4);
		filt[N-2][col+5] = (unsigned char) _mm256_extract_epi8(output_row0_even,5);
		filt[N-2][col+6] = (unsigned char) _mm256_extract_epi8(output_row0_even,6);
		filt[N-2][col+7] = (unsigned char) _mm256_extract_epi8(output_row0_even,7);

		filt[N-1][col] = (unsigned char) _mm256_extract_epi8(output_row1_even,0);
		filt[N-1][col+1] = (unsigned char) _mm256_extract_epi8(output_row1_even,1);
		filt[N-1][col+2] = (unsigned char) _mm256_extract_epi8(output_row1_even,2);
		filt[N-1][col+3] = (unsigned char) _mm256_extract_epi8(output_row1_even,3);
		filt[N-1][col+4] = (unsigned char) _mm256_extract_epi8(output_row1_even,4);
		filt[N-1][col+5] = (unsigned char) _mm256_extract_epi8(output_row1_even,5);
		filt[N-1][col+6] = (unsigned char) _mm256_extract_epi8(output_row1_even,6);
		filt[N-1][col+7] = (unsigned char) _mm256_extract_epi8(output_row1_even,7);
		  break;
	case 9:

		filt[N-2][col] = (unsigned char) _mm256_extract_epi8(output_row0_even,0);
		filt[N-2][col+1] = (unsigned char) _mm256_extract_epi8(output_row0_even,1);
		filt[N-2][col+2] = (unsigned char) _mm256_extract_epi8(output_row0_even,2);
		filt[N-2][col+3] = (unsigned char) _mm256_extract_epi8(output_row0_even,3);
		filt[N-2][col+4] = (unsigned char) _mm256_extract_epi8(output_row0_even,4);
		filt[N-2][col+5] = (unsigned char) _mm256_extract_epi8(output_row0_even,5);
		filt[N-2][col+6] = (unsigned char) _mm256_extract_epi8(output_row0_even,6);
		filt[N-2][col+7] = (unsigned char) _mm256_extract_epi8(output_row0_even,7);
		filt[N-2][col+8] = (unsigned char) _mm256_extract_epi8(output_row0_even,8);

		filt[N-1][col] = (unsigned char) _mm256_extract_epi8(output_row1_even,0);
		filt[N-1][col+1] = (unsigned char) _mm256_extract_epi8(output_row1_even,1);
		filt[N-1][col+2] = (unsigned char) _mm256_extract_epi8(output_row1_even,2);
		filt[N-1][col+3] = (unsigned char) _mm256_extract_epi8(output_row1_even,3);
		filt[N-1][col+4] = (unsigned char) _mm256_extract_epi8(output_row1_even,4);
		filt[N-1][col+5] = (unsigned char) _mm256_extract_epi8(output_row1_even,5);
		filt[N-1][col+6] = (unsigned char) _mm256_extract_epi8(output_row1_even,6);
		filt[N-1][col+7] = (unsigned char) _mm256_extract_epi8(output_row1_even,7);
		filt[N-1][col+8] = (unsigned char) _mm256_extract_epi8(output_row1_even,8);
		  break;
	case 10:

		filt[N-2][col] = (unsigned char) _mm256_extract_epi8(output_row0_even,0);
		filt[N-2][col+1] = (unsigned char) _mm256_extract_epi8(output_row0_even,1);
		filt[N-2][col+2] = (unsigned char) _mm256_extract_epi8(output_row0_even,2);
		filt[N-2][col+3] = (unsigned char) _mm256_extract_epi8(output_row0_even,3);
		filt[N-2][col+4] = (unsigned char) _mm256_extract_epi8(output_row0_even,4);
		filt[N-2][col+5] = (unsigned char) _mm256_extract_epi8(output_row0_even,5);
		filt[N-2][col+6] = (unsigned char) _mm256_extract_epi8(output_row0_even,6);
		filt[N-2][col+7] = (unsigned char) _mm256_extract_epi8(output_row0_even,7);
		filt[N-2][col+8] = (unsigned char) _mm256_extract_epi8(output_row0_even,8);
		filt[N-2][col+9] = (unsigned char) _mm256_extract_epi8(output_row0_even,9);

		filt[N-1][col] = (unsigned char) _mm256_extract_epi8(output_row1_even,0);
		filt[N-1][col+1] = (unsigned char) _mm256_extract_epi8(output_row1_even,1);
		filt[N-1][col+2] = (unsigned char) _mm256_extract_epi8(output_row1_even,2);
		filt[N-1][col+3] = (unsigned char) _mm256_extract_epi8(output_row1_even,3);
		filt[N-1][col+4] = (unsigned char) _mm256_extract_epi8(output_row1_even,4);
		filt[N-1][col+5] = (unsigned char) _mm256_extract_epi8(output_row1_even,5);
		filt[N-1][col+6] = (unsigned char) _mm256_extract_epi8(output_row1_even,6);
		filt[N-1][col+7] = (unsigned char) _mm256_extract_epi8(output_row1_even,7);
		filt[N-1][col+8] = (unsigned char) _mm256_extract_epi8(output_row1_even,8);
		filt[N-1][col+9] = (unsigned char) _mm256_extract_epi8(output_row1_even,9);
		  break;
	case 11:

		filt[N-2][col] = (unsigned char) _mm256_extract_epi8(output_row0_even,0);
		filt[N-2][col+1] = (unsigned char) _mm256_extract_epi8(output_row0_even,1);
		filt[N-2][col+2] = (unsigned char) _mm256_extract_epi8(output_row0_even,2);
		filt[N-2][col+3] = (unsigned char) _mm256_extract_epi8(output_row0_even,3);
		filt[N-2][col+4] = (unsigned char) _mm256_extract_epi8(output_row0_even,4);
		filt[N-2][col+5] = (unsigned char) _mm256_extract_epi8(output_row0_even,5);
		filt[N-2][col+6] = (unsigned char) _mm256_extract_epi8(output_row0_even,6);
		filt[N-2][col+7] = (unsigned char) _mm256_extract_epi8(output_row0_even,7);
		filt[N-2][col+8] = (unsigned char) _mm256_extract_epi8(output_row0_even,8);
		filt[N-2][col+9] = (unsigned char) _mm256_extract_epi8(output_row0_even,9);
		filt[N-2][col+10] = (unsigned char) _mm256_extract_epi8(output_row0_even,10);

		filt[N-1][col] = (unsigned char) _mm256_extract_epi8(output_row1_even,0);
		filt[N-1][col+1] = (unsigned char) _mm256_extract_epi8(output_row1_even,1);
		filt[N-1][col+2] = (unsigned char) _mm256_extract_epi8(output_row1_even,2);
		filt[N-1][col+3] = (unsigned char) _mm256_extract_epi8(output_row1_even,3);
		filt[N-1][col+4] = (unsigned char) _mm256_extract_epi8(output_row1_even,4);
		filt[N-1][col+5] = (unsigned char) _mm256_extract_epi8(output_row1_even,5);
		filt[N-1][col+6] = (unsigned char) _mm256_extract_epi8(output_row1_even,6);
		filt[N-1][col+7] = (unsigned char) _mm256_extract_epi8(output_row1_even,7);
		filt[N-1][col+8] = (unsigned char) _mm256_extract_epi8(output_row1_even,8);
		filt[N-1][col+9] = (unsigned char) _mm256_extract_epi8(output_row1_even,9);
		filt[N-1][col+10] = (unsigned char) _mm256_extract_epi8(output_row1_even,10);
		  break;
	case 12:

		filt[N-2][col] = (unsigned char) _mm256_extract_epi8(output_row0_even,0);
		filt[N-2][col+1] = (unsigned char) _mm256_extract_epi8(output_row0_even,1);
		filt[N-2][col+2] = (unsigned char) _mm256_extract_epi8(output_row0_even,2);
		filt[N-2][col+3] = (unsigned char) _mm256_extract_epi8(output_row0_even,3);
		filt[N-2][col+4] = (unsigned char) _mm256_extract_epi8(output_row0_even,4);
		filt[N-2][col+5] = (unsigned char) _mm256_extract_epi8(output_row0_even,5);
		filt[N-2][col+6] = (unsigned char) _mm256_extract_epi8(output_row0_even,6);
		filt[N-2][col+7] = (unsigned char) _mm256_extract_epi8(output_row0_even,7);
		filt[N-2][col+8] = (unsigned char) _mm256_extract_epi8(output_row0_even,8);
		filt[N-2][col+9] = (unsigned char) _mm256_extract_epi8(output_row0_even,9);
		filt[N-2][col+10] = (unsigned char) _mm256_extract_epi8(output_row0_even,10);
		filt[N-2][col+11] = (unsigned char) _mm256_extract_epi8(output_row0_even,11);

		filt[N-1][col] = (unsigned char) _mm256_extract_epi8(output_row1_even,0);
		filt[N-1][col+1] = (unsigned char) _mm256_extract_epi8(output_row1_even,1);
		filt[N-1][col+2] = (unsigned char) _mm256_extract_epi8(output_row1_even,2);
		filt[N-1][col+3] = (unsigned char) _mm256_extract_epi8(output_row1_even,3);
		filt[N-1][col+4] = (unsigned char) _mm256_extract_epi8(output_row1_even,4);
		filt[N-1][col+5] = (unsigned char) _mm256_extract_epi8(output_row1_even,5);
		filt[N-1][col+6] = (unsigned char) _mm256_extract_epi8(output_row1_even,6);
		filt[N-1][col+7] = (unsigned char) _mm256_extract_epi8(output_row1_even,7);
		filt[N-1][col+8] = (unsigned char) _mm256_extract_epi8(output_row1_even,8);
		filt[N-1][col+9] = (unsigned char) _mm256_extract_epi8(output_row1_even,9);
		filt[N-1][col+10] = (unsigned char) _mm256_extract_epi8(output_row1_even,10);
		filt[N-1][col+11] = (unsigned char) _mm256_extract_epi8(output_row1_even,11);
		  break;
	case 13:

		filt[N-2][col] = (unsigned char) _mm256_extract_epi8(output_row0_even,0);
		filt[N-2][col+1] = (unsigned char) _mm256_extract_epi8(output_row0_even,1);
		filt[N-2][col+2] = (unsigned char) _mm256_extract_epi8(output_row0_even,2);
		filt[N-2][col+3] = (unsigned char) _mm256_extract_epi8(output_row0_even,3);
		filt[N-2][col+4] = (unsigned char) _mm256_extract_epi8(output_row0_even,4);
		filt[N-2][col+5] = (unsigned char) _mm256_extract_epi8(output_row0_even,5);
		filt[N-2][col+6] = (unsigned char) _mm256_extract_epi8(output_row0_even,6);
		filt[N-2][col+7] = (unsigned char) _mm256_extract_epi8(output_row0_even,7);
		filt[N-2][col+8] = (unsigned char) _mm256_extract_epi8(output_row0_even,8);
		filt[N-2][col+9] = (unsigned char) _mm256_extract_epi8(output_row0_even,9);
		filt[N-2][col+10] = (unsigned char) _mm256_extract_epi8(output_row0_even,10);
		filt[N-2][col+11] = (unsigned char) _mm256_extract_epi8(output_row0_even,11);
		filt[N-2][col+12] = (unsigned char) _mm256_extract_epi8(output_row0_even,12);

		filt[N-1][col] = (unsigned char) _mm256_extract_epi8(output_row1_even,0);
		filt[N-1][col+1] = (unsigned char) _mm256_extract_epi8(output_row1_even,1);
		filt[N-1][col+2] = (unsigned char) _mm256_extract_epi8(output_row1_even,2);
		filt[N-1][col+3] = (unsigned char) _mm256_extract_epi8(output_row1_even,3);
		filt[N-1][col+4] = (unsigned char) _mm256_extract_epi8(output_row1_even,4);
		filt[N-1][col+5] = (unsigned char) _mm256_extract_epi8(output_row1_even,5);
		filt[N-1][col+6] = (unsigned char) _mm256_extract_epi8(output_row1_even,6);
		filt[N-1][col+7] = (unsigned char) _mm256_extract_epi8(output_row1_even,7);
		filt[N-1][col+8] = (unsigned char) _mm256_extract_epi8(output_row1_even,8);
		filt[N-1][col+9] = (unsigned char) _mm256_extract_epi8(output_row1_even,9);
		filt[N-1][col+10] = (unsigned char) _mm256_extract_epi8(output_row1_even,10);
		filt[N-1][col+11] = (unsigned char) _mm256_extract_epi8(output_row1_even,11);
		filt[N-1][col+12] = (unsigned char) _mm256_extract_epi8(output_row1_even,12);
		  break;
	case 14:

		filt[N-2][col] = (unsigned char) _mm256_extract_epi8(output_row0_even,0);
		filt[N-2][col+1] = (unsigned char) _mm256_extract_epi8(output_row0_even,1);
		filt[N-2][col+2] = (unsigned char) _mm256_extract_epi8(output_row0_even,2);
		filt[N-2][col+3] = (unsigned char) _mm256_extract_epi8(output_row0_even,3);
		filt[N-2][col+4] = (unsigned char) _mm256_extract_epi8(output_row0_even,4);
		filt[N-2][col+5] = (unsigned char) _mm256_extract_epi8(output_row0_even,5);
		filt[N-2][col+6] = (unsigned char) _mm256_extract_epi8(output_row0_even,6);
		filt[N-2][col+7] = (unsigned char) _mm256_extract_epi8(output_row0_even,7);
		filt[N-2][col+8] = (unsigned char) _mm256_extract_epi8(output_row0_even,8);
		filt[N-2][col+9] = (unsigned char) _mm256_extract_epi8(output_row0_even,9);
		filt[N-2][col+10] = (unsigned char) _mm256_extract_epi8(output_row0_even,10);
		filt[N-2][col+11] = (unsigned char) _mm256_extract_epi8(output_row0_even,11);
		filt[N-2][col+12] = (unsigned char) _mm256_extract_epi8(output_row0_even,12);
		filt[N-2][col+13] = (unsigned char) _mm256_extract_epi8(output_row0_even,13);

		filt[N-1][col] = (unsigned char) _mm256_extract_epi8(output_row1_even,0);
		filt[N-1][col+1] = (unsigned char) _mm256_extract_epi8(output_row1_even,1);
		filt[N-1][col+2] = (unsigned char) _mm256_extract_epi8(output_row1_even,2);
		filt[N-1][col+3] = (unsigned char) _mm256_extract_epi8(output_row1_even,3);
		filt[N-1][col+4] = (unsigned char) _mm256_extract_epi8(output_row1_even,4);
		filt[N-1][col+5] = (unsigned char) _mm256_extract_epi8(output_row1_even,5);
		filt[N-1][col+6] = (unsigned char) _mm256_extract_epi8(output_row1_even,6);
		filt[N-1][col+7] = (unsigned char) _mm256_extract_epi8(output_row1_even,7);
		filt[N-1][col+8] = (unsigned char) _mm256_extract_epi8(output_row1_even,8);
		filt[N-1][col+9] = (unsigned char) _mm256_extract_epi8(output_row1_even,9);
		filt[N-1][col+10] = (unsigned char) _mm256_extract_epi8(output_row1_even,10);
		filt[N-1][col+11] = (unsigned char) _mm256_extract_epi8(output_row1_even,11);
		filt[N-1][col+12] = (unsigned char) _mm256_extract_epi8(output_row1_even,12);
		filt[N-1][col+13] = (unsigned char) _mm256_extract_epi8(output_row1_even,13);
		  break;
	case 15:

		filt[N-2][col] = (unsigned char) _mm256_extract_epi8(output_row0_even,0);
		filt[N-2][col+1] = (unsigned char) _mm256_extract_epi8(output_row0_even,1);
		filt[N-2][col+2] = (unsigned char) _mm256_extract_epi8(output_row0_even,2);
		filt[N-2][col+3] = (unsigned char) _mm256_extract_epi8(output_row0_even,3);
		filt[N-2][col+4] = (unsigned char) _mm256_extract_epi8(output_row0_even,4);
		filt[N-2][col+5] = (unsigned char) _mm256_extract_epi8(output_row0_even,5);
		filt[N-2][col+6] = (unsigned char) _mm256_extract_epi8(output_row0_even,6);
		filt[N-2][col+7] = (unsigned char) _mm256_extract_epi8(output_row0_even,7);
		filt[N-2][col+8] = (unsigned char) _mm256_extract_epi8(output_row0_even,8);
		filt[N-2][col+9] = (unsigned char) _mm256_extract_epi8(output_row0_even,9);
		filt[N-2][col+10] = (unsigned char) _mm256_extract_epi8(output_row0_even,10);
		filt[N-2][col+11] = (unsigned char) _mm256_extract_epi8(output_row0_even,11);
		filt[N-2][col+12] = (unsigned char) _mm256_extract_epi8(output_row0_even,12);
		filt[N-2][col+13] = (unsigned char) _mm256_extract_epi8(output_row0_even,13);
		filt[N-2][col+14] = (unsigned char) _mm256_extract_epi8(output_row0_even,14);

		filt[N-1][col] = (unsigned char) _mm256_extract_epi8(output_row1_even,0);
		filt[N-1][col+1] = (unsigned char) _mm256_extract_epi8(output_row1_even,1);
		filt[N-1][col+2] = (unsigned char) _mm256_extract_epi8(output_row1_even,2);
		filt[N-1][col+3] = (unsigned char) _mm256_extract_epi8(output_row1_even,3);
		filt[N-1][col+4] = (unsigned char) _mm256_extract_epi8(output_row1_even,4);
		filt[N-1][col+5] = (unsigned char) _mm256_extract_epi8(output_row1_even,5);
		filt[N-1][col+6] = (unsigned char) _mm256_extract_epi8(output_row1_even,6);
		filt[N-1][col+7] = (unsigned char) _mm256_extract_epi8(output_row1_even,7);
		filt[N-1][col+8] = (unsigned char) _mm256_extract_epi8(output_row1_even,8);
		filt[N-1][col+9] = (unsigned char) _mm256_extract_epi8(output_row1_even,9);
		filt[N-1][col+10] = (unsigned char) _mm256_extract_epi8(output_row1_even,10);
		filt[N-1][col+11] = (unsigned char) _mm256_extract_epi8(output_row1_even,11);
		filt[N-1][col+12] = (unsigned char) _mm256_extract_epi8(output_row1_even,12);
		filt[N-1][col+13] = (unsigned char) _mm256_extract_epi8(output_row1_even,13);
		filt[N-1][col+14] = (unsigned char) _mm256_extract_epi8(output_row1_even,14);
		  break;
	case 16:
		_mm_storeu_si128( (__m128i *) &filt[N-2][col],_mm256_extractf128_si256(output_row0_even, 0)); //store low 128bit - 16pixels
		_mm_storeu_si128( (__m128i *) &filt[N-1][col],_mm256_extractf128_si256(output_row1_even, 0)); //store low 128bit - 16pixels
		  break;
	case 17:
		_mm_storeu_si128( (__m128i *) &filt[N-2][col],_mm256_extractf128_si256(output_row0_even, 0)); //store low 128bit - 16pixels
		filt[N-2][col+16] = (unsigned char) _mm256_extract_epi8(output_row0_even,16);

		_mm_storeu_si128( (__m128i *) &filt[N-1][col],_mm256_extractf128_si256(output_row1_even, 0)); //store low 128bit - 16pixels
		filt[N-1][col+16] = (unsigned char) _mm256_extract_epi8(output_row1_even,16);
		  break;
	case 18:

		_mm_storeu_si128( (__m128i *) &filt[N-2][col],_mm256_extractf128_si256(output_row0_even, 0)); //store low 128bit - 16pixels
		filt[N-2][col+16] = (unsigned char) _mm256_extract_epi8(output_row0_even,16);
		filt[N-2][col+17] = (unsigned char) _mm256_extract_epi8(output_row0_even,17);

		_mm_storeu_si128( (__m128i *) &filt[N-1][col],_mm256_extractf128_si256(output_row1_even, 0)); //store low 128bit - 16pixels
		filt[N-1][col+16] = (unsigned char) _mm256_extract_epi8(output_row1_even,16);
		filt[N-1][col+17] = (unsigned char) _mm256_extract_epi8(output_row1_even,17);
		  break;
	case 19:

		_mm_storeu_si128( (__m128i *) &filt[N-2][col],_mm256_extractf128_si256(output_row0_even, 0)); //store low 128bit - 16pixels
		filt[N-2][col+16] = (unsigned char) _mm256_extract_epi8(output_row0_even,16);
		filt[N-2][col+17] = (unsigned char) _mm256_extract_epi8(output_row0_even,17);
		filt[N-2][col+18] = (unsigned char) _mm256_extract_epi8(output_row0_even,18);

		_mm_storeu_si128( (__m128i *) &filt[N-1][col],_mm256_extractf128_si256(output_row1_even, 0)); //store low 128bit - 16pixels
		filt[N-1][col+16] = (unsigned char) _mm256_extract_epi8(output_row1_even,16);
		filt[N-1][col+17] = (unsigned char) _mm256_extract_epi8(output_row1_even,17);
		filt[N-1][col+18] = (unsigned char) _mm256_extract_epi8(output_row1_even,18);
		  break;
	case 20:

		_mm_storeu_si128( (__m128i *) &filt[N-2][col],_mm256_extractf128_si256(output_row0_even, 0)); //store low 128bit - 16pixels
		filt[N-2][col+16] = (unsigned char) _mm256_extract_epi8(output_row0_even,16);
		filt[N-2][col+17] = (unsigned char) _mm256_extract_epi8(output_row0_even,17);
		filt[N-2][col+18] = (unsigned char) _mm256_extract_epi8(output_row0_even,18);
		filt[N-2][col+19] = (unsigned char) _mm256_extract_epi8(output_row0_even,19);

		_mm_storeu_si128( (__m128i *) &filt[N-1][col],_mm256_extractf128_si256(output_row1_even, 0)); //store low 128bit - 16pixels
		filt[N-1][col+16] = (unsigned char) _mm256_extract_epi8(output_row1_even,16);
		filt[N-1][col+17] = (unsigned char) _mm256_extract_epi8(output_row1_even,17);
		filt[N-1][col+18] = (unsigned char) _mm256_extract_epi8(output_row1_even,18);
		filt[N-1][col+19] = (unsigned char) _mm256_extract_epi8(output_row1_even,19);
		  break;
	case 21:
		_mm_storeu_si128( (__m128i *) &filt[N-2][col],_mm256_extractf128_si256(output_row0_even, 0)); //store low 128bit - 16pixels
		filt[N-2][col+16] = (unsigned char) _mm256_extract_epi8(output_row0_even,16);
		filt[N-2][col+17] = (unsigned char) _mm256_extract_epi8(output_row0_even,17);
		filt[N-2][col+18] = (unsigned char) _mm256_extract_epi8(output_row0_even,18);
		filt[N-2][col+19] = (unsigned char) _mm256_extract_epi8(output_row0_even,19);
		filt[N-2][col+20] = (unsigned char) _mm256_extract_epi8(output_row0_even,20);

		_mm_storeu_si128( (__m128i *) &filt[N-1][col],_mm256_extractf128_si256(output_row1_even, 0)); //store low 128bit - 16pixels
		filt[N-1][col+16] = (unsigned char) _mm256_extract_epi8(output_row1_even,16);
		filt[N-1][col+17] = (unsigned char) _mm256_extract_epi8(output_row1_even,17);
		filt[N-1][col+18] = (unsigned char) _mm256_extract_epi8(output_row1_even,18);
		filt[N-1][col+19] = (unsigned char) _mm256_extract_epi8(output_row1_even,19);
		filt[N-1][col+20] = (unsigned char) _mm256_extract_epi8(output_row1_even,20);
		  break;
	case 22:
		_mm_storeu_si128( (__m128i *) &filt[N-2][col],_mm256_extractf128_si256(output_row0_even, 0)); //store low 128bit - 16pixels
		filt[N-2][col+16] = (unsigned char) _mm256_extract_epi8(output_row0_even,16);
		filt[N-2][col+17] = (unsigned char) _mm256_extract_epi8(output_row0_even,17);
		filt[N-2][col+18] = (unsigned char) _mm256_extract_epi8(output_row0_even,18);
		filt[N-2][col+19] = (unsigned char) _mm256_extract_epi8(output_row0_even,19);
		filt[N-2][col+20] = (unsigned char) _mm256_extract_epi8(output_row0_even,20);
		filt[N-2][col+21] = (unsigned char) _mm256_extract_epi8(output_row0_even,21);

		_mm_storeu_si128( (__m128i *) &filt[N-1][col],_mm256_extractf128_si256(output_row1_even, 0)); //store low 128bit - 16pixels
		filt[N-1][col+16] = (unsigned char) _mm256_extract_epi8(output_row1_even,16);
		filt[N-1][col+17] = (unsigned char) _mm256_extract_epi8(output_row1_even,17);
		filt[N-1][col+18] = (unsigned char) _mm256_extract_epi8(output_row1_even,18);
		filt[N-1][col+19] = (unsigned char) _mm256_extract_epi8(output_row1_even,19);
		filt[N-1][col+20] = (unsigned char) _mm256_extract_epi8(output_row1_even,20);
		filt[N-1][col+21] = (unsigned char) _mm256_extract_epi8(output_row1_even,21);
		  break;
	case 23:
		_mm_storeu_si128( (__m128i *) &filt[N-2][col],_mm256_extractf128_si256(output_row0_even, 0)); //store low 128bit - 16pixels
		filt[N-2][col+16] = (unsigned char) _mm256_extract_epi8(output_row0_even,16);
		filt[N-2][col+17] = (unsigned char) _mm256_extract_epi8(output_row0_even,17);
		filt[N-2][col+18] = (unsigned char) _mm256_extract_epi8(output_row0_even,18);
		filt[N-2][col+19] = (unsigned char) _mm256_extract_epi8(output_row0_even,19);
		filt[N-2][col+20] = (unsigned char) _mm256_extract_epi8(output_row0_even,20);
		filt[N-2][col+21] = (unsigned char) _mm256_extract_epi8(output_row0_even,21);
		filt[N-2][col+22] = (unsigned char) _mm256_extract_epi8(output_row0_even,22);

		_mm_storeu_si128( (__m128i *) &filt[N-1][col],_mm256_extractf128_si256(output_row1_even, 0)); //store low 128bit - 16pixels
		filt[N-1][col+16] = (unsigned char) _mm256_extract_epi8(output_row1_even,16);
		filt[N-1][col+17] = (unsigned char) _mm256_extract_epi8(output_row1_even,17);
		filt[N-1][col+18] = (unsigned char) _mm256_extract_epi8(output_row1_even,18);
		filt[N-1][col+19] = (unsigned char) _mm256_extract_epi8(output_row1_even,19);
		filt[N-1][col+20] = (unsigned char) _mm256_extract_epi8(output_row1_even,20);
		filt[N-1][col+21] = (unsigned char) _mm256_extract_epi8(output_row1_even,21);
		filt[N-1][col+22] = (unsigned char) _mm256_extract_epi8(output_row1_even,22);
		  break;
	case 24:
		_mm_storeu_si128( (__m128i *) &filt[N-2][col],_mm256_extractf128_si256(output_row0_even, 0)); //store low 128bit - 16pixels
		filt[N-2][col+16] = (unsigned char) _mm256_extract_epi8(output_row0_even,16);
		filt[N-2][col+17] = (unsigned char) _mm256_extract_epi8(output_row0_even,17);
		filt[N-2][col+18] = (unsigned char) _mm256_extract_epi8(output_row0_even,18);
		filt[N-2][col+19] = (unsigned char) _mm256_extract_epi8(output_row0_even,19);
		filt[N-2][col+20] = (unsigned char) _mm256_extract_epi8(output_row0_even,20);
		filt[N-2][col+21] = (unsigned char) _mm256_extract_epi8(output_row0_even,21);
		filt[N-2][col+22] = (unsigned char) _mm256_extract_epi8(output_row0_even,22);
		filt[N-2][col+23] = (unsigned char) _mm256_extract_epi8(output_row0_even,23);

		_mm_storeu_si128( (__m128i *) &filt[N-1][col],_mm256_extractf128_si256(output_row1_even, 0)); //store low 128bit - 16pixels
		filt[N-1][col+16] = (unsigned char) _mm256_extract_epi8(output_row1_even,16);
		filt[N-1][col+17] = (unsigned char) _mm256_extract_epi8(output_row1_even,17);
		filt[N-1][col+18] = (unsigned char) _mm256_extract_epi8(output_row1_even,18);
		filt[N-1][col+19] = (unsigned char) _mm256_extract_epi8(output_row1_even,19);
		filt[N-1][col+20] = (unsigned char) _mm256_extract_epi8(output_row1_even,20);
		filt[N-1][col+21] = (unsigned char) _mm256_extract_epi8(output_row1_even,21);
		filt[N-1][col+22] = (unsigned char) _mm256_extract_epi8(output_row1_even,22);
		filt[N-1][col+23] = (unsigned char) _mm256_extract_epi8(output_row1_even,23);
		  break;
	case 25:
		_mm_storeu_si128( (__m128i *) &filt[N-2][col],_mm256_extractf128_si256(output_row0_even, 0)); //store low 128bit - 16pixels
		filt[N-2][col+16] = (unsigned char) _mm256_extract_epi8(output_row0_even,16);
		filt[N-2][col+17] = (unsigned char) _mm256_extract_epi8(output_row0_even,17);
		filt[N-2][col+18] = (unsigned char) _mm256_extract_epi8(output_row0_even,18);
		filt[N-2][col+19] = (unsigned char) _mm256_extract_epi8(output_row0_even,19);
		filt[N-2][col+20] = (unsigned char) _mm256_extract_epi8(output_row0_even,20);
		filt[N-2][col+21] = (unsigned char) _mm256_extract_epi8(output_row0_even,21);
		filt[N-2][col+22] = (unsigned char) _mm256_extract_epi8(output_row0_even,22);
		filt[N-2][col+23] = (unsigned char) _mm256_extract_epi8(output_row0_even,23);
		filt[N-2][col+24] = (unsigned char) _mm256_extract_epi8(output_row0_even,24);

		_mm_storeu_si128( (__m128i *) &filt[N-1][col],_mm256_extractf128_si256(output_row1_even, 0)); //store low 128bit - 16pixels
		filt[N-1][col+16] = (unsigned char) _mm256_extract_epi8(output_row1_even,16);
		filt[N-1][col+17] = (unsigned char) _mm256_extract_epi8(output_row1_even,17);
		filt[N-1][col+18] = (unsigned char) _mm256_extract_epi8(output_row1_even,18);
		filt[N-1][col+19] = (unsigned char) _mm256_extract_epi8(output_row1_even,19);
		filt[N-1][col+20] = (unsigned char) _mm256_extract_epi8(output_row1_even,20);
		filt[N-1][col+21] = (unsigned char) _mm256_extract_epi8(output_row1_even,21);
		filt[N-1][col+22] = (unsigned char) _mm256_extract_epi8(output_row1_even,22);
		filt[N-1][col+23] = (unsigned char) _mm256_extract_epi8(output_row1_even,23);
		filt[N-1][col+24] = (unsigned char) _mm256_extract_epi8(output_row1_even,24);
		  break;
	case 26:
		_mm_storeu_si128( (__m128i *) &filt[N-2][col],_mm256_extractf128_si256(output_row0_even, 0)); //store low 128bit - 16pixels
		filt[N-2][col+16] = (unsigned char) _mm256_extract_epi8(output_row0_even,16);
		filt[N-2][col+17] = (unsigned char) _mm256_extract_epi8(output_row0_even,17);
		filt[N-2][col+18] = (unsigned char) _mm256_extract_epi8(output_row0_even,18);
		filt[N-2][col+19] = (unsigned char) _mm256_extract_epi8(output_row0_even,19);
		filt[N-2][col+20] = (unsigned char) _mm256_extract_epi8(output_row0_even,20);
		filt[N-2][col+21] = (unsigned char) _mm256_extract_epi8(output_row0_even,21);
		filt[N-2][col+22] = (unsigned char) _mm256_extract_epi8(output_row0_even,22);
		filt[N-2][col+23] = (unsigned char) _mm256_extract_epi8(output_row0_even,23);
		filt[N-2][col+24] = (unsigned char) _mm256_extract_epi8(output_row0_even,24);
		filt[N-2][col+25] = (unsigned char) _mm256_extract_epi8(output_row0_even,25);

		_mm_storeu_si128( (__m128i *) &filt[N-1][col],_mm256_extractf128_si256(output_row1_even, 0)); //store low 128bit - 16pixels
		filt[N-1][col+16] = (unsigned char) _mm256_extract_epi8(output_row1_even,16);
		filt[N-1][col+17] = (unsigned char) _mm256_extract_epi8(output_row1_even,17);
		filt[N-1][col+18] = (unsigned char) _mm256_extract_epi8(output_row1_even,18);
		filt[N-1][col+19] = (unsigned char) _mm256_extract_epi8(output_row1_even,19);
		filt[N-1][col+20] = (unsigned char) _mm256_extract_epi8(output_row1_even,20);
		filt[N-1][col+21] = (unsigned char) _mm256_extract_epi8(output_row1_even,21);
		filt[N-1][col+22] = (unsigned char) _mm256_extract_epi8(output_row1_even,22);
		filt[N-1][col+23] = (unsigned char) _mm256_extract_epi8(output_row1_even,23);
		filt[N-1][col+24] = (unsigned char) _mm256_extract_epi8(output_row1_even,24);
		filt[N-1][col+25] = (unsigned char) _mm256_extract_epi8(output_row1_even,25);
		  break;
	case 27:
		_mm_storeu_si128( (__m128i *) &filt[N-2][col],_mm256_extractf128_si256(output_row0_even, 0)); //store low 128bit - 16pixels
		filt[N-2][col+16] = (unsigned char) _mm256_extract_epi8(output_row0_even,16);
		filt[N-2][col+17] = (unsigned char) _mm256_extract_epi8(output_row0_even,17);
		filt[N-2][col+18] = (unsigned char) _mm256_extract_epi8(output_row0_even,18);
		filt[N-2][col+19] = (unsigned char) _mm256_extract_epi8(output_row0_even,19);
		filt[N-2][col+20] = (unsigned char) _mm256_extract_epi8(output_row0_even,20);
		filt[N-2][col+21] = (unsigned char) _mm256_extract_epi8(output_row0_even,21);
		filt[N-2][col+22] = (unsigned char) _mm256_extract_epi8(output_row0_even,22);
		filt[N-2][col+23] = (unsigned char) _mm256_extract_epi8(output_row0_even,23);
		filt[N-2][col+24] = (unsigned char) _mm256_extract_epi8(output_row0_even,24);
		filt[N-2][col+25] = (unsigned char) _mm256_extract_epi8(output_row0_even,25);
		filt[N-2][col+26] = (unsigned char) _mm256_extract_epi8(output_row0_even,26);

		_mm_storeu_si128( (__m128i *) &filt[N-1][col],_mm256_extractf128_si256(output_row1_even, 0)); //store low 128bit - 16pixels
		filt[N-1][col+16] = (unsigned char) _mm256_extract_epi8(output_row1_even,16);
		filt[N-1][col+17] = (unsigned char) _mm256_extract_epi8(output_row1_even,17);
		filt[N-1][col+18] = (unsigned char) _mm256_extract_epi8(output_row1_even,18);
		filt[N-1][col+19] = (unsigned char) _mm256_extract_epi8(output_row1_even,19);
		filt[N-1][col+20] = (unsigned char) _mm256_extract_epi8(output_row1_even,20);
		filt[N-1][col+21] = (unsigned char) _mm256_extract_epi8(output_row1_even,21);
		filt[N-1][col+22] = (unsigned char) _mm256_extract_epi8(output_row1_even,22);
		filt[N-1][col+23] = (unsigned char) _mm256_extract_epi8(output_row1_even,23);
		filt[N-1][col+24] = (unsigned char) _mm256_extract_epi8(output_row1_even,24);
		filt[N-1][col+25] = (unsigned char) _mm256_extract_epi8(output_row1_even,25);
		filt[N-1][col+26] = (unsigned char) _mm256_extract_epi8(output_row1_even,26);
		  break;
	case 28:
		_mm_storeu_si128( (__m128i *) &filt[N-2][col],_mm256_extractf128_si256(output_row0_even, 0)); //store low 128bit - 16pixels
		filt[N-2][col+16] = (unsigned char) _mm256_extract_epi8(output_row0_even,16);
		filt[N-2][col+17] = (unsigned char) _mm256_extract_epi8(output_row0_even,17);
		filt[N-2][col+18] = (unsigned char) _mm256_extract_epi8(output_row0_even,18);
		filt[N-2][col+19] = (unsigned char) _mm256_extract_epi8(output_row0_even,19);
		filt[N-2][col+20] = (unsigned char) _mm256_extract_epi8(output_row0_even,20);
		filt[N-2][col+21] = (unsigned char) _mm256_extract_epi8(output_row0_even,21);
		filt[N-2][col+22] = (unsigned char) _mm256_extract_epi8(output_row0_even,22);
		filt[N-2][col+23] = (unsigned char) _mm256_extract_epi8(output_row0_even,23);
		filt[N-2][col+24] = (unsigned char) _mm256_extract_epi8(output_row0_even,24);
		filt[N-2][col+25] = (unsigned char) _mm256_extract_epi8(output_row0_even,25);
		filt[N-2][col+26] = (unsigned char) _mm256_extract_epi8(output_row0_even,26);
		filt[N-2][col+27] = (unsigned char) _mm256_extract_epi8(output_row0_even,27);

		_mm_storeu_si128( (__m128i *) &filt[N-1][col],_mm256_extractf128_si256(output_row1_even, 0)); //store low 128bit - 16pixels
		filt[N-1][col+16] = (unsigned char) _mm256_extract_epi8(output_row1_even,16);
		filt[N-1][col+17] = (unsigned char) _mm256_extract_epi8(output_row1_even,17);
		filt[N-1][col+18] = (unsigned char) _mm256_extract_epi8(output_row1_even,18);
		filt[N-1][col+19] = (unsigned char) _mm256_extract_epi8(output_row1_even,19);
		filt[N-1][col+20] = (unsigned char) _mm256_extract_epi8(output_row1_even,20);
		filt[N-1][col+21] = (unsigned char) _mm256_extract_epi8(output_row1_even,21);
		filt[N-1][col+22] = (unsigned char) _mm256_extract_epi8(output_row1_even,22);
		filt[N-1][col+23] = (unsigned char) _mm256_extract_epi8(output_row1_even,23);
		filt[N-1][col+24] = (unsigned char) _mm256_extract_epi8(output_row1_even,24);
		filt[N-1][col+25] = (unsigned char) _mm256_extract_epi8(output_row1_even,25);
		filt[N-1][col+26] = (unsigned char) _mm256_extract_epi8(output_row1_even,26);
		filt[N-1][col+27] = (unsigned char) _mm256_extract_epi8(output_row1_even,27);
		  break;
	case 29:
		_mm_storeu_si128( (__m128i *) &filt[N-2][col],_mm256_extractf128_si256(output_row0_even, 0)); //store low 128bit - 16pixels
		filt[N-2][col+16] = (unsigned char) _mm256_extract_epi8(output_row0_even,16);
		filt[N-2][col+17] = (unsigned char) _mm256_extract_epi8(output_row0_even,17);
		filt[N-2][col+18] = (unsigned char) _mm256_extract_epi8(output_row0_even,18);
		filt[N-2][col+19] = (unsigned char) _mm256_extract_epi8(output_row0_even,19);
		filt[N-2][col+20] = (unsigned char) _mm256_extract_epi8(output_row0_even,20);
		filt[N-2][col+21] = (unsigned char) _mm256_extract_epi8(output_row0_even,21);
		filt[N-2][col+22] = (unsigned char) _mm256_extract_epi8(output_row0_even,22);
		filt[N-2][col+23] = (unsigned char) _mm256_extract_epi8(output_row0_even,23);
		filt[N-2][col+24] = (unsigned char) _mm256_extract_epi8(output_row0_even,24);
		filt[N-2][col+25] = (unsigned char) _mm256_extract_epi8(output_row0_even,25);
		filt[N-2][col+26] = (unsigned char) _mm256_extract_epi8(output_row0_even,26);
		filt[N-2][col+27] = (unsigned char) _mm256_extract_epi8(output_row0_even,27);
		filt[N-2][col+28] = (unsigned char) _mm256_extract_epi8(output_row0_even,28);

		_mm_storeu_si128( (__m128i *) &filt[N-1][col],_mm256_extractf128_si256(output_row1_even, 0)); //store low 128bit - 16pixels
		filt[N-1][col+16] = (unsigned char) _mm256_extract_epi8(output_row1_even,16);
		filt[N-1][col+17] = (unsigned char) _mm256_extract_epi8(output_row1_even,17);
		filt[N-1][col+18] = (unsigned char) _mm256_extract_epi8(output_row1_even,18);
		filt[N-1][col+19] = (unsigned char) _mm256_extract_epi8(output_row1_even,19);
		filt[N-1][col+20] = (unsigned char) _mm256_extract_epi8(output_row1_even,20);
		filt[N-1][col+21] = (unsigned char) _mm256_extract_epi8(output_row1_even,21);
		filt[N-1][col+22] = (unsigned char) _mm256_extract_epi8(output_row1_even,22);
		filt[N-1][col+23] = (unsigned char) _mm256_extract_epi8(output_row1_even,23);
		filt[N-1][col+24] = (unsigned char) _mm256_extract_epi8(output_row1_even,24);
		filt[N-1][col+25] = (unsigned char) _mm256_extract_epi8(output_row1_even,25);
		filt[N-1][col+26] = (unsigned char) _mm256_extract_epi8(output_row1_even,26);
		filt[N-1][col+27] = (unsigned char) _mm256_extract_epi8(output_row1_even,27);
		filt[N-1][col+28] = (unsigned char) _mm256_extract_epi8(output_row1_even,28);
		  break;
	case 30:

		_mm_storeu_si128( (__m128i *) &filt[N-2][col],_mm256_extractf128_si256(output_row0_even, 0)); //store low 128bit - 16pixels
		filt[N-2][col+16] = (unsigned char) _mm256_extract_epi8(output_row0_even,16);
		filt[N-2][col+17] = (unsigned char) _mm256_extract_epi8(output_row0_even,17);
		filt[N-2][col+18] = (unsigned char) _mm256_extract_epi8(output_row0_even,18);
		filt[N-2][col+19] = (unsigned char) _mm256_extract_epi8(output_row0_even,19);
		filt[N-2][col+20] = (unsigned char) _mm256_extract_epi8(output_row0_even,20);
		filt[N-2][col+21] = (unsigned char) _mm256_extract_epi8(output_row0_even,21);
		filt[N-2][col+22] = (unsigned char) _mm256_extract_epi8(output_row0_even,22);
		filt[N-2][col+23] = (unsigned char) _mm256_extract_epi8(output_row0_even,23);
		filt[N-2][col+24] = (unsigned char) _mm256_extract_epi8(output_row0_even,24);
		filt[N-2][col+25] = (unsigned char) _mm256_extract_epi8(output_row0_even,25);
		filt[N-2][col+26] = (unsigned char) _mm256_extract_epi8(output_row0_even,26);
		filt[N-2][col+27] = (unsigned char) _mm256_extract_epi8(output_row0_even,27);
		filt[N-2][col+28] = (unsigned char) _mm256_extract_epi8(output_row0_even,28);
		filt[N-2][col+29] = (unsigned char) _mm256_extract_epi8(output_row0_even,29);


		_mm_storeu_si128( (__m128i *) &filt[N-1][col],_mm256_extractf128_si256(output_row1_even, 0)); //store low 128bit - 16pixels
		filt[N-1][col+16] = (unsigned char) _mm256_extract_epi8(output_row1_even,16);
		filt[N-1][col+17] = (unsigned char) _mm256_extract_epi8(output_row1_even,17);
		filt[N-1][col+18] = (unsigned char) _mm256_extract_epi8(output_row1_even,18);
		filt[N-1][col+19] = (unsigned char) _mm256_extract_epi8(output_row1_even,19);
		filt[N-1][col+20] = (unsigned char) _mm256_extract_epi8(output_row1_even,20);
		filt[N-1][col+21] = (unsigned char) _mm256_extract_epi8(output_row1_even,21);
		filt[N-1][col+22] = (unsigned char) _mm256_extract_epi8(output_row1_even,22);
		filt[N-1][col+23] = (unsigned char) _mm256_extract_epi8(output_row1_even,23);
		filt[N-1][col+24] = (unsigned char) _mm256_extract_epi8(output_row1_even,24);
		filt[N-1][col+25] = (unsigned char) _mm256_extract_epi8(output_row1_even,25);
		filt[N-1][col+26] = (unsigned char) _mm256_extract_epi8(output_row1_even,26);
		filt[N-1][col+27] = (unsigned char) _mm256_extract_epi8(output_row1_even,27);
		filt[N-1][col+28] = (unsigned char) _mm256_extract_epi8(output_row1_even,28);
		filt[N-1][col+29] = (unsigned char) _mm256_extract_epi8(output_row1_even,29);
		  break;
	case 31:
		_mm_storeu_si128( (__m128i *) &filt[N-2][col],_mm256_extractf128_si256(output_row0_even, 0)); //store low 128bit - 16pixels
		filt[N-2][col+16] = (unsigned char) _mm256_extract_epi8(output_row0_even,16);
		filt[N-2][col+17] = (unsigned char) _mm256_extract_epi8(output_row0_even,17);
		filt[N-2][col+18] = (unsigned char) _mm256_extract_epi8(output_row0_even,18);
		filt[N-2][col+19] = (unsigned char) _mm256_extract_epi8(output_row0_even,19);
		filt[N-2][col+20] = (unsigned char) _mm256_extract_epi8(output_row0_even,20);
		filt[N-2][col+21] = (unsigned char) _mm256_extract_epi8(output_row0_even,21);
		filt[N-2][col+22] = (unsigned char) _mm256_extract_epi8(output_row0_even,22);
		filt[N-2][col+23] = (unsigned char) _mm256_extract_epi8(output_row0_even,23);
		filt[N-2][col+24] = (unsigned char) _mm256_extract_epi8(output_row0_even,24);
		filt[N-2][col+25] = (unsigned char) _mm256_extract_epi8(output_row0_even,25);
		filt[N-2][col+26] = (unsigned char) _mm256_extract_epi8(output_row0_even,26);
		filt[N-2][col+27] = (unsigned char) _mm256_extract_epi8(output_row0_even,27);
		filt[N-2][col+28] = (unsigned char) _mm256_extract_epi8(output_row0_even,28);
		filt[N-2][col+29] = (unsigned char) _mm256_extract_epi8(output_row0_even,29);


		_mm_storeu_si128( (__m128i *) &filt[N-1][col],_mm256_extractf128_si256(output_row1_even, 0)); //store low 128bit - 16pixels
		filt[N-1][col+16] = (unsigned char) _mm256_extract_epi8(output_row1_even,16);
		filt[N-1][col+17] = (unsigned char) _mm256_extract_epi8(output_row1_even,17);
		filt[N-1][col+18] = (unsigned char) _mm256_extract_epi8(output_row1_even,18);
		filt[N-1][col+19] = (unsigned char) _mm256_extract_epi8(output_row1_even,19);
		filt[N-1][col+20] = (unsigned char) _mm256_extract_epi8(output_row1_even,20);
		filt[N-1][col+21] = (unsigned char) _mm256_extract_epi8(output_row1_even,21);
		filt[N-1][col+22] = (unsigned char) _mm256_extract_epi8(output_row1_even,22);
		filt[N-1][col+23] = (unsigned char) _mm256_extract_epi8(output_row1_even,23);
		filt[N-1][col+24] = (unsigned char) _mm256_extract_epi8(output_row1_even,24);
		filt[N-1][col+25] = (unsigned char) _mm256_extract_epi8(output_row1_even,25);
		filt[N-1][col+26] = (unsigned char) _mm256_extract_epi8(output_row1_even,26);
		filt[N-1][col+27] = (unsigned char) _mm256_extract_epi8(output_row1_even,27);
		filt[N-1][col+28] = (unsigned char) _mm256_extract_epi8(output_row1_even,28);
		filt[N-1][col+29] = (unsigned char) _mm256_extract_epi8(output_row1_even,29);

		//the filt[0:2][M-1] pixel will be computed without vectorization
		int newPixel;
		for (int row=N-2;row<=N-1;row++){
		 newPixel = 0;
		for (int rowOffset=-2; rowOffset<=2; rowOffset++) {
			for (int colOffset=-2; colOffset<=2; colOffset++) {

			   if ( ((row+rowOffset)<N) && ((M-1+colOffset)<M) && ((row+rowOffset)>=0) && ((M-1+colOffset)>=0) )
	              newPixel += frame1[row+rowOffset][M-1+colOffset] * filter[2 + rowOffset][2 + colOffset];

			      }
		        }
	   filt[row][M-1] = (unsigned char) (newPixel / divisor);
		}

		  break;
	default:
		printf("\n something went wrong");
	}

return 0;

}







void Gaussian_Blur_optimized_5x5_step28_less_div_reg_blocking_caseC(unsigned char **frame1,unsigned char **filt,const unsigned int M, const unsigned int N, const unsigned short int divisor, signed char **filter5x5){

const signed char f00=filter5x5[0][0];
const signed char f01=filter5x5[0][1];
const signed char f02=filter5x5[0][2];
const signed char f03=filter5x5[0][3];
const signed char f04=filter5x5[0][4];

const signed char f10=filter5x5[1][0];
const signed char f11=filter5x5[1][1];
const signed char f12=filter5x5[1][2];
const signed char f13=filter5x5[1][3];
const signed char f14=filter5x5[1][4];

const signed char f20=filter5x5[2][0];
const signed char f21=filter5x5[2][1];
const signed char f22=filter5x5[2][2];
const signed char f23=filter5x5[2][3];
const signed char f24=filter5x5[2][4];


	const __m256i c0=_mm256_set_epi8(0,0,0,f04,f03,f02,f01,f00,0,f04,f03,f02,f01,f00,0,f04,f03,f02,f01,f00,0,f04,f03,f02,f01,f00,0,f04,f03,f02,f01,f00);
	const __m256i c1=_mm256_set_epi8(0,0,0,f14,f13,f12,f11,f10,0,f14,f13,f12,f11,f10,0,f14,f13,f12,f11,f10,0,f14,f13,f12,f11,f10,0,f14,f13,f12,f11,f10);
	const __m256i c2=_mm256_set_epi8(0,0,0,f24,f23,f22,f21,f20,0,f24,f23,f22,f21,f20,0,f24,f23,f22,f21,f20,0,f24,f23,f22,f21,f20,0,f24,f23,f22,f21,f20);

	const __m256i c0_sh1=_mm256_set_epi8(0,0,f04,f03,f02,f01,f00,0,f04,f03,f02,f01,f00,0,f04,f03,f02,f01,f00,0,f04,f03,f02,f01,f00,0,f04,f03,f02,f01,f00,0);
	const __m256i c1_sh1=_mm256_set_epi8(0,0,f14,f13,f12,f11,f10,0,f14,f13,f12,f11,f10,0,f14,f13,f12,f11,f10,0,f14,f13,f12,f11,f10,0,f14,f13,f12,f11,f10,0);
	const __m256i c2_sh1=_mm256_set_epi8(0,0,f24,f23,f22,f21,f20,0,f24,f23,f22,f21,f20,0,f24,f23,f22,f21,f20,0,f24,f23,f22,f21,f20,0,f24,f23,f22,f21,f20,0);

	const __m256i c0_sh2=_mm256_set_epi8(0,f04,f03,f02,f01,f00,0,f04,f03,f02,f01,f00,0,f04,f03,f02,f01,f00,0,f04,f03,f02,f01,f00,0,f04,f03,f02,f01,f00,0,0);
	const __m256i c1_sh2=_mm256_set_epi8(0,f14,f13,f12,f11,f10,0,f14,f13,f12,f11,f10,0,f14,f13,f12,f11,f10,0,f14,f13,f12,f11,f10,0,f14,f13,f12,f11,f10,0,0);
	const __m256i c2_sh2=_mm256_set_epi8(0,f24,f23,f22,f21,f20,0,f24,f23,f22,f21,f20,0,f24,f23,f22,f21,f20,0,f24,f23,f22,f21,f20,0,f24,f23,f22,f21,f20,0,0);

	const __m256i c0_sh3=_mm256_set_epi8(f04,f03,f02,f01,f00,0,f04,f03,f02,f01,f00,0,f04,f03,f02,f01,f00,0,f04,f03,f02,f01,f00,0,f04,f03,f02,f01,f00,0,0,0);
	const __m256i c1_sh3=_mm256_set_epi8(f14,f13,f12,f11,f10,0,f14,f13,f12,f11,f10,0,f14,f13,f12,f11,f10,0,f14,f13,f12,f11,f10,0,f14,f13,f12,f11,f10,0,0,0);
	const __m256i c2_sh3=_mm256_set_epi8(f24,f23,f22,f21,f20,0,f24,f23,f22,f21,f20,0,f24,f23,f22,f21,f20,0,f24,f23,f22,f21,f20,0,f24,f23,f22,f21,f20,0,0,0);

	const __m256i c0_sh4=_mm256_set_epi8(0,0,0,0,0,f04,f03,f02,f01,f00,0,f04,f03,f02,f01,f00,0,f04,f03,f02,f01,f00,0,f04,f03,f02,f01,f00,0,0,0,0);
	const __m256i c1_sh4=_mm256_set_epi8(0,0,0,0,0,f14,f13,f12,f11,f10,0,f14,f13,f12,f11,f10,0,f14,f13,f12,f11,f10,0,f14,f13,f12,f11,f10,0,0,0,0);
	const __m256i c2_sh4=_mm256_set_epi8(0,0,0,0,0,f24,f23,f22,f21,f20,0,f24,f23,f22,f21,f20,0,f24,f23,f22,f21,f20,0,f24,f23,f22,f21,f20,0,0,0,0);

	const __m256i c0_sh5=_mm256_set_epi8(0,0,0,0,f04,f03,f02,f01,f00,0,f04,f03,f02,f01,f00,0,f04,f03,f02,f01,f00,0,f04,f03,f02,f01,f00,0,0,0,0,0);
	const __m256i c1_sh5=_mm256_set_epi8(0,0,0,0,f14,f13,f12,f11,f10,0,f14,f13,f12,f11,f10,0,f14,f13,f12,f11,f10,0,f14,f13,f12,f11,f10,0,0,0,0,0);
	const __m256i c2_sh5=_mm256_set_epi8(0,0,0,0,f24,f23,f22,f21,f20,0,f24,f23,f22,f21,f20,0,f24,f23,f22,f21,f20,0,f24,f23,f22,f21,f20,0,0,0,0,0);



	//const __m256i f  = _mm256_set_epi16(52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759);
	//const __m256i mask  = _mm256_set_epi8(0,0,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255);
	//const __m256i mask2  = _mm256_set_epi8(0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0);
	//const __m256i mask6  = _mm256_set_epi8(0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0);

	const __m256i mask3  = _mm256_set_epi16(0,0,0,0,0,0,0,65535,0,0,0,0,0,0,0,0);
	const __m256i mask_prelude  = _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0);

	const __m256i output_mask      = _mm256_set_epi16(0,0,0,65535,0,0,65535,0,0,65535,0,0,65535,0,0,65535);
	const __m256i output_mask_sh1  = _mm256_set_epi16(0,0,65535,0,0,65535,0,0,65535,0,0,65535,0,0,65535,0);
	const __m256i output_mask_sh2  = _mm256_set_epi16(0,0,0,    0,65535,0,0,65535,0,0,65535,0,0,65535,0,0);



	const unsigned int REMINDER_ITERATIONS =  (M-((((M-32)/28)*28)+28)); //M-(last_col_value+28)
	//printf("\n%d",REMINDER_ITERATIONS);


	const unsigned int division_case=prepare_for_division(divisor); //determine which is the division case (A, B or C)
	const __m256i f = _mm256_load_si256( (__m256i *) &f_vector[0]);// initialize the division vector

//printf("\n%d %d",division_case,b);


#pragma omp parallel
{

unsigned int row,col;
register __m256i r0,r1,r2,r3,r4,r5,m0,m1,m2,m3,m4,m5;
__m256i output_row0_even,output_row1_even,output_row0_odd,output_row1_odd; //these are for processing row #0 and #1 only.
__m256i output_even,output_odd;
__m256i m0_prelude_row0,m0_prelude_row1;


/*---------------------- Gaussian Blur ---------------------------------*/

    #pragma omp for schedule(static)
	for (row = 1; row <= N-2; row+=2) {

		if (row==1){//in this case I calculate filt[0][:] and filt[1][:] too. No extra loads or multiplications are required for this
			  for (col = 0; col <= M-32; col+=28){

				  if (col==0){//this is a special case as the mask gets outside of the array (it is like adding two zeros in the beginning of frame[][]

						//load the 5 rows
						r0=_mm256_loadu_si256( (__m256i *) &frame1[0][0]);
						r1=_mm256_loadu_si256( (__m256i *) &frame1[1][0]);
						r2=_mm256_loadu_si256( (__m256i *) &frame1[2][0]);
						r3=_mm256_loadu_si256( (__m256i *) &frame1[3][0]);
						r4=_mm256_loadu_si256( (__m256i *) &frame1[4][0]);

				  			//START - extra code needed for prelude
				  					m0=_mm256_slli_si256(r0,2);//shift 2 elements left - equivalent to filling with two zeros inthe beginning
				  					m1=_mm256_slli_si256(r1,2);//shift 2 elements left - equivalent to filling with two zeros inthe beginning
				  					m2=_mm256_slli_si256(r2,2);//shift 2 elements left - equivalent to filling with two zeros inthe beginning
				  					m3=_mm256_slli_si256(r3,2);//shift 2 elements left - equivalent to filling with two zeros inthe beginning
				  					m4=_mm256_slli_si256(r4,2);//shift 2 elements left - equivalent to filling with two zeros inthe beginning

				  					r0=_mm256_and_si256(r0,mask_prelude);
				  					r0=_mm256_permute2f128_si256(r0,r0,1);
				  					r0=_mm256_srli_si256(r0,14);//shift 14 elements
				  					r0=_mm256_add_epi16(m0,r0);

				  					r1=_mm256_and_si256(r1,mask_prelude);
				  					r1=_mm256_permute2f128_si256(r1,r1,1);
				  					r1=_mm256_srli_si256(r1,14);//shift 14 elements
				  					r1=_mm256_add_epi16(m1,r1);

				  					r2=_mm256_and_si256(r2,mask_prelude);
				  					r2=_mm256_permute2f128_si256(r2,r2,1);
				  					r2=_mm256_srli_si256(r2,14);//shift 14 elements
				  					r2=_mm256_add_epi16(m2,r2);

				  					r3=_mm256_and_si256(r3,mask_prelude);
				  					r3=_mm256_permute2f128_si256(r3,r3,1);
				  					r3=_mm256_srli_si256(r3,14);//shift 14 elements
				  					r3=_mm256_add_epi16(m3,r3);

				  					r4=_mm256_and_si256(r4,mask_prelude);
				  					r4=_mm256_permute2f128_si256(r4,r4,1);
				  					r4=_mm256_srli_si256(r4,14);//shift 14 elements
				  					r4=_mm256_add_epi16(m4,r4);

				  			//END - extra code needed for prelude
				  		}
				  else {
						//load the 5 rows
						r0=_mm256_loadu_si256( (__m256i *) &frame1[0][col-2]);
						r1=_mm256_loadu_si256( (__m256i *) &frame1[1][col-2]);
						r2=_mm256_loadu_si256( (__m256i *) &frame1[2][col-2]);
						r3=_mm256_loadu_si256( (__m256i *) &frame1[3][col-2]);
						r4=_mm256_loadu_si256( (__m256i *) &frame1[4][col-2]);
				  }

				//col iteration computes output pixels of 2,8,14,20,26
				// col+1 iteration computes output pixels of 3,9,15,21,27
				// col+2 iteration computes output pixels of 4,10,16,22,28
				// col+3 iteration computes output pixels of 5,11,17,23,29
				// col+4 iteration computes output pixels of 6,12,18,24,30
				// col+5 iteration computes output pixels of 7,13,19,25,31
				//afterwards, col2 becomes 32 and repeat the above process

				//1st col iteration

				  //--------------row=2
				//multiply with the mask
				m0=_mm256_maddubs_epi16(r0,c0);
				m1=_mm256_maddubs_epi16(r1,c1);
				m2=_mm256_maddubs_epi16(r2,c2);
				m3=_mm256_maddubs_epi16(r3,c1);
				m4=_mm256_maddubs_epi16(r4,c0);

				//vertical add
				m0=_mm256_add_epi16(m0,m1);
				m0=_mm256_add_epi16(m0,m2);
				m0=_mm256_add_epi16(m0,m3);
				m0=_mm256_add_epi16(m0,m4);


				m1=_mm256_srli_si256(m0,2);
				m2=_mm256_add_epi16(m1,m0);

				m1=_mm256_srli_si256(m0,4);
				m2=_mm256_add_epi16(m1,m2);

				//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
				//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
				m4=_mm256_and_si256(m0,mask3);
				m4=_mm256_permute2f128_si256(m4,m4,1);
				m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions
				m2=_mm256_add_epi16(m2,m4);

				//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
				output_even=_mm256_and_si256(m2,output_mask);

				//--------------row=0
				//multiply with the mask
				m0=_mm256_maddubs_epi16(r0,c2);
				m1=_mm256_maddubs_epi16(r1,c1);
				m2=_mm256_maddubs_epi16(r2,c0);

				//vertical add
				m0=_mm256_add_epi16(m0,m1);
				m0=_mm256_add_epi16(m0,m2);


				m1=_mm256_srli_si256(m0,2);
				m2=_mm256_add_epi16(m1,m0);

				m1=_mm256_srli_si256(m0,4);
				m2=_mm256_add_epi16(m1,m2);

				//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
				//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
				m4=_mm256_and_si256(m0,mask3);
				m4=_mm256_permute2f128_si256(m4,m4,1);
				m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions
				m2=_mm256_add_epi16(m2,m4);

				//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
				output_row0_even=_mm256_and_si256(m2,output_mask);

				//--------------row=1
				//multiply with the mask
				m0=_mm256_maddubs_epi16(r0,c1);
				m1=_mm256_maddubs_epi16(r1,c2);
				m2=_mm256_maddubs_epi16(r2,c1);
				m3=_mm256_maddubs_epi16(r3,c0);

				//vertical add
				m0=_mm256_add_epi16(m0,m1);
				m0=_mm256_add_epi16(m0,m2);
				m0=_mm256_add_epi16(m0,m3);

				m1=_mm256_srli_si256(m0,2);
				m2=_mm256_add_epi16(m1,m0);

				m1=_mm256_srli_si256(m0,4);
				m2=_mm256_add_epi16(m1,m2);

				//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
				//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
				m4=_mm256_and_si256(m0,mask3);
				m4=_mm256_permute2f128_si256(m4,m4,1);
				m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions
				m2=_mm256_add_epi16(m2,m4);

				//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
				output_row1_even=_mm256_and_si256(m2,output_mask);

				//2nd col iteration
				//multiply with the mask

				//----row=2
				m0=_mm256_maddubs_epi16(r0,c0_sh1);
				m1=_mm256_maddubs_epi16(r1,c1_sh1);
				m2=_mm256_maddubs_epi16(r2,c2_sh1);
				m3=_mm256_maddubs_epi16(r3,c1_sh1);
				m4=_mm256_maddubs_epi16(r4,c0_sh1);

				//vertical add
				m0=_mm256_add_epi16(m0,m1);
				m0=_mm256_add_epi16(m0,m2);
				m0=_mm256_add_epi16(m0,m3);
				m0=_mm256_add_epi16(m0,m4);


				m1=_mm256_srli_si256(m0,2);
				m2=_mm256_add_epi16(m1,m0);

				m1=_mm256_srli_si256(m0,4);
				m2=_mm256_add_epi16(m1,m2);

				//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
				//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
				m4=_mm256_and_si256(m0,mask3);
				m4=_mm256_permute2f128_si256(m4,m4,1);
				m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions
				m2=_mm256_add_epi16(m2,m4);

				//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
				output_odd = _mm256_and_si256(m2,output_mask);

				//----row=0
				m0=_mm256_maddubs_epi16(r0,c2_sh1);
				m1=_mm256_maddubs_epi16(r1,c1_sh1);
				m2=_mm256_maddubs_epi16(r2,c0_sh1);

				//vertical add
				m0=_mm256_add_epi16(m0,m1);
				m0=_mm256_add_epi16(m0,m2);


				m1=_mm256_srli_si256(m0,2);
				m2=_mm256_add_epi16(m1,m0);

				m1=_mm256_srli_si256(m0,4);
				m2=_mm256_add_epi16(m1,m2);

				//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
				//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
				m4=_mm256_and_si256(m0,mask3);
				m4=_mm256_permute2f128_si256(m4,m4,1);
				m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions
				m2=_mm256_add_epi16(m2,m4);

				//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
				output_row0_odd = _mm256_and_si256(m2,output_mask);

				//----row=1
				m0=_mm256_maddubs_epi16(r0,c1_sh1);
				m1=_mm256_maddubs_epi16(r1,c2_sh1);
				m2=_mm256_maddubs_epi16(r2,c1_sh1);
				m3=_mm256_maddubs_epi16(r3,c0_sh1);

				//vertical add
				m0=_mm256_add_epi16(m0,m1);
				m0=_mm256_add_epi16(m0,m2);
				m0=_mm256_add_epi16(m0,m3);


				m1=_mm256_srli_si256(m0,2);
				m2=_mm256_add_epi16(m1,m0);

				m1=_mm256_srli_si256(m0,4);
				m2=_mm256_add_epi16(m1,m2);

				//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
				//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
				m4=_mm256_and_si256(m0,mask3);
				m4=_mm256_permute2f128_si256(m4,m4,1);
				m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions
				m2=_mm256_add_epi16(m2,m4);

				//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
				output_row1_odd = _mm256_and_si256(m2,output_mask);

		                 //3rd col iteration
				//---row=2
				//multiply with the mask
				m0=_mm256_maddubs_epi16(r0,c0_sh2);
				m1=_mm256_maddubs_epi16(r1,c1_sh2);
				m2=_mm256_maddubs_epi16(r2,c2_sh2);
				m3=_mm256_maddubs_epi16(r3,c1_sh2);
				m4=_mm256_maddubs_epi16(r4,c0_sh2);

				//vertical add
				m0=_mm256_add_epi16(m0,m1);
				m0=_mm256_add_epi16(m0,m2);
				m0=_mm256_add_epi16(m0,m3);
				m0=_mm256_add_epi16(m0,m4);


				m1=_mm256_srli_si256(m0,2);
				m2=_mm256_add_epi16(m1,m0);

				//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,_mm256_extract_epi16(m2,8),0,0,0,0,0,0,0);
				m4=_mm256_and_si256(m2,mask3);
				m4=_mm256_permute2f128_si256(m4,m4,1);
				m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions

				m1=_mm256_srli_si256(m0,4);
				m2=_mm256_add_epi16(m1,m2);
				m2=_mm256_add_epi16(m2,m4);

				//m2 has 16 16bit values now. the results I need are in positions 1,4,7,10,13. keep only those, discard others
				m2 = _mm256_and_si256(m2,output_mask_sh1);
				output_even = _mm256_add_epi16(output_even,m2);

				//---row=0
				//multiply with the mask

				m0=_mm256_maddubs_epi16(r0,c2_sh2);
				m1=_mm256_maddubs_epi16(r1,c1_sh2);
				m2=_mm256_maddubs_epi16(r2,c0_sh2);

				//vertical add
				m0=_mm256_add_epi16(m0,m1);
				m0=_mm256_add_epi16(m0,m2);


				m1=_mm256_srli_si256(m0,2);
				m2=_mm256_add_epi16(m1,m0);

				//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,_mm256_extract_epi16(m2,8),0,0,0,0,0,0,0);
				m4=_mm256_and_si256(m2,mask3);
				m4=_mm256_permute2f128_si256(m4,m4,1);
				m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions

				m1=_mm256_srli_si256(m0,4);
				m2=_mm256_add_epi16(m1,m2);

				//prepei na prosthesw to pixel pu thelw sto r2 kai meta na kanw diairesi me ola mazi

				m2=_mm256_add_epi16(m2,m4);

				//m2 has 16 16bit values now. the results I need are in positions 1,4,7,10,13. keep only those, discard others
				m2 = _mm256_and_si256(m2,output_mask_sh1);
				output_row0_even = _mm256_add_epi16(output_row0_even,m2);


				//---row=1
				//multiply with the mask

				m0=_mm256_maddubs_epi16(r0,c1_sh2);
				m1=_mm256_maddubs_epi16(r1,c2_sh2);
				m2=_mm256_maddubs_epi16(r2,c1_sh2);
				m3=_mm256_maddubs_epi16(r3,c0_sh2);

				//vertical add
				m0=_mm256_add_epi16(m0,m1);
				m0=_mm256_add_epi16(m0,m2);
				m0=_mm256_add_epi16(m0,m3);

				m1=_mm256_srli_si256(m0,2);
				m2=_mm256_add_epi16(m1,m0);

				//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,_mm256_extract_epi16(m2,8),0,0,0,0,0,0,0);
				m4=_mm256_and_si256(m2,mask3);
				m4=_mm256_permute2f128_si256(m4,m4,1);
				m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions

				m1=_mm256_srli_si256(m0,4);
				m2=_mm256_add_epi16(m1,m2);

				//prepei na prosthesw to pixel pu thelw sto r2 kai meta na kanw diairesi me ola mazi

				m2=_mm256_add_epi16(m2,m4);

				//m2 has 16 16bit values now. the results I need are in positions 1,4,7,10,13. keep only those, discard others
				m2 = _mm256_and_si256(m2,output_mask_sh1);
				output_row1_even = _mm256_add_epi16(output_row1_even,m2);

				//4th col iteration

				//4th col iteration

				//multiply with the mask
				m0=_mm256_maddubs_epi16(r0,c0_sh3);
				m1=_mm256_maddubs_epi16(r1,c1_sh3);
				m2=_mm256_maddubs_epi16(r2,c2_sh3);
				m3=_mm256_maddubs_epi16(r3,c1_sh3);
				m4=_mm256_maddubs_epi16(r4,c0_sh3);

				//vertical add
				m0=_mm256_add_epi16(m0,m1);
				m2=_mm256_add_epi16(m2,m3);
				m0=_mm256_add_epi16(m0,m2);
				m0=_mm256_add_epi16(m0,m4);

				//hozizontal additions
				//hadd(3:8)
				//hadd(9:14)
				//hadd(15:20)
				//hadd(21:26)
				//hadd(27:31)
				//result after division will be in 2,8,14,20,26

				m1=_mm256_srli_si256(m0,2);
				m4=_mm256_add_epi16(m1,m0);

				m1=_mm256_srli_si256(m0,4);
				m2=_mm256_add_epi16(m1,m4);

				//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
				//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
				m4=_mm256_and_si256(m4,mask3);
				m4=_mm256_permute2f128_si256(m4,m4,1);
				m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions

				m2=_mm256_add_epi16(m2,m4);

				//m2 has 16 16bit values now. the results I need are in positions 1,4,7,10,13. keep only those, discard others
				m2 = _mm256_and_si256(m2,output_mask_sh1);
				output_odd = _mm256_add_epi16(output_odd,m2);

				//row0
				//multiply with the mask
								m0=_mm256_maddubs_epi16(r0,c2_sh3);
								m1=_mm256_maddubs_epi16(r1,c1_sh3);
								m2=_mm256_maddubs_epi16(r2,c0_sh3);


								//vertical add
								m0=_mm256_add_epi16(m0,m1);
								m0=_mm256_add_epi16(m0,m2);


								m1=_mm256_srli_si256(m0,2);
								m4=_mm256_add_epi16(m1,m0);

								m1=_mm256_srli_si256(m0,4);
								m2=_mm256_add_epi16(m1,m4);

								//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
								//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
								m4=_mm256_and_si256(m4,mask3);
								m4=_mm256_permute2f128_si256(m4,m4,1);
								m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions

								m2=_mm256_add_epi16(m2,m4);

								//m2 has 16 16bit values now. the results I need are in positions 1,4,7,10,13. keep only those, discard others
								m2 = _mm256_and_si256(m2,output_mask_sh1);
								output_row0_odd = _mm256_add_epi16(output_row0_odd,m2);


								//-----------row1
								//multiply with the mask
												m0=_mm256_maddubs_epi16(r0,c1_sh3);
												m1=_mm256_maddubs_epi16(r1,c2_sh3);
												m2=_mm256_maddubs_epi16(r2,c1_sh3);
												m3=_mm256_maddubs_epi16(r3,c0_sh3);

												//vertical add
												m0=_mm256_add_epi16(m0,m1);
												m0=_mm256_add_epi16(m0,m2);
												m0=_mm256_add_epi16(m0,m3);

												m1=_mm256_srli_si256(m0,2);
												m4=_mm256_add_epi16(m1,m0);

												m1=_mm256_srli_si256(m0,4);
												m2=_mm256_add_epi16(m1,m4);

												//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
												//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
												m4=_mm256_and_si256(m4,mask3);
												m4=_mm256_permute2f128_si256(m4,m4,1);
												m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions

												m2=_mm256_add_epi16(m2,m4);

												//m2 has 16 16bit values now. the results I need are in positions 1,4,7,10,13. keep only those, discard others
												m2 = _mm256_and_si256(m2,output_mask_sh1);
												output_row1_odd = _mm256_add_epi16(output_row1_odd,m2);



				//5th col iteration
				//multiply with the mask
				m0=_mm256_maddubs_epi16(r0,c0_sh4);
				m1=_mm256_maddubs_epi16(r1,c1_sh4);
				m2=_mm256_maddubs_epi16(r2,c2_sh4);
				m3=_mm256_maddubs_epi16(r3,c1_sh4);
				m4=_mm256_maddubs_epi16(r4,c0_sh4);

				//vertical add
				m0=_mm256_add_epi16(m0,m1);
				m2=_mm256_add_epi16(m2,m3);
				m0=_mm256_add_epi16(m0,m2);
				m0=_mm256_add_epi16(m0,m4);

				//hozizontal additions
				//hadd(4:9)
				//hadd(10:15)
				//hadd(16:21)
				//hadd(22:27)
				//result after division will be in 4,10,16,22

				m1=_mm256_srli_si256(m0,2);
				m2=_mm256_add_epi16(m1,m0);

				m1=_mm256_srli_si256(m0,4);
				m2=_mm256_add_epi16(m1,m2);


				//m2 has 16 16bit values now. the results I need are in positions 2,5,8,11. keep only those, discard others
				m2 = _mm256_and_si256(m2,output_mask_sh2);
				output_even = _mm256_add_epi16(output_even,m2);

				//row0
				//multiply with the mask
				m0=_mm256_maddubs_epi16(r0,c2_sh4);
				m1=_mm256_maddubs_epi16(r1,c1_sh4);
				m2=_mm256_maddubs_epi16(r2,c0_sh4);


				//vertical add
				m0=_mm256_add_epi16(m0,m1);
				m0=_mm256_add_epi16(m0,m2);


				//hozizontal additions
				//hadd(4:9)
				//hadd(10:15)
				//hadd(16:21)
				//hadd(22:27)
				//result after division will be in 4,10,16,22

				m1=_mm256_srli_si256(m0,2);
				m2=_mm256_add_epi16(m1,m0);

				m1=_mm256_srli_si256(m0,4);
				m2=_mm256_add_epi16(m1,m2);


				//m2 has 16 16bit values now. the results I need are in positions 2,5,8,11. keep only those, discard others
				m2 = _mm256_and_si256(m2,output_mask_sh2);
				output_row0_even = _mm256_add_epi16(output_row0_even,m2);

				//row1
				//multiply with the mask
				m0=_mm256_maddubs_epi16(r0,c1_sh4);
				m1=_mm256_maddubs_epi16(r1,c2_sh4);
				m2=_mm256_maddubs_epi16(r2,c1_sh4);
				m3=_mm256_maddubs_epi16(r3,c0_sh4);


				//vertical add
				m0=_mm256_add_epi16(m0,m1);
				m0=_mm256_add_epi16(m0,m2);
				m0=_mm256_add_epi16(m0,m3);


				m1=_mm256_srli_si256(m0,2);
				m2=_mm256_add_epi16(m1,m0);

				m1=_mm256_srli_si256(m0,4);
				m2=_mm256_add_epi16(m1,m2);


				//m2 has 16 16bit values now. the results I need are in positions 2,5,8,11. keep only those, discard others
				m2 = _mm256_and_si256(m2,output_mask_sh2);
				output_row1_even = _mm256_add_epi16(output_row1_even,m2);


		                 //6th col iteration
				//multiply with the mask
				m0=_mm256_maddubs_epi16(r0,c0_sh5);
				m1=_mm256_maddubs_epi16(r1,c1_sh5);
				m2=_mm256_maddubs_epi16(r2,c2_sh5);
				m3=_mm256_maddubs_epi16(r3,c1_sh5);
				m4=_mm256_maddubs_epi16(r4,c0_sh5);

				//vertical add
				m0=_mm256_add_epi16(m0,m1);
				m2=_mm256_add_epi16(m2,m3);
				m0=_mm256_add_epi16(m0,m2);
				m0=_mm256_add_epi16(m0,m4);

				//hozizontal additions
				//hadd(5:10)
				//hadd(11:16)
				//hadd(17:22)
				//hadd(23:28)
				//result after division will be in 4,10,16,22

				m1=_mm256_srli_si256(m0,2);
				m2=_mm256_add_epi16(m1,m0);

				m1=_mm256_srli_si256(m0,4);
				m2=_mm256_add_epi16(m1,m2);

				//m2 has 16 16bit values now. the results I need are in positions 2,5,8,11. keep only those, discard others
				m2 = _mm256_and_si256(m2,output_mask_sh2);
				output_odd = _mm256_add_epi16(output_odd,m2);

				//now division follows
				m3 = _mm256_mulhi_epu16(output_even, f);             // multiply high unsigned words
				m1 = _mm256_sub_epi16(m2, m3);                     // subtract
				m1 = _mm256_srli_epi16(m1, 16);             // shift right logical with w
				m3 = _mm256_add_epi16(m3, m1);                    // add
				output_even=_mm256_srli_epi16(m3, b);            // shift right logical with b

				//now division follows
				m3 = _mm256_mulhi_epu16(output_odd, f);             // multiply high unsigned words
				m1 = _mm256_sub_epi16(m2, m3);                     // subtract
				m1 = _mm256_srli_epi16(m1, 16);             // shift right logical with w
				m3 = _mm256_add_epi16(m3, m1);                    // add
				output_odd=_mm256_srli_epi16(m3, b);            // shift right logical with b

				//shift odd 1 position and add to even
				output_odd = _mm256_slli_si256(output_odd,1);
				output_even = _mm256_add_epi8(output_even,output_odd);

				_mm256_storeu_si256( (__m256i *) &filt[2][col],output_even );

				//row0
				//multiply with the mask
				m0=_mm256_maddubs_epi16(r0,c2_sh5);
				m1=_mm256_maddubs_epi16(r1,c1_sh5);
				m2=_mm256_maddubs_epi16(r2,c0_sh5);

				//vertical add
				m0=_mm256_add_epi16(m0,m1);
				m0=_mm256_add_epi16(m0,m2);


				m1=_mm256_srli_si256(m0,2);
				m2=_mm256_add_epi16(m1,m0);

				m1=_mm256_srli_si256(m0,4);
				m2=_mm256_add_epi16(m1,m2);

				//m2 has 16 16bit values now. the results I need are in positions 2,5,8,11. keep only those, discard others
				m2 = _mm256_and_si256(m2,output_mask_sh2);
				output_row0_odd = _mm256_add_epi16(output_row0_odd,m2);

				//now division follows
				//now division follows
				m3 = _mm256_mulhi_epu16(output_row0_even, f);             // multiply high unsigned words
				m1 = _mm256_sub_epi16(m2, m3);                     // subtract
				m1 = _mm256_srli_epi16(m1, 16);             // shift right logical with w
				m3 = _mm256_add_epi16(m3, m1);                    // add
				output_row0_even=_mm256_srli_epi16(m3, b);            // shift right logical with b

				//now division follows
				m3 = _mm256_mulhi_epu16(output_row0_odd, f);             // multiply high unsigned words
				m1 = _mm256_sub_epi16(m2, m3);                     // subtract
				m1 = _mm256_srli_epi16(m1, 16);             // shift right logical with w
				m3 = _mm256_add_epi16(m3, m1);                    // add
				output_row0_odd=_mm256_srli_epi16(m3, b);            // shift right logical with b

				//shift odd 1 position and add to even
				output_row0_odd = _mm256_slli_si256(output_row0_odd,1);
				output_row0_even = _mm256_add_epi8(output_row0_even,output_row0_odd);


				_mm256_storeu_si256( (__m256i *) &filt[0][col],output_row0_even );

				//row1

				//multiply with the mask
				m0=_mm256_maddubs_epi16(r0,c1_sh5);
				m1=_mm256_maddubs_epi16(r1,c2_sh5);
				m2=_mm256_maddubs_epi16(r2,c1_sh5);
				m3=_mm256_maddubs_epi16(r3,c0_sh5);

				//vertical add
				m0=_mm256_add_epi16(m0,m1);
				m0=_mm256_add_epi16(m0,m2);
				m0=_mm256_add_epi16(m0,m3);


				m1=_mm256_srli_si256(m0,2);
				m2=_mm256_add_epi16(m1,m0);

				m1=_mm256_srli_si256(m0,4);
				m2=_mm256_add_epi16(m1,m2);

				//m2 has 16 16bit values now. the results I need are in positions 2,5,8,11. keep only those, discard others
				m2 = _mm256_and_si256(m2,output_mask_sh2);
				output_row1_odd = _mm256_add_epi16(output_row1_odd,m2);

				//now division follows
				m3 = _mm256_mulhi_epu16(output_row1_even, f);             // multiply high unsigned words
				m1 = _mm256_sub_epi16(m2, m3);                     // subtract
				m1 = _mm256_srli_epi16(m1, 16);             // shift right logical with w
				m3 = _mm256_add_epi16(m3, m1);                    // add
				output_row1_even=_mm256_srli_epi16(m3, b);            // shift right logical with b

				//now division follows
				m3 = _mm256_mulhi_epu16(output_row1_odd, f);             // multiply high unsigned words
				m1 = _mm256_sub_epi16(m2, m3);                     // subtract
				m1 = _mm256_srli_epi16(m1, 16);             // shift right logical with w
				m3 = _mm256_add_epi16(m3, m1);                    // add
				output_row1_odd=_mm256_srli_epi16(m3, b);            // shift right logical with b

				//shift odd 1 position and add to even
				output_row1_odd = _mm256_slli_si256(output_row1_odd,1);
				output_row1_even = _mm256_add_epi8(output_row1_even,output_row1_odd);


				_mm256_storeu_si256( (__m256i *) &filt[1][col],output_row1_even );


				}


		    loop_reminder_first_less_div(frame1,filt,M,N,col,REMINDER_ITERATIONS,division_case,c0,c1,c2,c0_sh1,c1_sh1,c2_sh1,c0_sh2,c1_sh2,c2_sh2,f,divisor,filter5x5);
		}

		else if ((row==N-3) ){//in this case I calculate filt[N-3][:], filt[N-2][:] and filt[N-1][:]. Below row0 refers to row=N-2 and row1 refers to row=N-1
			//printf("\nN=%d row=%d N-2=%d",N,row,N-2);
			for (col = 0; col <= M-32; col+=28){
						 //last col value that does not read outside of the array bounds is (col<M-29)

							  if (col==0){

									//load the 5 rows
									r0=_mm256_loadu_si256( (__m256i *) &frame1[N-5][0]);
									r1=_mm256_loadu_si256( (__m256i *) &frame1[N-4][0]);
									r2=_mm256_loadu_si256( (__m256i *) &frame1[N-3][0]);
									r3=_mm256_loadu_si256( (__m256i *) &frame1[N-2][0]);
									r4=_mm256_loadu_si256( (__m256i *) &frame1[N-1][0]);

							  			//START - extra code needed for prelude
							  					m0=_mm256_slli_si256(r0,2);//shift 2 elements left - equivalent to filling with two zeros inthe beginning
							  					m1=_mm256_slli_si256(r1,2);//shift 2 elements left - equivalent to filling with two zeros inthe beginning
							  					m2=_mm256_slli_si256(r2,2);//shift 2 elements left - equivalent to filling with two zeros inthe beginning
							  					m3=_mm256_slli_si256(r3,2);//shift 2 elements left - equivalent to filling with two zeros inthe beginning
							  					m4=_mm256_slli_si256(r4,2);//shift 2 elements left - equivalent to filling with two zeros inthe beginning

							  					r0=_mm256_and_si256(r0,mask_prelude);
							  					r0=_mm256_permute2f128_si256(r0,r0,1);
							  					r0=_mm256_srli_si256(r0,14);//shift 14 elements
							  					r0=_mm256_add_epi16(m0,r0);

							  					r1=_mm256_and_si256(r1,mask_prelude);
							  					r1=_mm256_permute2f128_si256(r1,r1,1);
							  					r1=_mm256_srli_si256(r1,14);//shift 14 elements
							  					r1=_mm256_add_epi16(m1,r1);

							  					r2=_mm256_and_si256(r2,mask_prelude);
							  					r2=_mm256_permute2f128_si256(r2,r2,1);
							  					r2=_mm256_srli_si256(r2,14);//shift 14 elements
							  					r2=_mm256_add_epi16(m2,r2);

							  					r3=_mm256_and_si256(r3,mask_prelude);
							  					r3=_mm256_permute2f128_si256(r3,r3,1);
							  					r3=_mm256_srli_si256(r3,14);//shift 14 elements
							  					r3=_mm256_add_epi16(m3,r3);

							  					r4=_mm256_and_si256(r4,mask_prelude);
							  					r4=_mm256_permute2f128_si256(r4,r4,1);
							  					r4=_mm256_srli_si256(r4,14);//shift 14 elements
							  					r4=_mm256_add_epi16(m4,r4);

							  			//END - extra code needed for prelude
							  		}
							  else {
									//load the 5 rows
									r0=_mm256_loadu_si256( (__m256i *) &frame1[N-5][col-2]);
									r1=_mm256_loadu_si256( (__m256i *) &frame1[N-4][col-2]);
									r2=_mm256_loadu_si256( (__m256i *) &frame1[N-3][col-2]);
									r3=_mm256_loadu_si256( (__m256i *) &frame1[N-2][col-2]);
									r4=_mm256_loadu_si256( (__m256i *) &frame1[N-1][col-2]);
							  }

							//col iteration computes output pixels of 2,8,14,20,26
							// col+1 iteration computes output pixels of 3,9,15,21,27
							// col+2 iteration computes output pixels of 4,10,16,22,28
							// col+3 iteration computes output pixels of 5,11,17,23,29
							// col+4 iteration computes output pixels of 6,12,18,24,30
							// col+5 iteration computes output pixels of 7,13,19,25,31
							//afterwards, col2 becomes 32 and repeat the above process

							//1st col iteration
							 //--------------row=2

							//multiply with the mask
							m0=_mm256_maddubs_epi16(r0,c0);
							m1=_mm256_maddubs_epi16(r1,c1);
							m2=_mm256_maddubs_epi16(r2,c2);
							m3=_mm256_maddubs_epi16(r3,c1);
							m4=_mm256_maddubs_epi16(r4,c0);

							//vertical add
							m4=_mm256_add_epi16(m4,m3);
							m4=_mm256_add_epi16(m4,m2);
							m4=_mm256_add_epi16(m4,m1);
							m0=_mm256_add_epi16(m4,m0);

							m1=_mm256_srli_si256(m0,2);
							m2=_mm256_add_epi16(m1,m0);

							m1=_mm256_srli_si256(m0,4);
							m2=_mm256_add_epi16(m1,m2);

							//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
							//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
							m4=_mm256_and_si256(m0,mask3);
							m4=_mm256_permute2f128_si256(m4,m4,1);
							m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions

							m2=_mm256_add_epi16(m2,m4);

							//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
							output_even=_mm256_and_si256(m2,output_mask);

							//--------------row=0
							//multiply with the mask
							m1=_mm256_maddubs_epi16(r1,c0);
							m2=_mm256_maddubs_epi16(r2,c1);
							m3=_mm256_maddubs_epi16(r3,c2);
							m4=_mm256_maddubs_epi16(r4,c1);

							//vertical add
							m4=_mm256_add_epi16(m4,m3);
							m4=_mm256_add_epi16(m4,m2);
							m4=_mm256_add_epi16(m4,m1); m0_prelude_row0=m4;

							m1=_mm256_srli_si256(m0_prelude_row0,2);
							m2=_mm256_add_epi16(m1,m0_prelude_row0);

							m1=_mm256_srli_si256(m0_prelude_row0,4);
							m2=_mm256_add_epi16(m1,m2);

							//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
							//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
							m4=_mm256_and_si256(m0_prelude_row0,mask3);
							m4=_mm256_permute2f128_si256(m4,m4,1);
							m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions

							m2=_mm256_add_epi16(m2,m4);

							//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
							output_row0_even=_mm256_and_si256(m2,output_mask);

							//--------------row=1
							//multiply with the mask
							m2=_mm256_maddubs_epi16(r2,c0);
							m3=_mm256_maddubs_epi16(r3,c1);
							m4=_mm256_maddubs_epi16(r4,c2);


							//vertical add
							m4=_mm256_add_epi16(m4,m3);
							m4=_mm256_add_epi16(m4,m2);m0_prelude_row1=m4;



							m1=_mm256_srli_si256(m0_prelude_row1,2);
							m2=_mm256_add_epi16(m1,m0_prelude_row1);

							m1=_mm256_srli_si256(m0_prelude_row1,4);
							m2=_mm256_add_epi16(m1,m2);

							//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
							//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
							m4=_mm256_and_si256(m0_prelude_row1,mask3);
							m4=_mm256_permute2f128_si256(m4,m4,1);
							m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions

							m2=_mm256_add_epi16(m2,m4);

							//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
							output_row1_even=_mm256_and_si256(m2,output_mask);

							//2nd col iteration
							//----row=2

							//multiply with the mask
							m0=_mm256_maddubs_epi16(r0,c0_sh1);
							m1=_mm256_maddubs_epi16(r1,c1_sh1);
							m2=_mm256_maddubs_epi16(r2,c2_sh1);
							m3=_mm256_maddubs_epi16(r3,c1_sh1);
							m4=_mm256_maddubs_epi16(r4,c0_sh1);

							//vertical add
							m4=_mm256_add_epi16(m4,m3);
							m4=_mm256_add_epi16(m4,m2);
							m4=_mm256_add_epi16(m4,m1);
							m0=_mm256_add_epi16(m4,m0);


							m1=_mm256_srli_si256(m0,2);
							m2=_mm256_add_epi16(m1,m0);

							m1=_mm256_srli_si256(m0,4);
							m2=_mm256_add_epi16(m1,m2);

							//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
							//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
							m4=_mm256_and_si256(m0,mask3);
							m4=_mm256_permute2f128_si256(m4,m4,1);
							m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions
							m2=_mm256_add_epi16(m2,m4);

							//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
							output_odd = _mm256_and_si256(m2,output_mask);

							//----row=0
							//multiply with the mask
							m1=_mm256_maddubs_epi16(r1,c0_sh1);
							m2=_mm256_maddubs_epi16(r2,c1_sh1);
							m3=_mm256_maddubs_epi16(r3,c2_sh1);
							m4=_mm256_maddubs_epi16(r4,c1_sh1);


							//vertical add
							m4=_mm256_add_epi16(m4,m3);
							m4=_mm256_add_epi16(m4,m2);
							m4=_mm256_add_epi16(m4,m1);m0_prelude_row0=m4;


							m1=_mm256_srli_si256(m0_prelude_row0,2);
							m2=_mm256_add_epi16(m1,m0_prelude_row0);

							m1=_mm256_srli_si256(m0_prelude_row0,4);
							m2=_mm256_add_epi16(m1,m2);

							//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
							//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
							m4=_mm256_and_si256(m0_prelude_row0,mask3);
							m4=_mm256_permute2f128_si256(m4,m4,1);
							m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions
							m2=_mm256_add_epi16(m2,m4);

							//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
							output_row0_odd = _mm256_and_si256(m2,output_mask);

							//----row=1
							//multiply with the mask
							m2=_mm256_maddubs_epi16(r2,c0_sh1);
							m3=_mm256_maddubs_epi16(r3,c1_sh1);
							m4=_mm256_maddubs_epi16(r4,c2_sh1);


							//vertical add
							m4=_mm256_add_epi16(m4,m3);
							m4=_mm256_add_epi16(m4,m2);m0_prelude_row1=m4;


							m1=_mm256_srli_si256(m0_prelude_row1,2);
							m2=_mm256_add_epi16(m1,m0_prelude_row1);

							m1=_mm256_srli_si256(m0_prelude_row1,4);
							m2=_mm256_add_epi16(m1,m2);

							//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
							//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
							m4=_mm256_and_si256(m0_prelude_row1,mask3);
							m4=_mm256_permute2f128_si256(m4,m4,1);
							m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions
							m2=_mm256_add_epi16(m2,m4);

							//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
							output_row1_odd = _mm256_and_si256(m2,output_mask);

					                 //3rd col iteration
							//---row=2
							//multiply with the mask
							m0=_mm256_maddubs_epi16(r0,c0_sh2);
							m1=_mm256_maddubs_epi16(r1,c1_sh2);
							m2=_mm256_maddubs_epi16(r2,c2_sh2);
							m3=_mm256_maddubs_epi16(r3,c1_sh2);
							m4=_mm256_maddubs_epi16(r4,c0_sh2);

							//vertical add
							m4=_mm256_add_epi16(m4,m3);
							m4=_mm256_add_epi16(m4,m2);
							m4=_mm256_add_epi16(m4,m1);
							m0=_mm256_add_epi16(m4,m0);


							m1=_mm256_srli_si256(m0,2);
							m2=_mm256_add_epi16(m1,m0);

							//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,_mm256_extract_epi16(m2,8),0,0,0,0,0,0,0);
							m4=_mm256_and_si256(m2,mask3);
							m4=_mm256_permute2f128_si256(m4,m4,1);
							m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions

							m1=_mm256_srli_si256(m0,4);
							m2=_mm256_add_epi16(m1,m2);

							//prepei na prosthesw to pixel pu thelw sto r2 kai meta na kanw diairesi me ola mazi

							m2=_mm256_add_epi16(m2,m4);

							//m2 has 16 16bit values now. the results I need are in positions 1,4,7,10,13. keep only those, discard others
							m2 = _mm256_and_si256(m2,output_mask_sh1);
							output_even = _mm256_add_epi16(output_even,m2);

							//---row=0
							//multiply with the mask
							m1=_mm256_maddubs_epi16(r1,c0_sh2);
							m2=_mm256_maddubs_epi16(r2,c1_sh2);
							m3=_mm256_maddubs_epi16(r3,c2_sh2);
							m4=_mm256_maddubs_epi16(r4,c1_sh2);


							//vertical add
							m4=_mm256_add_epi16(m4,m3);
							m4=_mm256_add_epi16(m4,m2);
							m4=_mm256_add_epi16(m4,m1);m0_prelude_row0=m4;

							m1=_mm256_srli_si256(m0_prelude_row0,2);
							m2=_mm256_add_epi16(m1,m0_prelude_row0);

							//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,_mm256_extract_epi16(m2,8),0,0,0,0,0,0,0);
							m4=_mm256_and_si256(m2,mask3);
							m4=_mm256_permute2f128_si256(m4,m4,1);
							m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions

							m1=_mm256_srli_si256(m0_prelude_row0,4);
							m2=_mm256_add_epi16(m1,m2);

							//prepei na prosthesw to pixel pu thelw sto r2 kai meta na kanw diairesi me ola mazi

							m2=_mm256_add_epi16(m2,m4);

							//m2 has 16 16bit values now. the results I need are in positions 1,4,7,10,13. keep only those, discard others
							m2 = _mm256_and_si256(m2,output_mask_sh1);
							output_row0_even = _mm256_add_epi16(output_row0_even,m2);


							//---row=1
							//multiply with the mask
							m2=_mm256_maddubs_epi16(r2,c0_sh2);
							m3=_mm256_maddubs_epi16(r3,c1_sh2);
							m4=_mm256_maddubs_epi16(r4,c2_sh2);


							//vertical add
							m4=_mm256_add_epi16(m4,m3);
							m4=_mm256_add_epi16(m4,m2);m0_prelude_row1=m4;


							m1=_mm256_srli_si256(m0_prelude_row1,2);
							m2=_mm256_add_epi16(m1,m0_prelude_row1);

							//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,_mm256_extract_epi16(m2,8),0,0,0,0,0,0,0);
							m4=_mm256_and_si256(m2,mask3);
							m4=_mm256_permute2f128_si256(m4,m4,1);
							m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions

							m1=_mm256_srli_si256(m0_prelude_row1,4);
							m2=_mm256_add_epi16(m1,m2);

							//prepei na prosthesw to pixel pu thelw sto r2 kai meta na kanw diairesi me ola mazi

							m2=_mm256_add_epi16(m2,m4);

							//m2 has 16 16bit values now. the results I need are in positions 1,4,7,10,13. keep only those, discard others
							m2 = _mm256_and_si256(m2,output_mask_sh1);
							output_row1_even = _mm256_add_epi16(output_row1_even,m2);

							//4th col iteration
							//multiply with the mask
							m0=_mm256_maddubs_epi16(r0,c0_sh3);
							m1=_mm256_maddubs_epi16(r1,c1_sh3);
							m2=_mm256_maddubs_epi16(r2,c2_sh3);
							m3=_mm256_maddubs_epi16(r3,c1_sh3);
							m4=_mm256_maddubs_epi16(r4,c0_sh3);

							//vertical add
							m0=_mm256_add_epi16(m0,m1);
							m2=_mm256_add_epi16(m2,m3);
							m0=_mm256_add_epi16(m0,m2);
							m0=_mm256_add_epi16(m0,m4);

							//hozizontal additions
							//hadd(3:8)
							//hadd(9:14)
							//hadd(15:20)
							//hadd(21:26)
							//hadd(27:31)
							//result after division will be in 2,8,14,20,26

							m1=_mm256_srli_si256(m0,2);
							m4=_mm256_add_epi16(m1,m0);

							m1=_mm256_srli_si256(m0,4);
							m2=_mm256_add_epi16(m1,m4);

							//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
							//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
							m4=_mm256_and_si256(m4,mask3);
							m4=_mm256_permute2f128_si256(m4,m4,1);
							m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions

							m2=_mm256_add_epi16(m2,m4);

							//m2 has 16 16bit values now. the results I need are in positions 1,4,7,10,13. keep only those, discard others
							m2 = _mm256_and_si256(m2,output_mask_sh1);
							output_odd = _mm256_add_epi16(output_odd,m2);

							//row0
							//multiply with the mask
								m1=_mm256_maddubs_epi16(r1,c0_sh3);
								m2=_mm256_maddubs_epi16(r2,c1_sh3);
								m3=_mm256_maddubs_epi16(r3,c2_sh3);
								m4=_mm256_maddubs_epi16(r4,c1_sh3);

								//vertical add
								m0=_mm256_add_epi16(m1,m2);
								m0=_mm256_add_epi16(m0,m3);
								m0=_mm256_add_epi16(m0,m4);


								m1=_mm256_srli_si256(m0,2);
								m4=_mm256_add_epi16(m1,m0);

								m1=_mm256_srli_si256(m0,4);
								m2=_mm256_add_epi16(m1,m4);

								//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
								//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
								m4=_mm256_and_si256(m4,mask3);
								m4=_mm256_permute2f128_si256(m4,m4,1);
								m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions

								m2=_mm256_add_epi16(m2,m4);

								//m2 has 16 16bit values now. the results I need are in positions 1,4,7,10,13. keep only those, discard others
								m2 = _mm256_and_si256(m2,output_mask_sh1);
								output_row0_odd = _mm256_add_epi16(output_row0_odd,m2);

								//row1
								//multiply with the mask
									m2=_mm256_maddubs_epi16(r2,c0_sh3);
									m3=_mm256_maddubs_epi16(r3,c1_sh3);
									m4=_mm256_maddubs_epi16(r4,c2_sh3);

									//vertical add
									m0=_mm256_add_epi16(m2,m3);
									m0=_mm256_add_epi16(m0,m4);


									m1=_mm256_srli_si256(m0,2);
									m4=_mm256_add_epi16(m1,m0);

									m1=_mm256_srli_si256(m0,4);
									m2=_mm256_add_epi16(m1,m4);

									//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
									//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
									m4=_mm256_and_si256(m4,mask3);
									m4=_mm256_permute2f128_si256(m4,m4,1);
									m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions

									m2=_mm256_add_epi16(m2,m4);

									//m2 has 16 16bit values now. the results I need are in positions 1,4,7,10,13. keep only those, discard others
									m2 = _mm256_and_si256(m2,output_mask_sh1);
									output_row1_odd = _mm256_add_epi16(output_row1_odd,m2);


							//5th col iteration
							//multiply with the mask
							m0=_mm256_maddubs_epi16(r0,c0_sh4);
							m1=_mm256_maddubs_epi16(r1,c1_sh4);
							m2=_mm256_maddubs_epi16(r2,c2_sh4);
							m3=_mm256_maddubs_epi16(r3,c1_sh4);
							m4=_mm256_maddubs_epi16(r4,c0_sh4);

							//vertical add
							m0=_mm256_add_epi16(m0,m1);
							m2=_mm256_add_epi16(m2,m3);
							m0=_mm256_add_epi16(m0,m2);
							m0=_mm256_add_epi16(m0,m4);

							//hozizontal additions
							//hadd(4:9)
							//hadd(10:15)
							//hadd(16:21)
							//hadd(22:27)
							//result after division will be in 4,10,16,22

							m1=_mm256_srli_si256(m0,2);
							m2=_mm256_add_epi16(m1,m0);

							m1=_mm256_srli_si256(m0,4);
							m2=_mm256_add_epi16(m1,m2);


							//m2 has 16 16bit values now. the results I need are in positions 2,5,8,11. keep only those, discard others
							m2 = _mm256_and_si256(m2,output_mask_sh2);
							output_even = _mm256_add_epi16(output_even,m2);

							//row0
							//multiply with the mask
							m1=_mm256_maddubs_epi16(r1,c0_sh4);
							m2=_mm256_maddubs_epi16(r2,c1_sh4);
							m3=_mm256_maddubs_epi16(r3,c2_sh4);
							m4=_mm256_maddubs_epi16(r4,c1_sh4);

							//vertical add
							m0=_mm256_add_epi16(m1,m2);
							m0=_mm256_add_epi16(m0,m3);
							m0=_mm256_add_epi16(m0,m4);


							m1=_mm256_srli_si256(m0,2);
							m2=_mm256_add_epi16(m1,m0);

							m1=_mm256_srli_si256(m0,4);
							m2=_mm256_add_epi16(m1,m2);


							//m2 has 16 16bit values now. the results I need are in positions 2,5,8,11. keep only those, discard others
							m2 = _mm256_and_si256(m2,output_mask_sh2);
							output_row0_even = _mm256_add_epi16(output_row0_even,m2);

							//row1
							//multiply with the mask
							m2=_mm256_maddubs_epi16(r2,c0_sh4);
							m3=_mm256_maddubs_epi16(r3,c1_sh4);
							m4=_mm256_maddubs_epi16(r4,c2_sh4);

							//vertical add
							m0=_mm256_add_epi16(m2,m3);
							m0=_mm256_add_epi16(m0,m4);


							m1=_mm256_srli_si256(m0,2);
							m2=_mm256_add_epi16(m1,m0);

							m1=_mm256_srli_si256(m0,4);
							m2=_mm256_add_epi16(m1,m2);


							//m2 has 16 16bit values now. the results I need are in positions 2,5,8,11. keep only those, discard others
							m2 = _mm256_and_si256(m2,output_mask_sh2);
							output_row1_even = _mm256_add_epi16(output_row1_even,m2);

					                 //6th col iteration
							//multiply with the mask
							m0=_mm256_maddubs_epi16(r0,c0_sh5);
							m1=_mm256_maddubs_epi16(r1,c1_sh5);
							m2=_mm256_maddubs_epi16(r2,c2_sh5);
							m3=_mm256_maddubs_epi16(r3,c1_sh5);
							m4=_mm256_maddubs_epi16(r4,c0_sh5);

							//vertical add
							m0=_mm256_add_epi16(m0,m1);
							m2=_mm256_add_epi16(m2,m3);
							m0=_mm256_add_epi16(m0,m2);
							m0=_mm256_add_epi16(m0,m4);

							//hozizontal additions
							//hadd(5:10)
							//hadd(11:16)
							//hadd(17:22)
							//hadd(23:28)
							//result after division will be in 4,10,16,22

							m1=_mm256_srli_si256(m0,2);
							m2=_mm256_add_epi16(m1,m0);

							m1=_mm256_srli_si256(m0,4);
							m2=_mm256_add_epi16(m1,m2);

							//m2 has 16 16bit values now. the results I need are in positions 2,5,8,11. keep only those, discard others
							m2 = _mm256_and_si256(m2,output_mask_sh2);
							output_odd = _mm256_add_epi16(output_odd,m2);

							//now division follows
							m3 = _mm256_mulhi_epu16(output_even, f);             // multiply high unsigned words
							m1 = _mm256_sub_epi16(m2, m3);                     // subtract
							m1 = _mm256_srli_epi16(m1, 16);             // shift right logical with w
							m3 = _mm256_add_epi16(m3, m1);                    // add
							output_even=_mm256_srli_epi16(m3, b);            // shift right logical with b

							//now division follows
							m3 = _mm256_mulhi_epu16(output_odd, f);             // multiply high unsigned words
							m1 = _mm256_sub_epi16(m2, m3);                     // subtract
							m1 = _mm256_srli_epi16(m1, 16);             // shift right logical with w
							m3 = _mm256_add_epi16(m3, m1);                    // add
							output_odd=_mm256_srli_epi16(m3, b);            // shift right logical with b

							//shift odd 1 position and add to even
							output_odd = _mm256_slli_si256(output_odd,1);
							output_even = _mm256_add_epi8(output_even,output_odd);


							_mm256_storeu_si256( (__m256i *) &filt[N-3][col],output_even );

							//row0
							//multiply with the mask
							m1=_mm256_maddubs_epi16(r1,c0_sh5);
							m2=_mm256_maddubs_epi16(r2,c1_sh5);
							m3=_mm256_maddubs_epi16(r3,c2_sh5);
							m4=_mm256_maddubs_epi16(r4,c1_sh5);

							//vertical add
							m0=_mm256_add_epi16(m1,m2);
							m0=_mm256_add_epi16(m0,m3);
							m0=_mm256_add_epi16(m0,m4);


							m1=_mm256_srli_si256(m0,2);
							m2=_mm256_add_epi16(m1,m0);

							m1=_mm256_srli_si256(m0,4);
							m2=_mm256_add_epi16(m1,m2);

							//m2 has 16 16bit values now. the results I need are in positions 2,5,8,11. keep only those, discard others
							m2 = _mm256_and_si256(m2,output_mask_sh2);
							output_row0_odd = _mm256_add_epi16(output_row0_odd,m2);

							//now division follows
							m3 = _mm256_mulhi_epu16(output_row0_even, f);             // multiply high unsigned words
							m1 = _mm256_sub_epi16(m2, m3);                     // subtract
							m1 = _mm256_srli_epi16(m1, 16);             // shift right logical with w
							m3 = _mm256_add_epi16(m3, m1);                    // add
							output_row0_even=_mm256_srli_epi16(m3, b);            // shift right logical with b

							//now division follows
							m3 = _mm256_mulhi_epu16(output_row0_odd, f);             // multiply high unsigned words
							m1 = _mm256_sub_epi16(m2, m3);                     // subtract
							m1 = _mm256_srli_epi16(m1, 16);             // shift right logical with w
							m3 = _mm256_add_epi16(m3, m1);                    // add
							output_row0_odd=_mm256_srli_epi16(m3, b);            // shift right logical with b

							//shift odd 1 position and add to even
							output_row0_odd = _mm256_slli_si256(output_row0_odd,1);
							output_row0_even = _mm256_add_epi8(output_row0_even,output_row0_odd);


							_mm256_storeu_si256( (__m256i *) &filt[N-2][col],output_row0_even );

							//row1
							//multiply with the mask
							m2=_mm256_maddubs_epi16(r2,c0_sh5);
							m3=_mm256_maddubs_epi16(r3,c1_sh5);
							m4=_mm256_maddubs_epi16(r4,c2_sh5);

							//vertical add
							m0=_mm256_add_epi16(m2,m3);
							m0=_mm256_add_epi16(m0,m4);

								m1=_mm256_srli_si256(m0,2);
							m2=_mm256_add_epi16(m1,m0);

							m1=_mm256_srli_si256(m0,4);
							m2=_mm256_add_epi16(m1,m2);

							//m2 has 16 16bit values now. the results I need are in positions 2,5,8,11. keep only those, discard others
							m2 = _mm256_and_si256(m2,output_mask_sh2);
							output_row1_odd = _mm256_add_epi16(output_row1_odd,m2);

							//now division follows
							m3 = _mm256_mulhi_epu16(output_row1_even, f);             // multiply high unsigned words
							m1 = _mm256_sub_epi16(m2, m3);                     // subtract
							m1 = _mm256_srli_epi16(m1, 16);             // shift right logical with w
							m3 = _mm256_add_epi16(m3, m1);                    // add
							output_row1_even=_mm256_srli_epi16(m3, b);            // shift right logical with b

							//now division follows
							m3 = _mm256_mulhi_epu16(output_row1_odd, f);             // multiply high unsigned words
							m1 = _mm256_sub_epi16(m2, m3);                     // subtract
							m1 = _mm256_srli_epi16(m1, 16);             // shift right logical with w
							m3 = _mm256_add_epi16(m3, m1);                    // add
							output_row1_odd=_mm256_srli_epi16(m3, b);            // shift right logical with b

							//shift odd 1 position and add to even
							output_row1_odd = _mm256_slli_si256(output_row1_odd,1);
							output_row1_even = _mm256_add_epi8(output_row1_even,output_row1_odd);


							_mm256_storeu_si256( (__m256i *) &filt[N-1][col],output_row1_even );


							}

		    loop_reminder_last_less_div(frame1,filt,M,N,col,REMINDER_ITERATIONS,division_case,c0,c1,c2,c0_sh1,c1_sh1,c2_sh1,c0_sh2,c1_sh2,c2_sh2,f,divisor,filter5x5);

					}
		//an extra condition is needed because of register blocking
		else if ((row==N-2) ){//in this case I calculate just filt[N-2][:] and filt[N-1][:]. Below row0 refers to row=N-2 and row1 refers to row=N-1
					//printf("\nN=%d row=%d N-2=%d",N,row,N-2);
					for (col = 0; col <= M-32; col+=28){
								 //last col value that does not read outside of the array bounds is (col<M-29)

									  if (col==0){

											r1=_mm256_loadu_si256( (__m256i *) &frame1[N-4][0]);
											r2=_mm256_loadu_si256( (__m256i *) &frame1[N-3][0]);
											r3=_mm256_loadu_si256( (__m256i *) &frame1[N-2][0]);
											r4=_mm256_loadu_si256( (__m256i *) &frame1[N-1][0]);

									  			//START - extra code needed for prelude
									  					m1=_mm256_slli_si256(r1,2);//shift 2 elements left - equivalent to filling with two zeros inthe beginning
									  					m2=_mm256_slli_si256(r2,2);//shift 2 elements left - equivalent to filling with two zeros inthe beginning
									  					m3=_mm256_slli_si256(r3,2);//shift 2 elements left - equivalent to filling with two zeros inthe beginning
									  					m4=_mm256_slli_si256(r4,2);//shift 2 elements left - equivalent to filling with two zeros inthe beginning

									  					r1=_mm256_and_si256(r1,mask_prelude);
									  					r1=_mm256_permute2f128_si256(r1,r1,1);
									  					r1=_mm256_srli_si256(r1,14);//shift 14 elements
									  					r1=_mm256_add_epi16(m1,r1);

									  					r2=_mm256_and_si256(r2,mask_prelude);
									  					r2=_mm256_permute2f128_si256(r2,r2,1);
									  					r2=_mm256_srli_si256(r2,14);//shift 14 elements
									  					r2=_mm256_add_epi16(m2,r2);

									  					r3=_mm256_and_si256(r3,mask_prelude);
									  					r3=_mm256_permute2f128_si256(r3,r3,1);
									  					r3=_mm256_srli_si256(r3,14);//shift 14 elements
									  					r3=_mm256_add_epi16(m3,r3);

									  					r4=_mm256_and_si256(r4,mask_prelude);
									  					r4=_mm256_permute2f128_si256(r4,r4,1);
									  					r4=_mm256_srli_si256(r4,14);//shift 14 elements
									  					r4=_mm256_add_epi16(m4,r4);

									  			//END - extra code needed for prelude
									  		}
									  else {

											r1=_mm256_loadu_si256( (__m256i *) &frame1[N-4][col-2]);
											r2=_mm256_loadu_si256( (__m256i *) &frame1[N-3][col-2]);
											r3=_mm256_loadu_si256( (__m256i *) &frame1[N-2][col-2]);
											r4=_mm256_loadu_si256( (__m256i *) &frame1[N-1][col-2]);
									  }

									//col iteration computes output pixels of 2,8,14,20,26
									// col+1 iteration computes output pixels of 3,9,15,21,27
									// col+2 iteration computes output pixels of 4,10,16,22,28
									// col+3 iteration computes output pixels of 5,11,17,23,29
									// col+4 iteration computes output pixels of 6,12,18,24,30
									// col+5 iteration computes output pixels of 7,13,19,25,31
									//afterwards, col2 becomes 32 and repeat the above process

									//1st col iteration

									//--------------row=0
									//multiply with the mask
									m1=_mm256_maddubs_epi16(r1,c0);
									m2=_mm256_maddubs_epi16(r2,c1);
									m3=_mm256_maddubs_epi16(r3,c2);
									m4=_mm256_maddubs_epi16(r4,c1);

									//vertical add
									m4=_mm256_add_epi16(m4,m3);
									m4=_mm256_add_epi16(m4,m2);
									m4=_mm256_add_epi16(m4,m1); m0_prelude_row0=m4;

									m1=_mm256_srli_si256(m0_prelude_row0,2);
									m2=_mm256_add_epi16(m1,m0_prelude_row0);

									m1=_mm256_srli_si256(m0_prelude_row0,4);
									m2=_mm256_add_epi16(m1,m2);

									//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
									//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
									m4=_mm256_and_si256(m0_prelude_row0,mask3);
									m4=_mm256_permute2f128_si256(m4,m4,1);
									m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions

									m2=_mm256_add_epi16(m2,m4);

									//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
									output_row0_even=_mm256_and_si256(m2,output_mask);

									//--------------row=1
									//multiply with the mask
									m2=_mm256_maddubs_epi16(r2,c0);
									m3=_mm256_maddubs_epi16(r3,c1);
									m4=_mm256_maddubs_epi16(r4,c2);


									//vertical add
									m4=_mm256_add_epi16(m4,m3);
									m4=_mm256_add_epi16(m4,m2);m0_prelude_row1=m4;



									m1=_mm256_srli_si256(m0_prelude_row1,2);
									m2=_mm256_add_epi16(m1,m0_prelude_row1);

									m1=_mm256_srli_si256(m0_prelude_row1,4);
									m2=_mm256_add_epi16(m1,m2);

									//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
									//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
									m4=_mm256_and_si256(m0_prelude_row1,mask3);
									m4=_mm256_permute2f128_si256(m4,m4,1);
									m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions

									m2=_mm256_add_epi16(m2,m4);

									//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
									output_row1_even=_mm256_and_si256(m2,output_mask);

									//2nd col iteration

									//----row=0
									//multiply with the mask
									m1=_mm256_maddubs_epi16(r1,c0_sh1);
									m2=_mm256_maddubs_epi16(r2,c1_sh1);
									m3=_mm256_maddubs_epi16(r3,c2_sh1);
									m4=_mm256_maddubs_epi16(r4,c1_sh1);


									//vertical add
									m4=_mm256_add_epi16(m4,m3);
									m4=_mm256_add_epi16(m4,m2);
									m4=_mm256_add_epi16(m4,m1);m0_prelude_row0=m4;


									m1=_mm256_srli_si256(m0_prelude_row0,2);
									m2=_mm256_add_epi16(m1,m0_prelude_row0);

									m1=_mm256_srli_si256(m0_prelude_row0,4);
									m2=_mm256_add_epi16(m1,m2);

									//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
									//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
									m4=_mm256_and_si256(m0_prelude_row0,mask3);
									m4=_mm256_permute2f128_si256(m4,m4,1);
									m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions
									m2=_mm256_add_epi16(m2,m4);

									//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
									output_row0_odd = _mm256_and_si256(m2,output_mask);

									//----row=1
									//multiply with the mask
									m2=_mm256_maddubs_epi16(r2,c0_sh1);
									m3=_mm256_maddubs_epi16(r3,c1_sh1);
									m4=_mm256_maddubs_epi16(r4,c2_sh1);


									//vertical add
									m4=_mm256_add_epi16(m4,m3);
									m4=_mm256_add_epi16(m4,m2);m0_prelude_row1=m4;


									m1=_mm256_srli_si256(m0_prelude_row1,2);
									m2=_mm256_add_epi16(m1,m0_prelude_row1);

									m1=_mm256_srli_si256(m0_prelude_row1,4);
									m2=_mm256_add_epi16(m1,m2);

									//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
									//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
									m4=_mm256_and_si256(m0_prelude_row1,mask3);
									m4=_mm256_permute2f128_si256(m4,m4,1);
									m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions
									m2=_mm256_add_epi16(m2,m4);

									//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
									output_row1_odd = _mm256_and_si256(m2,output_mask);

							                 //3rd col iteration


									//---row=0
									//multiply with the mask
									m1=_mm256_maddubs_epi16(r1,c0_sh2);
									m2=_mm256_maddubs_epi16(r2,c1_sh2);
									m3=_mm256_maddubs_epi16(r3,c2_sh2);
									m4=_mm256_maddubs_epi16(r4,c1_sh2);


									//vertical add
									m4=_mm256_add_epi16(m4,m3);
									m4=_mm256_add_epi16(m4,m2);
									m4=_mm256_add_epi16(m4,m1);m0_prelude_row0=m4;

									m1=_mm256_srli_si256(m0_prelude_row0,2);
									m2=_mm256_add_epi16(m1,m0_prelude_row0);

									//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,_mm256_extract_epi16(m2,8),0,0,0,0,0,0,0);
									m4=_mm256_and_si256(m2,mask3);
									m4=_mm256_permute2f128_si256(m4,m4,1);
									m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions

									m1=_mm256_srli_si256(m0_prelude_row0,4);
									m2=_mm256_add_epi16(m1,m2);

									//prepei na prosthesw to pixel pu thelw sto r2 kai meta na kanw diairesi me ola mazi

									m2=_mm256_add_epi16(m2,m4);

									//m2 has 16 16bit values now. the results I need are in positions 1,4,7,10,13. keep only those, discard others
									m2 = _mm256_and_si256(m2,output_mask_sh1);
									output_row0_even = _mm256_add_epi16(output_row0_even,m2);


									//---row=1
									//multiply with the mask
									m2=_mm256_maddubs_epi16(r2,c0_sh2);
									m3=_mm256_maddubs_epi16(r3,c1_sh2);
									m4=_mm256_maddubs_epi16(r4,c2_sh2);


									//vertical add
									m4=_mm256_add_epi16(m4,m3);
									m4=_mm256_add_epi16(m4,m2);m0_prelude_row1=m4;


									m1=_mm256_srli_si256(m0_prelude_row1,2);
									m2=_mm256_add_epi16(m1,m0_prelude_row1);

									//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,_mm256_extract_epi16(m2,8),0,0,0,0,0,0,0);
									m4=_mm256_and_si256(m2,mask3);
									m4=_mm256_permute2f128_si256(m4,m4,1);
									m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions

									m1=_mm256_srli_si256(m0_prelude_row1,4);
									m2=_mm256_add_epi16(m1,m2);

									//prepei na prosthesw to pixel pu thelw sto r2 kai meta na kanw diairesi me ola mazi

									m2=_mm256_add_epi16(m2,m4);

									//m2 has 16 16bit values now. the results I need are in positions 1,4,7,10,13. keep only those, discard others
									m2 = _mm256_and_si256(m2,output_mask_sh1);
									output_row1_even = _mm256_add_epi16(output_row1_even,m2);

									//4th col iteration

									//row0
									//multiply with the mask
										m1=_mm256_maddubs_epi16(r1,c0_sh3);
										m2=_mm256_maddubs_epi16(r2,c1_sh3);
										m3=_mm256_maddubs_epi16(r3,c2_sh3);
										m4=_mm256_maddubs_epi16(r4,c1_sh3);

										//vertical add
										m0=_mm256_add_epi16(m1,m2);
										m0=_mm256_add_epi16(m0,m3);
										m0=_mm256_add_epi16(m0,m4);


										m1=_mm256_srli_si256(m0,2);
										m4=_mm256_add_epi16(m1,m0);

										m1=_mm256_srli_si256(m0,4);
										m2=_mm256_add_epi16(m1,m4);

										//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
										//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
										m4=_mm256_and_si256(m4,mask3);
										m4=_mm256_permute2f128_si256(m4,m4,1);
										m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions

										m2=_mm256_add_epi16(m2,m4);

										//m2 has 16 16bit values now. the results I need are in positions 1,4,7,10,13. keep only those, discard others
										m2 = _mm256_and_si256(m2,output_mask_sh1);
										output_row0_odd = _mm256_add_epi16(output_row0_odd,m2);

										//row1
										//multiply with the mask
											m2=_mm256_maddubs_epi16(r2,c0_sh3);
											m3=_mm256_maddubs_epi16(r3,c1_sh3);
											m4=_mm256_maddubs_epi16(r4,c2_sh3);

											//vertical add
											m0=_mm256_add_epi16(m2,m3);
											m0=_mm256_add_epi16(m0,m4);


											m1=_mm256_srli_si256(m0,2);
											m4=_mm256_add_epi16(m1,m0);

											m1=_mm256_srli_si256(m0,4);
											m2=_mm256_add_epi16(m1,m4);

											//prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me ola mazi
											//m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
											m4=_mm256_and_si256(m4,mask3);
											m4=_mm256_permute2f128_si256(m4,m4,1);
											m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions

											m2=_mm256_add_epi16(m2,m4);

											//m2 has 16 16bit values now. the results I need are in positions 1,4,7,10,13. keep only those, discard others
											m2 = _mm256_and_si256(m2,output_mask_sh1);
											output_row1_odd = _mm256_add_epi16(output_row1_odd,m2);


									//5th col iteration


									//row0
									//multiply with the mask
									m1=_mm256_maddubs_epi16(r1,c0_sh4);
									m2=_mm256_maddubs_epi16(r2,c1_sh4);
									m3=_mm256_maddubs_epi16(r3,c2_sh4);
									m4=_mm256_maddubs_epi16(r4,c1_sh4);

									//vertical add
									m0=_mm256_add_epi16(m1,m2);
									m0=_mm256_add_epi16(m0,m3);
									m0=_mm256_add_epi16(m0,m4);


									m1=_mm256_srli_si256(m0,2);
									m2=_mm256_add_epi16(m1,m0);

									m1=_mm256_srli_si256(m0,4);
									m2=_mm256_add_epi16(m1,m2);


									//m2 has 16 16bit values now. the results I need are in positions 2,5,8,11. keep only those, discard others
									m2 = _mm256_and_si256(m2,output_mask_sh2);
									output_row0_even = _mm256_add_epi16(output_row0_even,m2);

									//row1
									//multiply with the mask
									m2=_mm256_maddubs_epi16(r2,c0_sh4);
									m3=_mm256_maddubs_epi16(r3,c1_sh4);
									m4=_mm256_maddubs_epi16(r4,c2_sh4);

									//vertical add
									m0=_mm256_add_epi16(m2,m3);
									m0=_mm256_add_epi16(m0,m4);


									m1=_mm256_srli_si256(m0,2);
									m2=_mm256_add_epi16(m1,m0);

									m1=_mm256_srli_si256(m0,4);
									m2=_mm256_add_epi16(m1,m2);


									//m2 has 16 16bit values now. the results I need are in positions 2,5,8,11. keep only those, discard others
									m2 = _mm256_and_si256(m2,output_mask_sh2);
									output_row1_even = _mm256_add_epi16(output_row1_even,m2);

							                 //6th col iteration

									//row0
									//multiply with the mask
									m1=_mm256_maddubs_epi16(r1,c0_sh5);
									m2=_mm256_maddubs_epi16(r2,c1_sh5);
									m3=_mm256_maddubs_epi16(r3,c2_sh5);
									m4=_mm256_maddubs_epi16(r4,c1_sh5);

									//vertical add
									m0=_mm256_add_epi16(m1,m2);
									m0=_mm256_add_epi16(m0,m3);
									m0=_mm256_add_epi16(m0,m4);


									m1=_mm256_srli_si256(m0,2);
									m2=_mm256_add_epi16(m1,m0);

									m1=_mm256_srli_si256(m0,4);
									m2=_mm256_add_epi16(m1,m2);

									//m2 has 16 16bit values now. the results I need are in positions 2,5,8,11. keep only those, discard others
									m2 = _mm256_and_si256(m2,output_mask_sh2);
									output_row0_odd = _mm256_add_epi16(output_row0_odd,m2);

									//now division follows
									m3 = _mm256_mulhi_epu16(output_row0_even, f);             // multiply high unsigned words
									m1 = _mm256_sub_epi16(m2, m3);                     // subtract
									m1 = _mm256_srli_epi16(m1, 16);             // shift right logical with w
									m3 = _mm256_add_epi16(m3, m1);                    // add
									output_row0_even=_mm256_srli_epi16(m3, b);            // shift right logical with b

									//now division follows
									m3 = _mm256_mulhi_epu16(output_row0_odd, f);             // multiply high unsigned words
									m1 = _mm256_sub_epi16(m2, m3);                     // subtract
									m1 = _mm256_srli_epi16(m1, 16);             // shift right logical with w
									m3 = _mm256_add_epi16(m3, m1);                    // add
									output_row0_odd=_mm256_srli_epi16(m3, b);            // shift right logical with b

									//shift odd 1 position and add to even
									output_row0_odd = _mm256_slli_si256(output_row0_odd,1);
									output_row0_even = _mm256_add_epi8(output_row0_even,output_row0_odd);


									_mm256_storeu_si256( (__m256i *) &filt[N-2][col],output_row0_even );

									//row1
									//multiply with the mask
									m2=_mm256_maddubs_epi16(r2,c0_sh5);
									m3=_mm256_maddubs_epi16(r3,c1_sh5);
									m4=_mm256_maddubs_epi16(r4,c2_sh5);

									//vertical add
									m0=_mm256_add_epi16(m2,m3);
									m0=_mm256_add_epi16(m0,m4);

										m1=_mm256_srli_si256(m0,2);
									m2=_mm256_add_epi16(m1,m0);

									m1=_mm256_srli_si256(m0,4);
									m2=_mm256_add_epi16(m1,m2);

									//m2 has 16 16bit values now. the results I need are in positions 2,5,8,11. keep only those, discard others
									m2 = _mm256_and_si256(m2,output_mask_sh2);
									output_row1_odd = _mm256_add_epi16(output_row1_odd,m2);

									//now division follows
									m3 = _mm256_mulhi_epu16(output_row1_even, f);             // multiply high unsigned words
									m1 = _mm256_sub_epi16(m2, m3);                     // subtract
									m1 = _mm256_srli_epi16(m1, 16);             // shift right logical with w
									m3 = _mm256_add_epi16(m3, m1);                    // add
									output_row1_even=_mm256_srli_epi16(m3, b);            // shift right logical with b

									//now division follows
									m3 = _mm256_mulhi_epu16(output_row1_odd, f);             // multiply high unsigned words
									m1 = _mm256_sub_epi16(m2, m3);                     // subtract
									m1 = _mm256_srli_epi16(m1, 16);             // shift right logical with w
									m3 = _mm256_add_epi16(m3, m1);                    // add
									output_row1_odd=_mm256_srli_epi16(m3, b);            // shift right logical with b

									//shift odd 1 position and add to even
									output_row1_odd = _mm256_slli_si256(output_row1_odd,1);
									output_row1_even = _mm256_add_epi8(output_row1_even,output_row1_odd);


									_mm256_storeu_si256( (__m256i *) &filt[N-1][col],output_row1_even );


									}

				    loop_reminder_last_less_div_special_case(frame1,filt,M,N,col,REMINDER_ITERATIONS,division_case,c0,c1,c2,c0_sh1,c1_sh1,c2_sh1,c0_sh2,c1_sh2,c2_sh2,f,divisor,filter5x5);

							}

	else {


	  for (col = 0; col <= M-32; col+=28){
	 //last col value that does not read outside of the array bounds is (col<M-29)

		  if (col==0){

				//load the 5 rows
				r0=_mm256_loadu_si256( (__m256i *) &frame1[row-2][0]);
				r1=_mm256_loadu_si256( (__m256i *) &frame1[row-1][0]);
				r2=_mm256_loadu_si256( (__m256i *) &frame1[row][0]);
				r3=_mm256_loadu_si256( (__m256i *) &frame1[row+1][0]);
				r4=_mm256_loadu_si256( (__m256i *) &frame1[row+2][0]);
				r5=_mm256_loadu_si256( (__m256i *) &frame1[row+3][0]);


		  			//START - extra code needed for prelude
		  					m0=_mm256_slli_si256(r0,2);//shift 2 elements left - equivalent to filling with two zeros inthe beginning
		  					m1=_mm256_slli_si256(r1,2);//shift 2 elements left - equivalent to filling with two zeros inthe beginning
		  					m2=_mm256_slli_si256(r2,2);//shift 2 elements left - equivalent to filling with two zeros inthe beginning
		  					m3=_mm256_slli_si256(r3,2);//shift 2 elements left - equivalent to filling with two zeros inthe beginning
		  					m4=_mm256_slli_si256(r4,2);//shift 2 elements left - equivalent to filling with two zeros inthe beginning
		  					m5=_mm256_slli_si256(r5,2);//shift 2 elements left - equivalent to filling with two zeros inthe beginning


		  					r0=_mm256_and_si256(r0,mask_prelude);
		  					r0=_mm256_permute2f128_si256(r0,r0,1);
		  					r0=_mm256_srli_si256(r0,14);//shift 14 elements
		  					r0=_mm256_add_epi16(m0,r0);

		  					r1=_mm256_and_si256(r1,mask_prelude);
		  					r1=_mm256_permute2f128_si256(r1,r1,1);
		  					r1=_mm256_srli_si256(r1,14);//shift 14 elements
		  					r1=_mm256_add_epi16(m1,r1);

		  					r2=_mm256_and_si256(r2,mask_prelude);
		  					r2=_mm256_permute2f128_si256(r2,r2,1);
		  					r2=_mm256_srli_si256(r2,14);//shift 14 elements
		  					r2=_mm256_add_epi16(m2,r2);

		  					r3=_mm256_and_si256(r3,mask_prelude);
		  					r3=_mm256_permute2f128_si256(r3,r3,1);
		  					r3=_mm256_srli_si256(r3,14);//shift 14 elements
		  					r3=_mm256_add_epi16(m3,r3);

		  					r4=_mm256_and_si256(r4,mask_prelude);
		  					r4=_mm256_permute2f128_si256(r4,r4,1);
		  					r4=_mm256_srli_si256(r4,14);//shift 14 elements
		  					r4=_mm256_add_epi16(m4,r4);

		  					r5=_mm256_and_si256(r5,mask_prelude);
		  					r5=_mm256_permute2f128_si256(r5,r5,1);
		  					r5=_mm256_srli_si256(r5,14);//shift 14 elements
		  					r5=_mm256_add_epi16(m5,r5);

		  			//END - extra code needed for prelude
		  		}
		  else {
				//load the 5 rows
				r0=_mm256_loadu_si256( (__m256i *) &frame1[row-2][col-2]);
				r1=_mm256_loadu_si256( (__m256i *) &frame1[row-1][col-2]);
				r2=_mm256_loadu_si256( (__m256i *) &frame1[row][col-2]);
				r3=_mm256_loadu_si256( (__m256i *) &frame1[row+1][col-2]);
				r4=_mm256_loadu_si256( (__m256i *) &frame1[row+2][col-2]);
				r5=_mm256_loadu_si256( (__m256i *) &frame1[row+3][col-2]);

		  }

		//col iteration computes output pixels of 2,8,14,20,26
		// col+1 iteration computes output pixels of 3,9,15,21,27
		// col+2 iteration computes output pixels of 4,10,16,22,28
		// col+3 iteration computes output pixels of 5,11,17,23,29
		// col+4 iteration computes output pixels of 6,12,18,24
		// col+5 iteration computes output pixels of 7,13,19,25
		//afterwards, col becomes 30 and repeat the above process

		//1st col iteration


		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,c0);
		m1=_mm256_maddubs_epi16(r1,c1);
		m2=_mm256_maddubs_epi16(r2,c2);
		m3=_mm256_maddubs_epi16(r3,c1);
		m4=_mm256_maddubs_epi16(r4,c0);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m2=_mm256_add_epi16(m2,m3);
		m0=_mm256_add_epi16(m0,m2);
		m0=_mm256_add_epi16(m0,m4);

		//hozizontal additions
		//hadd(0:5)
		//hadd(6:11)
		//hadd(12:17)
		//hadd(18:23)
		//hadd(24:28)

		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		m1=_mm256_srli_si256(m0,4);
		m2=_mm256_add_epi16(m1,m2);

		//after shifts the 15th element is lost as it cannot be propagated to the 16th position (AVX registers are managed as two seperate SSE registers)
		m4=_mm256_and_si256(m0,mask3);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions
		m2=_mm256_add_epi16(m2,m4);

		//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
		output_even=_mm256_and_si256(m2,output_mask);


		//2nd col iteration
		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,c0_sh1);
		m1=_mm256_maddubs_epi16(r1,c1_sh1);
		m2=_mm256_maddubs_epi16(r2,c2_sh1);
		m3=_mm256_maddubs_epi16(r3,c1_sh1);
		m4=_mm256_maddubs_epi16(r4,c0_sh1);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m2=_mm256_add_epi16(m2,m3);
		m0=_mm256_add_epi16(m0,m2);
		m0=_mm256_add_epi16(m0,m4);

		//hozizontal additions
		//hadd(1:6)
		//hadd(7:12)
		//hadd(13:18)
		//hadd(19:24)
		//hadd(25:29)

		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		m1=_mm256_srli_si256(m0,4);
		m2=_mm256_add_epi16(m1,m2);

		//after shifts the 15th element is lost as it cannot be propagated to the 16th position (AVX registers are managed as two seperate SSE registers)
		m4=_mm256_and_si256(m0,mask3);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions
		m2=_mm256_add_epi16(m2,m4);

		//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
		output_odd = _mm256_and_si256(m2,output_mask);



         //3rd col iteration
		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,c0_sh2);
		m1=_mm256_maddubs_epi16(r1,c1_sh2);
		m2=_mm256_maddubs_epi16(r2,c2_sh2);
		m3=_mm256_maddubs_epi16(r3,c1_sh2);
		m4=_mm256_maddubs_epi16(r4,c0_sh2);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m2=_mm256_add_epi16(m2,m3);
		m0=_mm256_add_epi16(m0,m2);
		m0=_mm256_add_epi16(m0,m4);

		//hozizontal additions
		//hadd(2:7)
		//hadd(8:13)
		//hadd(14:19)
		//hadd(20:25)
		//hadd(26:30)

		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		m4=_mm256_and_si256(m2,mask3);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions

		m1=_mm256_srli_si256(m0,4);
		m2=_mm256_add_epi16(m1,m2);
		m2=_mm256_add_epi16(m2,m4);

		//m2 has 16 16bit values now. the results I need are in positions 1,4,7,10,13. keep only those, discard others
		m2 = _mm256_and_si256(m2,output_mask_sh1);
		output_even = _mm256_add_epi16(output_even,m2);


		//4th col iteration

		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,c0_sh3);
		m1=_mm256_maddubs_epi16(r1,c1_sh3);
		m2=_mm256_maddubs_epi16(r2,c2_sh3);
		m3=_mm256_maddubs_epi16(r3,c1_sh3);
		m4=_mm256_maddubs_epi16(r4,c0_sh3);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m2=_mm256_add_epi16(m2,m3);
		m0=_mm256_add_epi16(m0,m2);
		m0=_mm256_add_epi16(m0,m4);

		//hozizontal additions
		//hadd(3:8)
		//hadd(9:14)
		//hadd(15:20)
		//hadd(21:26)
		//hadd(27:31)
		//result after division will be in 2,8,14,20,26

		m1=_mm256_srli_si256(m0,2);
		m4=_mm256_add_epi16(m1,m0);

		m1=_mm256_srli_si256(m0,4);
		m2=_mm256_add_epi16(m1,m4);


		m4=_mm256_and_si256(m4,mask3);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions

		m2=_mm256_add_epi16(m2,m4);

		//m2 has 16 16bit values now. the results I need are in positions 1,4,7,10,13. keep only those, discard others
		m2 = _mm256_and_si256(m2,output_mask_sh1);
		output_odd = _mm256_add_epi16(output_odd,m2);




		//5th col iteration
		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,c0_sh4);
		m1=_mm256_maddubs_epi16(r1,c1_sh4);
		m2=_mm256_maddubs_epi16(r2,c2_sh4);
		m3=_mm256_maddubs_epi16(r3,c1_sh4);
		m4=_mm256_maddubs_epi16(r4,c0_sh4);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m2=_mm256_add_epi16(m2,m3);
		m0=_mm256_add_epi16(m0,m2);
		m0=_mm256_add_epi16(m0,m4);

		//hozizontal additions
		//hadd(4:9)
		//hadd(10:15)
		//hadd(16:21)
		//hadd(22:27)
		//result after division will be in 4,10,16,22

		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		m1=_mm256_srli_si256(m0,4);
		m2=_mm256_add_epi16(m1,m2);


		//m2 has 16 16bit values now. the results I need are in positions 2,5,8,11. keep only those, discard others
		m2 = _mm256_and_si256(m2,output_mask_sh2);
		output_even = _mm256_add_epi16(output_even,m2);



                 //6th col iteration
		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,c0_sh5);
		m1=_mm256_maddubs_epi16(r1,c1_sh5);
		m2=_mm256_maddubs_epi16(r2,c2_sh5);
		m3=_mm256_maddubs_epi16(r3,c1_sh5);
		m4=_mm256_maddubs_epi16(r4,c0_sh5);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m2=_mm256_add_epi16(m2,m3);
		m0=_mm256_add_epi16(m0,m2);
		m0=_mm256_add_epi16(m0,m4);

		//hozizontal additions
		//hadd(5:10)
		//hadd(11:16)
		//hadd(17:22)
		//hadd(23:28)
		//result after division will be in 4,10,16,22

		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		m1=_mm256_srli_si256(m0,4);
		m2=_mm256_add_epi16(m1,m2);

		//m2 has 16 16bit values now. the results I need are in positions 2,5,8,11. keep only those, discard others
		m2 = _mm256_and_si256(m2,output_mask_sh2);
		output_odd = _mm256_add_epi16(output_odd,m2);

		//now division follows
		m3 = _mm256_mulhi_epu16(output_even, f);             // multiply high unsigned words
		m1 = _mm256_sub_epi16(m2, m3);                     // subtract
		m1 = _mm256_srli_epi16(m1, 16);             // shift right logical with w
		m3 = _mm256_add_epi16(m3, m1);                    // add
		output_even=_mm256_srli_epi16(m3, b);            // shift right logical with b

		//now division follows
		m3 = _mm256_mulhi_epu16(output_odd, f);             // multiply high unsigned words
		m1 = _mm256_sub_epi16(m2, m3);                     // subtract
		m1 = _mm256_srli_epi16(m1, 16);             // shift right logical with w
		m3 = _mm256_add_epi16(m3, m1);                    // add
		output_odd=_mm256_srli_epi16(m3, b);            // shift right logical with b


		//shift odd 1 position and add to even
		output_odd = _mm256_slli_si256(output_odd,1);
		output_even = _mm256_add_epi8(output_even,output_odd);


		_mm256_storeu_si256( (__m256i *) &filt[row][col],output_even );


		//row+1


		//multiply with the mask
		m0=_mm256_maddubs_epi16(r1,c0);
		m1=_mm256_maddubs_epi16(r2,c1);
		m2=_mm256_maddubs_epi16(r3,c2);
		m3=_mm256_maddubs_epi16(r4,c1);
		m4=_mm256_maddubs_epi16(r5,c0);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m2=_mm256_add_epi16(m2,m3);
		m0=_mm256_add_epi16(m0,m2);
		m0=_mm256_add_epi16(m0,m4);

		//hozizontal additions
		//hadd(0:5)
		//hadd(6:11)
		//hadd(12:17)
		//hadd(18:23)
		//hadd(24:28)

		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		m1=_mm256_srli_si256(m0,4);
		m2=_mm256_add_epi16(m1,m2);

		//after shifts the 15th element is lost as it cannot be propagated to the 16th position (AVX registers are managed as two seperate SSE registers)
		m4=_mm256_and_si256(m0,mask3);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions
		m2=_mm256_add_epi16(m2,m4);

		//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
		output_even=_mm256_and_si256(m2,output_mask);


		//2nd col iteration
		//multiply with the mask
		m0=_mm256_maddubs_epi16(r1,c0_sh1);
		m1=_mm256_maddubs_epi16(r2,c1_sh1);
		m2=_mm256_maddubs_epi16(r3,c2_sh1);
		m3=_mm256_maddubs_epi16(r4,c1_sh1);
		m4=_mm256_maddubs_epi16(r5,c0_sh1);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m2=_mm256_add_epi16(m2,m3);
		m0=_mm256_add_epi16(m0,m2);
		m0=_mm256_add_epi16(m0,m4);

		//hozizontal additions
		//hadd(1:6)
		//hadd(7:12)
		//hadd(13:18)
		//hadd(19:24)
		//hadd(25:29)

		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		m1=_mm256_srli_si256(m0,4);
		m2=_mm256_add_epi16(m1,m2);

		//after shifts the 15th element is lost as it cannot be propagated to the 16th position (AVX registers are managed as two seperate SSE registers)
		m4=_mm256_and_si256(m0,mask3);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions
		m2=_mm256_add_epi16(m2,m4);

		//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
		output_odd = _mm256_and_si256(m2,output_mask);



         //3rd col iteration
		//multiply with the mask
		m0=_mm256_maddubs_epi16(r1,c0_sh2);
		m1=_mm256_maddubs_epi16(r2,c1_sh2);
		m2=_mm256_maddubs_epi16(r3,c2_sh2);
		m3=_mm256_maddubs_epi16(r4,c1_sh2);
		m4=_mm256_maddubs_epi16(r5,c0_sh2);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m2=_mm256_add_epi16(m2,m3);
		m0=_mm256_add_epi16(m0,m2);
		m0=_mm256_add_epi16(m0,m4);

		//hozizontal additions
		//hadd(2:7)
		//hadd(8:13)
		//hadd(14:19)
		//hadd(20:25)
		//hadd(26:30)

		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		m4=_mm256_and_si256(m2,mask3);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions

		m1=_mm256_srli_si256(m0,4);
		m2=_mm256_add_epi16(m1,m2);
		m2=_mm256_add_epi16(m2,m4);

		//m2 has 16 16bit values now. the results I need are in positions 1,4,7,10,13. keep only those, discard others
		m2 = _mm256_and_si256(m2,output_mask_sh1);
		output_even = _mm256_add_epi16(output_even,m2);


		//4th col iteration

		//multiply with the mask
		m0=_mm256_maddubs_epi16(r1,c0_sh3);
		m1=_mm256_maddubs_epi16(r2,c1_sh3);
		m2=_mm256_maddubs_epi16(r3,c2_sh3);
		m3=_mm256_maddubs_epi16(r4,c1_sh3);
		m4=_mm256_maddubs_epi16(r5,c0_sh3);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m2=_mm256_add_epi16(m2,m3);
		m0=_mm256_add_epi16(m0,m2);
		m0=_mm256_add_epi16(m0,m4);

		//hozizontal additions
		//hadd(3:8)
		//hadd(9:14)
		//hadd(15:20)
		//hadd(21:26)
		//hadd(27:31)
		//result after division will be in 2,8,14,20,26

		m1=_mm256_srli_si256(m0,2);
		m4=_mm256_add_epi16(m1,m0);

		m1=_mm256_srli_si256(m0,4);
		m2=_mm256_add_epi16(m1,m4);


		m4=_mm256_and_si256(m4,mask3);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions

		m2=_mm256_add_epi16(m2,m4);

		//m2 has 16 16bit values now. the results I need are in positions 1,4,7,10,13. keep only those, discard others
		m2 = _mm256_and_si256(m2,output_mask_sh1);
		output_odd = _mm256_add_epi16(output_odd,m2);




		//5th col iteration
		//multiply with the mask
		m0=_mm256_maddubs_epi16(r1,c0_sh4);
		m1=_mm256_maddubs_epi16(r2,c1_sh4);
		m2=_mm256_maddubs_epi16(r3,c2_sh4);
		m3=_mm256_maddubs_epi16(r4,c1_sh4);
		m4=_mm256_maddubs_epi16(r5,c0_sh4);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m2=_mm256_add_epi16(m2,m3);
		m0=_mm256_add_epi16(m0,m2);
		m0=_mm256_add_epi16(m0,m4);

		//hozizontal additions
		//hadd(4:9)
		//hadd(10:15)
		//hadd(16:21)
		//hadd(22:27)
		//result after division will be in 4,10,16,22

		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		m1=_mm256_srli_si256(m0,4);
		m2=_mm256_add_epi16(m1,m2);


		//m2 has 16 16bit values now. the results I need are in positions 2,5,8,11. keep only those, discard others
		m2 = _mm256_and_si256(m2,output_mask_sh2);
		output_even = _mm256_add_epi16(output_even,m2);



                 //6th col iteration
		//multiply with the mask
		m0=_mm256_maddubs_epi16(r1,c0_sh5);
		m1=_mm256_maddubs_epi16(r2,c1_sh5);
		m2=_mm256_maddubs_epi16(r3,c2_sh5);
		m3=_mm256_maddubs_epi16(r4,c1_sh5);
		m4=_mm256_maddubs_epi16(r5,c0_sh5);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m2=_mm256_add_epi16(m2,m3);
		m0=_mm256_add_epi16(m0,m2);
		m0=_mm256_add_epi16(m0,m4);

		//hozizontal additions
		//hadd(5:10)
		//hadd(11:16)
		//hadd(17:22)
		//hadd(23:28)
		//result after division will be in 4,10,16,22

		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		m1=_mm256_srli_si256(m0,4);
		m2=_mm256_add_epi16(m1,m2);

		//m2 has 16 16bit values now. the results I need are in positions 2,5,8,11. keep only those, discard others
		m2 = _mm256_and_si256(m2,output_mask_sh2);
		output_odd = _mm256_add_epi16(output_odd,m2);

		//now division follows
		m3 = _mm256_mulhi_epu16(output_even, f);             // multiply high unsigned words
		m1 = _mm256_sub_epi16(m2, m3);                     // subtract
		m1 = _mm256_srli_epi16(m1, 16);             // shift right logical with w
		m3 = _mm256_add_epi16(m3, m1);                    // add
		output_even=_mm256_srli_epi16(m3, b);            // shift right logical with b

		//now division follows
		m3 = _mm256_mulhi_epu16(output_odd, f);             // multiply high unsigned words
		m1 = _mm256_sub_epi16(m2, m3);                     // subtract
		m1 = _mm256_srli_epi16(m1, 16);             // shift right logical with w
		m3 = _mm256_add_epi16(m3, m1);                    // add
		output_odd=_mm256_srli_epi16(m3, b);            // shift right logical with b

		//shift odd 1 position and add to even
		output_odd = _mm256_slli_si256(output_odd,1);
		output_even = _mm256_add_epi8(output_even,output_odd);


		_mm256_storeu_si256( (__m256i *) &filt[row+1][col],output_even );


		}

        if (REMINDER_ITERATIONS>=29){
        	loop_reminder_high_reminder_values_less_div(frame1,filt,M,N,row,col,REMINDER_ITERATIONS,division_case,c0,c1,c2,c0_sh1,c1_sh1,c2_sh1,c0_sh2,c1_sh2,c2_sh2,f,divisor,filter5x5);
        	loop_reminder_high_reminder_values_less_div(frame1,filt,M,N,row+1,col,REMINDER_ITERATIONS,division_case,c0,c1,c2,c0_sh1,c1_sh1,c2_sh1,c0_sh2,c1_sh2,c2_sh2,f,divisor,filter5x5);

        }
        else{
        	loop_reminder_low_reminder_values_less_div(frame1,filt,M,N,row,col,REMINDER_ITERATIONS,division_case,c0,c1,c2,c0_sh1,c1_sh1,c2_sh1,c0_sh2,c1_sh2,c2_sh2,c0_sh3,c1_sh3,c2_sh3,c0_sh4,c1_sh4,c2_sh4,c0_sh5,c1_sh5,c2_sh5,f);
        	loop_reminder_low_reminder_values_less_div(frame1,filt,M,N,row+1,col,REMINDER_ITERATIONS,division_case,c0,c1,c2,c0_sh1,c1_sh1,c2_sh1,c0_sh2,c1_sh2,c2_sh2,c0_sh3,c1_sh3,c2_sh3,c0_sh4,c1_sh4,c2_sh4,c0_sh5,c1_sh5,c2_sh5,f);

        }
        //padding code. The last three iterations  ABOVE get outside of the array bounds. The last three col iterations read outside but the garbage values are multiplied by zeros (last three mask zeros). From now on, I must include another mask with extra zeros.
//It is faster to read outside of the array's bounds and then fill the right values. there is an insert command that inserts a value to the vector
//TOMORROW: USE Gaussian_Blur_AVX_ver4_plus_less_load ROUTINE FOR LOOP REMINDER. LOAD OUTSIDE OF THE ARRAY AND THEN ZERO THE VALUES NEEDED.
	}

}

}//end of parallel


}





void Gaussian_Blur_optimized_3x3(unsigned char **frame1,unsigned char **filt,const unsigned int M, const unsigned int N, const unsigned short int divisor, signed char **filter){

const signed char f00=filter[0][0];
const signed char f01=filter[0][1];
const signed char f02=filter[0][2];

const signed char f10=filter[1][0];
const signed char f11=filter[1][1];
const signed char f12=filter[1][2];


	const __m256i c0=_mm256_set_epi8(0,f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00);
	const __m256i c1=_mm256_set_epi8(0,f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10);

	const __m256i c0_sh1=_mm256_set_epi8(f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00,0);
	const __m256i c1_sh1=_mm256_set_epi8(f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10,0);


	const __m256i c0_sh2=_mm256_set_epi8(0,0,0,f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00,0,0);
	const __m256i c1_sh2=_mm256_set_epi8(0,0,0,f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10,0,0);

	const __m256i c0_sh3=_mm256_set_epi8(0,0,f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00,0,0,0);
	const __m256i c1_sh3=_mm256_set_epi8(0,0,f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10,0,0,0);


	const __m256i mask3  = _mm256_set_epi16(0,0,0,0,0,0,0,65535,0,0,0,0,0,0,0,0);
	const __m256i mask_prelude  = _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0);

	const __m256i output_mask      = _mm256_set_epi16(0,65535,0,65535,0,65535,0,65535,0,65535,0,65535,0,65535,0,65535);
	const __m256i output_mask_sh1  = _mm256_set_epi16(0,0,65535,0,65535,0,65535,0,65535,0,65535,0,65535,0,65535,0);




	const unsigned int REMINDER_ITERATIONS =  (M-((((M-32)/30)*30)+30)); //M-(last_col_value+30)
	//printf("\n%d",REMINDER_ITERATIONS);


	const unsigned int division_case=prepare_for_division(divisor); //determine which is the division case (A, B or C)
	const __m256i f = _mm256_load_si256( (__m256i *) &f_vector[0]);// initialize the division vector

//printf("\n%d %d",division_case,b);


#pragma omp parallel
{

unsigned int row,col;
register __m256i r0,r1,r2,m0,m1,m2,m4;
//__m256i output_row0_even,output_row1_even,output_row0_odd,output_row1_odd; //these are for processing row #0 and #1 only.
__m256i output_even,output_odd;
//__m256i m0_prelude_row0,m0_prelude_row1;


/*---------------------- Gaussian Blur ---------------------------------*/

    #pragma omp for schedule(static)
	for (row = 1; row < N-1; row++) {

	if (row==1){//special case compute filt[0:1][:]

		for (col = 0; col <= M-32; col+=30){

		  if (col==0){

				//load the 5 rows
				r0=_mm256_loadu_si256( (__m256i *) &frame1[0][0]);
				r1=_mm256_loadu_si256( (__m256i *) &frame1[1][0]);
				r2=_mm256_loadu_si256( (__m256i *) &frame1[2][0]);


		  			//START - extra code needed for prelude
		  					m0=_mm256_slli_si256(r0,1);//shift 1 elements left - equivalent to filling with one zero inthe beginning
		  					m1=_mm256_slli_si256(r1,1);//shift 1 elements left - equivalent to filling with one zero inthe beginning
		  					m2=_mm256_slli_si256(r2,1);//shift 1 elements left - equivalent to filling with one zero inthe beginning

		  					//preserve the element being lost because of the shift above
		  					r0=_mm256_and_si256(r0,mask_prelude);
		  					r0=_mm256_permute2f128_si256(r0,r0,1);
		  					r0=_mm256_srli_si256(r0,15);//shift 15 elements
		  					r0=_mm256_add_epi16(m0,r0);

		  					r1=_mm256_and_si256(r1,mask_prelude);
		  					r1=_mm256_permute2f128_si256(r1,r1,1);
		  					r1=_mm256_srli_si256(r1,15);//shift 15 elements
		  					r1=_mm256_add_epi16(m1,r1);

		  					r2=_mm256_and_si256(r2,mask_prelude);
		  					r2=_mm256_permute2f128_si256(r2,r2,1);
		  					r2=_mm256_srli_si256(r2,15);//shift 15 elements
		  					r2=_mm256_add_epi16(m2,r2);

		  			//END - extra code needed for prelude
		  		}
		  else {
				//load the 5 rows
				r0=_mm256_loadu_si256( (__m256i *) &frame1[0][col-1]);
				r1=_mm256_loadu_si256( (__m256i *) &frame1[1][col-1]);
				r2=_mm256_loadu_si256( (__m256i *) &frame1[2][col-1]);
		  }

		//col iteration computes output pixels of    1,5,9,13,17,21,25,29
		// col+1 iteration computes output pixels of 2,6,10,14,18,22,26,30
		// col+2 iteration computes output pixels of 3,7,11,15,19,23,27
		// col+3 iteration computes output pixels of 4,8,12,16,20,24,28
		//afterwards, col becomes 30 and repeat the above process

		 //row==1
		//1st col iteration

		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,c0);
		m1=_mm256_maddubs_epi16(r1,c1);
		m2=_mm256_maddubs_epi16(r2,c0);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m0=_mm256_add_epi16(m0,m2);

		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others. later on they will be stored into 0,4,8,12,16,20,24,28 positions (8bit register).
		output_even=_mm256_and_si256(m2,output_mask);


		//2nd col iteration
		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,c0_sh1);
		m1=_mm256_maddubs_epi16(r1,c1_sh1);
		m2=_mm256_maddubs_epi16(r2,c0_sh1);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m0=_mm256_add_epi16(m0,m2);

		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);


		//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others.
		output_odd = _mm256_and_si256(m2,output_mask);


       //3rd col iteration
		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,c0_sh2);
		m1=_mm256_maddubs_epi16(r1,c1_sh2);
		m2=_mm256_maddubs_epi16(r2,c0_sh2);


		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m0=_mm256_add_epi16(m0,m2);


		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		m4=_mm256_and_si256(m0,mask3);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
		m2=_mm256_add_epi16(m2,m4);

		//m2 has 16 16bit values now. the results I need are in positions 1,3,5,7,9,11,13. keep only those, discard others
		m2 = _mm256_and_si256(m2,output_mask_sh1);
		output_even = _mm256_add_epi16(output_even,m2);


		//4th col iteration

		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,c0_sh3);
		m1=_mm256_maddubs_epi16(r1,c1_sh3);
		m2=_mm256_maddubs_epi16(r2,c0_sh3);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m0=_mm256_add_epi16(m0,m2);


		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		m4=_mm256_and_si256(m0,mask3);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
		m2=_mm256_add_epi16(m2,m4);

		//m2 has 16 16bit values now. the results I need are in positions 1,3,5,7,9,11,13. keep only those, discard others
		m2 = _mm256_and_si256(m2,output_mask_sh1);
		output_odd = _mm256_add_epi16(output_odd,m2);


		//now division follows
		output_even=division(division_case,output_even,f);
		output_odd=division(division_case,output_odd,f);

		//shift odd 1 position and add to even
		output_odd = _mm256_slli_si256(output_odd,1);
		output_even = _mm256_add_epi8(output_even,output_odd);


		_mm256_storeu_si256( (__m256i *) &filt[1][col],output_even );

		 //row==0
		//1st col iteration

		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,c1);
		m1=_mm256_maddubs_epi16(r1,c0);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);

		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others. later on they will be stored into 0,4,8,12,16,20,24,28 positions (8bit register).
		output_even=_mm256_and_si256(m2,output_mask);


		//2nd col iteration
		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,c1_sh1);
		m1=_mm256_maddubs_epi16(r1,c0_sh1);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);

		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);


		//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others.
		output_odd = _mm256_and_si256(m2,output_mask);


      //3rd col iteration
		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,c1_sh2);
		m1=_mm256_maddubs_epi16(r1,c0_sh2);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);


		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		m4=_mm256_and_si256(m0,mask3);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
		m2=_mm256_add_epi16(m2,m4);

		//m2 has 16 16bit values now. the results I need are in positions 1,3,5,7,9,11,13. keep only those, discard others
		m2 = _mm256_and_si256(m2,output_mask_sh1);
		output_even = _mm256_add_epi16(output_even,m2);


		//4th col iteration

		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,c1_sh3);
		m1=_mm256_maddubs_epi16(r1,c0_sh3);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);

		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		m4=_mm256_and_si256(m0,mask3);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
		m2=_mm256_add_epi16(m2,m4);

		//m2 has 16 16bit values now. the results I need are in positions 1,3,5,7,9,11,13. keep only those, discard others
		m2 = _mm256_and_si256(m2,output_mask_sh1);
		output_odd = _mm256_add_epi16(output_odd,m2);


		//now division follows
		output_even=division(division_case,output_even,f);
		output_odd=division(division_case,output_odd,f);

		//shift odd 1 position and add to even
		output_odd = _mm256_slli_si256(output_odd,1);
		output_even = _mm256_add_epi8(output_even,output_odd);


		_mm256_storeu_si256( (__m256i *) &filt[0][col],output_even );
		}
	loop_reminder_3x3_first_values(frame1,filt,M,N,col,REMINDER_ITERATIONS,division_case,divisor,filter,c0,c1,c0_sh1,c1_sh1,c0_sh2,c1_sh2,c0_sh3,c1_sh3,f);

	}

	else if (row==N-2){//special case compute filt[N-2:N-1][:]
		for (col = 0; col <= M-32; col+=30){

		  if (col==0){

				//load the 5 rows
				r0=_mm256_loadu_si256( (__m256i *) &frame1[N-3][0]);
				r1=_mm256_loadu_si256( (__m256i *) &frame1[N-2][0]);
				r2=_mm256_loadu_si256( (__m256i *) &frame1[N-1][0]);


		  			//START - extra code needed for prelude
		  					m0=_mm256_slli_si256(r0,1);//shift 1 elements left - equivalent to filling with one zero inthe beginning
		  					m1=_mm256_slli_si256(r1,1);//shift 1 elements left - equivalent to filling with one zero inthe beginning
		  					m2=_mm256_slli_si256(r2,1);//shift 1 elements left - equivalent to filling with one zero inthe beginning

		  					//preserve the element being lost because of the shift above
		  					r0=_mm256_and_si256(r0,mask_prelude);
		  					r0=_mm256_permute2f128_si256(r0,r0,1);
		  					r0=_mm256_srli_si256(r0,15);//shift 15 elements
		  					r0=_mm256_add_epi16(m0,r0);

		  					r1=_mm256_and_si256(r1,mask_prelude);
		  					r1=_mm256_permute2f128_si256(r1,r1,1);
		  					r1=_mm256_srli_si256(r1,15);//shift 15 elements
		  					r1=_mm256_add_epi16(m1,r1);

		  					r2=_mm256_and_si256(r2,mask_prelude);
		  					r2=_mm256_permute2f128_si256(r2,r2,1);
		  					r2=_mm256_srli_si256(r2,15);//shift 15 elements
		  					r2=_mm256_add_epi16(m2,r2);

		  			//END - extra code needed for prelude
		  		}
		  else {
				//load the 5 rows
				r0=_mm256_loadu_si256( (__m256i *) &frame1[N-3][col-1]);
				r1=_mm256_loadu_si256( (__m256i *) &frame1[N-2][col-1]);
				r2=_mm256_loadu_si256( (__m256i *) &frame1[N-1][col-1]);
		  }

		//col iteration computes output pixels of    1,5,9,13,17,21,25,29
		// col+1 iteration computes output pixels of 2,6,10,14,18,22,26,30
		// col+2 iteration computes output pixels of 3,7,11,15,19,23,27
		// col+3 iteration computes output pixels of 4,8,12,16,20,24,28
		//afterwards, col becomes 30 and repeat the above process

		 //row==N-2
		//1st col iteration

		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,c0);
		m1=_mm256_maddubs_epi16(r1,c1);
		m2=_mm256_maddubs_epi16(r2,c0);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m0=_mm256_add_epi16(m0,m2);

		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others. later on they will be stored into 0,4,8,12,16,20,24,28 positions (8bit register).
		output_even=_mm256_and_si256(m2,output_mask);


		//2nd col iteration
		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,c0_sh1);
		m1=_mm256_maddubs_epi16(r1,c1_sh1);
		m2=_mm256_maddubs_epi16(r2,c0_sh1);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m0=_mm256_add_epi16(m0,m2);

		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);


		//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others.
		output_odd = _mm256_and_si256(m2,output_mask);


       //3rd col iteration
		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,c0_sh2);
		m1=_mm256_maddubs_epi16(r1,c1_sh2);
		m2=_mm256_maddubs_epi16(r2,c0_sh2);


		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m0=_mm256_add_epi16(m0,m2);


		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		m4=_mm256_and_si256(m0,mask3);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
		m2=_mm256_add_epi16(m2,m4);

		//m2 has 16 16bit values now. the results I need are in positions 1,3,5,7,9,11,13. keep only those, discard others
		m2 = _mm256_and_si256(m2,output_mask_sh1);
		output_even = _mm256_add_epi16(output_even,m2);


		//4th col iteration

		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,c0_sh3);
		m1=_mm256_maddubs_epi16(r1,c1_sh3);
		m2=_mm256_maddubs_epi16(r2,c0_sh3);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m0=_mm256_add_epi16(m0,m2);


		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		m4=_mm256_and_si256(m0,mask3);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
		m2=_mm256_add_epi16(m2,m4);

		//m2 has 16 16bit values now. the results I need are in positions 1,3,5,7,9,11,13. keep only those, discard others
		m2 = _mm256_and_si256(m2,output_mask_sh1);
		output_odd = _mm256_add_epi16(output_odd,m2);


		//now division follows
		output_even=division(division_case,output_even,f);
		output_odd=division(division_case,output_odd,f);

		//shift odd 1 position and add to even
		output_odd = _mm256_slli_si256(output_odd,1);
		output_even = _mm256_add_epi8(output_even,output_odd);


		_mm256_storeu_si256( (__m256i *) &filt[N-2][col],output_even );

		 //row==N-1
		//1st col iteration

		//multiply with the mask
		m1=_mm256_maddubs_epi16(r1,c0);
		m2=_mm256_maddubs_epi16(r2,c1);

		//vertical add
		m0=_mm256_add_epi16(m1,m2);

		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others. later on they will be stored into 0,4,8,12,16,20,24,28 positions (8bit register).
		output_even=_mm256_and_si256(m2,output_mask);


		//2nd col iteration
		//multiply with the mask
		m1=_mm256_maddubs_epi16(r1,c0_sh1);
		m2=_mm256_maddubs_epi16(r2,c1_sh1);

		//vertical add
		m0=_mm256_add_epi16(m1,m2);

		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);


		//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others.
		output_odd = _mm256_and_si256(m2,output_mask);


       //3rd col iteration
		//multiply with the mask
		m1=_mm256_maddubs_epi16(r1,c0_sh2);
		m2=_mm256_maddubs_epi16(r2,c1_sh2);


		//vertical add
		m0=_mm256_add_epi16(m1,m2);


		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		m4=_mm256_and_si256(m0,mask3);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
		m2=_mm256_add_epi16(m2,m4);

		//m2 has 16 16bit values now. the results I need are in positions 1,3,5,7,9,11,13. keep only those, discard others
		m2 = _mm256_and_si256(m2,output_mask_sh1);
		output_even = _mm256_add_epi16(output_even,m2);


		//4th col iteration

		//multiply with the mask
		m1=_mm256_maddubs_epi16(r1,c0_sh3);
		m2=_mm256_maddubs_epi16(r2,c1_sh3);

		//vertical add
		m0=_mm256_add_epi16(m1,m2);

		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		m4=_mm256_and_si256(m0,mask3);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
		m2=_mm256_add_epi16(m2,m4);

		//m2 has 16 16bit values now. the results I need are in positions 1,3,5,7,9,11,13. keep only those, discard others
		m2 = _mm256_and_si256(m2,output_mask_sh1);
		output_odd = _mm256_add_epi16(output_odd,m2);


		//now division follows
		output_even=division(division_case,output_even,f);
		output_odd=division(division_case,output_odd,f);

		//shift odd 1 position and add to even
		output_odd = _mm256_slli_si256(output_odd,1);
		output_even = _mm256_add_epi8(output_even,output_odd);


		_mm256_storeu_si256( (__m256i *) &filt[N-1][col],output_even );
		}
	loop_reminder_3x3_last_values(frame1,filt,M,N,col,REMINDER_ITERATIONS,division_case,divisor,filter,c0,c1,c0_sh1,c1_sh1,c0_sh2,c1_sh2,c0_sh3,c1_sh3,f);

	}

	else {//main loop

	  for (col = 0; col <= M-32; col+=30){

		  if (col==0){

				//load the 5 rows
				r0=_mm256_loadu_si256( (__m256i *) &frame1[row-1][0]);
				r1=_mm256_loadu_si256( (__m256i *) &frame1[row][0]);
				r2=_mm256_loadu_si256( (__m256i *) &frame1[row+1][0]);


		  			//START - extra code needed for prelude
		  					m0=_mm256_slli_si256(r0,1);//shift 1 elements left - equivalent to filling with one zero inthe beginning
		  					m1=_mm256_slli_si256(r1,1);//shift 1 elements left - equivalent to filling with one zero inthe beginning
		  					m2=_mm256_slli_si256(r2,1);//shift 1 elements left - equivalent to filling with one zero inthe beginning

		  					//preserve the element being lost because of the shift above
		  					r0=_mm256_and_si256(r0,mask_prelude);
		  					r0=_mm256_permute2f128_si256(r0,r0,1);
		  					r0=_mm256_srli_si256(r0,15);//shift 15 elements
		  					r0=_mm256_add_epi16(m0,r0);

		  					r1=_mm256_and_si256(r1,mask_prelude);
		  					r1=_mm256_permute2f128_si256(r1,r1,1);
		  					r1=_mm256_srli_si256(r1,15);//shift 15 elements
		  					r1=_mm256_add_epi16(m1,r1);

		  					r2=_mm256_and_si256(r2,mask_prelude);
		  					r2=_mm256_permute2f128_si256(r2,r2,1);
		  					r2=_mm256_srli_si256(r2,15);//shift 15 elements
		  					r2=_mm256_add_epi16(m2,r2);

		  			//END - extra code needed for prelude
		  		}
		  else {
				//load the 5 rows
				r0=_mm256_loadu_si256( (__m256i *) &frame1[row-1][col-1]);
				r1=_mm256_loadu_si256( (__m256i *) &frame1[row][col-1]);
				r2=_mm256_loadu_si256( (__m256i *) &frame1[row+1][col-1]);
		  }

		//col iteration computes output pixels of    1,5,9,13,17,21,25,29
		// col+1 iteration computes output pixels of 2,6,10,14,18,22,26,30
		// col+2 iteration computes output pixels of 3,7,11,15,19,23,27
		// col+3 iteration computes output pixels of 4,8,12,16,20,24,28
		//afterwards, col becomes 30 and repeat the above process

		//1st col iteration

		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,c0);
		m1=_mm256_maddubs_epi16(r1,c1);
		m2=_mm256_maddubs_epi16(r2,c0);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m0=_mm256_add_epi16(m0,m2);

		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others. later on they will be stored into 0,4,8,12,16,20,24,28 positions (8bit register).
		output_even=_mm256_and_si256(m2,output_mask);


		//2nd col iteration
		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,c0_sh1);
		m1=_mm256_maddubs_epi16(r1,c1_sh1);
		m2=_mm256_maddubs_epi16(r2,c0_sh1);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m0=_mm256_add_epi16(m0,m2);

		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);


		//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others.
		output_odd = _mm256_and_si256(m2,output_mask);


         //3rd col iteration
		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,c0_sh2);
		m1=_mm256_maddubs_epi16(r1,c1_sh2);
		m2=_mm256_maddubs_epi16(r2,c0_sh2);


		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m0=_mm256_add_epi16(m0,m2);


		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		m4=_mm256_and_si256(m0,mask3);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
		m2=_mm256_add_epi16(m2,m4);

		//m2 has 16 16bit values now. the results I need are in positions 1,3,5,7,9,11,13. keep only those, discard others
		m2 = _mm256_and_si256(m2,output_mask_sh1);
		output_even = _mm256_add_epi16(output_even,m2);


		//4th col iteration

		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,c0_sh3);
		m1=_mm256_maddubs_epi16(r1,c1_sh3);
		m2=_mm256_maddubs_epi16(r2,c0_sh3);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m0=_mm256_add_epi16(m0,m2);


		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		m4=_mm256_and_si256(m0,mask3);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
		m2=_mm256_add_epi16(m2,m4);

		//m2 has 16 16bit values now. the results I need are in positions 1,3,5,7,9,11,13. keep only those, discard others
		m2 = _mm256_and_si256(m2,output_mask_sh1);
		output_odd = _mm256_add_epi16(output_odd,m2);


		//now division follows
		output_even=division(division_case,output_even,f);
		output_odd=division(division_case,output_odd,f);

		//shift odd 1 position and add to even
		output_odd = _mm256_slli_si256(output_odd,1);
		output_even = _mm256_add_epi8(output_even,output_odd);


		_mm256_storeu_si256( (__m256i *) &filt[row][col],output_even );


		}
	  loop_reminder_3x3(frame1,filt,M,N,row,col,REMINDER_ITERATIONS,division_case,divisor,filter,c0,c1,c0_sh1,c1_sh1,c0_sh2,c1_sh2,c0_sh3,c1_sh3,f);

	}

}

}//end of parallel


}





int loop_reminder_3x3(unsigned char **frame1,unsigned char **filt,const unsigned int M, const unsigned int N,const unsigned int row, const unsigned int col,const unsigned int REMINDER_ITERATIONS,const unsigned int division_case,const unsigned short int divisor,signed char **filter,const __m256i c0,const __m256i c1,const __m256i c0_sh1,const __m256i c1_sh1,const __m256i c0_sh2,const __m256i c1_sh2, const __m256i c0_sh3,const __m256i c1_sh3,const __m256i f){

register	__m256i r0,r1,r2,m0,m1,m2,m4,output_even,output_odd;


const __m256i mask3  = _mm256_set_epi16(0,0,0,0,0,0,0,65535,0,0,0,0,0,0,0,0);

const __m256i output_mask      = _mm256_set_epi16(0,65535,0,65535,0,65535,0,65535,0,65535,0,65535,0,65535,0,65535);
const __m256i output_mask_sh1  = _mm256_set_epi16(0,0,65535,0,65535,0,65535,0,65535,0,65535,0,65535,0,65535,0);

 __m256i reminder_mask1;

if (REMINDER_ITERATIONS == 0){
	return 0; //no loop reminder is needed
}

	reminder_mask1=_mm256_load_si256( (__m256i *) &reminder_msk1_3x3[REMINDER_ITERATIONS-1][0]);
	//reminder_mask2=_mm256_load_si256( (__m256i *) &reminder_msk2[REMINDER_ITERATIONS-1][0]);


	 //1st col iteration
	//load the 5 rows
	r0=_mm256_loadu_si256( (__m256i *) &frame1[row-1][col-1]);
	r1=_mm256_loadu_si256( (__m256i *) &frame1[row][col-1]);
	r2=_mm256_loadu_si256( (__m256i *) &frame1[row+1][col-1]);


	/*
	 * The above load operations load outside of the array bounds; these elements are filled with zeros just after
	 * Furthermore, I add one extra zero in the end to compute col=N-1. The last value of col is N-1, not N
	 */

	//AND r0-r4 with reminder_mask
	r0=_mm256_and_si256(r0,reminder_mask1);
	r1=_mm256_and_si256(r1,reminder_mask1);
	r2=_mm256_and_si256(r2,reminder_mask1);


	//col iteration computes output pixels of    1,5,9,13,17,21,25,29
	// col+1 iteration computes output pixels of 2,6,10,14,18,22,26,30
	// col+2 iteration computes output pixels of 3,7,11,15,19,23,27
	// col+3 iteration computes output pixels of 4,8,12,16,20,24,28
	//afterwards, col becomes 30 and repeat the above process

	//1st col iteration

	//multiply with the mask
	m0=_mm256_maddubs_epi16(r0,c0);
	m1=_mm256_maddubs_epi16(r1,c1);
	m2=_mm256_maddubs_epi16(r2,c0);

	//vertical add
	m0=_mm256_add_epi16(m0,m1);
	m0=_mm256_add_epi16(m0,m2);

	//hozizontal additions
	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);

	//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others. later on they will be stored into 0,4,8,12,16,20,24,28 positions (8bit register).
	output_even=_mm256_and_si256(m2,output_mask);


	//2nd col iteration
	//multiply with the mask
	m0=_mm256_maddubs_epi16(r0,c0_sh1);
	m1=_mm256_maddubs_epi16(r1,c1_sh1);
	m2=_mm256_maddubs_epi16(r2,c0_sh1);

	//vertical add
	m0=_mm256_add_epi16(m0,m1);
	m0=_mm256_add_epi16(m0,m2);

	//hozizontal additions
	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);


	//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others.
	output_odd = _mm256_and_si256(m2,output_mask);


     //3rd col iteration
	//multiply with the mask
	m0=_mm256_maddubs_epi16(r0,c0_sh2);
	m1=_mm256_maddubs_epi16(r1,c1_sh2);
	m2=_mm256_maddubs_epi16(r2,c0_sh2);


	//vertical add
	m0=_mm256_add_epi16(m0,m1);
	m0=_mm256_add_epi16(m0,m2);


	//hozizontal additions
	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);

	m4=_mm256_and_si256(m0,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
	m2=_mm256_add_epi16(m2,m4);

	//m2 has 16 16bit values now. the results I need are in positions 1,3,5,7,9,11,13. keep only those, discard others
	m2 = _mm256_and_si256(m2,output_mask_sh1);
	output_even = _mm256_add_epi16(output_even,m2);


	//4th col iteration

	//multiply with the mask
	m0=_mm256_maddubs_epi16(r0,c0_sh3);
	m1=_mm256_maddubs_epi16(r1,c1_sh3);
	m2=_mm256_maddubs_epi16(r2,c0_sh3);

	//vertical add
	m0=_mm256_add_epi16(m0,m1);
	m0=_mm256_add_epi16(m0,m2);


	//hozizontal additions
	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);

	m4=_mm256_and_si256(m0,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
	m2=_mm256_add_epi16(m2,m4);

	//m2 has 16 16bit values now. the results I need are in positions 1,3,5,7,9,11,13. keep only those, discard others
	m2 = _mm256_and_si256(m2,output_mask_sh1);
	output_odd = _mm256_add_epi16(output_odd,m2);


	//now division follows
	output_even=division(division_case,output_even,f);
	output_odd=division(division_case,output_odd,f);

	//shift odd 1 position and add to even
	output_odd = _mm256_slli_si256(output_odd,1);
	output_even = _mm256_add_epi8(output_even,output_odd);


 	switch (REMINDER_ITERATIONS){
	case 1:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
	  break;
	case 2:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		  break;
	case 3:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		  break;
	case 4:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output_even,3);
		  break;
	case 5:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output_even,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output_even,4);
		  break;
	case 6:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output_even,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output_even,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output_even,5);
		  break;
	case 7:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output_even,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output_even,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output_even,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output_even,6);
		  break;
	case 8:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output_even,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output_even,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output_even,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output_even,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output_even,7);
		  break;
	case 9:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output_even,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output_even,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output_even,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output_even,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output_even,7);
		filt[row][col+8] = (unsigned char) _mm256_extract_epi8(output_even,8);
		  break;
	case 10:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output_even,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output_even,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output_even,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output_even,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output_even,7);
		filt[row][col+8] = (unsigned char) _mm256_extract_epi8(output_even,8);
		filt[row][col+9] = (unsigned char) _mm256_extract_epi8(output_even,9);
		  break;
	case 11:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output_even,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output_even,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output_even,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output_even,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output_even,7);
		filt[row][col+8] = (unsigned char) _mm256_extract_epi8(output_even,8);
		filt[row][col+9] = (unsigned char) _mm256_extract_epi8(output_even,9);
		filt[row][col+10] = (unsigned char) _mm256_extract_epi8(output_even,10);
		  break;
	case 12:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output_even,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output_even,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output_even,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output_even,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output_even,7);
		filt[row][col+8] = (unsigned char) _mm256_extract_epi8(output_even,8);
		filt[row][col+9] = (unsigned char) _mm256_extract_epi8(output_even,9);
		filt[row][col+10] = (unsigned char) _mm256_extract_epi8(output_even,10);
		filt[row][col+11] = (unsigned char) _mm256_extract_epi8(output_even,11);
		  break;
	case 13:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output_even,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output_even,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output_even,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output_even,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output_even,7);
		filt[row][col+8] = (unsigned char) _mm256_extract_epi8(output_even,8);
		filt[row][col+9] = (unsigned char) _mm256_extract_epi8(output_even,9);
		filt[row][col+10] = (unsigned char) _mm256_extract_epi8(output_even,10);
		filt[row][col+11] = (unsigned char) _mm256_extract_epi8(output_even,11);
		filt[row][col+12] = (unsigned char) _mm256_extract_epi8(output_even,12);
		  break;
	case 14:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output_even,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output_even,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output_even,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output_even,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output_even,7);
		filt[row][col+8] = (unsigned char) _mm256_extract_epi8(output_even,8);
		filt[row][col+9] = (unsigned char) _mm256_extract_epi8(output_even,9);
		filt[row][col+10] = (unsigned char) _mm256_extract_epi8(output_even,10);
		filt[row][col+11] = (unsigned char) _mm256_extract_epi8(output_even,11);
		filt[row][col+12] = (unsigned char) _mm256_extract_epi8(output_even,12);
		filt[row][col+13] = (unsigned char) _mm256_extract_epi8(output_even,13);
		  break;
	case 15:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output_even,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output_even,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output_even,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output_even,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output_even,7);
		filt[row][col+8] = (unsigned char) _mm256_extract_epi8(output_even,8);
		filt[row][col+9] = (unsigned char) _mm256_extract_epi8(output_even,9);
		filt[row][col+10] = (unsigned char) _mm256_extract_epi8(output_even,10);
		filt[row][col+11] = (unsigned char) _mm256_extract_epi8(output_even,11);
		filt[row][col+12] = (unsigned char) _mm256_extract_epi8(output_even,12);
		filt[row][col+13] = (unsigned char) _mm256_extract_epi8(output_even,13);
		filt[row][col+14] = (unsigned char) _mm256_extract_epi8(output_even,14);
		  break;
	case 16:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
		  break;
	case 17:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
		  break;
	case 18:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
		  break;
	case 19:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
		  break;
	case 20:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output_even,19);
		  break;
	case 21:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output_even,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output_even,20);
		  break;
	case 22:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output_even,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output_even,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output_even,21);
		break;
	case 23:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output_even,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output_even,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output_even,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output_even,22);
		  break;
	case 24:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output_even,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output_even,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output_even,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output_even,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output_even,23);
		  break;
	case 25:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output_even,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output_even,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output_even,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output_even,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output_even,23);
	filt[row][col+24] = (unsigned char) _mm256_extract_epi8(output_even,24);
		  break;
	case 26:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output_even,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output_even,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output_even,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output_even,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output_even,23);
	filt[row][col+24] = (unsigned char) _mm256_extract_epi8(output_even,24);
	filt[row][col+25] = (unsigned char) _mm256_extract_epi8(output_even,25);
		  break;
	case 27:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output_even,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output_even,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output_even,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output_even,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output_even,23);
	filt[row][col+24] = (unsigned char) _mm256_extract_epi8(output_even,24);
	filt[row][col+25] = (unsigned char) _mm256_extract_epi8(output_even,25);
	filt[row][col+26] = (unsigned char) _mm256_extract_epi8(output_even,26);
		  break;
	case 28:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output_even,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output_even,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output_even,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output_even,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output_even,23);
	filt[row][col+24] = (unsigned char) _mm256_extract_epi8(output_even,24);
	filt[row][col+25] = (unsigned char) _mm256_extract_epi8(output_even,25);
	filt[row][col+26] = (unsigned char) _mm256_extract_epi8(output_even,26);
	filt[row][col+27] = (unsigned char) _mm256_extract_epi8(output_even,27);
		  break;
	case 29:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output_even,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output_even,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output_even,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output_even,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output_even,23);
	filt[row][col+24] = (unsigned char) _mm256_extract_epi8(output_even,24);
	filt[row][col+25] = (unsigned char) _mm256_extract_epi8(output_even,25);
	filt[row][col+26] = (unsigned char) _mm256_extract_epi8(output_even,26);
	filt[row][col+27] = (unsigned char) _mm256_extract_epi8(output_even,27);
	filt[row][col+28] = (unsigned char) _mm256_extract_epi8(output_even,28);
		  break;
	case 30:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output_even,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output_even,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output_even,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output_even,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output_even,23);
	filt[row][col+24] = (unsigned char) _mm256_extract_epi8(output_even,24);
	filt[row][col+25] = (unsigned char) _mm256_extract_epi8(output_even,25);
	filt[row][col+26] = (unsigned char) _mm256_extract_epi8(output_even,26);
	filt[row][col+27] = (unsigned char) _mm256_extract_epi8(output_even,27);
	filt[row][col+28] = (unsigned char) _mm256_extract_epi8(output_even,28);

	//the filt[row][col+29] is computed unvectorized
	int newPixel = 0;
	newPixel += frame1[row-1][M-1-1] * filter[0][0];
	newPixel += frame1[row-1][M-1] * filter[0][1];

	newPixel += frame1[row][M-1-1] * filter[1][0];
	newPixel += frame1[row][M-1] * filter[1][1];

	newPixel += frame1[row+1][M-1-1] * filter[2][0];
	newPixel += frame1[row+1][M-1] * filter[2][1];

	filt[row][M-1] = (unsigned char) (newPixel / divisor);

		  break;

	case 31:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output_even,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output_even,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output_even,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output_even,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output_even,23);
	filt[row][col+24] = (unsigned char) _mm256_extract_epi8(output_even,24);
	filt[row][col+25] = (unsigned char) _mm256_extract_epi8(output_even,25);
	filt[row][col+26] = (unsigned char) _mm256_extract_epi8(output_even,26);
	filt[row][col+27] = (unsigned char) _mm256_extract_epi8(output_even,27);
	filt[row][col+28] = (unsigned char) _mm256_extract_epi8(output_even,28);

	//the filt[row][col+29/30] are computed unvectorized
	newPixel = 0;
	newPixel += frame1[row-1][M-2-1] * filter[0][0];
	newPixel += frame1[row-1][M-2] * filter[0][1];
	newPixel += frame1[row-1][M-2+1] * filter[0][2];


	newPixel += frame1[row][M-2-1] * filter[1][0];
	newPixel += frame1[row][M-2] * filter[1][1];
	newPixel += frame1[row][M-2+1] * filter[1][2];


	newPixel += frame1[row+1][M-2-1] * filter[2][0];
	newPixel += frame1[row+1][M-2] * filter[2][1];
	newPixel += frame1[row+1][M-2+1] * filter[2][2];

	filt[row][M-2] = (unsigned char) (newPixel / divisor);

	newPixel = 0;
	newPixel += frame1[row-1][M-1-1] * filter[0][0];
	newPixel += frame1[row-1][M-1] * filter[0][1];

	newPixel += frame1[row][M-1-1] * filter[1][0];
	newPixel += frame1[row][M-1] * filter[1][1];

	newPixel += frame1[row+1][M-1-1] * filter[2][0];
	newPixel += frame1[row+1][M-1] * filter[2][1];

	filt[row][M-1] = (unsigned char) (newPixel / divisor);
		  break;
	default:
		printf("\nsomething went wrong");
		return -1;
 	}

return 0;

}


int loop_reminder_3x3_first_values(unsigned char **frame1,unsigned char **filt,const unsigned int M, const unsigned int N, const unsigned int col,const unsigned int REMINDER_ITERATIONS,const unsigned int division_case,const unsigned short int divisor,signed char **filter,const __m256i c0,const __m256i c1,const __m256i c0_sh1,const __m256i c1_sh1,const __m256i c0_sh2,const __m256i c1_sh2, const __m256i c0_sh3,const __m256i c1_sh3,const __m256i f){

register	__m256i r0,r1,r2,m0,m1,m2,m4,output_even,output_odd,output_row0,output_row1;
int newPixel;
unsigned int i;
const __m256i mask3  = _mm256_set_epi16(0,0,0,0,0,0,0,65535,0,0,0,0,0,0,0,0);

const __m256i output_mask      = _mm256_set_epi16(0,65535,0,65535,0,65535,0,65535,0,65535,0,65535,0,65535,0,65535);
const __m256i output_mask_sh1  = _mm256_set_epi16(0,0,65535,0,65535,0,65535,0,65535,0,65535,0,65535,0,65535,0);

 __m256i reminder_mask1;

if (REMINDER_ITERATIONS == 0){
	return 0; //no loop reminder is needed
}

	reminder_mask1=_mm256_load_si256( (__m256i *) &reminder_msk1_3x3[REMINDER_ITERATIONS-1][0]);
	//reminder_mask2=_mm256_load_si256( (__m256i *) &reminder_msk2[REMINDER_ITERATIONS-1][0]);


	 //1st col iteration
	//load the 5 rows
	r0=_mm256_loadu_si256( (__m256i *) &frame1[0][col-1]);
	r1=_mm256_loadu_si256( (__m256i *) &frame1[1][col-1]);
	r2=_mm256_loadu_si256( (__m256i *) &frame1[2][col-1]);


	/*
	 * The above load operations load outside of the array bounds; these elements are filled with zeros just after
	 * Furthermore, I add one extra zero in the end to compute col=N-1. The last value of col is N-1, not N
	 */

	//AND r0-r4 with reminder_mask
	r0=_mm256_and_si256(r0,reminder_mask1);
	r1=_mm256_and_si256(r1,reminder_mask1);
	r2=_mm256_and_si256(r2,reminder_mask1);


	//col iteration computes output pixels of    1,5,9,13,17,21,25,29
	// col+1 iteration computes output pixels of 2,6,10,14,18,22,26,30
	// col+2 iteration computes output pixels of 3,7,11,15,19,23,27
	// col+3 iteration computes output pixels of 4,8,12,16,20,24,28
	//afterwards, col becomes 30 and repeat the above process

	//1st col iteration

	//multiply with the mask
	m0=_mm256_maddubs_epi16(r0,c0);
	m1=_mm256_maddubs_epi16(r1,c1);
	m2=_mm256_maddubs_epi16(r2,c0);

	//vertical add
	m0=_mm256_add_epi16(m0,m1);
	m0=_mm256_add_epi16(m0,m2);

	//hozizontal additions
	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);

	//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others. later on they will be stored into 0,4,8,12,16,20,24,28 positions (8bit register).
	output_even=_mm256_and_si256(m2,output_mask);


	//2nd col iteration
	//multiply with the mask
	m0=_mm256_maddubs_epi16(r0,c0_sh1);
	m1=_mm256_maddubs_epi16(r1,c1_sh1);
	m2=_mm256_maddubs_epi16(r2,c0_sh1);

	//vertical add
	m0=_mm256_add_epi16(m0,m1);
	m0=_mm256_add_epi16(m0,m2);

	//hozizontal additions
	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);


	//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others.
	output_odd = _mm256_and_si256(m2,output_mask);


     //3rd col iteration
	//multiply with the mask
	m0=_mm256_maddubs_epi16(r0,c0_sh2);
	m1=_mm256_maddubs_epi16(r1,c1_sh2);
	m2=_mm256_maddubs_epi16(r2,c0_sh2);


	//vertical add
	m0=_mm256_add_epi16(m0,m1);
	m0=_mm256_add_epi16(m0,m2);


	//hozizontal additions
	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);

	m4=_mm256_and_si256(m0,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
	m2=_mm256_add_epi16(m2,m4);

	//m2 has 16 16bit values now. the results I need are in positions 1,3,5,7,9,11,13. keep only those, discard others
	m2 = _mm256_and_si256(m2,output_mask_sh1);
	output_even = _mm256_add_epi16(output_even,m2);


	//4th col iteration

	//multiply with the mask
	m0=_mm256_maddubs_epi16(r0,c0_sh3);
	m1=_mm256_maddubs_epi16(r1,c1_sh3);
	m2=_mm256_maddubs_epi16(r2,c0_sh3);

	//vertical add
	m0=_mm256_add_epi16(m0,m1);
	m0=_mm256_add_epi16(m0,m2);


	//hozizontal additions
	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);

	m4=_mm256_and_si256(m0,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
	m2=_mm256_add_epi16(m2,m4);

	//m2 has 16 16bit values now. the results I need are in positions 1,3,5,7,9,11,13. keep only those, discard others
	m2 = _mm256_and_si256(m2,output_mask_sh1);
	output_odd = _mm256_add_epi16(output_odd,m2);


	//now division follows
	output_even=division(division_case,output_even,f);
	output_odd=division(division_case,output_odd,f);

	//shift odd 1 position and add to even
	output_odd = _mm256_slli_si256(output_odd,1);
	output_even = _mm256_add_epi8(output_even,output_odd);

	output_row1=output_even;

	 //row==0
	//1st col iteration

	//multiply with the mask
	m0=_mm256_maddubs_epi16(r0,c1);
	m1=_mm256_maddubs_epi16(r1,c0);

	//vertical add
	m0=_mm256_add_epi16(m0,m1);

	//hozizontal additions
	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);

	//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others. later on they will be stored into 0,4,8,12,16,20,24,28 positions (8bit register).
	output_even=_mm256_and_si256(m2,output_mask);


	//2nd col iteration
	//multiply with the mask
	m0=_mm256_maddubs_epi16(r0,c1_sh1);
	m1=_mm256_maddubs_epi16(r1,c0_sh1);

	//vertical add
	m0=_mm256_add_epi16(m0,m1);

	//hozizontal additions
	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);


	//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others.
	output_odd = _mm256_and_si256(m2,output_mask);


 //3rd col iteration
	//multiply with the mask
	m0=_mm256_maddubs_epi16(r0,c1_sh2);
	m1=_mm256_maddubs_epi16(r1,c0_sh2);

	//vertical add
	m0=_mm256_add_epi16(m0,m1);


	//hozizontal additions
	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);

	m4=_mm256_and_si256(m0,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
	m2=_mm256_add_epi16(m2,m4);

	//m2 has 16 16bit values now. the results I need are in positions 1,3,5,7,9,11,13. keep only those, discard others
	m2 = _mm256_and_si256(m2,output_mask_sh1);
	output_even = _mm256_add_epi16(output_even,m2);


	//4th col iteration

	//multiply with the mask
	m0=_mm256_maddubs_epi16(r0,c1_sh3);
	m1=_mm256_maddubs_epi16(r1,c0_sh3);

	//vertical add
	m0=_mm256_add_epi16(m0,m1);

	//hozizontal additions
	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);

	m4=_mm256_and_si256(m0,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
	m2=_mm256_add_epi16(m2,m4);

	//m2 has 16 16bit values now. the results I need are in positions 1,3,5,7,9,11,13. keep only those, discard others
	m2 = _mm256_and_si256(m2,output_mask_sh1);
	output_odd = _mm256_add_epi16(output_odd,m2);


	//now division follows
	output_even=division(division_case,output_even,f);
	output_odd=division(division_case,output_odd,f);

	//shift odd 1 position and add to even
	output_odd = _mm256_slli_si256(output_odd,1);
	output_even = _mm256_add_epi8(output_even,output_odd);

	output_row0=output_even;

 	switch (REMINDER_ITERATIONS){
	case 1:
		filt[1][col] = (unsigned char) _mm256_extract_epi8(output_row1,0);
		filt[0][col] = (unsigned char) _mm256_extract_epi8(output_row0,0);

	  break;
	case 2:
		for (i=0;i<=1;i++){
			filt[1][col+i] = (unsigned char) _mm256_extract_epi8(output_row1,i);
			filt[0][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 3:
		for (i=0;i<=2;i++){
			filt[1][col+i] = (unsigned char) _mm256_extract_epi8(output_row1,i);
			filt[0][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 4:
		for (i=0;i<=3;i++){
			filt[1][col+i] = (unsigned char) _mm256_extract_epi8(output_row1,i);
			filt[0][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 5:
		for (i=0;i<=4;i++){
			filt[1][col+i] = (unsigned char) _mm256_extract_epi8(output_row1,i);
			filt[0][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 6:
		for (i=0;i<=5;i++){
			filt[1][col+i] = (unsigned char) _mm256_extract_epi8(output_row1,i);
			filt[0][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 7:
		for (i=0;i<=6;i++){
			filt[1][col+i] = (unsigned char) _mm256_extract_epi8(output_row1,i);
			filt[0][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 8:
		for (i=0;i<=7;i++){
			filt[1][col+i] = (unsigned char) _mm256_extract_epi8(output_row1,i);
			filt[0][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 9:
		for (i=0;i<=8;i++){
			filt[1][col+i] = (unsigned char) _mm256_extract_epi8(output_row1,i);
			filt[0][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 10:
		for (i=0;i<=9;i++){
			filt[1][col+i] = (unsigned char) _mm256_extract_epi8(output_row1,i);
			filt[0][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 11:
		for (i=0;i<=10;i++){
			filt[1][col+i] = (unsigned char) _mm256_extract_epi8(output_row1,i);
			filt[0][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 12:
		for (i=0;i<=11;i++){
			filt[1][col+i] = (unsigned char) _mm256_extract_epi8(output_row1,i);
			filt[0][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 13:
		for (i=0;i<=12;i++){
			filt[1][col+i] = (unsigned char) _mm256_extract_epi8(output_row1,i);
			filt[0][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 14:
		for (i=0;i<=13;i++){
			filt[1][col+i] = (unsigned char) _mm256_extract_epi8(output_row1,i);
			filt[0][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 15:
		for (i=0;i<=14;i++){
			filt[1][col+i] = (unsigned char) _mm256_extract_epi8(output_row1,i);
			filt[0][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 16:
  	_mm_storeu_si128( (__m128i *) &filt[1][col],_mm256_extractf128_si256(output_row1, 0)); //store low 128bit - 16pixels
  	_mm_storeu_si128( (__m128i *) &filt[0][col],_mm256_extractf128_si256(output_row0, 0)); //store low 128bit - 16pixels

		  break;
	case 17:
  	_mm_storeu_si128( (__m128i *) &filt[1][col],_mm256_extractf128_si256(output_row1, 0)); //store low 128bit - 16pixels
  	_mm_storeu_si128( (__m128i *) &filt[0][col],_mm256_extractf128_si256(output_row0, 0)); //store low 128bit - 16pixels
	filt[1][col+16] = (unsigned char) _mm256_extract_epi8(output_row1,16);
	filt[0][col+16] = (unsigned char) _mm256_extract_epi8(output_row0,16);
		  break;
	case 18:
  	_mm_storeu_si128( (__m128i *) &filt[1][col],_mm256_extractf128_si256(output_row1, 0)); //store low 128bit - 16pixels
  	_mm_storeu_si128( (__m128i *) &filt[0][col],_mm256_extractf128_si256(output_row0, 0)); //store low 128bit - 16pixels

	for (i=16;i<=17;i++){
	 filt[1][col+i] = (unsigned char) _mm256_extract_epi8(output_row1,i);
	 filt[0][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
	}
		  break;
	case 19:
	  	_mm_storeu_si128( (__m128i *) &filt[1][col],_mm256_extractf128_si256(output_row1, 0)); //store low 128bit - 16pixels
	  	_mm_storeu_si128( (__m128i *) &filt[0][col],_mm256_extractf128_si256(output_row0, 0)); //store low 128bit - 16pixels

		for (i=16;i<=18;i++){
		 filt[1][col+i] = (unsigned char) _mm256_extract_epi8(output_row1,i);
		 filt[0][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 20:
	  	_mm_storeu_si128( (__m128i *) &filt[1][col],_mm256_extractf128_si256(output_row1, 0)); //store low 128bit - 16pixels
	  	_mm_storeu_si128( (__m128i *) &filt[0][col],_mm256_extractf128_si256(output_row0, 0)); //store low 128bit - 16pixels

		for (i=16;i<=19;i++){
		 filt[1][col+i] = (unsigned char) _mm256_extract_epi8(output_row1,i);
		 filt[0][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 21:
	  	_mm_storeu_si128( (__m128i *) &filt[1][col],_mm256_extractf128_si256(output_row1, 0)); //store low 128bit - 16pixels
	  	_mm_storeu_si128( (__m128i *) &filt[0][col],_mm256_extractf128_si256(output_row0, 0)); //store low 128bit - 16pixels

		for (i=16;i<=20;i++){
		 filt[1][col+i] = (unsigned char) _mm256_extract_epi8(output_row1,i);
		 filt[0][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 22:
	  	_mm_storeu_si128( (__m128i *) &filt[1][col],_mm256_extractf128_si256(output_row1, 0)); //store low 128bit - 16pixels
	  	_mm_storeu_si128( (__m128i *) &filt[0][col],_mm256_extractf128_si256(output_row0, 0)); //store low 128bit - 16pixels

		for (i=16;i<=21;i++){
		 filt[1][col+i] = (unsigned char) _mm256_extract_epi8(output_row1,i);
		 filt[0][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		break;
	case 23:
	  	_mm_storeu_si128( (__m128i *) &filt[1][col],_mm256_extractf128_si256(output_row1, 0)); //store low 128bit - 16pixels
	  	_mm_storeu_si128( (__m128i *) &filt[0][col],_mm256_extractf128_si256(output_row0, 0)); //store low 128bit - 16pixels

		for (i=16;i<=22;i++){
		 filt[1][col+i] = (unsigned char) _mm256_extract_epi8(output_row1,i);
		 filt[0][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 24:
	  	_mm_storeu_si128( (__m128i *) &filt[1][col],_mm256_extractf128_si256(output_row1, 0)); //store low 128bit - 16pixels
	  	_mm_storeu_si128( (__m128i *) &filt[0][col],_mm256_extractf128_si256(output_row0, 0)); //store low 128bit - 16pixels

		for (i=16;i<=23;i++){
		 filt[1][col+i] = (unsigned char) _mm256_extract_epi8(output_row1,i);
		 filt[0][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 25:
	  	_mm_storeu_si128( (__m128i *) &filt[1][col],_mm256_extractf128_si256(output_row1, 0)); //store low 128bit - 16pixels
	  	_mm_storeu_si128( (__m128i *) &filt[0][col],_mm256_extractf128_si256(output_row0, 0)); //store low 128bit - 16pixels

		for (i=16;i<=24;i++){
		 filt[1][col+i] = (unsigned char) _mm256_extract_epi8(output_row1,i);
		 filt[0][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 26:
	  	_mm_storeu_si128( (__m128i *) &filt[1][col],_mm256_extractf128_si256(output_row1, 0)); //store low 128bit - 16pixels
	  	_mm_storeu_si128( (__m128i *) &filt[0][col],_mm256_extractf128_si256(output_row0, 0)); //store low 128bit - 16pixels

		for (i=16;i<=25;i++){
		 filt[1][col+i] = (unsigned char) _mm256_extract_epi8(output_row1,i);
		 filt[0][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}

		  break;
	case 27:
	  	_mm_storeu_si128( (__m128i *) &filt[1][col],_mm256_extractf128_si256(output_row1, 0)); //store low 128bit - 16pixels
	  	_mm_storeu_si128( (__m128i *) &filt[0][col],_mm256_extractf128_si256(output_row0, 0)); //store low 128bit - 16pixels

		for (i=16;i<=26;i++){
		 filt[1][col+i] = (unsigned char) _mm256_extract_epi8(output_row1,i);
		 filt[0][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}

		  break;
	case 28:
	  	_mm_storeu_si128( (__m128i *) &filt[1][col],_mm256_extractf128_si256(output_row1, 0)); //store low 128bit - 16pixels
	  	_mm_storeu_si128( (__m128i *) &filt[0][col],_mm256_extractf128_si256(output_row0, 0)); //store low 128bit - 16pixels

		for (i=16;i<=27;i++){
		 filt[1][col+i] = (unsigned char) _mm256_extract_epi8(output_row1,i);
		 filt[0][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 29:
	  	_mm_storeu_si128( (__m128i *) &filt[1][col],_mm256_extractf128_si256(output_row1, 0)); //store low 128bit - 16pixels
	  	_mm_storeu_si128( (__m128i *) &filt[0][col],_mm256_extractf128_si256(output_row0, 0)); //store low 128bit - 16pixels

		filt[1][col+16] = (unsigned char) _mm256_extract_epi8(output_row1,16);
		filt[1][col+17] = (unsigned char) _mm256_extract_epi8(output_row1,17);
		filt[1][col+18] = (unsigned char) _mm256_extract_epi8(output_row1,18);
		filt[1][col+19] = (unsigned char) _mm256_extract_epi8(output_row1,19);
		filt[1][col+20] = (unsigned char) _mm256_extract_epi8(output_row1,20);
		filt[1][col+21] = (unsigned char) _mm256_extract_epi8(output_row1,21);
		filt[1][col+22] = (unsigned char) _mm256_extract_epi8(output_row1,22);
		filt[1][col+23] = (unsigned char) _mm256_extract_epi8(output_row1,23);
		filt[1][col+24] = (unsigned char) _mm256_extract_epi8(output_row1,24);
		filt[1][col+25] = (unsigned char) _mm256_extract_epi8(output_row1,25);
		filt[1][col+26] = (unsigned char) _mm256_extract_epi8(output_row1,26);
		filt[1][col+27] = (unsigned char) _mm256_extract_epi8(output_row1,27);
		filt[1][col+28] = (unsigned char) _mm256_extract_epi8(output_row1,28);

		filt[0][col+16] = (unsigned char) _mm256_extract_epi8(output_row0,16);
		filt[0][col+17] = (unsigned char) _mm256_extract_epi8(output_row0,17);
		filt[0][col+18] = (unsigned char) _mm256_extract_epi8(output_row0,18);
		filt[0][col+19] = (unsigned char) _mm256_extract_epi8(output_row0,19);
		filt[0][col+20] = (unsigned char) _mm256_extract_epi8(output_row0,20);
		filt[0][col+21] = (unsigned char) _mm256_extract_epi8(output_row0,21);
		filt[0][col+22] = (unsigned char) _mm256_extract_epi8(output_row0,22);
		filt[0][col+23] = (unsigned char) _mm256_extract_epi8(output_row0,23);
		filt[0][col+24] = (unsigned char) _mm256_extract_epi8(output_row0,24);
		filt[0][col+25] = (unsigned char) _mm256_extract_epi8(output_row0,25);
		filt[0][col+26] = (unsigned char) _mm256_extract_epi8(output_row0,26);
		filt[0][col+27] = (unsigned char) _mm256_extract_epi8(output_row0,27);
		filt[0][col+28] = (unsigned char) _mm256_extract_epi8(output_row0,28);

		  break;
	case 30:
	  	_mm_storeu_si128( (__m128i *) &filt[1][col],_mm256_extractf128_si256(output_row1, 0)); //store low 128bit - 16pixels
	  	_mm_storeu_si128( (__m128i *) &filt[0][col],_mm256_extractf128_si256(output_row0, 0)); //store low 128bit - 16pixels

		filt[1][col+16] = (unsigned char) _mm256_extract_epi8(output_row1,16);
		filt[1][col+17] = (unsigned char) _mm256_extract_epi8(output_row1,17);
		filt[1][col+18] = (unsigned char) _mm256_extract_epi8(output_row1,18);
		filt[1][col+19] = (unsigned char) _mm256_extract_epi8(output_row1,19);
		filt[1][col+20] = (unsigned char) _mm256_extract_epi8(output_row1,20);
		filt[1][col+21] = (unsigned char) _mm256_extract_epi8(output_row1,21);
		filt[1][col+22] = (unsigned char) _mm256_extract_epi8(output_row1,22);
		filt[1][col+23] = (unsigned char) _mm256_extract_epi8(output_row1,23);
		filt[1][col+24] = (unsigned char) _mm256_extract_epi8(output_row1,24);
		filt[1][col+25] = (unsigned char) _mm256_extract_epi8(output_row1,25);
		filt[1][col+26] = (unsigned char) _mm256_extract_epi8(output_row1,26);
		filt[1][col+27] = (unsigned char) _mm256_extract_epi8(output_row1,27);
		filt[1][col+28] = (unsigned char) _mm256_extract_epi8(output_row1,28);

		filt[0][col+16] = (unsigned char) _mm256_extract_epi8(output_row0,16);
		filt[0][col+17] = (unsigned char) _mm256_extract_epi8(output_row0,17);
		filt[0][col+18] = (unsigned char) _mm256_extract_epi8(output_row0,18);
		filt[0][col+19] = (unsigned char) _mm256_extract_epi8(output_row0,19);
		filt[0][col+20] = (unsigned char) _mm256_extract_epi8(output_row0,20);
		filt[0][col+21] = (unsigned char) _mm256_extract_epi8(output_row0,21);
		filt[0][col+22] = (unsigned char) _mm256_extract_epi8(output_row0,22);
		filt[0][col+23] = (unsigned char) _mm256_extract_epi8(output_row0,23);
		filt[0][col+24] = (unsigned char) _mm256_extract_epi8(output_row0,24);
		filt[0][col+25] = (unsigned char) _mm256_extract_epi8(output_row0,25);
		filt[0][col+26] = (unsigned char) _mm256_extract_epi8(output_row0,26);
		filt[0][col+27] = (unsigned char) _mm256_extract_epi8(output_row0,27);
		filt[0][col+28] = (unsigned char) _mm256_extract_epi8(output_row0,28);

	//the filt[row][col+29] is computed unvectorized
	newPixel = 0;
	newPixel += frame1[1-1][M-1-1] * filter[0][0];
	newPixel += frame1[1-1][M-1] * filter[0][1];

	newPixel += frame1[1][M-1-1] * filter[1][0];
	newPixel += frame1[1][M-1] * filter[1][1];

	newPixel += frame1[1+1][M-1-1] * filter[2][0];
	newPixel += frame1[1+1][M-1] * filter[2][1];

	filt[1][M-1] = (unsigned char) (newPixel / divisor);

	newPixel = 0;
	newPixel += frame1[1-1][M-1-1] * filter[1][0];
	newPixel += frame1[1-1][M-1] * filter[1][1];

	newPixel += frame1[1][M-1-1] * filter[2][0];
	newPixel += frame1[1][M-1] * filter[2][1];

	filt[0][M-1] = (unsigned char) (newPixel / divisor);

		  break;

	case 31:
	  	_mm_storeu_si128( (__m128i *) &filt[1][col],_mm256_extractf128_si256(output_row1, 0)); //store low 128bit - 16pixels
	  	_mm_storeu_si128( (__m128i *) &filt[0][col],_mm256_extractf128_si256(output_row0, 0)); //store low 128bit - 16pixels

		filt[1][col+16] = (unsigned char) _mm256_extract_epi8(output_row1,16);
		filt[1][col+17] = (unsigned char) _mm256_extract_epi8(output_row1,17);
		filt[1][col+18] = (unsigned char) _mm256_extract_epi8(output_row1,18);
		filt[1][col+19] = (unsigned char) _mm256_extract_epi8(output_row1,19);
		filt[1][col+20] = (unsigned char) _mm256_extract_epi8(output_row1,20);
		filt[1][col+21] = (unsigned char) _mm256_extract_epi8(output_row1,21);
		filt[1][col+22] = (unsigned char) _mm256_extract_epi8(output_row1,22);
		filt[1][col+23] = (unsigned char) _mm256_extract_epi8(output_row1,23);
		filt[1][col+24] = (unsigned char) _mm256_extract_epi8(output_row1,24);
		filt[1][col+25] = (unsigned char) _mm256_extract_epi8(output_row1,25);
		filt[1][col+26] = (unsigned char) _mm256_extract_epi8(output_row1,26);
		filt[1][col+27] = (unsigned char) _mm256_extract_epi8(output_row1,27);
		filt[1][col+28] = (unsigned char) _mm256_extract_epi8(output_row1,28);

		filt[0][col+16] = (unsigned char) _mm256_extract_epi8(output_row0,16);
		filt[0][col+17] = (unsigned char) _mm256_extract_epi8(output_row0,17);
		filt[0][col+18] = (unsigned char) _mm256_extract_epi8(output_row0,18);
		filt[0][col+19] = (unsigned char) _mm256_extract_epi8(output_row0,19);
		filt[0][col+20] = (unsigned char) _mm256_extract_epi8(output_row0,20);
		filt[0][col+21] = (unsigned char) _mm256_extract_epi8(output_row0,21);
		filt[0][col+22] = (unsigned char) _mm256_extract_epi8(output_row0,22);
		filt[0][col+23] = (unsigned char) _mm256_extract_epi8(output_row0,23);
		filt[0][col+24] = (unsigned char) _mm256_extract_epi8(output_row0,24);
		filt[0][col+25] = (unsigned char) _mm256_extract_epi8(output_row0,25);
		filt[0][col+26] = (unsigned char) _mm256_extract_epi8(output_row0,26);
		filt[0][col+27] = (unsigned char) _mm256_extract_epi8(output_row0,27);
		filt[0][col+28] = (unsigned char) _mm256_extract_epi8(output_row0,28);

	//the filt[1][col+29/30] are computed unvectorized
		newPixel = 0;
		newPixel += frame1[1-1][M-2-1] * filter[0][0];
		newPixel += frame1[1-1][M-2] * filter[0][1];
		newPixel += frame1[1-1][M-2+1] * filter[0][2];

		newPixel += frame1[1][M-2-1] * filter[1][0];
		newPixel += frame1[1][M-2] * filter[1][1];
		newPixel += frame1[1][M-2+1] * filter[1][2];

		newPixel += frame1[1+1][M-2-1] * filter[2][0];
		newPixel += frame1[1+1][M-2] * filter[2][1];
		newPixel += frame1[1+1][M-2+1] * filter[2][2];

		filt[1][M-2] = (unsigned char) (newPixel / divisor);

		newPixel = 0;
		newPixel += frame1[1-1][M-1-1] * filter[0][0];
		newPixel += frame1[1-1][M-1] * filter[0][1];

		newPixel += frame1[1][M-1-1] * filter[1][0];
		newPixel += frame1[1][M-1] * filter[1][1];

		newPixel += frame1[1+1][M-1-1] * filter[2][0];
		newPixel += frame1[1+1][M-1] * filter[2][1];

		filt[1][M-1] = (unsigned char) (newPixel / divisor);

		newPixel = 0;

		newPixel += frame1[0][M-2-1] * filter[1][0];
		newPixel += frame1[0][M-2] * filter[1][1];
		newPixel += frame1[0][M-2+1] * filter[1][2];

		newPixel += frame1[1][M-2-1] * filter[2][0];
		newPixel += frame1[1][M-2] * filter[2][1];
		newPixel += frame1[1][M-2+1] * filter[2][2];

		filt[0][M-2] = (unsigned char) (newPixel / divisor);

		newPixel = 0;

		newPixel += frame1[0][M-1-1] * filter[1][0];
		newPixel += frame1[0][M-1] * filter[1][1];

		newPixel += frame1[1][M-1-1] * filter[2][0];
		newPixel += frame1[1][M-1] * filter[2][1];

		filt[0][M-1] = (unsigned char) (newPixel / divisor);


		  break;

	default:
		printf("\nsomething went wrong");
		return -1;
 	}

return 0;

}



int loop_reminder_3x3_last_values(unsigned char **frame1,unsigned char **filt,const unsigned int M, const unsigned int N, const unsigned int col,const unsigned int REMINDER_ITERATIONS,const unsigned int division_case,const unsigned short int divisor,signed char **filter,const __m256i c0,const __m256i c1,const __m256i c0_sh1,const __m256i c1_sh1,const __m256i c0_sh2,const __m256i c1_sh2, const __m256i c0_sh3,const __m256i c1_sh3,const __m256i f){

register	__m256i r0,r1,r2,m0,m1,m2,m4,output_even,output_odd,output_row0,output_row1;
int newPixel;
unsigned int i;
const __m256i mask3  = _mm256_set_epi16(0,0,0,0,0,0,0,65535,0,0,0,0,0,0,0,0);

const __m256i output_mask      = _mm256_set_epi16(0,65535,0,65535,0,65535,0,65535,0,65535,0,65535,0,65535,0,65535);
const __m256i output_mask_sh1  = _mm256_set_epi16(0,0,65535,0,65535,0,65535,0,65535,0,65535,0,65535,0,65535,0);

 __m256i reminder_mask1;

if (REMINDER_ITERATIONS == 0){
	return 0; //no loop reminder is needed
}

	reminder_mask1=_mm256_load_si256( (__m256i *) &reminder_msk1_3x3[REMINDER_ITERATIONS-1][0]);
	//reminder_mask2=_mm256_load_si256( (__m256i *) &reminder_msk2[REMINDER_ITERATIONS-1][0]);


	 //1st col iteration
	//load the 5 rows
	r0=_mm256_loadu_si256( (__m256i *) &frame1[N-3][col-1]);
	r1=_mm256_loadu_si256( (__m256i *) &frame1[N-2][col-1]);
	r2=_mm256_loadu_si256( (__m256i *) &frame1[N-1][col-1]);


	/*
	 * The above load operations load outside of the array bounds; these elements are filled with zeros just after
	 * Furthermore, I add one extra zero in the end to compute col=N-1. The last value of col is N-1, not N
	 */

	//AND r0-r4 with reminder_mask
	r0=_mm256_and_si256(r0,reminder_mask1);
	r1=_mm256_and_si256(r1,reminder_mask1);
	r2=_mm256_and_si256(r2,reminder_mask1);


	//col iteration computes output pixels of    1,5,9,13,17,21,25,29
	// col+1 iteration computes output pixels of 2,6,10,14,18,22,26,30
	// col+2 iteration computes output pixels of 3,7,11,15,19,23,27
	// col+3 iteration computes output pixels of 4,8,12,16,20,24,28
	//afterwards, col becomes 30 and repeat the above process

	//1st col iteration

	//multiply with the mask
	m0=_mm256_maddubs_epi16(r0,c0);
	m1=_mm256_maddubs_epi16(r1,c1);
	m2=_mm256_maddubs_epi16(r2,c0);

	//vertical add
	m0=_mm256_add_epi16(m0,m1);
	m0=_mm256_add_epi16(m0,m2);

	//hozizontal additions
	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);

	//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others. later on they will be stored into 0,4,8,12,16,20,24,28 positions (8bit register).
	output_even=_mm256_and_si256(m2,output_mask);


	//2nd col iteration
	//multiply with the mask
	m0=_mm256_maddubs_epi16(r0,c0_sh1);
	m1=_mm256_maddubs_epi16(r1,c1_sh1);
	m2=_mm256_maddubs_epi16(r2,c0_sh1);

	//vertical add
	m0=_mm256_add_epi16(m0,m1);
	m0=_mm256_add_epi16(m0,m2);

	//hozizontal additions
	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);


	//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others.
	output_odd = _mm256_and_si256(m2,output_mask);


     //3rd col iteration
	//multiply with the mask
	m0=_mm256_maddubs_epi16(r0,c0_sh2);
	m1=_mm256_maddubs_epi16(r1,c1_sh2);
	m2=_mm256_maddubs_epi16(r2,c0_sh2);


	//vertical add
	m0=_mm256_add_epi16(m0,m1);
	m0=_mm256_add_epi16(m0,m2);


	//hozizontal additions
	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);

	m4=_mm256_and_si256(m0,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
	m2=_mm256_add_epi16(m2,m4);

	//m2 has 16 16bit values now. the results I need are in positions 1,3,5,7,9,11,13. keep only those, discard others
	m2 = _mm256_and_si256(m2,output_mask_sh1);
	output_even = _mm256_add_epi16(output_even,m2);


	//4th col iteration

	//multiply with the mask
	m0=_mm256_maddubs_epi16(r0,c0_sh3);
	m1=_mm256_maddubs_epi16(r1,c1_sh3);
	m2=_mm256_maddubs_epi16(r2,c0_sh3);

	//vertical add
	m0=_mm256_add_epi16(m0,m1);
	m0=_mm256_add_epi16(m0,m2);


	//hozizontal additions
	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);

	m4=_mm256_and_si256(m0,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
	m2=_mm256_add_epi16(m2,m4);

	//m2 has 16 16bit values now. the results I need are in positions 1,3,5,7,9,11,13. keep only those, discard others
	m2 = _mm256_and_si256(m2,output_mask_sh1);
	output_odd = _mm256_add_epi16(output_odd,m2);


	//now division follows
	output_even=division(division_case,output_even,f);
	output_odd=division(division_case,output_odd,f);

	//shift odd 1 position and add to even
	output_odd = _mm256_slli_si256(output_odd,1);
	output_even = _mm256_add_epi8(output_even,output_odd);

	output_row1=output_even;

	 //row==N-1
	 //row==N-1
	//1st col iteration

	//multiply with the mask
	m1=_mm256_maddubs_epi16(r1,c0);
	m2=_mm256_maddubs_epi16(r2,c1);

	//vertical add
	m0=_mm256_add_epi16(m1,m2);

	//hozizontal additions
	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);

	//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others. later on they will be stored into 0,4,8,12,16,20,24,28 positions (8bit register).
	output_even=_mm256_and_si256(m2,output_mask);


	//2nd col iteration
	//multiply with the mask
	m1=_mm256_maddubs_epi16(r1,c0_sh1);
	m2=_mm256_maddubs_epi16(r2,c1_sh1);

	//vertical add
	m0=_mm256_add_epi16(m1,m2);

	//hozizontal additions
	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);


	//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others.
	output_odd = _mm256_and_si256(m2,output_mask);


  //3rd col iteration
	//multiply with the mask
	m1=_mm256_maddubs_epi16(r1,c0_sh2);
	m2=_mm256_maddubs_epi16(r2,c1_sh2);


	//vertical add
	m0=_mm256_add_epi16(m1,m2);


	//hozizontal additions
	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);

	m4=_mm256_and_si256(m0,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
	m2=_mm256_add_epi16(m2,m4);

	//m2 has 16 16bit values now. the results I need are in positions 1,3,5,7,9,11,13. keep only those, discard others
	m2 = _mm256_and_si256(m2,output_mask_sh1);
	output_even = _mm256_add_epi16(output_even,m2);


	//4th col iteration

	//multiply with the mask
	m1=_mm256_maddubs_epi16(r1,c0_sh3);
	m2=_mm256_maddubs_epi16(r2,c1_sh3);

	//vertical add
	m0=_mm256_add_epi16(m1,m2);

	//hozizontal additions
	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);

	m4=_mm256_and_si256(m0,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
	m2=_mm256_add_epi16(m2,m4);

	//m2 has 16 16bit values now. the results I need are in positions 1,3,5,7,9,11,13. keep only those, discard others
	m2 = _mm256_and_si256(m2,output_mask_sh1);
	output_odd = _mm256_add_epi16(output_odd,m2);


	//now division follows
	output_even=division(division_case,output_even,f);
	output_odd=division(division_case,output_odd,f);

	//shift odd 1 position and add to even
	output_odd = _mm256_slli_si256(output_odd,1);
	output_even = _mm256_add_epi8(output_even,output_odd);


	output_row0=output_even;

 	switch (REMINDER_ITERATIONS){
	case 1:
		filt[N-2][col] = (unsigned char) _mm256_extract_epi8(output_row1,0);
		filt[N-1][col] = (unsigned char) _mm256_extract_epi8(output_row0,0);

	  break;
	case 2:
		for (i=0;i<=1;i++){
			filt[N-2][col+i] = (unsigned char) _mm256_extract_epi8(output_row1,i);
			filt[N-1][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 3:
		for (i=0;i<=2;i++){
			filt[N-2][col+i] = (unsigned char) _mm256_extract_epi8(output_row1,i);
			filt[N-1][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 4:
		for (i=0;i<=3;i++){
			filt[N-2][col+i] = (unsigned char) _mm256_extract_epi8(output_row1,i);
			filt[N-1][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 5:
		for (i=0;i<=4;i++){
			filt[N-2][col+i] = (unsigned char) _mm256_extract_epi8(output_row1,i);
			filt[N-1][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 6:
		for (i=0;i<=5;i++){
			filt[N-2][col+i] = (unsigned char) _mm256_extract_epi8(output_row1,i);
			filt[N-1][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 7:
		for (i=0;i<=6;i++){
			filt[N-2][col+i] = (unsigned char) _mm256_extract_epi8(output_row1,i);
			filt[N-1][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 8:
		for (i=0;i<=7;i++){
			filt[N-2][col+i] = (unsigned char) _mm256_extract_epi8(output_row1,i);
			filt[N-1][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 9:
		for (i=0;i<=8;i++){
			filt[N-2][col+i] = (unsigned char) _mm256_extract_epi8(output_row1,i);
			filt[N-1][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 10:
		for (i=0;i<=9;i++){
			filt[N-2][col+i] = (unsigned char) _mm256_extract_epi8(output_row1,i);
			filt[N-1][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 11:
		for (i=0;i<=10;i++){
			filt[N-2][col+i] = (unsigned char) _mm256_extract_epi8(output_row1,i);
			filt[N-1][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 12:
		for (i=0;i<=11;i++){
			filt[N-2][col+i] = (unsigned char) _mm256_extract_epi8(output_row1,i);
			filt[N-1][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 13:
		for (i=0;i<=12;i++){
			filt[N-2][col+i] = (unsigned char) _mm256_extract_epi8(output_row1,i);
			filt[N-1][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 14:
		for (i=0;i<=13;i++){
			filt[N-2][col+i] = (unsigned char) _mm256_extract_epi8(output_row1,i);
			filt[N-1][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 15:
		for (i=0;i<=14;i++){
			filt[N-2][col+i] = (unsigned char) _mm256_extract_epi8(output_row1,i);
			filt[N-1][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 16:
  	_mm_storeu_si128( (__m128i *) &filt[N-2][col],_mm256_extractf128_si256(output_row1, 0)); //store low 128bit - 16pixels
  	_mm_storeu_si128( (__m128i *) &filt[N-1][col],_mm256_extractf128_si256(output_row0, 0)); //store low 128bit - 16pixels

		  break;
	case 17:
  	_mm_storeu_si128( (__m128i *) &filt[N-2][col],_mm256_extractf128_si256(output_row1, 0)); //store low 128bit - 16pixels
  	_mm_storeu_si128( (__m128i *) &filt[N-1][col],_mm256_extractf128_si256(output_row0, 0)); //store low 128bit - 16pixels
	filt[N-2][col+16] = (unsigned char) _mm256_extract_epi8(output_row1,16);
	filt[N-1][col+16] = (unsigned char) _mm256_extract_epi8(output_row0,16);
		  break;
	case 18:
  	_mm_storeu_si128( (__m128i *) &filt[N-2][col],_mm256_extractf128_si256(output_row1, 0)); //store low 128bit - 16pixels
  	_mm_storeu_si128( (__m128i *) &filt[N-1][col],_mm256_extractf128_si256(output_row0, 0)); //store low 128bit - 16pixels

	for (i=16;i<=17;i++){
	 filt[N-2][col+i] = (unsigned char) _mm256_extract_epi8(output_row1,i);
	 filt[N-1][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
	}
		  break;
	case 19:
	  	_mm_storeu_si128( (__m128i *) &filt[N-2][col],_mm256_extractf128_si256(output_row1, 0)); //store low 128bit - 16pixels
	  	_mm_storeu_si128( (__m128i *) &filt[N-1][col],_mm256_extractf128_si256(output_row0, 0)); //store low 128bit - 16pixels

		for (i=16;i<=18;i++){
		 filt[N-2][col+i] = (unsigned char) _mm256_extract_epi8(output_row1,i);
		 filt[N-1][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 20:
	  	_mm_storeu_si128( (__m128i *) &filt[N-2][col],_mm256_extractf128_si256(output_row1, 0)); //store low 128bit - 16pixels
	  	_mm_storeu_si128( (__m128i *) &filt[N-1][col],_mm256_extractf128_si256(output_row0, 0)); //store low 128bit - 16pixels

		for (i=16;i<=19;i++){
		 filt[N-2][col+i] = (unsigned char) _mm256_extract_epi8(output_row1,i);
		 filt[N-1][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 21:
	  	_mm_storeu_si128( (__m128i *) &filt[N-2][col],_mm256_extractf128_si256(output_row1, 0)); //store low 128bit - 16pixels
	  	_mm_storeu_si128( (__m128i *) &filt[N-1][col],_mm256_extractf128_si256(output_row0, 0)); //store low 128bit - 16pixels

		for (i=16;i<=20;i++){
		 filt[N-2][col+i] = (unsigned char) _mm256_extract_epi8(output_row1,i);
		 filt[N-1][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 22:
	  	_mm_storeu_si128( (__m128i *) &filt[N-2][col],_mm256_extractf128_si256(output_row1, 0)); //store low 128bit - 16pixels
	  	_mm_storeu_si128( (__m128i *) &filt[N-1][col],_mm256_extractf128_si256(output_row0, 0)); //store low 128bit - 16pixels

		for (i=16;i<=21;i++){
		 filt[N-2][col+i] = (unsigned char) _mm256_extract_epi8(output_row1,i);
		 filt[N-1][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		break;
	case 23:
	  	_mm_storeu_si128( (__m128i *) &filt[N-2][col],_mm256_extractf128_si256(output_row1, 0)); //store low 128bit - 16pixels
	  	_mm_storeu_si128( (__m128i *) &filt[N-1][col],_mm256_extractf128_si256(output_row0, 0)); //store low 128bit - 16pixels

		for (i=16;i<=22;i++){
		 filt[N-2][col+i] = (unsigned char) _mm256_extract_epi8(output_row1,i);
		 filt[N-1][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 24:
	  	_mm_storeu_si128( (__m128i *) &filt[N-2][col],_mm256_extractf128_si256(output_row1, 0)); //store low 128bit - 16pixels
	  	_mm_storeu_si128( (__m128i *) &filt[N-1][col],_mm256_extractf128_si256(output_row0, 0)); //store low 128bit - 16pixels

		for (i=16;i<=23;i++){
		 filt[N-2][col+i] = (unsigned char) _mm256_extract_epi8(output_row1,i);
		 filt[N-1][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 25:
	  	_mm_storeu_si128( (__m128i *) &filt[N-2][col],_mm256_extractf128_si256(output_row1, 0)); //store low 128bit - 16pixels
	  	_mm_storeu_si128( (__m128i *) &filt[N-1][col],_mm256_extractf128_si256(output_row0, 0)); //store low 128bit - 16pixels

		for (i=16;i<=24;i++){
		 filt[N-2][col+i] = (unsigned char) _mm256_extract_epi8(output_row1,i);
		 filt[N-1][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 26:
	  	_mm_storeu_si128( (__m128i *) &filt[N-2][col],_mm256_extractf128_si256(output_row1, 0)); //store low 128bit - 16pixels
	  	_mm_storeu_si128( (__m128i *) &filt[N-1][col],_mm256_extractf128_si256(output_row0, 0)); //store low 128bit - 16pixels

		for (i=16;i<=25;i++){
		 filt[N-2][col+i] = (unsigned char) _mm256_extract_epi8(output_row1,i);
		 filt[N-1][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}

		  break;
	case 27:
	  	_mm_storeu_si128( (__m128i *) &filt[N-2][col],_mm256_extractf128_si256(output_row1, 0)); //store low 128bit - 16pixels
	  	_mm_storeu_si128( (__m128i *) &filt[N-1][col],_mm256_extractf128_si256(output_row0, 0)); //store low 128bit - 16pixels

		for (i=16;i<=26;i++){
		 filt[N-2][col+i] = (unsigned char) _mm256_extract_epi8(output_row1,i);
		 filt[N-1][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}

		  break;
	case 28:
	  	_mm_storeu_si128( (__m128i *) &filt[N-2][col],_mm256_extractf128_si256(output_row1, 0)); //store low 128bit - 16pixels
	  	_mm_storeu_si128( (__m128i *) &filt[N-1][col],_mm256_extractf128_si256(output_row0, 0)); //store low 128bit - 16pixels

		for (i=16;i<=27;i++){
		 filt[N-2][col+i] = (unsigned char) _mm256_extract_epi8(output_row1,i);
		 filt[N-1][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 29:
	  	_mm_storeu_si128( (__m128i *) &filt[N-2][col],_mm256_extractf128_si256(output_row1, 0)); //store low 128bit - 16pixels
	  	_mm_storeu_si128( (__m128i *) &filt[N-1][col],_mm256_extractf128_si256(output_row0, 0)); //store low 128bit - 16pixels

		filt[N-2][col+16] = (unsigned char) _mm256_extract_epi8(output_row1,16);
		filt[N-2][col+17] = (unsigned char) _mm256_extract_epi8(output_row1,17);
		filt[N-2][col+18] = (unsigned char) _mm256_extract_epi8(output_row1,18);
		filt[N-2][col+19] = (unsigned char) _mm256_extract_epi8(output_row1,19);
		filt[N-2][col+20] = (unsigned char) _mm256_extract_epi8(output_row1,20);
		filt[N-2][col+21] = (unsigned char) _mm256_extract_epi8(output_row1,21);
		filt[N-2][col+22] = (unsigned char) _mm256_extract_epi8(output_row1,22);
		filt[N-2][col+23] = (unsigned char) _mm256_extract_epi8(output_row1,23);
		filt[N-2][col+24] = (unsigned char) _mm256_extract_epi8(output_row1,24);
		filt[N-2][col+25] = (unsigned char) _mm256_extract_epi8(output_row1,25);
		filt[N-2][col+26] = (unsigned char) _mm256_extract_epi8(output_row1,26);
		filt[N-2][col+27] = (unsigned char) _mm256_extract_epi8(output_row1,27);
		filt[N-2][col+28] = (unsigned char) _mm256_extract_epi8(output_row1,28);

		filt[N-1][col+16] = (unsigned char) _mm256_extract_epi8(output_row0,16);
		filt[N-1][col+17] = (unsigned char) _mm256_extract_epi8(output_row0,17);
		filt[N-1][col+18] = (unsigned char) _mm256_extract_epi8(output_row0,18);
		filt[N-1][col+19] = (unsigned char) _mm256_extract_epi8(output_row0,19);
		filt[N-1][col+20] = (unsigned char) _mm256_extract_epi8(output_row0,20);
		filt[N-1][col+21] = (unsigned char) _mm256_extract_epi8(output_row0,21);
		filt[N-1][col+22] = (unsigned char) _mm256_extract_epi8(output_row0,22);
		filt[N-1][col+23] = (unsigned char) _mm256_extract_epi8(output_row0,23);
		filt[N-1][col+24] = (unsigned char) _mm256_extract_epi8(output_row0,24);
		filt[N-1][col+25] = (unsigned char) _mm256_extract_epi8(output_row0,25);
		filt[N-1][col+26] = (unsigned char) _mm256_extract_epi8(output_row0,26);
		filt[N-1][col+27] = (unsigned char) _mm256_extract_epi8(output_row0,27);
		filt[N-1][col+28] = (unsigned char) _mm256_extract_epi8(output_row0,28);

		  break;
	case 30:
	  	_mm_storeu_si128( (__m128i *) &filt[N-2][col],_mm256_extractf128_si256(output_row1, 0)); //store low 128bit - 16pixels
	  	_mm_storeu_si128( (__m128i *) &filt[N-1][col],_mm256_extractf128_si256(output_row0, 0)); //store low 128bit - 16pixels

		filt[N-2][col+16] = (unsigned char) _mm256_extract_epi8(output_row1,16);
		filt[N-2][col+17] = (unsigned char) _mm256_extract_epi8(output_row1,17);
		filt[N-2][col+18] = (unsigned char) _mm256_extract_epi8(output_row1,18);
		filt[N-2][col+19] = (unsigned char) _mm256_extract_epi8(output_row1,19);
		filt[N-2][col+20] = (unsigned char) _mm256_extract_epi8(output_row1,20);
		filt[N-2][col+21] = (unsigned char) _mm256_extract_epi8(output_row1,21);
		filt[N-2][col+22] = (unsigned char) _mm256_extract_epi8(output_row1,22);
		filt[N-2][col+23] = (unsigned char) _mm256_extract_epi8(output_row1,23);
		filt[N-2][col+24] = (unsigned char) _mm256_extract_epi8(output_row1,24);
		filt[N-2][col+25] = (unsigned char) _mm256_extract_epi8(output_row1,25);
		filt[N-2][col+26] = (unsigned char) _mm256_extract_epi8(output_row1,26);
		filt[N-2][col+27] = (unsigned char) _mm256_extract_epi8(output_row1,27);
		filt[N-2][col+28] = (unsigned char) _mm256_extract_epi8(output_row1,28);

		filt[N-1][col+16] = (unsigned char) _mm256_extract_epi8(output_row0,16);
		filt[N-1][col+17] = (unsigned char) _mm256_extract_epi8(output_row0,17);
		filt[N-1][col+18] = (unsigned char) _mm256_extract_epi8(output_row0,18);
		filt[N-1][col+19] = (unsigned char) _mm256_extract_epi8(output_row0,19);
		filt[N-1][col+20] = (unsigned char) _mm256_extract_epi8(output_row0,20);
		filt[N-1][col+21] = (unsigned char) _mm256_extract_epi8(output_row0,21);
		filt[N-1][col+22] = (unsigned char) _mm256_extract_epi8(output_row0,22);
		filt[N-1][col+23] = (unsigned char) _mm256_extract_epi8(output_row0,23);
		filt[N-1][col+24] = (unsigned char) _mm256_extract_epi8(output_row0,24);
		filt[N-1][col+25] = (unsigned char) _mm256_extract_epi8(output_row0,25);
		filt[N-1][col+26] = (unsigned char) _mm256_extract_epi8(output_row0,26);
		filt[N-1][col+27] = (unsigned char) _mm256_extract_epi8(output_row0,27);
		filt[N-1][col+28] = (unsigned char) _mm256_extract_epi8(output_row0,28);

	//the filt[row][col+29] is computed unvectorized
	newPixel = 0;
	newPixel += frame1[N-2-1][M-1-1] * filter[0][0];
	newPixel += frame1[N-2-1][M-1] * filter[0][1];

	newPixel += frame1[N-2][M-1-1] * filter[1][0];
	newPixel += frame1[N-2][M-1] * filter[1][1];

	newPixel += frame1[N-2+1][M-1-1] * filter[2][0];
	newPixel += frame1[N-2+1][M-1] * filter[2][1];

	filt[N-2][M-1] = (unsigned char) (newPixel / divisor);

	newPixel = 0;
	newPixel += frame1[N-1-1][M-1-1] * filter[0][0];
	newPixel += frame1[N-1-1][M-1] * filter[0][1];

	newPixel += frame1[N-1][M-1-1] * filter[1][0];
	newPixel += frame1[N-1][M-1] * filter[1][1];

	filt[N-1][M-1] = (unsigned char) (newPixel / divisor);

		  break;

	case 31:
	  	_mm_storeu_si128( (__m128i *) &filt[N-2][col],_mm256_extractf128_si256(output_row1, 0)); //store low 128bit - 16pixels
	  	_mm_storeu_si128( (__m128i *) &filt[N-1][col],_mm256_extractf128_si256(output_row0, 0)); //store low 128bit - 16pixels

		filt[N-2][col+16] = (unsigned char) _mm256_extract_epi8(output_row1,16);
		filt[N-2][col+17] = (unsigned char) _mm256_extract_epi8(output_row1,17);
		filt[N-2][col+18] = (unsigned char) _mm256_extract_epi8(output_row1,18);
		filt[N-2][col+19] = (unsigned char) _mm256_extract_epi8(output_row1,19);
		filt[N-2][col+20] = (unsigned char) _mm256_extract_epi8(output_row1,20);
		filt[N-2][col+21] = (unsigned char) _mm256_extract_epi8(output_row1,21);
		filt[N-2][col+22] = (unsigned char) _mm256_extract_epi8(output_row1,22);
		filt[N-2][col+23] = (unsigned char) _mm256_extract_epi8(output_row1,23);
		filt[N-2][col+24] = (unsigned char) _mm256_extract_epi8(output_row1,24);
		filt[N-2][col+25] = (unsigned char) _mm256_extract_epi8(output_row1,25);
		filt[N-2][col+26] = (unsigned char) _mm256_extract_epi8(output_row1,26);
		filt[N-2][col+27] = (unsigned char) _mm256_extract_epi8(output_row1,27);
		filt[N-2][col+28] = (unsigned char) _mm256_extract_epi8(output_row1,28);

		filt[N-1][col+16] = (unsigned char) _mm256_extract_epi8(output_row0,16);
		filt[N-1][col+17] = (unsigned char) _mm256_extract_epi8(output_row0,17);
		filt[N-1][col+18] = (unsigned char) _mm256_extract_epi8(output_row0,18);
		filt[N-1][col+19] = (unsigned char) _mm256_extract_epi8(output_row0,19);
		filt[N-1][col+20] = (unsigned char) _mm256_extract_epi8(output_row0,20);
		filt[N-1][col+21] = (unsigned char) _mm256_extract_epi8(output_row0,21);
		filt[N-1][col+22] = (unsigned char) _mm256_extract_epi8(output_row0,22);
		filt[N-1][col+23] = (unsigned char) _mm256_extract_epi8(output_row0,23);
		filt[N-1][col+24] = (unsigned char) _mm256_extract_epi8(output_row0,24);
		filt[N-1][col+25] = (unsigned char) _mm256_extract_epi8(output_row0,25);
		filt[N-1][col+26] = (unsigned char) _mm256_extract_epi8(output_row0,26);
		filt[N-1][col+27] = (unsigned char) _mm256_extract_epi8(output_row0,27);
		filt[N-1][col+28] = (unsigned char) _mm256_extract_epi8(output_row0,28);

	//the filt[N-2][col+29/30] are computed unvectorized
		newPixel = 0;
		newPixel += frame1[N-2-1][M-2-1] * filter[0][0];
		newPixel += frame1[N-2-1][M-2] * filter[0][1];
		newPixel += frame1[N-2-1][M-2+1] * filter[0][2];

		newPixel += frame1[N-2][M-2-1] * filter[1][0];
		newPixel += frame1[N-2][M-2] * filter[1][1];
		newPixel += frame1[N-2][M-2+1] * filter[1][2];

		newPixel += frame1[N-2+1][M-2-1] * filter[2][0];
		newPixel += frame1[N-2+1][M-2] * filter[2][1];
		newPixel += frame1[N-2+1][M-2+1] * filter[2][2];

		filt[N-2][M-2] = (unsigned char) (newPixel / divisor);

		newPixel = 0;
		newPixel += frame1[N-2-1][M-1-1] * filter[0][0];
		newPixel += frame1[N-2-1][M-1] * filter[0][1];

		newPixel += frame1[N-2][M-1-1] * filter[1][0];
		newPixel += frame1[N-2][M-1] * filter[1][1];

		newPixel += frame1[N-2+1][M-1-1] * filter[2][0];
		newPixel += frame1[N-2+1][M-1] * filter[2][1];

		filt[N-2][M-1] = (unsigned char) (newPixel / divisor);

		newPixel = 0;

		newPixel += frame1[N-1-1][M-2-1] * filter[0][0];
		newPixel += frame1[N-1-1][M-2] * filter[0][1];
		newPixel += frame1[N-1-1][M-2+1] * filter[0][2];

		newPixel += frame1[N-1][M-2-1] * filter[1][0];
		newPixel += frame1[N-1][M-2] * filter[1][1];
		newPixel += frame1[N-1][M-2+1] * filter[1][2];

		filt[N-1][M-2] = (unsigned char) (newPixel / divisor);

		newPixel = 0;

		newPixel += frame1[N-1-1][M-1-1] * filter[0][0];
		newPixel += frame1[N-1-1][M-1] * filter[0][1];

		newPixel += frame1[N-1][M-1-1] * filter[1][0];
		newPixel += frame1[N-1][M-1] * filter[1][1];

		filt[N-1][M-1] = (unsigned char) (newPixel / divisor);


		  break;

	default:
		printf("\nsomething went wrong");
		return -1;
 	}

return 0;

}








int loop_reminder_3x3_last_row_only(unsigned char **frame1,unsigned char **filt,const unsigned int M, const unsigned int N, const unsigned int col,const unsigned int REMINDER_ITERATIONS,const unsigned int division_case,const unsigned short int divisor,signed char **filter,const __m256i c0,const __m256i c1,const __m256i c0_sh1,const __m256i c1_sh1,const __m256i c0_sh2,const __m256i c1_sh2, const __m256i c0_sh3,const __m256i c1_sh3,const __m256i f){

register	__m256i r1,r2,m0,m1,m2,m4,output_even,output_odd,output_row0;
int newPixel;
unsigned int i;
const __m256i mask3  = _mm256_set_epi16(0,0,0,0,0,0,0,65535,0,0,0,0,0,0,0,0);

const __m256i output_mask      = _mm256_set_epi16(0,65535,0,65535,0,65535,0,65535,0,65535,0,65535,0,65535,0,65535);
const __m256i output_mask_sh1  = _mm256_set_epi16(0,0,65535,0,65535,0,65535,0,65535,0,65535,0,65535,0,65535,0);

 __m256i reminder_mask1;

if (REMINDER_ITERATIONS == 0){
	return 0; //no loop reminder is needed
}

	reminder_mask1=_mm256_load_si256( (__m256i *) &reminder_msk1_3x3[REMINDER_ITERATIONS-1][0]);
	//reminder_mask2=_mm256_load_si256( (__m256i *) &reminder_msk2[REMINDER_ITERATIONS-1][0]);


	 //1st col iteration
	r1=_mm256_loadu_si256( (__m256i *) &frame1[N-2][col-1]);
	r2=_mm256_loadu_si256( (__m256i *) &frame1[N-1][col-1]);


	/*
	 * The above load operations load outside of the array bounds; these elements are filled with zeros just after
	 * Furthermore, I add one extra zero in the end to compute col=N-1. The last value of col is N-1, not N
	 */

	//AND r0-r4 with reminder_mask
	r1=_mm256_and_si256(r1,reminder_mask1);
	r2=_mm256_and_si256(r2,reminder_mask1);



	 //row==N-1
	 //row==N-1
	//1st col iteration

	//multiply with the mask
	m1=_mm256_maddubs_epi16(r1,c0);
	m2=_mm256_maddubs_epi16(r2,c1);

	//vertical add
	m0=_mm256_add_epi16(m1,m2);

	//hozizontal additions
	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);

	//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others. later on they will be stored into 0,4,8,12,16,20,24,28 positions (8bit register).
	output_even=_mm256_and_si256(m2,output_mask);


	//2nd col iteration
	//multiply with the mask
	m1=_mm256_maddubs_epi16(r1,c0_sh1);
	m2=_mm256_maddubs_epi16(r2,c1_sh1);

	//vertical add
	m0=_mm256_add_epi16(m1,m2);

	//hozizontal additions
	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);


	//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others.
	output_odd = _mm256_and_si256(m2,output_mask);


  //3rd col iteration
	//multiply with the mask
	m1=_mm256_maddubs_epi16(r1,c0_sh2);
	m2=_mm256_maddubs_epi16(r2,c1_sh2);


	//vertical add
	m0=_mm256_add_epi16(m1,m2);


	//hozizontal additions
	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);

	m4=_mm256_and_si256(m0,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
	m2=_mm256_add_epi16(m2,m4);

	//m2 has 16 16bit values now. the results I need are in positions 1,3,5,7,9,11,13. keep only those, discard others
	m2 = _mm256_and_si256(m2,output_mask_sh1);
	output_even = _mm256_add_epi16(output_even,m2);


	//4th col iteration

	//multiply with the mask
	m1=_mm256_maddubs_epi16(r1,c0_sh3);
	m2=_mm256_maddubs_epi16(r2,c1_sh3);

	//vertical add
	m0=_mm256_add_epi16(m1,m2);

	//hozizontal additions
	m1=_mm256_srli_si256(m0,2);
	m2=_mm256_add_epi16(m1,m0);

	m4=_mm256_and_si256(m0,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
	m2=_mm256_add_epi16(m2,m4);

	//m2 has 16 16bit values now. the results I need are in positions 1,3,5,7,9,11,13. keep only those, discard others
	m2 = _mm256_and_si256(m2,output_mask_sh1);
	output_odd = _mm256_add_epi16(output_odd,m2);


	//now division follows
	output_even=division(division_case,output_even,f);
	output_odd=division(division_case,output_odd,f);

	//shift odd 1 position and add to even
	output_odd = _mm256_slli_si256(output_odd,1);
	output_even = _mm256_add_epi8(output_even,output_odd);


	output_row0=output_even;

 	switch (REMINDER_ITERATIONS){
	case 1:
		filt[N-1][col] = (unsigned char) _mm256_extract_epi8(output_row0,0);

	  break;
	case 2:
		for (i=0;i<=1;i++){
			filt[N-1][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 3:
		for (i=0;i<=2;i++){
			filt[N-1][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 4:
		for (i=0;i<=3;i++){
			filt[N-1][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 5:
		for (i=0;i<=4;i++){
			filt[N-1][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 6:
		for (i=0;i<=5;i++){
			filt[N-1][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 7:
		for (i=0;i<=6;i++){
			filt[N-1][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 8:
		for (i=0;i<=7;i++){
			filt[N-1][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 9:
		for (i=0;i<=8;i++){
			filt[N-1][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 10:
		for (i=0;i<=9;i++){
			filt[N-1][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 11:
		for (i=0;i<=10;i++){
			filt[N-1][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 12:
		for (i=0;i<=11;i++){
			filt[N-1][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 13:
		for (i=0;i<=12;i++){
			filt[N-1][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 14:
		for (i=0;i<=13;i++){
			filt[N-1][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 15:
		for (i=0;i<=14;i++){
			filt[N-1][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 16:
  	_mm_storeu_si128( (__m128i *) &filt[N-1][col],_mm256_extractf128_si256(output_row0, 0)); //store low 128bit - 16pixels

		  break;
	case 17:
  	_mm_storeu_si128( (__m128i *) &filt[N-1][col],_mm256_extractf128_si256(output_row0, 0)); //store low 128bit - 16pixels
	filt[N-1][col+16] = (unsigned char) _mm256_extract_epi8(output_row0,16);
		  break;
	case 18:
  	_mm_storeu_si128( (__m128i *) &filt[N-1][col],_mm256_extractf128_si256(output_row0, 0)); //store low 128bit - 16pixels

	for (i=16;i<=17;i++){
	 filt[N-1][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
	}
		  break;
	case 19:
	  	_mm_storeu_si128( (__m128i *) &filt[N-1][col],_mm256_extractf128_si256(output_row0, 0)); //store low 128bit - 16pixels

		for (i=16;i<=18;i++){
		 filt[N-1][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 20:
	  	_mm_storeu_si128( (__m128i *) &filt[N-1][col],_mm256_extractf128_si256(output_row0, 0)); //store low 128bit - 16pixels

		for (i=16;i<=19;i++){
		 filt[N-1][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 21:
	  	_mm_storeu_si128( (__m128i *) &filt[N-1][col],_mm256_extractf128_si256(output_row0, 0)); //store low 128bit - 16pixels

		for (i=16;i<=20;i++){
		 filt[N-1][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 22:
	  	_mm_storeu_si128( (__m128i *) &filt[N-1][col],_mm256_extractf128_si256(output_row0, 0)); //store low 128bit - 16pixels

		for (i=16;i<=21;i++){
		 filt[N-1][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		break;
	case 23:
	  	_mm_storeu_si128( (__m128i *) &filt[N-1][col],_mm256_extractf128_si256(output_row0, 0)); //store low 128bit - 16pixels

		for (i=16;i<=22;i++){
		 filt[N-1][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 24:
	  	_mm_storeu_si128( (__m128i *) &filt[N-1][col],_mm256_extractf128_si256(output_row0, 0)); //store low 128bit - 16pixels

		for (i=16;i<=23;i++){
		 filt[N-1][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 25:
	  	_mm_storeu_si128( (__m128i *) &filt[N-1][col],_mm256_extractf128_si256(output_row0, 0)); //store low 128bit - 16pixels

		for (i=16;i<=24;i++){
		 filt[N-1][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 26:
	  	_mm_storeu_si128( (__m128i *) &filt[N-1][col],_mm256_extractf128_si256(output_row0, 0)); //store low 128bit - 16pixels

		for (i=16;i<=25;i++){
		 filt[N-1][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}

		  break;
	case 27:
	  	_mm_storeu_si128( (__m128i *) &filt[N-1][col],_mm256_extractf128_si256(output_row0, 0)); //store low 128bit - 16pixels

		for (i=16;i<=26;i++){
		 filt[N-1][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}

		  break;
	case 28:
	  	_mm_storeu_si128( (__m128i *) &filt[N-1][col],_mm256_extractf128_si256(output_row0, 0)); //store low 128bit - 16pixels

		for (i=16;i<=27;i++){
		 filt[N-1][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 29:
	  	_mm_storeu_si128( (__m128i *) &filt[N-1][col],_mm256_extractf128_si256(output_row0, 0)); //store low 128bit - 16pixels

		filt[N-1][col+16] = (unsigned char) _mm256_extract_epi8(output_row0,16);
		filt[N-1][col+17] = (unsigned char) _mm256_extract_epi8(output_row0,17);
		filt[N-1][col+18] = (unsigned char) _mm256_extract_epi8(output_row0,18);
		filt[N-1][col+19] = (unsigned char) _mm256_extract_epi8(output_row0,19);
		filt[N-1][col+20] = (unsigned char) _mm256_extract_epi8(output_row0,20);
		filt[N-1][col+21] = (unsigned char) _mm256_extract_epi8(output_row0,21);
		filt[N-1][col+22] = (unsigned char) _mm256_extract_epi8(output_row0,22);
		filt[N-1][col+23] = (unsigned char) _mm256_extract_epi8(output_row0,23);
		filt[N-1][col+24] = (unsigned char) _mm256_extract_epi8(output_row0,24);
		filt[N-1][col+25] = (unsigned char) _mm256_extract_epi8(output_row0,25);
		filt[N-1][col+26] = (unsigned char) _mm256_extract_epi8(output_row0,26);
		filt[N-1][col+27] = (unsigned char) _mm256_extract_epi8(output_row0,27);
		filt[N-1][col+28] = (unsigned char) _mm256_extract_epi8(output_row0,28);

		  break;
	case 30:
	  	_mm_storeu_si128( (__m128i *) &filt[N-1][col],_mm256_extractf128_si256(output_row0, 0)); //store low 128bit - 16pixels

		filt[N-1][col+16] = (unsigned char) _mm256_extract_epi8(output_row0,16);
		filt[N-1][col+17] = (unsigned char) _mm256_extract_epi8(output_row0,17);
		filt[N-1][col+18] = (unsigned char) _mm256_extract_epi8(output_row0,18);
		filt[N-1][col+19] = (unsigned char) _mm256_extract_epi8(output_row0,19);
		filt[N-1][col+20] = (unsigned char) _mm256_extract_epi8(output_row0,20);
		filt[N-1][col+21] = (unsigned char) _mm256_extract_epi8(output_row0,21);
		filt[N-1][col+22] = (unsigned char) _mm256_extract_epi8(output_row0,22);
		filt[N-1][col+23] = (unsigned char) _mm256_extract_epi8(output_row0,23);
		filt[N-1][col+24] = (unsigned char) _mm256_extract_epi8(output_row0,24);
		filt[N-1][col+25] = (unsigned char) _mm256_extract_epi8(output_row0,25);
		filt[N-1][col+26] = (unsigned char) _mm256_extract_epi8(output_row0,26);
		filt[N-1][col+27] = (unsigned char) _mm256_extract_epi8(output_row0,27);
		filt[N-1][col+28] = (unsigned char) _mm256_extract_epi8(output_row0,28);

	//the filt[row][col+29] is computed unvectorized

	newPixel = 0;
	newPixel += frame1[N-1-1][M-1-1] * filter[0][0];
	newPixel += frame1[N-1-1][M-1] * filter[0][1];

	newPixel += frame1[N-1][M-1-1] * filter[1][0];
	newPixel += frame1[N-1][M-1] * filter[1][1];

	filt[N-1][M-1] = (unsigned char) (newPixel / divisor);

		  break;

	case 31:
	  	_mm_storeu_si128( (__m128i *) &filt[N-1][col],_mm256_extractf128_si256(output_row0, 0)); //store low 128bit - 16pixels


		filt[N-1][col+16] = (unsigned char) _mm256_extract_epi8(output_row0,16);
		filt[N-1][col+17] = (unsigned char) _mm256_extract_epi8(output_row0,17);
		filt[N-1][col+18] = (unsigned char) _mm256_extract_epi8(output_row0,18);
		filt[N-1][col+19] = (unsigned char) _mm256_extract_epi8(output_row0,19);
		filt[N-1][col+20] = (unsigned char) _mm256_extract_epi8(output_row0,20);
		filt[N-1][col+21] = (unsigned char) _mm256_extract_epi8(output_row0,21);
		filt[N-1][col+22] = (unsigned char) _mm256_extract_epi8(output_row0,22);
		filt[N-1][col+23] = (unsigned char) _mm256_extract_epi8(output_row0,23);
		filt[N-1][col+24] = (unsigned char) _mm256_extract_epi8(output_row0,24);
		filt[N-1][col+25] = (unsigned char) _mm256_extract_epi8(output_row0,25);
		filt[N-1][col+26] = (unsigned char) _mm256_extract_epi8(output_row0,26);
		filt[N-1][col+27] = (unsigned char) _mm256_extract_epi8(output_row0,27);
		filt[N-1][col+28] = (unsigned char) _mm256_extract_epi8(output_row0,28);

	//the filt[N-2][col+29/30] are computed unvectorized

		newPixel = 0;

		newPixel += frame1[N-1-1][M-2-1] * filter[0][0];
		newPixel += frame1[N-1-1][M-2] * filter[0][1];
		newPixel += frame1[N-1-1][M-2+1] * filter[0][2];

		newPixel += frame1[N-1][M-2-1] * filter[1][0];
		newPixel += frame1[N-1][M-2] * filter[1][1];
		newPixel += frame1[N-1][M-2+1] * filter[1][2];

		filt[N-1][M-2] = (unsigned char) (newPixel / divisor);

		newPixel = 0;

		newPixel += frame1[N-1-1][M-1-1] * filter[0][0];
		newPixel += frame1[N-1-1][M-1] * filter[0][1];

		newPixel += frame1[N-1][M-1-1] * filter[1][0];
		newPixel += frame1[N-1][M-1] * filter[1][1];

		filt[N-1][M-1] = (unsigned char) (newPixel / divisor);


		  break;

	default:
		printf("\nsomething went wrong");
		return -1;
 	}

return 0;

}



void Gaussian_Blur_optimized_3x3_16_reg_blocking(unsigned char **frame1,unsigned char **filt,const unsigned int M, const unsigned int N, const unsigned short int divisor, signed char **filter){

const signed char f00=filter[0][0];
const signed char f01=filter[0][1];
const signed char f02=filter[0][2];

const signed char f10=filter[1][0];
const signed char f11=filter[1][1];
const signed char f12=filter[1][2];


	const __m256i c0=_mm256_set_epi8(0,f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00);
	const __m256i c1=_mm256_set_epi8(0,f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10);

	const __m256i c0_sh1=_mm256_set_epi8(f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00,0);
	const __m256i c1_sh1=_mm256_set_epi8(f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10,0);


	const __m256i c0_sh2=_mm256_set_epi8(0,0,0,f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00,0,0);
	const __m256i c1_sh2=_mm256_set_epi8(0,0,0,f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10,0,0);

	const __m256i c0_sh3=_mm256_set_epi8(0,0,f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00,0,0,0);
	const __m256i c1_sh3=_mm256_set_epi8(0,0,f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10,0,0,0);


	const __m256i mask3  = _mm256_set_epi16(0,0,0,0,0,0,0,65535,0,0,0,0,0,0,0,0);
	const __m256i mask_prelude  = _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0);

	const __m256i output_mask      = _mm256_set_epi16(0,65535,0,65535,0,65535,0,65535,0,65535,0,65535,0,65535,0,65535);
	const __m256i output_mask_sh1  = _mm256_set_epi16(0,0,65535,0,65535,0,65535,0,65535,0,65535,0,65535,0,65535,0);




	const unsigned int REMINDER_ITERATIONS =  (M-((((M-32)/30)*30)+30)); //M-(last_col_value+30)
	//printf("\n%d",REMINDER_ITERATIONS);


	const unsigned int division_case=prepare_for_division(divisor); //determine which is the division case (A, B or C)
	const __m256i f = _mm256_load_si256( (__m256i *) &f_vector[0]);// initialize the division vector

//printf("\n%d %d",division_case,b);


#pragma omp parallel
{

int row,col;
register __m256i r0,r1,r2,r3,r4,m0,m1,m2,m3,m4;
//__m256i output_row0_even,output_row1_even,output_row0_odd,output_row1_odd; //these are for processing row #0 and #1 only.
__m256i output_even,output_odd;
//__m256i m0_prelude_row0,m0_prelude_row1;


/*---------------------- Gaussian Blur ---------------------------------*/

    #pragma omp for schedule(static)
	for (row = -1; row < N; row+=3) {

	if (row==-1){//special case compute filt[0:1][:]

		for (col = 0; col <= M-32; col+=30){

		  if (col==0){

				//load the 5 rows
				r0=_mm256_load_si256( (__m256i *) &frame1[0][0]);
				r1=_mm256_load_si256( (__m256i *) &frame1[1][0]);
				r2=_mm256_load_si256( (__m256i *) &frame1[2][0]);


		  			//START - extra code needed for prelude
		  					m0=_mm256_slli_si256(r0,1);//shift 1 elements left - equivalent to filling with one zero inthe beginning
		  					m1=_mm256_slli_si256(r1,1);//shift 1 elements left - equivalent to filling with one zero inthe beginning
		  					m2=_mm256_slli_si256(r2,1);//shift 1 elements left - equivalent to filling with one zero inthe beginning

		  					//preserve the element being lost because of the shift above
		  					r0=_mm256_and_si256(r0,mask_prelude);
		  					r0=_mm256_permute2f128_si256(r0,r0,1);
		  					r0=_mm256_srli_si256(r0,15);//shift 15 elements
		  					r0=_mm256_add_epi16(m0,r0);

		  					r1=_mm256_and_si256(r1,mask_prelude);
		  					r1=_mm256_permute2f128_si256(r1,r1,1);
		  					r1=_mm256_srli_si256(r1,15);//shift 15 elements
		  					r1=_mm256_add_epi16(m1,r1);

		  					r2=_mm256_and_si256(r2,mask_prelude);
		  					r2=_mm256_permute2f128_si256(r2,r2,1);
		  					r2=_mm256_srli_si256(r2,15);//shift 15 elements
		  					r2=_mm256_add_epi16(m2,r2);

		  			//END - extra code needed for prelude
		  		}
		  else {
				//load the 5 rows
				r0=_mm256_loadu_si256( (__m256i *) &frame1[0][col-1]);
				r1=_mm256_loadu_si256( (__m256i *) &frame1[1][col-1]);
				r2=_mm256_loadu_si256( (__m256i *) &frame1[2][col-1]);
		  }

		//col iteration computes output pixels of    1,5,9,13,17,21,25,29
		// col+1 iteration computes output pixels of 2,6,10,14,18,22,26,30
		// col+2 iteration computes output pixels of 3,7,11,15,19,23,27
		// col+3 iteration computes output pixels of 4,8,12,16,20,24,28
		//afterwards, col becomes 30 and repeat the above process

		 //row==1
		//1st col iteration

		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,c0);
		m1=_mm256_maddubs_epi16(r1,c1);
		m2=_mm256_maddubs_epi16(r2,c0);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m0=_mm256_add_epi16(m0,m2);

		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others. later on they will be stored into 0,4,8,12,16,20,24,28 positions (8bit register).
		output_even=_mm256_and_si256(m2,output_mask);


		//2nd col iteration
		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,c0_sh1);
		m1=_mm256_maddubs_epi16(r1,c1_sh1);
		m2=_mm256_maddubs_epi16(r2,c0_sh1);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m0=_mm256_add_epi16(m0,m2);

		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);


		//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others.
		output_odd = _mm256_and_si256(m2,output_mask);


       //3rd col iteration
		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,c0_sh2);
		m1=_mm256_maddubs_epi16(r1,c1_sh2);
		m2=_mm256_maddubs_epi16(r2,c0_sh2);


		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m0=_mm256_add_epi16(m0,m2);


		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		m4=_mm256_and_si256(m0,mask3);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
		m2=_mm256_add_epi16(m2,m4);

		//m2 has 16 16bit values now. the results I need are in positions 1,3,5,7,9,11,13. keep only those, discard others
		m2 = _mm256_and_si256(m2,output_mask_sh1);
		output_even = _mm256_add_epi16(output_even,m2);


		//4th col iteration

		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,c0_sh3);
		m1=_mm256_maddubs_epi16(r1,c1_sh3);
		m2=_mm256_maddubs_epi16(r2,c0_sh3);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m0=_mm256_add_epi16(m0,m2);


		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		m4=_mm256_and_si256(m0,mask3);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
		m2=_mm256_add_epi16(m2,m4);

		//m2 has 16 16bit values now. the results I need are in positions 1,3,5,7,9,11,13. keep only those, discard others
		m2 = _mm256_and_si256(m2,output_mask_sh1);
		output_odd = _mm256_add_epi16(output_odd,m2);


		//now division follows
		output_even=division(division_case,output_even,f);
		output_odd=division(division_case,output_odd,f);

		//shift odd 1 position and add to even
		output_odd = _mm256_slli_si256(output_odd,1);
		output_even = _mm256_add_epi8(output_even,output_odd);


		_mm256_storeu_si256( (__m256i *) &filt[1][col],output_even );

		 //row==0
		//1st col iteration

		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,c1);
		m1=_mm256_maddubs_epi16(r1,c0);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);

		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others. later on they will be stored into 0,4,8,12,16,20,24,28 positions (8bit register).
		output_even=_mm256_and_si256(m2,output_mask);


		//2nd col iteration
		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,c1_sh1);
		m1=_mm256_maddubs_epi16(r1,c0_sh1);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);

		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);


		//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others.
		output_odd = _mm256_and_si256(m2,output_mask);


      //3rd col iteration
		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,c1_sh2);
		m1=_mm256_maddubs_epi16(r1,c0_sh2);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);


		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		m4=_mm256_and_si256(m0,mask3);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
		m2=_mm256_add_epi16(m2,m4);

		//m2 has 16 16bit values now. the results I need are in positions 1,3,5,7,9,11,13. keep only those, discard others
		m2 = _mm256_and_si256(m2,output_mask_sh1);
		output_even = _mm256_add_epi16(output_even,m2);


		//4th col iteration

		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,c1_sh3);
		m1=_mm256_maddubs_epi16(r1,c0_sh3);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);

		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		m4=_mm256_and_si256(m0,mask3);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
		m2=_mm256_add_epi16(m2,m4);

		//m2 has 16 16bit values now. the results I need are in positions 1,3,5,7,9,11,13. keep only those, discard others
		m2 = _mm256_and_si256(m2,output_mask_sh1);
		output_odd = _mm256_add_epi16(output_odd,m2);


		//now division follows
		output_even=division(division_case,output_even,f);
		output_odd=division(division_case,output_odd,f);

		//shift odd 1 position and add to even
		output_odd = _mm256_slli_si256(output_odd,1);
		output_even = _mm256_add_epi8(output_even,output_odd);


		_mm256_storeu_si256( (__m256i *) &filt[0][col],output_even );
		}
	loop_reminder_3x3_first_values(frame1,filt,M,N,col,REMINDER_ITERATIONS,division_case,divisor,filter,c0,c1,c0_sh1,c1_sh1,c0_sh2,c1_sh2,c0_sh3,c1_sh3,f);

	}

	else if (row==N-2){//special case compute filt[N-2:N-1][:]
		//printf("\nddd");
		for (col = 0; col <= M-32; col+=30){

		  if (col==0){

				//load the 5 rows
				r0=_mm256_load_si256( (__m256i *) &frame1[N-3][0]);
				r1=_mm256_load_si256( (__m256i *) &frame1[N-2][0]);
				r2=_mm256_load_si256( (__m256i *) &frame1[N-1][0]);


		  			//START - extra code needed for prelude
		  					m0=_mm256_slli_si256(r0,1);//shift 1 elements left - equivalent to filling with one zero inthe beginning
		  					m1=_mm256_slli_si256(r1,1);//shift 1 elements left - equivalent to filling with one zero inthe beginning
		  					m2=_mm256_slli_si256(r2,1);//shift 1 elements left - equivalent to filling with one zero inthe beginning

		  					//preserve the element being lost because of the shift above
		  					r0=_mm256_and_si256(r0,mask_prelude);
		  					r0=_mm256_permute2f128_si256(r0,r0,1);
		  					r0=_mm256_srli_si256(r0,15);//shift 15 elements
		  					r0=_mm256_add_epi16(m0,r0);

		  					r1=_mm256_and_si256(r1,mask_prelude);
		  					r1=_mm256_permute2f128_si256(r1,r1,1);
		  					r1=_mm256_srli_si256(r1,15);//shift 15 elements
		  					r1=_mm256_add_epi16(m1,r1);

		  					r2=_mm256_and_si256(r2,mask_prelude);
		  					r2=_mm256_permute2f128_si256(r2,r2,1);
		  					r2=_mm256_srli_si256(r2,15);//shift 15 elements
		  					r2=_mm256_add_epi16(m2,r2);

		  			//END - extra code needed for prelude
		  		}
		  else {
				//load the 5 rows
				r0=_mm256_loadu_si256( (__m256i *) &frame1[N-3][col-1]);
				r1=_mm256_loadu_si256( (__m256i *) &frame1[N-2][col-1]);
				r2=_mm256_loadu_si256( (__m256i *) &frame1[N-1][col-1]);
		  }

		//col iteration computes output pixels of    1,5,9,13,17,21,25,29
		// col+1 iteration computes output pixels of 2,6,10,14,18,22,26,30
		// col+2 iteration computes output pixels of 3,7,11,15,19,23,27
		// col+3 iteration computes output pixels of 4,8,12,16,20,24,28
		//afterwards, col becomes 30 and repeat the above process

		 //row==N-2
		//1st col iteration

		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,c0);
		m1=_mm256_maddubs_epi16(r1,c1);
		m2=_mm256_maddubs_epi16(r2,c0);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m0=_mm256_add_epi16(m0,m2);

		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others. later on they will be stored into 0,4,8,12,16,20,24,28 positions (8bit register).
		output_even=_mm256_and_si256(m2,output_mask);


		//2nd col iteration
		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,c0_sh1);
		m1=_mm256_maddubs_epi16(r1,c1_sh1);
		m2=_mm256_maddubs_epi16(r2,c0_sh1);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m0=_mm256_add_epi16(m0,m2);

		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);


		//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others.
		output_odd = _mm256_and_si256(m2,output_mask);


       //3rd col iteration
		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,c0_sh2);
		m1=_mm256_maddubs_epi16(r1,c1_sh2);
		m2=_mm256_maddubs_epi16(r2,c0_sh2);


		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m0=_mm256_add_epi16(m0,m2);


		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		m4=_mm256_and_si256(m0,mask3);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
		m2=_mm256_add_epi16(m2,m4);

		//m2 has 16 16bit values now. the results I need are in positions 1,3,5,7,9,11,13. keep only those, discard others
		m2 = _mm256_and_si256(m2,output_mask_sh1);
		output_even = _mm256_add_epi16(output_even,m2);


		//4th col iteration

		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,c0_sh3);
		m1=_mm256_maddubs_epi16(r1,c1_sh3);
		m2=_mm256_maddubs_epi16(r2,c0_sh3);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m0=_mm256_add_epi16(m0,m2);


		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		m4=_mm256_and_si256(m0,mask3);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
		m2=_mm256_add_epi16(m2,m4);

		//m2 has 16 16bit values now. the results I need are in positions 1,3,5,7,9,11,13. keep only those, discard others
		m2 = _mm256_and_si256(m2,output_mask_sh1);
		output_odd = _mm256_add_epi16(output_odd,m2);


		//now division follows
		output_even=division(division_case,output_even,f);
		output_odd=division(division_case,output_odd,f);

		//shift odd 1 position and add to even
		output_odd = _mm256_slli_si256(output_odd,1);
		output_even = _mm256_add_epi8(output_even,output_odd);


		_mm256_storeu_si256( (__m256i *) &filt[N-2][col],output_even );

		 //row==N-1
		//1st col iteration

		//multiply with the mask
		m1=_mm256_maddubs_epi16(r1,c0);
		m2=_mm256_maddubs_epi16(r2,c1);

		//vertical add
		m0=_mm256_add_epi16(m1,m2);

		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others. later on they will be stored into 0,4,8,12,16,20,24,28 positions (8bit register).
		output_even=_mm256_and_si256(m2,output_mask);


		//2nd col iteration
		//multiply with the mask
		m1=_mm256_maddubs_epi16(r1,c0_sh1);
		m2=_mm256_maddubs_epi16(r2,c1_sh1);

		//vertical add
		m0=_mm256_add_epi16(m1,m2);

		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);


		//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others.
		output_odd = _mm256_and_si256(m2,output_mask);


       //3rd col iteration
		//multiply with the mask
		m1=_mm256_maddubs_epi16(r1,c0_sh2);
		m2=_mm256_maddubs_epi16(r2,c1_sh2);


		//vertical add
		m0=_mm256_add_epi16(m1,m2);


		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		m4=_mm256_and_si256(m0,mask3);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
		m2=_mm256_add_epi16(m2,m4);

		//m2 has 16 16bit values now. the results I need are in positions 1,3,5,7,9,11,13. keep only those, discard others
		m2 = _mm256_and_si256(m2,output_mask_sh1);
		output_even = _mm256_add_epi16(output_even,m2);


		//4th col iteration

		//multiply with the mask
		m1=_mm256_maddubs_epi16(r1,c0_sh3);
		m2=_mm256_maddubs_epi16(r2,c1_sh3);

		//vertical add
		m0=_mm256_add_epi16(m1,m2);

		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		m4=_mm256_and_si256(m0,mask3);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
		m2=_mm256_add_epi16(m2,m4);

		//m2 has 16 16bit values now. the results I need are in positions 1,3,5,7,9,11,13. keep only those, discard others
		m2 = _mm256_and_si256(m2,output_mask_sh1);
		output_odd = _mm256_add_epi16(output_odd,m2);


		//now division follows
		output_even=division(division_case,output_even,f);
		output_odd=division(division_case,output_odd,f);

		//shift odd 1 position and add to even
		output_odd = _mm256_slli_si256(output_odd,1);
		output_even = _mm256_add_epi8(output_even,output_odd);


		_mm256_storeu_si256( (__m256i *) &filt[N-1][col],output_even );
		}
	loop_reminder_3x3_last_values(frame1,filt,M,N,col,REMINDER_ITERATIONS,division_case,divisor,filter,c0,c1,c0_sh1,c1_sh1,c0_sh2,c1_sh2,c0_sh3,c1_sh3,f);

	}

	else if (row==N-1){//special case compute filt[N-1][:]

		for (col = 0; col <= M-32; col+=30){

		  if (col==0){

				r1=_mm256_load_si256( (__m256i *) &frame1[N-2][0]);
				r2=_mm256_load_si256( (__m256i *) &frame1[N-1][0]);


		  			//START - extra code needed for prelude
		  					m1=_mm256_slli_si256(r1,1);//shift 1 elements left - equivalent to filling with one zero inthe beginning
		  					m2=_mm256_slli_si256(r2,1);//shift 1 elements left - equivalent to filling with one zero inthe beginning

		  					r1=_mm256_and_si256(r1,mask_prelude);
		  					r1=_mm256_permute2f128_si256(r1,r1,1);
		  					r1=_mm256_srli_si256(r1,15);//shift 15 elements
		  					r1=_mm256_add_epi16(m1,r1);

		  					r2=_mm256_and_si256(r2,mask_prelude);
		  					r2=_mm256_permute2f128_si256(r2,r2,1);
		  					r2=_mm256_srli_si256(r2,15);//shift 15 elements
		  					r2=_mm256_add_epi16(m2,r2);

		  			//END - extra code needed for prelude
		  		}
		  else {
				r1=_mm256_loadu_si256( (__m256i *) &frame1[N-2][col-1]);
				r2=_mm256_loadu_si256( (__m256i *) &frame1[N-1][col-1]);
		  }


		 //row==N-1
		//1st col iteration

		//multiply with the mask
		m1=_mm256_maddubs_epi16(r1,c0);
		m2=_mm256_maddubs_epi16(r2,c1);

		//vertical add
		m0=_mm256_add_epi16(m1,m2);

		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others. later on they will be stored into 0,4,8,12,16,20,24,28 positions (8bit register).
		output_even=_mm256_and_si256(m2,output_mask);


		//2nd col iteration
		//multiply with the mask
		m1=_mm256_maddubs_epi16(r1,c0_sh1);
		m2=_mm256_maddubs_epi16(r2,c1_sh1);

		//vertical add
		m0=_mm256_add_epi16(m1,m2);

		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);


		//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others.
		output_odd = _mm256_and_si256(m2,output_mask);


       //3rd col iteration
		//multiply with the mask
		m1=_mm256_maddubs_epi16(r1,c0_sh2);
		m2=_mm256_maddubs_epi16(r2,c1_sh2);


		//vertical add
		m0=_mm256_add_epi16(m1,m2);


		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		m4=_mm256_and_si256(m0,mask3);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
		m2=_mm256_add_epi16(m2,m4);

		//m2 has 16 16bit values now. the results I need are in positions 1,3,5,7,9,11,13. keep only those, discard others
		m2 = _mm256_and_si256(m2,output_mask_sh1);
		output_even = _mm256_add_epi16(output_even,m2);


		//4th col iteration

		//multiply with the mask
		m1=_mm256_maddubs_epi16(r1,c0_sh3);
		m2=_mm256_maddubs_epi16(r2,c1_sh3);

		//vertical add
		m0=_mm256_add_epi16(m1,m2);

		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		m4=_mm256_and_si256(m0,mask3);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
		m2=_mm256_add_epi16(m2,m4);

		//m2 has 16 16bit values now. the results I need are in positions 1,3,5,7,9,11,13. keep only those, discard others
		m2 = _mm256_and_si256(m2,output_mask_sh1);
		output_odd = _mm256_add_epi16(output_odd,m2);


		//now division follows
		output_even=division(division_case,output_even,f);
		output_odd=division(division_case,output_odd,f);

		//shift odd 1 position and add to even
		output_odd = _mm256_slli_si256(output_odd,1);
		output_even = _mm256_add_epi8(output_even,output_odd);


		_mm256_storeu_si256( (__m256i *) &filt[N-1][col],output_even );
		}
	loop_reminder_3x3_last_row_only(frame1,filt,M,N,col,REMINDER_ITERATIONS,division_case,divisor,filter,c0,c1,c0_sh1,c1_sh1,c0_sh2,c1_sh2,c0_sh3,c1_sh3,f);

	}

	else if (row==N-3){//special case compute filt[N-2:N-1][:] and filt[N-3][:]

		for (col = 0; col <= M-32; col+=30){

		  if (col==0){

				//load the 5 rows
				r0=_mm256_load_si256( (__m256i *) &frame1[N-4][0]);
				r1=_mm256_load_si256( (__m256i *) &frame1[N-3][0]);
				r2=_mm256_load_si256( (__m256i *) &frame1[N-2][0]);
				r3=_mm256_load_si256( (__m256i *) &frame1[N-1][0]);


		  			//START - extra code needed for prelude
		  					m0=_mm256_slli_si256(r0,1);//shift 1 elements left - equivalent to filling with one zero inthe beginning
		  					m1=_mm256_slli_si256(r1,1);//shift 1 elements left - equivalent to filling with one zero inthe beginning
		  					m2=_mm256_slli_si256(r2,1);//shift 1 elements left - equivalent to filling with one zero inthe beginning
		  					m3=_mm256_slli_si256(r3,1);//shift 1 elements left - equivalent to filling with one zero inthe beginning

		  					//preserve the element being lost because of the shift above
		  					r0=_mm256_and_si256(r0,mask_prelude);
		  					r0=_mm256_permute2f128_si256(r0,r0,1);
		  					r0=_mm256_srli_si256(r0,15);//shift 15 elements
		  					r0=_mm256_add_epi16(m0,r0);

		  					r1=_mm256_and_si256(r1,mask_prelude);
		  					r1=_mm256_permute2f128_si256(r1,r1,1);
		  					r1=_mm256_srli_si256(r1,15);//shift 15 elements
		  					r1=_mm256_add_epi16(m1,r1);

		  					r2=_mm256_and_si256(r2,mask_prelude);
		  					r2=_mm256_permute2f128_si256(r2,r2,1);
		  					r2=_mm256_srli_si256(r2,15);//shift 15 elements
		  					r2=_mm256_add_epi16(m2,r2);

		  					r3=_mm256_and_si256(r3,mask_prelude);
		  					r3=_mm256_permute2f128_si256(r3,r3,1);
		  					r3=_mm256_srli_si256(r3,15);//shift 15 elements
		  					r3=_mm256_add_epi16(m3,r3);

		  			//END - extra code needed for prelude
		  		}
		  else {
				r0=_mm256_loadu_si256( (__m256i *) &frame1[N-4][col-1]);
				r1=_mm256_loadu_si256( (__m256i *) &frame1[N-3][col-1]);
				r2=_mm256_loadu_si256( (__m256i *) &frame1[N-2][col-1]);
				r3=_mm256_loadu_si256( (__m256i *) &frame1[N-1][col-1]);
		  }

		//col iteration computes output pixels of    1,5,9,13,17,21,25,29
		// col+1 iteration computes output pixels of 2,6,10,14,18,22,26,30
		// col+2 iteration computes output pixels of 3,7,11,15,19,23,27
		// col+3 iteration computes output pixels of 4,8,12,16,20,24,28
		//afterwards, col becomes 30 and repeat the above process

		 //row==N-3
		//1st col iteration

		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,c0);
		m1=_mm256_maddubs_epi16(r1,c1);
		m2=_mm256_maddubs_epi16(r2,c0);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m0=_mm256_add_epi16(m0,m2);

		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others. later on they will be stored into 0,4,8,12,16,20,24,28 positions (8bit register).
		output_even=_mm256_and_si256(m2,output_mask);


		//2nd col iteration
		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,c0_sh1);
		m1=_mm256_maddubs_epi16(r1,c1_sh1);
		m2=_mm256_maddubs_epi16(r2,c0_sh1);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m0=_mm256_add_epi16(m0,m2);

		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);


		//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others.
		output_odd = _mm256_and_si256(m2,output_mask);


       //3rd col iteration
		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,c0_sh2);
		m1=_mm256_maddubs_epi16(r1,c1_sh2);
		m2=_mm256_maddubs_epi16(r2,c0_sh2);


		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m0=_mm256_add_epi16(m0,m2);


		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		m4=_mm256_and_si256(m0,mask3);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
		m2=_mm256_add_epi16(m2,m4);

		//m2 has 16 16bit values now. the results I need are in positions 1,3,5,7,9,11,13. keep only those, discard others
		m2 = _mm256_and_si256(m2,output_mask_sh1);
		output_even = _mm256_add_epi16(output_even,m2);


		//4th col iteration

		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,c0_sh3);
		m1=_mm256_maddubs_epi16(r1,c1_sh3);
		m2=_mm256_maddubs_epi16(r2,c0_sh3);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m0=_mm256_add_epi16(m0,m2);


		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		m4=_mm256_and_si256(m0,mask3);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
		m2=_mm256_add_epi16(m2,m4);

		//m2 has 16 16bit values now. the results I need are in positions 1,3,5,7,9,11,13. keep only those, discard others
		m2 = _mm256_and_si256(m2,output_mask_sh1);
		output_odd = _mm256_add_epi16(output_odd,m2);


		//now division follows
		output_even=division(division_case,output_even,f);
		output_odd=division(division_case,output_odd,f);

		//shift odd 1 position and add to even
		output_odd = _mm256_slli_si256(output_odd,1);
		output_even = _mm256_add_epi8(output_even,output_odd);


		_mm256_storeu_si256( (__m256i *) &filt[N-3][col],output_even );

		 //row==N-2
		//1st col iteration

		//multiply with the mask
		m0=_mm256_maddubs_epi16(r1,c0);
		m1=_mm256_maddubs_epi16(r2,c1);
		m2=_mm256_maddubs_epi16(r3,c0);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m0=_mm256_add_epi16(m0,m2);

		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others. later on they will be stored into 0,4,8,12,16,20,24,28 positions (8bit register).
		output_even=_mm256_and_si256(m2,output_mask);


		//2nd col iteration
		//multiply with the mask
		m0=_mm256_maddubs_epi16(r1,c0_sh1);
		m1=_mm256_maddubs_epi16(r2,c1_sh1);
		m2=_mm256_maddubs_epi16(r3,c0_sh1);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m0=_mm256_add_epi16(m0,m2);

		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);


		//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others.
		output_odd = _mm256_and_si256(m2,output_mask);


      //3rd col iteration
		//multiply with the mask
		m0=_mm256_maddubs_epi16(r1,c0_sh2);
		m1=_mm256_maddubs_epi16(r2,c1_sh2);
		m2=_mm256_maddubs_epi16(r3,c0_sh2);


		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m0=_mm256_add_epi16(m0,m2);


		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		m4=_mm256_and_si256(m0,mask3);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
		m2=_mm256_add_epi16(m2,m4);

		//m2 has 16 16bit values now. the results I need are in positions 1,3,5,7,9,11,13. keep only those, discard others
		m2 = _mm256_and_si256(m2,output_mask_sh1);
		output_even = _mm256_add_epi16(output_even,m2);


		//4th col iteration

		//multiply with the mask
		m0=_mm256_maddubs_epi16(r1,c0_sh3);
		m1=_mm256_maddubs_epi16(r2,c1_sh3);
		m2=_mm256_maddubs_epi16(r3,c0_sh3);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m0=_mm256_add_epi16(m0,m2);


		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		m4=_mm256_and_si256(m0,mask3);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
		m2=_mm256_add_epi16(m2,m4);

		//m2 has 16 16bit values now. the results I need are in positions 1,3,5,7,9,11,13. keep only those, discard others
		m2 = _mm256_and_si256(m2,output_mask_sh1);
		output_odd = _mm256_add_epi16(output_odd,m2);


		//now division follows
		output_even=division(division_case,output_even,f);
		output_odd=division(division_case,output_odd,f);

		//shift odd 1 position and add to even
		output_odd = _mm256_slli_si256(output_odd,1);
		output_even = _mm256_add_epi8(output_even,output_odd);


		_mm256_storeu_si256( (__m256i *) &filt[N-2][col],output_even );

		 //row==N-1
		//1st col iteration

		//multiply with the mask
		m1=_mm256_maddubs_epi16(r2,c0);
		m2=_mm256_maddubs_epi16(r3,c1);

		//vertical add
		m0=_mm256_add_epi16(m1,m2);

		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others. later on they will be stored into 0,4,8,12,16,20,24,28 positions (8bit register).
		output_even=_mm256_and_si256(m2,output_mask);


		//2nd col iteration
		//multiply with the mask
		m1=_mm256_maddubs_epi16(r2,c0_sh1);
		m2=_mm256_maddubs_epi16(r3,c1_sh1);

		//vertical add
		m0=_mm256_add_epi16(m1,m2);

		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);


		//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others.
		output_odd = _mm256_and_si256(m2,output_mask);


       //3rd col iteration
		//multiply with the mask
		m1=_mm256_maddubs_epi16(r2,c0_sh2);
		m2=_mm256_maddubs_epi16(r3,c1_sh2);


		//vertical add
		m0=_mm256_add_epi16(m1,m2);


		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		m4=_mm256_and_si256(m0,mask3);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
		m2=_mm256_add_epi16(m2,m4);

		//m2 has 16 16bit values now. the results I need are in positions 1,3,5,7,9,11,13. keep only those, discard others
		m2 = _mm256_and_si256(m2,output_mask_sh1);
		output_even = _mm256_add_epi16(output_even,m2);


		//4th col iteration

		//multiply with the mask
		m1=_mm256_maddubs_epi16(r2,c0_sh3);
		m2=_mm256_maddubs_epi16(r3,c1_sh3);

		//vertical add
		m0=_mm256_add_epi16(m1,m2);

		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		m4=_mm256_and_si256(m0,mask3);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
		m2=_mm256_add_epi16(m2,m4);

		//m2 has 16 16bit values now. the results I need are in positions 1,3,5,7,9,11,13. keep only those, discard others
		m2 = _mm256_and_si256(m2,output_mask_sh1);
		output_odd = _mm256_add_epi16(output_odd,m2);


		//now division follows
		output_even=division(division_case,output_even,f);
		output_odd=division(division_case,output_odd,f);

		//shift odd 1 position and add to even
		output_odd = _mm256_slli_si256(output_odd,1);
		output_even = _mm256_add_epi8(output_even,output_odd);


		_mm256_storeu_si256( (__m256i *) &filt[N-1][col],output_even );
		}
	loop_reminder_3x3_last_values(frame1,filt,M,N,col,REMINDER_ITERATIONS,division_case,divisor,filter,c0,c1,c0_sh1,c1_sh1,c0_sh2,c1_sh2,c0_sh3,c1_sh3,f);
	loop_reminder_3x3(frame1,filt,M,N,N-3,col,REMINDER_ITERATIONS,division_case,divisor,filter,c0,c1,c0_sh1,c1_sh1,c0_sh2,c1_sh2,c0_sh3,c1_sh3,f);

	}

	else {//main loop

	  for (col = 0; col <= M-32; col+=30){

		  if (col==0){

				//load the 5 rows
				r0=_mm256_load_si256( (__m256i *) &frame1[row-1][0]);
				r1=_mm256_load_si256( (__m256i *) &frame1[row][0]);
				r2=_mm256_load_si256( (__m256i *) &frame1[row+1][0]);
				r3=_mm256_load_si256( (__m256i *) &frame1[row+2][0]);
				r4=_mm256_load_si256( (__m256i *) &frame1[row+3][0]);



		  			//START - extra code needed for prelude
		  					m0=_mm256_slli_si256(r0,1);//shift 1 elements left - equivalent to filling with one zero inthe beginning
		  					m1=_mm256_slli_si256(r1,1);//shift 1 elements left - equivalent to filling with one zero inthe beginning
		  					m2=_mm256_slli_si256(r2,1);//shift 1 elements left - equivalent to filling with one zero inthe beginning
		  					m3=_mm256_slli_si256(r3,1);//shift 1 elements left - equivalent to filling with one zero inthe beginning
		  					m4=_mm256_slli_si256(r4,1);//shift 1 elements left - equivalent to filling with one zero inthe beginning


		  					//preserve the element being lost because of the shift above
		  					r0=_mm256_and_si256(r0,mask_prelude);
		  					r0=_mm256_permute2f128_si256(r0,r0,1);
		  					r0=_mm256_srli_si256(r0,15);//shift 15 elements
		  					r0=_mm256_add_epi16(m0,r0);

		  					r1=_mm256_and_si256(r1,mask_prelude);
		  					r1=_mm256_permute2f128_si256(r1,r1,1);
		  					r1=_mm256_srli_si256(r1,15);//shift 15 elements
		  					r1=_mm256_add_epi16(m1,r1);

		  					r2=_mm256_and_si256(r2,mask_prelude);
		  					r2=_mm256_permute2f128_si256(r2,r2,1);
		  					r2=_mm256_srli_si256(r2,15);//shift 15 elements
		  					r2=_mm256_add_epi16(m2,r2);

		  					r3=_mm256_and_si256(r3,mask_prelude);
		  					r3=_mm256_permute2f128_si256(r3,r3,1);
		  					r3=_mm256_srli_si256(r3,15);//shift 15 elements
		  					r3=_mm256_add_epi16(m3,r3);

		  					r4=_mm256_and_si256(r4,mask_prelude);
		  					r4=_mm256_permute2f128_si256(r4,r4,1);
		  					r4=_mm256_srli_si256(r4,15);//shift 15 elements
		  					r4=_mm256_add_epi16(m4,r4);

		  			//END - extra code needed for prelude
		  		}
		  else {
				//load the 5 rows
				r0=_mm256_loadu_si256( (__m256i *) &frame1[row-1][col-1]);
				r1=_mm256_loadu_si256( (__m256i *) &frame1[row][col-1]);
				r2=_mm256_loadu_si256( (__m256i *) &frame1[row+1][col-1]);
				r3=_mm256_loadu_si256( (__m256i *) &frame1[row+2][col-1]);
				r4=_mm256_loadu_si256( (__m256i *) &frame1[row+3][col-1]);

		  }

		//col iteration computes output pixels of    1,5,9,13,17,21,25,29
		// col+1 iteration computes output pixels of 2,6,10,14,18,22,26,30
		// col+2 iteration computes output pixels of 3,7,11,15,19,23,27
		// col+3 iteration computes output pixels of 4,8,12,16,20,24,28
		//afterwards, col becomes 30 and repeat the above process

		//first row
		//1st col iteration

		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,c0);
		m1=_mm256_maddubs_epi16(r1,c1);
		m2=_mm256_maddubs_epi16(r2,c0);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m0=_mm256_add_epi16(m0,m2);

		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others. later on they will be stored into 0,4,8,12,16,20,24,28 positions (8bit register).
		output_even=_mm256_and_si256(m2,output_mask);


		//2nd col iteration
		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,c0_sh1);
		m1=_mm256_maddubs_epi16(r1,c1_sh1);
		m2=_mm256_maddubs_epi16(r2,c0_sh1);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m0=_mm256_add_epi16(m0,m2);

		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);


		//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others.
		output_odd = _mm256_and_si256(m2,output_mask);


         //3rd col iteration
		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,c0_sh2);
		m1=_mm256_maddubs_epi16(r1,c1_sh2);
		m2=_mm256_maddubs_epi16(r2,c0_sh2);


		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m0=_mm256_add_epi16(m0,m2);


		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		m4=_mm256_and_si256(m0,mask3);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
		m2=_mm256_add_epi16(m2,m4);

		//m2 has 16 16bit values now. the results I need are in positions 1,3,5,7,9,11,13. keep only those, discard others
		m2 = _mm256_and_si256(m2,output_mask_sh1);
		output_even = _mm256_add_epi16(output_even,m2);


		//4th col iteration

		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,c0_sh3);
		m1=_mm256_maddubs_epi16(r1,c1_sh3);
		m2=_mm256_maddubs_epi16(r2,c0_sh3);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m0=_mm256_add_epi16(m0,m2);


		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		m4=_mm256_and_si256(m0,mask3);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
		m2=_mm256_add_epi16(m2,m4);

		//m2 has 16 16bit values now. the results I need are in positions 1,3,5,7,9,11,13. keep only those, discard others
		m2 = _mm256_and_si256(m2,output_mask_sh1);
		output_odd = _mm256_add_epi16(output_odd,m2);


		//now division follows
		output_even=division(division_case,output_even,f);
		output_odd=division(division_case,output_odd,f);

		//shift odd 1 position and add to even
		output_odd = _mm256_slli_si256(output_odd,1);
		output_even = _mm256_add_epi8(output_even,output_odd);


		_mm256_storeu_si256( (__m256i *) &filt[row][col],output_even );

		//2nd row
		//1st col iteration

		//multiply with the mask
		m0=_mm256_maddubs_epi16(r1,c0);
		m1=_mm256_maddubs_epi16(r2,c1);
		m2=_mm256_maddubs_epi16(r3,c0);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m0=_mm256_add_epi16(m0,m2);

		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others. later on they will be stored into 0,4,8,12,16,20,24,28 positions (8bit register).
		output_even=_mm256_and_si256(m2,output_mask);


		//2nd col iteration
		//multiply with the mask
		m0=_mm256_maddubs_epi16(r1,c0_sh1);
		m1=_mm256_maddubs_epi16(r2,c1_sh1);
		m2=_mm256_maddubs_epi16(r3,c0_sh1);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m0=_mm256_add_epi16(m0,m2);

		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);


		//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others.
		output_odd = _mm256_and_si256(m2,output_mask);


         //3rd col iteration
		//multiply with the mask
		m0=_mm256_maddubs_epi16(r1,c0_sh2);
		m1=_mm256_maddubs_epi16(r2,c1_sh2);
		m2=_mm256_maddubs_epi16(r3,c0_sh2);


		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m0=_mm256_add_epi16(m0,m2);


		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		m4=_mm256_and_si256(m0,mask3);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
		m2=_mm256_add_epi16(m2,m4);

		//m2 has 16 16bit values now. the results I need are in positions 1,3,5,7,9,11,13. keep only those, discard others
		m2 = _mm256_and_si256(m2,output_mask_sh1);
		output_even = _mm256_add_epi16(output_even,m2);


		//4th col iteration

		//multiply with the mask
		m0=_mm256_maddubs_epi16(r1,c0_sh3);
		m1=_mm256_maddubs_epi16(r2,c1_sh3);
		m2=_mm256_maddubs_epi16(r3,c0_sh3);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m0=_mm256_add_epi16(m0,m2);


		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		m4=_mm256_and_si256(m0,mask3);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
		m2=_mm256_add_epi16(m2,m4);

		//m2 has 16 16bit values now. the results I need are in positions 1,3,5,7,9,11,13. keep only those, discard others
		m2 = _mm256_and_si256(m2,output_mask_sh1);
		output_odd = _mm256_add_epi16(output_odd,m2);


		//now division follows
		output_even=division(division_case,output_even,f);
		output_odd=division(division_case,output_odd,f);

		//shift odd 1 position and add to even
		output_odd = _mm256_slli_si256(output_odd,1);
		output_even = _mm256_add_epi8(output_even,output_odd);


		_mm256_storeu_si256( (__m256i *) &filt[row+1][col],output_even );

		//3rd row
		//1st col iteration

		//multiply with the mask
		m0=_mm256_maddubs_epi16(r2,c0);
		m1=_mm256_maddubs_epi16(r3,c1);
		m2=_mm256_maddubs_epi16(r4,c0);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m0=_mm256_add_epi16(m0,m2);

		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others. later on they will be stored into 0,4,8,12,16,20,24,28 positions (8bit register).
		output_even=_mm256_and_si256(m2,output_mask);


		//2nd col iteration
		//multiply with the mask
		m0=_mm256_maddubs_epi16(r2,c0_sh1);
		m1=_mm256_maddubs_epi16(r3,c1_sh1);
		m2=_mm256_maddubs_epi16(r4,c0_sh1);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m0=_mm256_add_epi16(m0,m2);

		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);


		//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others.
		output_odd = _mm256_and_si256(m2,output_mask);


         //3rd col iteration
		//multiply with the mask
		m0=_mm256_maddubs_epi16(r2,c0_sh2);
		m1=_mm256_maddubs_epi16(r3,c1_sh2);
		m2=_mm256_maddubs_epi16(r4,c0_sh2);


		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m0=_mm256_add_epi16(m0,m2);


		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		m4=_mm256_and_si256(m0,mask3);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
		m2=_mm256_add_epi16(m2,m4);

		//m2 has 16 16bit values now. the results I need are in positions 1,3,5,7,9,11,13. keep only those, discard others
		m2 = _mm256_and_si256(m2,output_mask_sh1);
		output_even = _mm256_add_epi16(output_even,m2);


		//4th col iteration

		//multiply with the mask
		m0=_mm256_maddubs_epi16(r2,c0_sh3);
		m1=_mm256_maddubs_epi16(r3,c1_sh3);
		m2=_mm256_maddubs_epi16(r4,c0_sh3);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m0=_mm256_add_epi16(m0,m2);


		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		m4=_mm256_and_si256(m0,mask3);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
		m2=_mm256_add_epi16(m2,m4);

		//m2 has 16 16bit values now. the results I need are in positions 1,3,5,7,9,11,13. keep only those, discard others
		m2 = _mm256_and_si256(m2,output_mask_sh1);
		output_odd = _mm256_add_epi16(output_odd,m2);


		//now division follows
		output_even=division(division_case,output_even,f);
		output_odd=division(division_case,output_odd,f);

		//shift odd 1 position and add to even
		output_odd = _mm256_slli_si256(output_odd,1);
		output_even = _mm256_add_epi8(output_even,output_odd);


		_mm256_storeu_si256( (__m256i *) &filt[row+2][col],output_even );


		}
	  loop_reminder_3x3(frame1,filt,M,N,row,col,REMINDER_ITERATIONS,division_case,divisor,filter,c0,c1,c0_sh1,c1_sh1,c0_sh2,c1_sh2,c0_sh3,c1_sh3,f);
	  loop_reminder_3x3(frame1,filt,M,N,row+1,col,REMINDER_ITERATIONS,division_case,divisor,filter,c0,c1,c0_sh1,c1_sh1,c0_sh2,c1_sh2,c0_sh3,c1_sh3,f);
	  loop_reminder_3x3(frame1,filt,M,N,row+2,col,REMINDER_ITERATIONS,division_case,divisor,filter,c0,c1,c0_sh1,c1_sh1,c0_sh2,c1_sh2,c0_sh3,c1_sh3,f);

	}

}

}//end of parallel


}



void Gaussian_Blur_optimized_5x5_coeffprop(unsigned char **frame1,unsigned char **filt,const unsigned int M, const unsigned int N, const unsigned short int divisor, signed char **filter5x5){

	const signed char f00=filter5x5[0][0];
	const signed char f01=filter5x5[0][1];
	const signed char f02=filter5x5[0][2];
	const signed char f03=filter5x5[0][3];
	const signed char f04=filter5x5[0][4];

	const signed char f10=filter5x5[1][0];
	const signed char f11=filter5x5[1][1];
	const signed char f12=filter5x5[1][2];
	const signed char f13=filter5x5[1][3];
	const signed char f14=filter5x5[1][4];

	const signed char f20=filter5x5[2][0];
	const signed char f21=filter5x5[2][1];
	const signed char f22=filter5x5[2][2];
	const signed char f23=filter5x5[2][3];
	const signed char f24=filter5x5[2][4];


	const __m256i c00=_mm256_set_epi16(f00,f00,f00,f00,f00,f00,f00,f00,f00,f00,f00,f00,f00,f00,f00,f00);
	const __m256i c01=_mm256_set_epi16(f01,f01,f01,f01,f01,f01,f01,f01,f01,f01,f01,f01,f01,f01,f01,f01);
	const __m256i c02=_mm256_set_epi16(f02,f02,f02,f02,f02,f02,f02,f02,f02,f02,f02,f02,f02,f02,f02,f02);
	const __m256i c03=_mm256_set_epi16(f03,f03,f03,f03,f03,f03,f03,f03,f03,f03,f03,f03,f03,f03,f03,f03);
	const __m256i c04=_mm256_set_epi16(f04,f04,f04,f04,f04,f04,f04,f04,f04,f04,f04,f04,f04,f04,f04,f04);

	const __m256i c10=_mm256_set_epi16(f10,f10,f10,f10,f10,f10,f10,f10,f10,f10,f10,f10,f10,f10,f10,f10);
	const __m256i c11=_mm256_set_epi16(f11,f11,f11,f11,f11,f11,f11,f11,f11,f11,f11,f11,f11,f11,f11,f11);
	const __m256i c12=_mm256_set_epi16(f12,f12,f12,f12,f12,f12,f12,f12,f12,f12,f12,f12,f12,f12,f12,f12);
	const __m256i c13=_mm256_set_epi16(f13,f13,f13,f13,f13,f13,f13,f13,f13,f13,f13,f13,f13,f13,f13,f13);
	const __m256i c14=_mm256_set_epi16(f14,f14,f14,f14,f14,f14,f14,f14,f14,f14,f14,f14,f14,f14,f14,f14);

	const __m256i c20=_mm256_set_epi16(f20,f20,f20,f20,f20,f20,f20,f20,f20,f20,f20,f20,f20,f20,f20,f20);
	const __m256i c21=_mm256_set_epi16(f21,f21,f21,f21,f21,f21,f21,f21,f21,f21,f21,f21,f21,f21,f21,f21);
	const __m256i c22=_mm256_set_epi16(f22,f22,f22,f22,f22,f22,f22,f22,f22,f22,f22,f22,f22,f22,f22,f22);
	const __m256i c23=_mm256_set_epi16(f23,f23,f23,f23,f23,f23,f23,f23,f23,f23,f23,f23,f23,f23,f23,f23);
	const __m256i c24=_mm256_set_epi16(f24,f24,f24,f24,f24,f24,f24,f24,f24,f24,f24,f24,f24,f24,f24,f24);

	const __m256i mask_prelude  = _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0);
	const __m256i mask3  = _mm256_set_epi16(0,0,0,0,0,0,0,65535,0,0,0,0,0,0,0,0);

    __m256i cx = _mm256_set_epi8(255, 255, 255, 255, 255, 255, 255, 255, 14, 12, 10, 8, 6, 4, 2, 0, 255, 255, 255, 255, 255, 255, 255, 255, 14, 12, 10, 8, 6, 4, 2, 0);

	const unsigned int division_case=prepare_for_division(divisor); //determine which is the division case (A, B or C)
	const __m256i f = _mm256_load_si256( (__m256i *) &f_vector[0]);// initialize the division vector

//printf("\n%d %d",division_case,b);


#pragma omp parallel
{

unsigned int row,col;
register __m256i r0,r1,r2,r3,r4,m0,m1,m2,m3,m4,t0,t1,t2,t3,t4,t0b,t1b,t2b,t3b,t4b;
__m128i r0l,r1l,r2l,r3l,r4l;

/*---------------------- Gaussian Blur ---------------------------------*/

    #pragma omp for schedule(static)
	for (row = 2; row < N-2; row++) {
			  for (col = 2; col < M-16; col+=12){

				  if (col==0){//this is a special case as the mask gets outside of the array (it is like adding two zeros in the beginning of frame[][]

						//load the 5 rows
						r0=_mm256_loadu_si256( (__m256i *) &frame1[row-2][0]);
						r1=_mm256_loadu_si256( (__m256i *) &frame1[row-1][0]);
						r2=_mm256_loadu_si256( (__m256i *) &frame1[row][0]);
						r3=_mm256_loadu_si256( (__m256i *) &frame1[row+1][0]);
						r4=_mm256_loadu_si256( (__m256i *) &frame1[row+2][0]);

				  			//START - extra code needed for prelude
				  					m0=_mm256_slli_si256(r0,2);//shift 2 elements left - equivalent to filling with two zeros inthe beginning
				  					m1=_mm256_slli_si256(r1,2);//shift 2 elements left - equivalent to filling with two zeros inthe beginning
				  					m2=_mm256_slli_si256(r2,2);//shift 2 elements left - equivalent to filling with two zeros inthe beginning
				  					m3=_mm256_slli_si256(r3,2);//shift 2 elements left - equivalent to filling with two zeros inthe beginning
				  					m4=_mm256_slli_si256(r4,2);//shift 2 elements left - equivalent to filling with two zeros inthe beginning

				  					r0=_mm256_and_si256(r0,mask_prelude);
				  					r0=_mm256_permute2f128_si256(r0,r0,1);
				  					r0=_mm256_srli_si256(r0,14);//shift 14 elements
				  					r0=_mm256_add_epi16(m0,r0);

				  					r1=_mm256_and_si256(r1,mask_prelude);
				  					r1=_mm256_permute2f128_si256(r1,r1,1);
				  					r1=_mm256_srli_si256(r1,14);//shift 14 elements
				  					r1=_mm256_add_epi16(m1,r1);

				  					r2=_mm256_and_si256(r2,mask_prelude);
				  					r2=_mm256_permute2f128_si256(r2,r2,1);
				  					r2=_mm256_srli_si256(r2,14);//shift 14 elements
				  					r2=_mm256_add_epi16(m2,r2);

				  					r3=_mm256_and_si256(r3,mask_prelude);
				  					r3=_mm256_permute2f128_si256(r3,r3,1);
				  					r3=_mm256_srli_si256(r3,14);//shift 14 elements
				  					r3=_mm256_add_epi16(m3,r3);

				  					r4=_mm256_and_si256(r4,mask_prelude);
				  					r4=_mm256_permute2f128_si256(r4,r4,1);
				  					r4=_mm256_srli_si256(r4,14);//shift 14 elements
				  					r4=_mm256_add_epi16(m4,r4);

				  			//END - extra code needed for prelude
				  		}
				  else {
						//load the 5 rows
						r0l=_mm_loadu_si128( (__m128i *) &frame1[row-2][col-2]);
						r1l=_mm_loadu_si128( (__m128i *) &frame1[row-1][col-2]);
						r2l=_mm_loadu_si128( (__m128i *) &frame1[row][col-2]);
						r3l=_mm_loadu_si128( (__m128i *) &frame1[row+1][col-2]);
						r4l=_mm_loadu_si128( (__m128i *) &frame1[row+2][col-2]);

						 t0=_mm256_cvtepu8_epi16 (r0l);//make 8bit 16 bit
						 t1=_mm256_cvtepu8_epi16 (r1l);//make 8bit 16 bit
						 t2=_mm256_cvtepu8_epi16 (r2l);//make 8bit 16 bit
						 t3=_mm256_cvtepu8_epi16 (r3l);//make 8bit 16 bit
						 t4=_mm256_cvtepu8_epi16 (r4l);//make 8bit 16 bit
				  }

				  //r0h = _mm256_extractf128_si256(r0, 0);
			/*	  r0l = _mm256_extractf128_si256(r0, 1);
				  r1l = _mm256_extractf128_si256(r1, 1);
				  r2l = _mm256_extractf128_si256(r2, 1);
				  r3l = _mm256_extractf128_si256(r3, 1);
				  r4l = _mm256_extractf128_si256(r4, 1);*/


				  //mul by c00
				  m0=_mm256_mullo_epi16 (t0,c00);
				  m1=_mm256_mullo_epi16 (t1,c10);
				  m2=_mm256_mullo_epi16 (t2,c20);
				  m3=_mm256_mullo_epi16 (t3,c10);
				  m4=_mm256_mullo_epi16 (t4,c00);

				 //1st shift
					t0b=_mm256_srli_si256(t0,2);//shift by 1 elements left
					t1b=_mm256_srli_si256(t1,2);//shift by 1 elements left
					t2b=_mm256_srli_si256(t2,2);//shift by 1 elements left
					t3b=_mm256_srli_si256(t3,2);//shift by 1 elements left
					t4b=_mm256_srli_si256(t4,2);//shift by 1 elements left

					//after shifts the 15th element is lost as it cannot be propagated to the 16th position (AVX registers are managed as two seperate SSE registers)
					t0=_mm256_and_si256(t0,mask3);
					t0=_mm256_permute2f128_si256(t0,t0,1);
					t0=_mm256_slli_si256(t0,14);//shift 7 short int positions or 14 char positions
					t0=_mm256_add_epi16(t0b,t0);

					t1=_mm256_and_si256(t1,mask3);
					t1=_mm256_permute2f128_si256(t1,t1,1);
					t1=_mm256_slli_si256(t1,14);//shift 7 short int positions or 14 char positions
					t1=_mm256_add_epi16(t1b,t1);

					t2=_mm256_and_si256(t2,mask3);
					t2=_mm256_permute2f128_si256(t2,t2,1);
					t2=_mm256_slli_si256(t2,14);//shift 7 short int positions or 14 char positions
					t2=_mm256_add_epi16(t2b,t2);

					t3=_mm256_and_si256(t3,mask3);
					t3=_mm256_permute2f128_si256(t3,t3,1);
					t3=_mm256_slli_si256(t3,14);//shift 7 short int positions or 14 char positions
					t3=_mm256_add_epi16(t3b,t3);

					t4=_mm256_and_si256(t4,mask3);
					t4=_mm256_permute2f128_si256(t4,t4,1);
					t4=_mm256_slli_si256(t4,14);//shift 7 short int positions or 14 char positions
					t4=_mm256_add_epi16(t4b,t4);

				  //mul by c01
				  m0+=_mm256_mullo_epi16 (t0,c01);
				  m1+=_mm256_mullo_epi16 (t1,c11);
				  m2+=_mm256_mullo_epi16 (t2,c21);
				  m3+=_mm256_mullo_epi16 (t3,c11);
				  m4+=_mm256_mullo_epi16 (t4,c01);

					 //2nd shift
					t0b=_mm256_srli_si256(t0,2);//shift by 1 elements left
					t1b=_mm256_srli_si256(t1,2);//shift by 1 elements left
					t2b=_mm256_srli_si256(t2,2);//shift by 1 elements left
					t3b=_mm256_srli_si256(t3,2);//shift by 1 elements left
					t4b=_mm256_srli_si256(t4,2);//shift by 1 elements left

					//after shifts the 15th element is lost as it cannot be propagated to the 16th position (AVX registers are managed as two seperate SSE registers)
					t0=_mm256_and_si256(t0,mask3);
					t0=_mm256_permute2f128_si256(t0,t0,1);
					t0=_mm256_slli_si256(t0,14);//shift 7 short int positions or 14 char positions
					t0=_mm256_add_epi16(t0b,t0);

					t1=_mm256_and_si256(t1,mask3);
					t1=_mm256_permute2f128_si256(t1,t1,1);
					t1=_mm256_slli_si256(t1,14);//shift 7 short int positions or 14 char positions
					t1=_mm256_add_epi16(t1b,t1);

					t2=_mm256_and_si256(t2,mask3);
					t2=_mm256_permute2f128_si256(t2,t2,1);
					t2=_mm256_slli_si256(t2,14);//shift 7 short int positions or 14 char positions
					t2=_mm256_add_epi16(t2b,t2);

					t3=_mm256_and_si256(t3,mask3);
					t3=_mm256_permute2f128_si256(t3,t3,1);
					t3=_mm256_slli_si256(t3,14);//shift 7 short int positions or 14 char positions
					t3=_mm256_add_epi16(t3b,t3);

					t4=_mm256_and_si256(t4,mask3);
					t4=_mm256_permute2f128_si256(t4,t4,1);
					t4=_mm256_slli_si256(t4,14);//shift 7 short int positions or 14 char positions
					t4=_mm256_add_epi16(t4b,t4);


					  //mul by c02
					  m0+=_mm256_mullo_epi16 (t0,c02);
					  m1+=_mm256_mullo_epi16 (t1,c12);
					  m2+=_mm256_mullo_epi16 (t2,c22);
					  m3+=_mm256_mullo_epi16 (t3,c12);
					  m4+=_mm256_mullo_epi16 (t4,c02);

						 //3rd shift
						t0b=_mm256_srli_si256(t0,2);//shift by 1 elements left
						t1b=_mm256_srli_si256(t1,2);//shift by 1 elements left
						t2b=_mm256_srli_si256(t2,2);//shift by 1 elements left
						t3b=_mm256_srli_si256(t3,2);//shift by 1 elements left
						t4b=_mm256_srli_si256(t4,2);//shift by 1 elements left

						//after shifts the 15th element is lost as it cannot be propagated to the 16th position (AVX registers are managed as two seperate SSE registers)
						t0=_mm256_and_si256(t0,mask3);
						t0=_mm256_permute2f128_si256(t0,t0,1);
						t0=_mm256_slli_si256(t0,14);//shift 7 short int positions or 14 char positions
						t0=_mm256_add_epi16(t0b,t0);

						t1=_mm256_and_si256(t1,mask3);
						t1=_mm256_permute2f128_si256(t1,t1,1);
						t1=_mm256_slli_si256(t1,14);//shift 7 short int positions or 14 char positions
						t1=_mm256_add_epi16(t1b,t1);

						t2=_mm256_and_si256(t2,mask3);
						t2=_mm256_permute2f128_si256(t2,t2,1);
						t2=_mm256_slli_si256(t2,14);//shift 7 short int positions or 14 char positions
						t2=_mm256_add_epi16(t2b,t2);

						t3=_mm256_and_si256(t3,mask3);
						t3=_mm256_permute2f128_si256(t3,t3,1);
						t3=_mm256_slli_si256(t3,14);//shift 7 short int positions or 14 char positions
						t3=_mm256_add_epi16(t3b,t3);

						t4=_mm256_and_si256(t4,mask3);
						t4=_mm256_permute2f128_si256(t4,t4,1);
						t4=_mm256_slli_si256(t4,14);//shift 7 short int positions or 14 char positions
						t4=_mm256_add_epi16(t4b,t4);


						  //mul by c03
						  m0+=_mm256_mullo_epi16 (t0,c03);
						  m1+=_mm256_mullo_epi16 (t1,c13);
						  m2+=_mm256_mullo_epi16 (t2,c23);
						  m3+=_mm256_mullo_epi16 (t3,c13);
						  m4+=_mm256_mullo_epi16 (t4,c03);

							 //4th shift
							t0b=_mm256_srli_si256(t0,2);//shift by 1 elements left
							t1b=_mm256_srli_si256(t1,2);//shift by 1 elements left
							t2b=_mm256_srli_si256(t2,2);//shift by 1 elements left
							t3b=_mm256_srli_si256(t3,2);//shift by 1 elements left
							t4b=_mm256_srli_si256(t4,2);//shift by 1 elements left

							//after shifts the 15th element is lost as it cannot be propagated to the 16th position (AVX registers are managed as two seperate SSE registers)
							t0=_mm256_and_si256(t0,mask3);
							t0=_mm256_permute2f128_si256(t0,t0,1);
							t0=_mm256_slli_si256(t0,14);//shift 7 short int positions or 14 char positions
							t0=_mm256_add_epi16(t0b,t0);

							t1=_mm256_and_si256(t1,mask3);
							t1=_mm256_permute2f128_si256(t1,t1,1);
							t1=_mm256_slli_si256(t1,14);//shift 7 short int positions or 14 char positions
							t1=_mm256_add_epi16(t1b,t1);

							t2=_mm256_and_si256(t2,mask3);
							t2=_mm256_permute2f128_si256(t2,t2,1);
							t2=_mm256_slli_si256(t2,14);//shift 7 short int positions or 14 char positions
							t2=_mm256_add_epi16(t2b,t2);

							t3=_mm256_and_si256(t3,mask3);
							t3=_mm256_permute2f128_si256(t3,t3,1);
							t3=_mm256_slli_si256(t3,14);//shift 7 short int positions or 14 char positions
							t3=_mm256_add_epi16(t3b,t3);

							t4=_mm256_and_si256(t4,mask3);
							t4=_mm256_permute2f128_si256(t4,t4,1);
							t4=_mm256_slli_si256(t4,14);//shift 7 short int positions or 14 char positions
							t4=_mm256_add_epi16(t4b,t4);

							  //mul by c01
							  m0+=_mm256_mullo_epi16 (t0,c04);
							  m1+=_mm256_mullo_epi16 (t1,c14);
							  m2+=_mm256_mullo_epi16 (t2,c24);
							  m3+=_mm256_mullo_epi16 (t3,c14);
							  m4+=_mm256_mullo_epi16 (t4,c04);

							 //add rows
			  				m0=_mm256_add_epi16(m0,m1);
			  				m0=_mm256_add_epi16(m0,m2);
			  				m0=_mm256_add_epi16(m0,m3);
			  				m0=_mm256_add_epi16(m0,m4);

							m0=division(division_case,m0,f);

						    //take the result after division and make it 8bit.
						    r0 = _mm256_shuffle_epi8(m0, cx);//propagate only the even elements and discard the odd. I get 0,2,4,6,8,10,12,14,0,0,0,0,0,0,0,0,16,18,20,22,24,26,28,30,0,0,0,0,0,0,0,0
						    r1 = _mm256_permute2f128_si256(r0, r0, 1);
						    r1 = _mm256_slli_si256(r1, 8);
						    r0 = _mm256_add_epi16(r0, r1);

						    //now store first 128bit in memory
			  				_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(r0, 0) );

			  }

	}

}

}





void Gaussian_Blur_optimized_3x3_coeffprop(unsigned char **frame1,unsigned char **filt,const unsigned int M, const unsigned int N, const unsigned short int divisor, signed char **filter){

	const signed char f00=filter[0][0];
	const signed char f01=filter[0][1];
	const signed char f02=filter[0][2];

	const signed char f10=filter[1][0];
	const signed char f11=filter[1][1];
	const signed char f12=filter[1][2];


	const __m256i c00=_mm256_set_epi16(f00,f00,f00,f00,f00,f00,f00,f00,f00,f00,f00,f00,f00,f00,f00,f00);
	const __m256i c01=_mm256_set_epi16(f01,f01,f01,f01,f01,f01,f01,f01,f01,f01,f01,f01,f01,f01,f01,f01);
	const __m256i c02=_mm256_set_epi16(f02,f02,f02,f02,f02,f02,f02,f02,f02,f02,f02,f02,f02,f02,f02,f02);

	const __m256i c10=_mm256_set_epi16(f10,f10,f10,f10,f10,f10,f10,f10,f10,f10,f10,f10,f10,f10,f10,f10);
	const __m256i c11=_mm256_set_epi16(f11,f11,f11,f11,f11,f11,f11,f11,f11,f11,f11,f11,f11,f11,f11,f11);
	const __m256i c12=_mm256_set_epi16(f12,f12,f12,f12,f12,f12,f12,f12,f12,f12,f12,f12,f12,f12,f12,f12);


	const __m256i mask_prelude  = _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0);
	const __m256i mask3  = _mm256_set_epi16(0,0,0,0,0,0,0,65535,0,0,0,0,0,0,0,0);

    __m256i cx = _mm256_set_epi8(255, 255, 255, 255, 255, 255, 255, 255, 14, 12, 10, 8, 6, 4, 2, 0, 255, 255, 255, 255, 255, 255, 255, 255, 14, 12, 10, 8, 6, 4, 2, 0);

	const unsigned int division_case=prepare_for_division(divisor); //determine which is the division case (A, B or C)
	const __m256i f = _mm256_load_si256( (__m256i *) &f_vector[0]);// initialize the division vector

//printf("\n%d %d",division_case,b);


#pragma omp parallel
{

unsigned int row,col;
register __m256i r0,r1,r2,r3,r4,m0,m1,m2,m3,m4,t0,t1,t2,t0b,t1b,t2b;
__m128i r0l,r1l,r2l;

/*---------------------- Gaussian Blur ---------------------------------*/

    #pragma omp for schedule(static)
	for (row = 2; row < N-2; row++) {
			  for (col = 2; col < M-16; col+=14){

				  if (col==0){//this is a special case as the mask gets outside of the array (it is like adding two zeros in the beginning of frame[][]

						//load the 5 rows
						r0=_mm256_loadu_si256( (__m256i *) &frame1[row-2][0]);
						r1=_mm256_loadu_si256( (__m256i *) &frame1[row-1][0]);
						r2=_mm256_loadu_si256( (__m256i *) &frame1[row][0]);
						r3=_mm256_loadu_si256( (__m256i *) &frame1[row+1][0]);
						r4=_mm256_loadu_si256( (__m256i *) &frame1[row+2][0]);

				  			//START - extra code needed for prelude
				  					m0=_mm256_slli_si256(r0,2);//shift 2 elements left - equivalent to filling with two zeros inthe beginning
				  					m1=_mm256_slli_si256(r1,2);//shift 2 elements left - equivalent to filling with two zeros inthe beginning
				  					m2=_mm256_slli_si256(r2,2);//shift 2 elements left - equivalent to filling with two zeros inthe beginning
				  					m3=_mm256_slli_si256(r3,2);//shift 2 elements left - equivalent to filling with two zeros inthe beginning
				  					m4=_mm256_slli_si256(r4,2);//shift 2 elements left - equivalent to filling with two zeros inthe beginning

				  					r0=_mm256_and_si256(r0,mask_prelude);
				  					r0=_mm256_permute2f128_si256(r0,r0,1);
				  					r0=_mm256_srli_si256(r0,14);//shift 14 elements
				  					r0=_mm256_add_epi16(m0,r0);

				  					r1=_mm256_and_si256(r1,mask_prelude);
				  					r1=_mm256_permute2f128_si256(r1,r1,1);
				  					r1=_mm256_srli_si256(r1,14);//shift 14 elements
				  					r1=_mm256_add_epi16(m1,r1);

				  					r2=_mm256_and_si256(r2,mask_prelude);
				  					r2=_mm256_permute2f128_si256(r2,r2,1);
				  					r2=_mm256_srli_si256(r2,14);//shift 14 elements
				  					r2=_mm256_add_epi16(m2,r2);

				  					r3=_mm256_and_si256(r3,mask_prelude);
				  					r3=_mm256_permute2f128_si256(r3,r3,1);
				  					r3=_mm256_srli_si256(r3,14);//shift 14 elements
				  					r3=_mm256_add_epi16(m3,r3);

				  					r4=_mm256_and_si256(r4,mask_prelude);
				  					r4=_mm256_permute2f128_si256(r4,r4,1);
				  					r4=_mm256_srli_si256(r4,14);//shift 14 elements
				  					r4=_mm256_add_epi16(m4,r4);

				  			//END - extra code needed for prelude
				  		}
				  else {
						//load the 5 rows
						r0l=_mm_loadu_si128( (__m128i *) &frame1[row-1][col-1]);
						r1l=_mm_loadu_si128( (__m128i *) &frame1[row][col-1]);
						r2l=_mm_loadu_si128( (__m128i *) &frame1[row+1][col-1]);

						 t0=_mm256_cvtepu8_epi16 (r0l);//make 8bit 16 bit
						 t1=_mm256_cvtepu8_epi16 (r1l);//make 8bit 16 bit
						 t2=_mm256_cvtepu8_epi16 (r2l);//make 8bit 16 bit
				  }

				  //r0h = _mm256_extractf128_si256(r0, 0);
			/*	  r0l = _mm256_extractf128_si256(r0, 1);
				  r1l = _mm256_extractf128_si256(r1, 1);
				  r2l = _mm256_extractf128_si256(r2, 1);
				  r3l = _mm256_extractf128_si256(r3, 1);
				  r4l = _mm256_extractf128_si256(r4, 1);*/


				  //mul by c00
				  m0=_mm256_mullo_epi16 (t0,c00);
				  m1=_mm256_mullo_epi16 (t1,c10);
				  m2=_mm256_mullo_epi16 (t2,c00);

				 //1st shift
					t0b=_mm256_srli_si256(t0,2);//shift by 1 elements left
					t1b=_mm256_srli_si256(t1,2);//shift by 1 elements left
					t2b=_mm256_srli_si256(t2,2);//shift by 1 elements left


					//after shifts the 15th element is lost as it cannot be propagated to the 16th position (AVX registers are managed as two seperate SSE registers)
					t0=_mm256_and_si256(t0,mask3);
					t0=_mm256_permute2f128_si256(t0,t0,1);
					t0=_mm256_slli_si256(t0,14);//shift 7 short int positions or 14 char positions
					t0=_mm256_add_epi16(t0b,t0);

					t1=_mm256_and_si256(t1,mask3);
					t1=_mm256_permute2f128_si256(t1,t1,1);
					t1=_mm256_slli_si256(t1,14);//shift 7 short int positions or 14 char positions
					t1=_mm256_add_epi16(t1b,t1);

					t2=_mm256_and_si256(t2,mask3);
					t2=_mm256_permute2f128_si256(t2,t2,1);
					t2=_mm256_slli_si256(t2,14);//shift 7 short int positions or 14 char positions
					t2=_mm256_add_epi16(t2b,t2);


				  //mul by c01
				  m0+=_mm256_mullo_epi16 (t0,c01);
				  m1+=_mm256_mullo_epi16 (t1,c11);
				  m2+=_mm256_mullo_epi16 (t2,c01);


					 //2nd shift
					t0b=_mm256_srli_si256(t0,2);//shift by 1 elements left
					t1b=_mm256_srli_si256(t1,2);//shift by 1 elements left
					t2b=_mm256_srli_si256(t2,2);//shift by 1 elements left


					//after shifts the 15th element is lost as it cannot be propagated to the 16th position (AVX registers are managed as two seperate SSE registers)
					t0=_mm256_and_si256(t0,mask3);
					t0=_mm256_permute2f128_si256(t0,t0,1);
					t0=_mm256_slli_si256(t0,14);//shift 7 short int positions or 14 char positions
					t0=_mm256_add_epi16(t0b,t0);

					t1=_mm256_and_si256(t1,mask3);
					t1=_mm256_permute2f128_si256(t1,t1,1);
					t1=_mm256_slli_si256(t1,14);//shift 7 short int positions or 14 char positions
					t1=_mm256_add_epi16(t1b,t1);

					t2=_mm256_and_si256(t2,mask3);
					t2=_mm256_permute2f128_si256(t2,t2,1);
					t2=_mm256_slli_si256(t2,14);//shift 7 short int positions or 14 char positions
					t2=_mm256_add_epi16(t2b,t2);


					  //mul by c02
					  m0+=_mm256_mullo_epi16 (t0,c02);
					  m1+=_mm256_mullo_epi16 (t1,c12);
					  m2+=_mm256_mullo_epi16 (t2,c02);



							 //add rows
			  				m0=_mm256_add_epi16(m0,m1);
			  				m0=_mm256_add_epi16(m0,m2);

							m0=division(division_case,m0,f);

						    //take the result after division and make it 8bit.
						    r0 = _mm256_shuffle_epi8(m0, cx);//propagate only the even elements and discard the odd. I get 0,2,4,6,8,10,12,14,0,0,0,0,0,0,0,0,16,18,20,22,24,26,28,30,0,0,0,0,0,0,0,0
						    r1 = _mm256_permute2f128_si256(r0, r0, 1);
						    r1 = _mm256_slli_si256(r1, 8);
						    r0 = _mm256_add_epi16(r0, r1);

						    //now store first 128bit in memory
			  				_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(r0, 0) );

			  }

	}

}

}


void print_vector_8(__m256i v){

	unsigned char temp[32] __attribute__((aligned(64))) ;
	_mm256_store_si256( (__m256i *) &temp[0], v);

	  printf("\n");
	   for (int i=0;i<32;i++)
		printf("%u ", temp[i]);


}

void print_vector_16(__m256i v){

	unsigned short int temp[16] __attribute__((aligned(64))) ;
	_mm256_store_si256( (__m256i *) &temp[0], v);

	  printf("\n");
	   for (int i=0;i<16;i++)
		printf("%u ", temp[i]);


}

void print_vector_32(__m256i v){

	int temp[8] __attribute__((aligned(64))) ;
	_mm256_store_si256( (__m256i *) &temp[0], v);

	  printf("\n");
	   for (int i=0;i<8;i++)
		printf("%u ", temp[i]);

}




void Gaussian_Blur_optimized_3x3_32(unsigned char **frame1,unsigned char **filt,const unsigned int M, const unsigned int N, const unsigned short int divisor, signed char **filter){

const signed char f00=filter[0][0];
const signed char f01=filter[0][1];
const signed char f02=filter[0][2];

const signed char f10=filter[1][0];
const signed char f11=filter[1][1];
const signed char f12=filter[1][2];


	const __m256i c0=_mm256_set_epi8(0,f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00);
	const __m256i c1=_mm256_set_epi8(0,f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10);

	const __m256i c0_sh1=_mm256_set_epi8(f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00,0);
	const __m256i c1_sh1=_mm256_set_epi8(f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10,0);


	const __m256i c0_sh2=_mm256_set_epi8(0,0,0,f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00,0,0);
	const __m256i c1_sh2=_mm256_set_epi8(0,0,0,f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10,0,0);

	const __m256i c0_sh3=_mm256_set_epi8(0,0,f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00,0,0,0);
	const __m256i c1_sh3=_mm256_set_epi8(0,0,f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10,0,0,0);


	const __m256i mask3  = _mm256_set_epi16(0,0,0,0,0,0,0,65535,0,0,0,0,0,0,0,0);
	const __m256i mask_prelude  = _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0);




	const unsigned int REMINDER_ITERATIONS =  (M-((((M-32)/30)*30)+30)); //M-(last_col_value+30)
	//printf("\n%d",REMINDER_ITERATIONS);


	const unsigned int division_case=prepare_for_division_32(divisor); //determine which is the division case (A, B or C)
	const __m256i f = _mm256_load_si256( (__m256i *) &f_vector[0]);// initialize the division vector

//printf("\nDiv case %d , b=%d",division_case,b);

//const __m256i x      = _mm256_set_epi32(100000,200000,50000,1000,1000000,200200,50001,16034);
//__m256i res = division_32(division_case,x,f);
//print_vector_32(res);

//return 0;

#pragma omp parallel
{

unsigned int row,col;
register __m256i r0,r1,r2,m0,m1,m2,m3,m4;
//__m256i output_row0_even,output_row1_even,output_row0_odd,output_row1_odd; //these are for processing row #0 and #1 only.
__m256i output1,output2,output3,output4;
//__m256i m0_prelude_row0,m0_prelude_row1;


/*---------------------- Gaussian Blur ---------------------------------*/

    #pragma omp for schedule(static)
	for (row = 1; row < N-1; row++) {

	if (row==1){//special case compute filt[0:1][:]

		for (col = 0; col <= M-32; col+=30){

		  if (col==0){

				//load the 5 rows
				r0=_mm256_loadu_si256( (__m256i *) &frame1[0][0]);
				r1=_mm256_loadu_si256( (__m256i *) &frame1[1][0]);
				r2=_mm256_loadu_si256( (__m256i *) &frame1[2][0]);


		  			//START - extra code needed for prelude
		  					m0=_mm256_slli_si256(r0,1);//shift 1 elements left - equivalent to filling with one zero inthe beginning
		  					m1=_mm256_slli_si256(r1,1);//shift 1 elements left - equivalent to filling with one zero inthe beginning
		  					m2=_mm256_slli_si256(r2,1);//shift 1 elements left - equivalent to filling with one zero inthe beginning

		  					//preserve the element being lost because of the shift above
		  					r0=_mm256_and_si256(r0,mask_prelude);
		  					r0=_mm256_permute2f128_si256(r0,r0,1);
		  					r0=_mm256_srli_si256(r0,15);//shift 15 elements
		  					r0=_mm256_add_epi16(m0,r0);

		  					r1=_mm256_and_si256(r1,mask_prelude);
		  					r1=_mm256_permute2f128_si256(r1,r1,1);
		  					r1=_mm256_srli_si256(r1,15);//shift 15 elements
		  					r1=_mm256_add_epi16(m1,r1);

		  					r2=_mm256_and_si256(r2,mask_prelude);
		  					r2=_mm256_permute2f128_si256(r2,r2,1);
		  					r2=_mm256_srli_si256(r2,15);//shift 15 elements
		  					r2=_mm256_add_epi16(m2,r2);

		  			//END - extra code needed for prelude
		  		}
		  else {
				//load the 5 rows
				r0=_mm256_loadu_si256( (__m256i *) &frame1[0][col-1]);
				r1=_mm256_loadu_si256( (__m256i *) &frame1[1][col-1]);
				r2=_mm256_loadu_si256( (__m256i *) &frame1[2][col-1]);
		  }

		//col iteration computes output pixels of    1,5,9,13,17,21,25,29
		// col+1 iteration computes output pixels of 2,6,10,14,18,22,26,30
		// col+2 iteration computes output pixels of 3,7,11,15,19,23,27
		// col+3 iteration computes output pixels of 4,8,12,16,20,24,28
		//afterwards, col becomes 30 and repeat the above process

		 //row==1
			//1st col iteration

			//multiply with the mask
			m0=_mm256_maddubs_epi16(r0,c0);
			m1=_mm256_maddubs_epi16(r1,c1);
			m2=_mm256_maddubs_epi16(r2,c0);

			//make 16-bit values 32-bit (horizontal add)
			__m256i ones=_mm256_set1_epi16(1);
			m0=_mm256_madd_epi16(m0,ones);
			m1=_mm256_madd_epi16(m1,ones);
			m2=_mm256_madd_epi16(m2,ones);

			//vertical add
			m0=_mm256_add_epi32(m0,m1);
			m0=_mm256_add_epi32(m0,m2);

			//m0 has 8 32bit values now.
			output1 = division_32(division_case,m0,f);


			//2nd col iteration
			//multiply with the mask
			m0=_mm256_maddubs_epi16(r0,c0_sh1);
			m1=_mm256_maddubs_epi16(r1,c1_sh1);
			m2=_mm256_maddubs_epi16(r2,c0_sh1);

			//make 16-bit values 32-bit (horizontal add)
			m0=_mm256_madd_epi16(m0,ones);
			m1=_mm256_madd_epi16(m1,ones);
			m2=_mm256_madd_epi16(m2,ones);

			//vertical add
			m0=_mm256_add_epi32(m0,m1);
			m0=_mm256_add_epi32(m0,m2);

			//m0 has 8 32bit values now.
			output2 = division_32(division_case,m0,f);


	         //3rd col iteration
			//multiply with the mask
			m0=_mm256_maddubs_epi16(r0,c0_sh2);
			m1=_mm256_maddubs_epi16(r1,c1_sh2);
			m2=_mm256_maddubs_epi16(r2,c0_sh2);

			//shift m0 by one position because hadd follows
			m3=_mm256_srli_si256(m0,2);
			m4=_mm256_and_si256(m0,mask3);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
			m0=_mm256_add_epi16(m3,m4);

			//shift m1 by one position because hadd follows
			m3=_mm256_srli_si256(m1,2);
			m4=_mm256_and_si256(m1,mask3);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
			m1=_mm256_add_epi16(m3,m4);

			//shift m2 by one position because hadd follows
			m3=_mm256_srli_si256(m2,2);
			m4=_mm256_and_si256(m2,mask3);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
			m2=_mm256_add_epi16(m3,m4);

			//make 16-bit values 32-bit (horizontal add)
			m0=_mm256_madd_epi16(m0,ones);
			m1=_mm256_madd_epi16(m1,ones);
			m2=_mm256_madd_epi16(m2,ones);

			//vertical add
			m0=_mm256_add_epi32(m0,m1);
			m0=_mm256_add_epi32(m0,m2);

			//m0 has 8 32bit values now.
			output3 = division_32(division_case,m0,f);

			//4th col iteration

			//multiply with the mask
			m0=_mm256_maddubs_epi16(r0,c0_sh3);
			m1=_mm256_maddubs_epi16(r1,c1_sh3);
			m2=_mm256_maddubs_epi16(r2,c0_sh3);

			//shift m0 by one position because hadd follows
			m3=_mm256_srli_si256(m0,2);
			m4=_mm256_and_si256(m0,mask3);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
			m0=_mm256_add_epi16(m3,m4);

			//shift m1 by one position because hadd follows
			m3=_mm256_srli_si256(m1,2);
			m4=_mm256_and_si256(m1,mask3);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
			m1=_mm256_add_epi16(m3,m4);

			//shift m2 by one position because hadd follows
			m3=_mm256_srli_si256(m2,2);
			m4=_mm256_and_si256(m2,mask3);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
			m2=_mm256_add_epi16(m3,m4);

			//make 16-bit values 32-bit (horizontal add)
			m0=_mm256_madd_epi16(m0,ones);
			m1=_mm256_madd_epi16(m1,ones);
			m2=_mm256_madd_epi16(m2,ones);

			//vertical add
			m0=_mm256_add_epi32(m0,m1);
			m0=_mm256_add_epi32(m0,m2);

			//m0 has 8 32bit values now.
			output4 = division_32(division_case,m0,f);

			//the 32-bit division generates 8-bit values that are stored into memory.

			//blend output1, output2, output3, output4 into one register
			output2 = _mm256_slli_si256(output2,1);
			output1 = _mm256_add_epi8(output1,output2);
			output3 = _mm256_slli_si256(output3,2);
			output1 = _mm256_add_epi8(output1,output3);
			output4 = _mm256_slli_si256(output4,3);
			output1 = _mm256_add_epi8(output1,output4);

			_mm256_storeu_si256( (__m256i *) &filt[1][col],output1 );

		 //row==0
			//1st col iteration

			//multiply with the mask
			m0=_mm256_maddubs_epi16(r0,c1);
			m1=_mm256_maddubs_epi16(r1,c0);

			//make 16-bit values 32-bit (horizontal add)
			ones=_mm256_set1_epi16(1);
			m0=_mm256_madd_epi16(m0,ones);
			m1=_mm256_madd_epi16(m1,ones);

			//vertical add
			m0=_mm256_add_epi32(m0,m1);

			//m0 has 8 32bit values now.
			output1 = division_32(division_case,m0,f);


			//2nd col iteration
			//multiply with the mask
			m0=_mm256_maddubs_epi16(r0,c1_sh1);
			m1=_mm256_maddubs_epi16(r1,c0_sh1);

			//make 16-bit values 32-bit (horizontal add)
			m0=_mm256_madd_epi16(m0,ones);
			m1=_mm256_madd_epi16(m1,ones);

			//vertical add
			m0=_mm256_add_epi32(m0,m1);

			//m0 has 8 32bit values now.
			output2 = division_32(division_case,m0,f);


	         //3rd col iteration
			//multiply with the mask
			m0=_mm256_maddubs_epi16(r0,c1_sh2);
			m1=_mm256_maddubs_epi16(r1,c0_sh2);

			//shift m0 by one position because hadd follows
			m3=_mm256_srli_si256(m0,2);
			m4=_mm256_and_si256(m0,mask3);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
			m0=_mm256_add_epi16(m3,m4);

			//shift m1 by one position because hadd follows
			m3=_mm256_srli_si256(m1,2);
			m4=_mm256_and_si256(m1,mask3);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
			m1=_mm256_add_epi16(m3,m4);


			//make 16-bit values 32-bit (horizontal add)
			m0=_mm256_madd_epi16(m0,ones);
			m1=_mm256_madd_epi16(m1,ones);

			//vertical add
			m0=_mm256_add_epi32(m0,m1);

			//m0 has 8 32bit values now.
			output3 = division_32(division_case,m0,f);

			//4th col iteration

			//multiply with the mask
			m0=_mm256_maddubs_epi16(r0,c1_sh3);
			m1=_mm256_maddubs_epi16(r1,c0_sh3);

			//shift m0 by one position because hadd follows
			m3=_mm256_srli_si256(m0,2);
			m4=_mm256_and_si256(m0,mask3);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
			m0=_mm256_add_epi16(m3,m4);

			//shift m1 by one position because hadd follows
			m3=_mm256_srli_si256(m1,2);
			m4=_mm256_and_si256(m1,mask3);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
			m1=_mm256_add_epi16(m3,m4);


			//make 16-bit values 32-bit (horizontal add)
			m0=_mm256_madd_epi16(m0,ones);
			m1=_mm256_madd_epi16(m1,ones);

			//vertical add
			m0=_mm256_add_epi32(m0,m1);

			//m0 has 8 32bit values now.
			output4 = division_32(division_case,m0,f);

			//the 32-bit division generates 8-bit values that are stored into memory.

			//blend output1, output2, output3, output4 into one register
			output2 = _mm256_slli_si256(output2,1);
			output1 = _mm256_add_epi8(output1,output2);
			output3 = _mm256_slli_si256(output3,2);
			output1 = _mm256_add_epi8(output1,output3);
			output4 = _mm256_slli_si256(output4,3);
			output1 = _mm256_add_epi8(output1,output4);

			_mm256_storeu_si256( (__m256i *) &filt[0][col],output1 );
		}
	loop_reminder_3x3_first_values_32(frame1,filt,M,N,col,REMINDER_ITERATIONS,division_case,divisor,filter,c0,c1,c0_sh1,c1_sh1,c0_sh2,c1_sh2,c0_sh3,c1_sh3,f);

	}

	else if (row==N-2){//special case compute filt[N-2:N-1][:]
		for (col = 0; col <= M-32; col+=30){

		  if (col==0){

				//load the 5 rows
				r0=_mm256_loadu_si256( (__m256i *) &frame1[N-3][0]);
				r1=_mm256_loadu_si256( (__m256i *) &frame1[N-2][0]);
				r2=_mm256_loadu_si256( (__m256i *) &frame1[N-1][0]);


		  			//START - extra code needed for prelude
		  					m0=_mm256_slli_si256(r0,1);//shift 1 elements left - equivalent to filling with one zero inthe beginning
		  					m1=_mm256_slli_si256(r1,1);//shift 1 elements left - equivalent to filling with one zero inthe beginning
		  					m2=_mm256_slli_si256(r2,1);//shift 1 elements left - equivalent to filling with one zero inthe beginning

		  					//preserve the element being lost because of the shift above
		  					r0=_mm256_and_si256(r0,mask_prelude);
		  					r0=_mm256_permute2f128_si256(r0,r0,1);
		  					r0=_mm256_srli_si256(r0,15);//shift 15 elements
		  					r0=_mm256_add_epi16(m0,r0);

		  					r1=_mm256_and_si256(r1,mask_prelude);
		  					r1=_mm256_permute2f128_si256(r1,r1,1);
		  					r1=_mm256_srli_si256(r1,15);//shift 15 elements
		  					r1=_mm256_add_epi16(m1,r1);

		  					r2=_mm256_and_si256(r2,mask_prelude);
		  					r2=_mm256_permute2f128_si256(r2,r2,1);
		  					r2=_mm256_srli_si256(r2,15);//shift 15 elements
		  					r2=_mm256_add_epi16(m2,r2);

		  			//END - extra code needed for prelude
		  		}
		  else {
				//load the 5 rows
				r0=_mm256_loadu_si256( (__m256i *) &frame1[N-3][col-1]);
				r1=_mm256_loadu_si256( (__m256i *) &frame1[N-2][col-1]);
				r2=_mm256_loadu_si256( (__m256i *) &frame1[N-1][col-1]);
		  }

		//col iteration computes output pixels of    1,5,9,13,17,21,25,29
		// col+1 iteration computes output pixels of 2,6,10,14,18,22,26,30
		// col+2 iteration computes output pixels of 3,7,11,15,19,23,27
		// col+3 iteration computes output pixels of 4,8,12,16,20,24,28
		//afterwards, col becomes 30 and repeat the above process

		 //row==N-2
			//1st col iteration

			//multiply with the mask
			m0=_mm256_maddubs_epi16(r0,c0);
			m1=_mm256_maddubs_epi16(r1,c1);
			m2=_mm256_maddubs_epi16(r2,c0);

			//make 16-bit values 32-bit (horizontal add)
			__m256i ones=_mm256_set1_epi16(1);
			m0=_mm256_madd_epi16(m0,ones);
			m1=_mm256_madd_epi16(m1,ones);
			m2=_mm256_madd_epi16(m2,ones);

			//vertical add
			m0=_mm256_add_epi32(m0,m1);
			m0=_mm256_add_epi32(m0,m2);

			//m0 has 8 32bit values now.
			output1 = division_32(division_case,m0,f);


			//2nd col iteration
			//multiply with the mask
			m0=_mm256_maddubs_epi16(r0,c0_sh1);
			m1=_mm256_maddubs_epi16(r1,c1_sh1);
			m2=_mm256_maddubs_epi16(r2,c0_sh1);

			//make 16-bit values 32-bit (horizontal add)
			m0=_mm256_madd_epi16(m0,ones);
			m1=_mm256_madd_epi16(m1,ones);
			m2=_mm256_madd_epi16(m2,ones);

			//vertical add
			m0=_mm256_add_epi32(m0,m1);
			m0=_mm256_add_epi32(m0,m2);

			//m0 has 8 32bit values now.
			output2 = division_32(division_case,m0,f);


	         //3rd col iteration
			//multiply with the mask
			m0=_mm256_maddubs_epi16(r0,c0_sh2);
			m1=_mm256_maddubs_epi16(r1,c1_sh2);
			m2=_mm256_maddubs_epi16(r2,c0_sh2);

			//shift m0 by one position because hadd follows
			m3=_mm256_srli_si256(m0,2);
			m4=_mm256_and_si256(m0,mask3);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
			m0=_mm256_add_epi16(m3,m4);

			//shift m1 by one position because hadd follows
			m3=_mm256_srli_si256(m1,2);
			m4=_mm256_and_si256(m1,mask3);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
			m1=_mm256_add_epi16(m3,m4);

			//shift m2 by one position because hadd follows
			m3=_mm256_srli_si256(m2,2);
			m4=_mm256_and_si256(m2,mask3);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
			m2=_mm256_add_epi16(m3,m4);

			//make 16-bit values 32-bit (horizontal add)
			m0=_mm256_madd_epi16(m0,ones);
			m1=_mm256_madd_epi16(m1,ones);
			m2=_mm256_madd_epi16(m2,ones);

			//vertical add
			m0=_mm256_add_epi32(m0,m1);
			m0=_mm256_add_epi32(m0,m2);

			//m0 has 8 32bit values now.
			output3 = division_32(division_case,m0,f);

			//4th col iteration

			//multiply with the mask
			m0=_mm256_maddubs_epi16(r0,c0_sh3);
			m1=_mm256_maddubs_epi16(r1,c1_sh3);
			m2=_mm256_maddubs_epi16(r2,c0_sh3);

			//shift m0 by one position because hadd follows
			m3=_mm256_srli_si256(m0,2);
			m4=_mm256_and_si256(m0,mask3);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
			m0=_mm256_add_epi16(m3,m4);

			//shift m1 by one position because hadd follows
			m3=_mm256_srli_si256(m1,2);
			m4=_mm256_and_si256(m1,mask3);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
			m1=_mm256_add_epi16(m3,m4);

			//shift m2 by one position because hadd follows
			m3=_mm256_srli_si256(m2,2);
			m4=_mm256_and_si256(m2,mask3);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
			m2=_mm256_add_epi16(m3,m4);

			//make 16-bit values 32-bit (horizontal add)
			m0=_mm256_madd_epi16(m0,ones);
			m1=_mm256_madd_epi16(m1,ones);
			m2=_mm256_madd_epi16(m2,ones);

			//vertical add
			m0=_mm256_add_epi32(m0,m1);
			m0=_mm256_add_epi32(m0,m2);

			//m0 has 8 32bit values now.
			output4 = division_32(division_case,m0,f);

			//the 32-bit division generates 8-bit values that are stored into memory.

			//blend output1, output2, output3, output4 into one register
			output2 = _mm256_slli_si256(output2,1);
			output1 = _mm256_add_epi8(output1,output2);
			output3 = _mm256_slli_si256(output3,2);
			output1 = _mm256_add_epi8(output1,output3);
			output4 = _mm256_slli_si256(output4,3);
			output1 = _mm256_add_epi8(output1,output4);

			_mm256_storeu_si256( (__m256i *) &filt[N-2][col],output1 );

		 //row==N-1
			//1st col iteration

			//multiply with the mask
			m0=_mm256_maddubs_epi16(r1,c0);
			m1=_mm256_maddubs_epi16(r2,c1);

			//make 16-bit values 32-bit (horizontal add)
			ones=_mm256_set1_epi16(1);
			m0=_mm256_madd_epi16(m0,ones);
			m1=_mm256_madd_epi16(m1,ones);

			//vertical add
			m0=_mm256_add_epi32(m0,m1);

			//m0 has 8 32bit values now.
			output1 = division_32(division_case,m0,f);


			//2nd col iteration
			//multiply with the mask
			m0=_mm256_maddubs_epi16(r1,c0_sh1);
			m1=_mm256_maddubs_epi16(r2,c1_sh1);

			//make 16-bit values 32-bit (horizontal add)
			m0=_mm256_madd_epi16(m0,ones);
			m1=_mm256_madd_epi16(m1,ones);

			//vertical add
			m0=_mm256_add_epi32(m0,m1);

			//m0 has 8 32bit values now.
			output2 = division_32(division_case,m0,f);


	         //3rd col iteration
			//multiply with the mask
			m0=_mm256_maddubs_epi16(r1,c0_sh2);
			m1=_mm256_maddubs_epi16(r2,c1_sh2);

			//shift m0 by one position because hadd follows
			m3=_mm256_srli_si256(m0,2);
			m4=_mm256_and_si256(m0,mask3);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
			m0=_mm256_add_epi16(m3,m4);

			//shift m1 by one position because hadd follows
			m3=_mm256_srli_si256(m1,2);
			m4=_mm256_and_si256(m1,mask3);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
			m1=_mm256_add_epi16(m3,m4);


			//make 16-bit values 32-bit (horizontal add)
			m0=_mm256_madd_epi16(m0,ones);
			m1=_mm256_madd_epi16(m1,ones);

			//vertical add
			m0=_mm256_add_epi32(m0,m1);

			//m0 has 8 32bit values now.
			output3 = division_32(division_case,m0,f);

			//4th col iteration

			//multiply with the mask
			m0=_mm256_maddubs_epi16(r1,c0_sh3);
			m1=_mm256_maddubs_epi16(r2,c1_sh3);

			//shift m0 by one position because hadd follows
			m3=_mm256_srli_si256(m0,2);
			m4=_mm256_and_si256(m0,mask3);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
			m0=_mm256_add_epi16(m3,m4);

			//shift m1 by one position because hadd follows
			m3=_mm256_srli_si256(m1,2);
			m4=_mm256_and_si256(m1,mask3);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
			m1=_mm256_add_epi16(m3,m4);


			//make 16-bit values 32-bit (horizontal add)
			m0=_mm256_madd_epi16(m0,ones);
			m1=_mm256_madd_epi16(m1,ones);

			//vertical add
			m0=_mm256_add_epi32(m0,m1);

			//m0 has 8 32bit values now.
			output4 = division_32(division_case,m0,f);

			//the 32-bit division generates 8-bit values that are stored into memory.

			//blend output1, output2, output3, output4 into one register
			output2 = _mm256_slli_si256(output2,1);
			output1 = _mm256_add_epi8(output1,output2);
			output3 = _mm256_slli_si256(output3,2);
			output1 = _mm256_add_epi8(output1,output3);
			output4 = _mm256_slli_si256(output4,3);
			output1 = _mm256_add_epi8(output1,output4);

			_mm256_storeu_si256( (__m256i *) &filt[N-1][col],output1 );
		}
	loop_reminder_3x3_last_values_32(frame1,filt,M,N,col,REMINDER_ITERATIONS,division_case,divisor,filter,c0,c1,c0_sh1,c1_sh1,c0_sh2,c1_sh2,c0_sh3,c1_sh3,f);

	}

	else {//main loop

	  for (col = 0; col <= M-32; col+=30){

		  if (col==0){

				//load the 5 rows
				r0=_mm256_loadu_si256( (__m256i *) &frame1[row-1][0]);
				r1=_mm256_loadu_si256( (__m256i *) &frame1[row][0]);
				r2=_mm256_loadu_si256( (__m256i *) &frame1[row+1][0]);


		  			//START - extra code needed for prelude
		  					m0=_mm256_slli_si256(r0,1);//shift 1 elements left - equivalent to filling with one zero inthe beginning
		  					m1=_mm256_slli_si256(r1,1);//shift 1 elements left - equivalent to filling with one zero inthe beginning
		  					m2=_mm256_slli_si256(r2,1);//shift 1 elements left - equivalent to filling with one zero inthe beginning

		  					//preserve the element being lost because of the shift above
		  					r0=_mm256_and_si256(r0,mask_prelude);
		  					r0=_mm256_permute2f128_si256(r0,r0,1);
		  					r0=_mm256_srli_si256(r0,15);//shift 15 elements
		  					r0=_mm256_add_epi16(m0,r0);

		  					r1=_mm256_and_si256(r1,mask_prelude);
		  					r1=_mm256_permute2f128_si256(r1,r1,1);
		  					r1=_mm256_srli_si256(r1,15);//shift 15 elements
		  					r1=_mm256_add_epi16(m1,r1);

		  					r2=_mm256_and_si256(r2,mask_prelude);
		  					r2=_mm256_permute2f128_si256(r2,r2,1);
		  					r2=_mm256_srli_si256(r2,15);//shift 15 elements
		  					r2=_mm256_add_epi16(m2,r2);

		  			//END - extra code needed for prelude
		  		}
		  else {
				//load the 5 rows
				r0=_mm256_loadu_si256( (__m256i *) &frame1[row-1][col-1]);
				r1=_mm256_loadu_si256( (__m256i *) &frame1[row][col-1]);
				r2=_mm256_loadu_si256( (__m256i *) &frame1[row+1][col-1]);
		  }

		//col iteration computes output pixels of    1,5,9,13,17,21,25,29
		// col+1 iteration computes output pixels of 2,6,10,14,18,22,26,30
		// col+2 iteration computes output pixels of 3,7,11,15,19,23,27
		// col+3 iteration computes output pixels of 4,8,12,16,20,24,28
		//afterwards, col becomes 30 and repeat the above process

		//1st col iteration

		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,c0);
		m1=_mm256_maddubs_epi16(r1,c1);
		m2=_mm256_maddubs_epi16(r2,c0);

		//make 16-bit values 32-bit (horizontal add)
		__m256i ones=_mm256_set1_epi16(1);
		m0=_mm256_madd_epi16(m0,ones);
		m1=_mm256_madd_epi16(m1,ones);
		m2=_mm256_madd_epi16(m2,ones);


		//vertical add
		m0=_mm256_add_epi32(m0,m1);
		m0=_mm256_add_epi32(m0,m2);


		//m0 has 8 32bit values now.
		output1 = division_32(division_case,m0,f);


		//2nd col iteration
		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,c0_sh1);
		m1=_mm256_maddubs_epi16(r1,c1_sh1);
		m2=_mm256_maddubs_epi16(r2,c0_sh1);

		//make 16-bit values 32-bit (horizontal add)
		m0=_mm256_madd_epi16(m0,ones);
		m1=_mm256_madd_epi16(m1,ones);
		m2=_mm256_madd_epi16(m2,ones);

		//vertical add
		m0=_mm256_add_epi32(m0,m1);
		m0=_mm256_add_epi32(m0,m2);

		//m0 has 8 32bit values now.
		output2 = division_32(division_case,m0,f);


         //3rd col iteration
		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,c0_sh2);
		m1=_mm256_maddubs_epi16(r1,c1_sh2);
		m2=_mm256_maddubs_epi16(r2,c0_sh2);

		//shift m0 by one position because hadd follows
		m3=_mm256_srli_si256(m0,2);
		m4=_mm256_and_si256(m0,mask3);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
		m0=_mm256_add_epi16(m3,m4);

		//shift m1 by one position because hadd follows
		m3=_mm256_srli_si256(m1,2);
		m4=_mm256_and_si256(m1,mask3);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
		m1=_mm256_add_epi16(m3,m4);

		//shift m2 by one position because hadd follows
		m3=_mm256_srli_si256(m2,2);
		m4=_mm256_and_si256(m2,mask3);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
		m2=_mm256_add_epi16(m3,m4);

		//make 16-bit values 32-bit (horizontal add)
		m0=_mm256_madd_epi16(m0,ones);
		m1=_mm256_madd_epi16(m1,ones);
		m2=_mm256_madd_epi16(m2,ones);

		//vertical add
		m0=_mm256_add_epi32(m0,m1);
		m0=_mm256_add_epi32(m0,m2);

		//m0 has 8 32bit values now.
		output3 = division_32(division_case,m0,f);

		//4th col iteration

		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,c0_sh3);
		m1=_mm256_maddubs_epi16(r1,c1_sh3);
		m2=_mm256_maddubs_epi16(r2,c0_sh3);

		//shift m0 by one position because hadd follows
		m3=_mm256_srli_si256(m0,2);
		m4=_mm256_and_si256(m0,mask3);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
		m0=_mm256_add_epi16(m3,m4);

		//shift m1 by one position because hadd follows
		m3=_mm256_srli_si256(m1,2);
		m4=_mm256_and_si256(m1,mask3);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
		m1=_mm256_add_epi16(m3,m4);

		//shift m2 by one position because hadd follows
		m3=_mm256_srli_si256(m2,2);
		m4=_mm256_and_si256(m2,mask3);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
		m2=_mm256_add_epi16(m3,m4);

		//make 16-bit values 32-bit (horizontal add)
		m0=_mm256_madd_epi16(m0,ones);
		m1=_mm256_madd_epi16(m1,ones);
		m2=_mm256_madd_epi16(m2,ones);

		//vertical add
		m0=_mm256_add_epi32(m0,m1);
		m0=_mm256_add_epi32(m0,m2);

		//m0 has 8 32bit values now.
		output4 = division_32(division_case,m0,f);

		//the 32-bit division generates 8-bit values that are stored into memory.

		//blend output1, output2, output3, output4 into one register
		output2 = _mm256_slli_si256(output2,1);
		output1 = _mm256_add_epi8(output1,output2);
		output3 = _mm256_slli_si256(output3,2);
		output1 = _mm256_add_epi8(output1,output3);
		output4 = _mm256_slli_si256(output4,3);
		output1 = _mm256_add_epi8(output1,output4);

		_mm256_storeu_si256( (__m256i *) &filt[row][col],output1 );


		}
	  loop_reminder_3x3_32(frame1,filt,M,N,row,col,REMINDER_ITERATIONS,division_case,divisor,filter,c0,c1,c0_sh1,c1_sh1,c0_sh2,c1_sh2,c0_sh3,c1_sh3,f);

	}

}

}//end of parallel


}



int loop_reminder_3x3_32(unsigned char **frame1,unsigned char **filt,const unsigned int M, const unsigned int N,const unsigned int row, const unsigned int col,const unsigned int REMINDER_ITERATIONS,const unsigned int division_case,const unsigned short int divisor,signed char **filter,const __m256i c0,const __m256i c1,const __m256i c0_sh1,const __m256i c1_sh1,const __m256i c0_sh2,const __m256i c1_sh2, const __m256i c0_sh3,const __m256i c1_sh3,const __m256i f){

register	__m256i r0,r1,r2,m0,m1,m2,m3,m4,output1,output2,output3,output4;


const __m256i mask3  = _mm256_set_epi16(0,0,0,0,0,0,0,65535,0,0,0,0,0,0,0,0);


 __m256i reminder_mask1;

if (REMINDER_ITERATIONS == 0){
	return 0; //no loop reminder is needed
}

	reminder_mask1=_mm256_load_si256( (__m256i *) &reminder_msk1_3x3[REMINDER_ITERATIONS-1][0]);
	//reminder_mask2=_mm256_load_si256( (__m256i *) &reminder_msk2[REMINDER_ITERATIONS-1][0]);


	 //1st col iteration
	//load the 5 rows
	r0=_mm256_loadu_si256( (__m256i *) &frame1[row-1][col-1]);
	r1=_mm256_loadu_si256( (__m256i *) &frame1[row][col-1]);
	r2=_mm256_loadu_si256( (__m256i *) &frame1[row+1][col-1]);


	/*
	 * The above load operations load outside of the array bounds; these elements are filled with zeros just after
	 * Furthermore, I add one extra zero in the end to compute col=N-1. The last value of col is N-1, not N
	 */

	//AND r0-r4 with reminder_mask
	r0=_mm256_and_si256(r0,reminder_mask1);
	r1=_mm256_and_si256(r1,reminder_mask1);
	r2=_mm256_and_si256(r2,reminder_mask1);


	//col iteration computes output pixels of    1,5,9,13,17,21,25,29
	// col+1 iteration computes output pixels of 2,6,10,14,18,22,26,30
	// col+2 iteration computes output pixels of 3,7,11,15,19,23,27
	// col+3 iteration computes output pixels of 4,8,12,16,20,24,28
	//afterwards, col becomes 30 and repeat the above process

	//1st col iteration

	//multiply with the mask
	m0=_mm256_maddubs_epi16(r0,c0);
	m1=_mm256_maddubs_epi16(r1,c1);
	m2=_mm256_maddubs_epi16(r2,c0);

	//make 16-bit values 32-bit (horizontal add)
	__m256i ones=_mm256_set1_epi16(1);
	m0=_mm256_madd_epi16(m0,ones);
	m1=_mm256_madd_epi16(m1,ones);
	m2=_mm256_madd_epi16(m2,ones);

	//vertical add
	m0=_mm256_add_epi32(m0,m1);
	m0=_mm256_add_epi32(m0,m2);

	//m0 has 8 32bit values now.
	output1 = division_32(division_case,m0,f);


	//2nd col iteration
	//multiply with the mask
	m0=_mm256_maddubs_epi16(r0,c0_sh1);
	m1=_mm256_maddubs_epi16(r1,c1_sh1);
	m2=_mm256_maddubs_epi16(r2,c0_sh1);

	//make 16-bit values 32-bit (horizontal add)
	m0=_mm256_madd_epi16(m0,ones);
	m1=_mm256_madd_epi16(m1,ones);
	m2=_mm256_madd_epi16(m2,ones);

	//vertical add
	m0=_mm256_add_epi32(m0,m1);
	m0=_mm256_add_epi32(m0,m2);

	//m0 has 8 32bit values now.
	output2 = division_32(division_case,m0,f);


     //3rd col iteration
	//multiply with the mask
	m0=_mm256_maddubs_epi16(r0,c0_sh2);
	m1=_mm256_maddubs_epi16(r1,c1_sh2);
	m2=_mm256_maddubs_epi16(r2,c0_sh2);

	//shift m0 by one position because hadd follows
	m3=_mm256_srli_si256(m0,2);
	m4=_mm256_and_si256(m0,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
	m0=_mm256_add_epi16(m3,m4);

	//shift m1 by one position because hadd follows
	m3=_mm256_srli_si256(m1,2);
	m4=_mm256_and_si256(m1,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
	m1=_mm256_add_epi16(m3,m4);

	//shift m2 by one position because hadd follows
	m3=_mm256_srli_si256(m2,2);
	m4=_mm256_and_si256(m2,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
	m2=_mm256_add_epi16(m3,m4);

	//make 16-bit values 32-bit (horizontal add)
	m0=_mm256_madd_epi16(m0,ones);
	m1=_mm256_madd_epi16(m1,ones);
	m2=_mm256_madd_epi16(m2,ones);

	//vertical add
	m0=_mm256_add_epi32(m0,m1);
	m0=_mm256_add_epi32(m0,m2);

	//m0 has 8 32bit values now.
	output3 = division_32(division_case,m0,f);

	//4th col iteration

	//multiply with the mask
	m0=_mm256_maddubs_epi16(r0,c0_sh3);
	m1=_mm256_maddubs_epi16(r1,c1_sh3);
	m2=_mm256_maddubs_epi16(r2,c0_sh3);

	//shift m0 by one position because hadd follows
	m3=_mm256_srli_si256(m0,2);
	m4=_mm256_and_si256(m0,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
	m0=_mm256_add_epi16(m3,m4);

	//shift m1 by one position because hadd follows
	m3=_mm256_srli_si256(m1,2);
	m4=_mm256_and_si256(m1,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
	m1=_mm256_add_epi16(m3,m4);

	//shift m2 by one position because hadd follows
	m3=_mm256_srli_si256(m2,2);
	m4=_mm256_and_si256(m2,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
	m2=_mm256_add_epi16(m3,m4);

	//make 16-bit values 32-bit (horizontal add)
	m0=_mm256_madd_epi16(m0,ones);
	m1=_mm256_madd_epi16(m1,ones);
	m2=_mm256_madd_epi16(m2,ones);

	//vertical add
	m0=_mm256_add_epi32(m0,m1);
	m0=_mm256_add_epi32(m0,m2);

	//m0 has 8 32bit values now.
	output4 = division_32(division_case,m0,f);

	//the 32-bit division generates 8-bit values that are stored into memory.

	//blend output1, output2, output3, output4 into one register
	output2 = _mm256_slli_si256(output2,1);
	output1 = _mm256_add_epi8(output1,output2);
	output3 = _mm256_slli_si256(output3,2);
	output1 = _mm256_add_epi8(output1,output3);
	output4 = _mm256_slli_si256(output4,3);
	output1 = _mm256_add_epi8(output1,output4);




 	switch (REMINDER_ITERATIONS){
	case 1:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
	  break;
	case 2:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		  break;
	case 3:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		  break;
	case 4:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output1,3);
		  break;
	case 5:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output1,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output1,4);
		  break;
	case 6:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output1,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output1,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output1,5);
		  break;
	case 7:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output1,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output1,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output1,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output1,6);
		  break;
	case 8:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output1,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output1,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output1,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output1,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output1,7);
		  break;
	case 9:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output1,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output1,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output1,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output1,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output1,7);
		filt[row][col+8] = (unsigned char) _mm256_extract_epi8(output1,8);
		  break;
	case 10:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output1,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output1,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output1,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output1,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output1,7);
		filt[row][col+8] = (unsigned char) _mm256_extract_epi8(output1,8);
		filt[row][col+9] = (unsigned char) _mm256_extract_epi8(output1,9);
		  break;
	case 11:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output1,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output1,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output1,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output1,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output1,7);
		filt[row][col+8] = (unsigned char) _mm256_extract_epi8(output1,8);
		filt[row][col+9] = (unsigned char) _mm256_extract_epi8(output1,9);
		filt[row][col+10] = (unsigned char) _mm256_extract_epi8(output1,10);
		  break;
	case 12:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output1,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output1,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output1,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output1,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output1,7);
		filt[row][col+8] = (unsigned char) _mm256_extract_epi8(output1,8);
		filt[row][col+9] = (unsigned char) _mm256_extract_epi8(output1,9);
		filt[row][col+10] = (unsigned char) _mm256_extract_epi8(output1,10);
		filt[row][col+11] = (unsigned char) _mm256_extract_epi8(output1,11);
		  break;
	case 13:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output1,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output1,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output1,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output1,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output1,7);
		filt[row][col+8] = (unsigned char) _mm256_extract_epi8(output1,8);
		filt[row][col+9] = (unsigned char) _mm256_extract_epi8(output1,9);
		filt[row][col+10] = (unsigned char) _mm256_extract_epi8(output1,10);
		filt[row][col+11] = (unsigned char) _mm256_extract_epi8(output1,11);
		filt[row][col+12] = (unsigned char) _mm256_extract_epi8(output1,12);
		  break;
	case 14:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output1,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output1,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output1,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output1,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output1,7);
		filt[row][col+8] = (unsigned char) _mm256_extract_epi8(output1,8);
		filt[row][col+9] = (unsigned char) _mm256_extract_epi8(output1,9);
		filt[row][col+10] = (unsigned char) _mm256_extract_epi8(output1,10);
		filt[row][col+11] = (unsigned char) _mm256_extract_epi8(output1,11);
		filt[row][col+12] = (unsigned char) _mm256_extract_epi8(output1,12);
		filt[row][col+13] = (unsigned char) _mm256_extract_epi8(output1,13);
		  break;
	case 15:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output1,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output1,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output1,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output1,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output1,7);
		filt[row][col+8] = (unsigned char) _mm256_extract_epi8(output1,8);
		filt[row][col+9] = (unsigned char) _mm256_extract_epi8(output1,9);
		filt[row][col+10] = (unsigned char) _mm256_extract_epi8(output1,10);
		filt[row][col+11] = (unsigned char) _mm256_extract_epi8(output1,11);
		filt[row][col+12] = (unsigned char) _mm256_extract_epi8(output1,12);
		filt[row][col+13] = (unsigned char) _mm256_extract_epi8(output1,13);
		filt[row][col+14] = (unsigned char) _mm256_extract_epi8(output1,14);
		  break;
	case 16:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
		  break;
	case 17:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
		  break;
	case 18:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
		  break;
	case 19:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
		  break;
	case 20:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output1,19);
		  break;
	case 21:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output1,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output1,20);
		  break;
	case 22:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output1,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output1,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output1,21);
		break;
	case 23:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output1,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output1,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output1,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output1,22);
		  break;
	case 24:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output1,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output1,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output1,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output1,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output1,23);
		  break;
	case 25:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output1,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output1,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output1,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output1,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output1,23);
	filt[row][col+24] = (unsigned char) _mm256_extract_epi8(output1,24);
		  break;
	case 26:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output1,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output1,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output1,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output1,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output1,23);
	filt[row][col+24] = (unsigned char) _mm256_extract_epi8(output1,24);
	filt[row][col+25] = (unsigned char) _mm256_extract_epi8(output1,25);
		  break;
	case 27:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output1,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output1,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output1,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output1,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output1,23);
	filt[row][col+24] = (unsigned char) _mm256_extract_epi8(output1,24);
	filt[row][col+25] = (unsigned char) _mm256_extract_epi8(output1,25);
	filt[row][col+26] = (unsigned char) _mm256_extract_epi8(output1,26);
		  break;
	case 28:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output1,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output1,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output1,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output1,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output1,23);
	filt[row][col+24] = (unsigned char) _mm256_extract_epi8(output1,24);
	filt[row][col+25] = (unsigned char) _mm256_extract_epi8(output1,25);
	filt[row][col+26] = (unsigned char) _mm256_extract_epi8(output1,26);
	filt[row][col+27] = (unsigned char) _mm256_extract_epi8(output1,27);
		  break;
	case 29:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output1,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output1,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output1,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output1,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output1,23);
	filt[row][col+24] = (unsigned char) _mm256_extract_epi8(output1,24);
	filt[row][col+25] = (unsigned char) _mm256_extract_epi8(output1,25);
	filt[row][col+26] = (unsigned char) _mm256_extract_epi8(output1,26);
	filt[row][col+27] = (unsigned char) _mm256_extract_epi8(output1,27);
	filt[row][col+28] = (unsigned char) _mm256_extract_epi8(output1,28);
		  break;
	case 30:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output1,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output1,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output1,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output1,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output1,23);
	filt[row][col+24] = (unsigned char) _mm256_extract_epi8(output1,24);
	filt[row][col+25] = (unsigned char) _mm256_extract_epi8(output1,25);
	filt[row][col+26] = (unsigned char) _mm256_extract_epi8(output1,26);
	filt[row][col+27] = (unsigned char) _mm256_extract_epi8(output1,27);
	filt[row][col+28] = (unsigned char) _mm256_extract_epi8(output1,28);

	//the filt[row][col+29] is computed unvectorized
	int newPixel = 0;
	newPixel += frame1[row-1][M-1-1] * filter[0][0];
	newPixel += frame1[row-1][M-1] * filter[0][1];

	newPixel += frame1[row][M-1-1] * filter[1][0];
	newPixel += frame1[row][M-1] * filter[1][1];

	newPixel += frame1[row+1][M-1-1] * filter[2][0];
	newPixel += frame1[row+1][M-1] * filter[2][1];

	filt[row][M-1] = (unsigned char) (newPixel / divisor);

		  break;

	case 31:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output1,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output1,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output1,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output1,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output1,23);
	filt[row][col+24] = (unsigned char) _mm256_extract_epi8(output1,24);
	filt[row][col+25] = (unsigned char) _mm256_extract_epi8(output1,25);
	filt[row][col+26] = (unsigned char) _mm256_extract_epi8(output1,26);
	filt[row][col+27] = (unsigned char) _mm256_extract_epi8(output1,27);
	filt[row][col+28] = (unsigned char) _mm256_extract_epi8(output1,28);

	//the filt[row][col+29/30] are computed unvectorized
	newPixel = 0;
	newPixel += frame1[row-1][M-2-1] * filter[0][0];
	newPixel += frame1[row-1][M-2] * filter[0][1];
	newPixel += frame1[row-1][M-2+1] * filter[0][2];


	newPixel += frame1[row][M-2-1] * filter[1][0];
	newPixel += frame1[row][M-2] * filter[1][1];
	newPixel += frame1[row][M-2+1] * filter[1][2];


	newPixel += frame1[row+1][M-2-1] * filter[2][0];
	newPixel += frame1[row+1][M-2] * filter[2][1];
	newPixel += frame1[row+1][M-2+1] * filter[2][2];

	filt[row][M-2] = (unsigned char) (newPixel / divisor);

	newPixel = 0;
	newPixel += frame1[row-1][M-1-1] * filter[0][0];
	newPixel += frame1[row-1][M-1] * filter[0][1];

	newPixel += frame1[row][M-1-1] * filter[1][0];
	newPixel += frame1[row][M-1] * filter[1][1];

	newPixel += frame1[row+1][M-1-1] * filter[2][0];
	newPixel += frame1[row+1][M-1] * filter[2][1];

	filt[row][M-1] = (unsigned char) (newPixel / divisor);
		  break;
	default:
		printf("\nsomething went wrong");
		return -1;
 	}

return 0;

}



int loop_reminder_3x3_first_values_32(unsigned char **frame1,unsigned char **filt,const unsigned int M, const unsigned int N, const unsigned int col,const unsigned int REMINDER_ITERATIONS,const unsigned int division_case,const unsigned short int divisor,signed char **filter,const __m256i c0,const __m256i c1,const __m256i c0_sh1,const __m256i c1_sh1,const __m256i c0_sh2,const __m256i c1_sh2, const __m256i c0_sh3,const __m256i c1_sh3,const __m256i f){

register	__m256i r0,r1,r2,m0,m1,m2,m3,m4,output_row0,output_row1,output1,output2,output3,output4;
int newPixel;
unsigned int i;
const __m256i mask3  = _mm256_set_epi16(0,0,0,0,0,0,0,65535,0,0,0,0,0,0,0,0);

 __m256i reminder_mask1;

if (REMINDER_ITERATIONS == 0){
	return 0; //no loop reminder is needed
}

	reminder_mask1=_mm256_load_si256( (__m256i *) &reminder_msk1_3x3[REMINDER_ITERATIONS-1][0]);
	//reminder_mask2=_mm256_load_si256( (__m256i *) &reminder_msk2[REMINDER_ITERATIONS-1][0]);


	 //1st col iteration
	//load the 5 rows
	r0=_mm256_loadu_si256( (__m256i *) &frame1[0][col-1]);
	r1=_mm256_loadu_si256( (__m256i *) &frame1[1][col-1]);
	r2=_mm256_loadu_si256( (__m256i *) &frame1[2][col-1]);


	/*
	 * The above load operations load outside of the array bounds; these elements are filled with zeros just after
	 * Furthermore, I add one extra zero in the end to compute col=N-1. The last value of col is N-1, not N
	 */

	//AND r0-r4 with reminder_mask
	r0=_mm256_and_si256(r0,reminder_mask1);
	r1=_mm256_and_si256(r1,reminder_mask1);
	r2=_mm256_and_si256(r2,reminder_mask1);


	//col iteration computes output pixels of    1,5,9,13,17,21,25,29
	// col+1 iteration computes output pixels of 2,6,10,14,18,22,26,30
	// col+2 iteration computes output pixels of 3,7,11,15,19,23,27
	// col+3 iteration computes output pixels of 4,8,12,16,20,24,28
	//afterwards, col becomes 30 and repeat the above process

	 //row==1
			//1st col iteration

			//multiply with the mask
			m0=_mm256_maddubs_epi16(r0,c0);
			m1=_mm256_maddubs_epi16(r1,c1);
			m2=_mm256_maddubs_epi16(r2,c0);

			//make 16-bit values 32-bit (horizontal add)
			__m256i ones=_mm256_set1_epi16(1);
			m0=_mm256_madd_epi16(m0,ones);
			m1=_mm256_madd_epi16(m1,ones);
			m2=_mm256_madd_epi16(m2,ones);

			//vertical add
			m0=_mm256_add_epi32(m0,m1);
			m0=_mm256_add_epi32(m0,m2);

			//m0 has 8 32bit values now.
			output1 = division_32(division_case,m0,f);


			//2nd col iteration
			//multiply with the mask
			m0=_mm256_maddubs_epi16(r0,c0_sh1);
			m1=_mm256_maddubs_epi16(r1,c1_sh1);
			m2=_mm256_maddubs_epi16(r2,c0_sh1);

			//make 16-bit values 32-bit (horizontal add)
			m0=_mm256_madd_epi16(m0,ones);
			m1=_mm256_madd_epi16(m1,ones);
			m2=_mm256_madd_epi16(m2,ones);

			//vertical add
			m0=_mm256_add_epi32(m0,m1);
			m0=_mm256_add_epi32(m0,m2);

			//m0 has 8 32bit values now.
			output2 = division_32(division_case,m0,f);


	         //3rd col iteration
			//multiply with the mask
			m0=_mm256_maddubs_epi16(r0,c0_sh2);
			m1=_mm256_maddubs_epi16(r1,c1_sh2);
			m2=_mm256_maddubs_epi16(r2,c0_sh2);

			//shift m0 by one position because hadd follows
			m3=_mm256_srli_si256(m0,2);
			m4=_mm256_and_si256(m0,mask3);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
			m0=_mm256_add_epi16(m3,m4);

			//shift m1 by one position because hadd follows
			m3=_mm256_srli_si256(m1,2);
			m4=_mm256_and_si256(m1,mask3);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
			m1=_mm256_add_epi16(m3,m4);

			//shift m2 by one position because hadd follows
			m3=_mm256_srli_si256(m2,2);
			m4=_mm256_and_si256(m2,mask3);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
			m2=_mm256_add_epi16(m3,m4);

			//make 16-bit values 32-bit (horizontal add)
			m0=_mm256_madd_epi16(m0,ones);
			m1=_mm256_madd_epi16(m1,ones);
			m2=_mm256_madd_epi16(m2,ones);

			//vertical add
			m0=_mm256_add_epi32(m0,m1);
			m0=_mm256_add_epi32(m0,m2);

			//m0 has 8 32bit values now.
			output3 = division_32(division_case,m0,f);

			//4th col iteration

			//multiply with the mask
			m0=_mm256_maddubs_epi16(r0,c0_sh3);
			m1=_mm256_maddubs_epi16(r1,c1_sh3);
			m2=_mm256_maddubs_epi16(r2,c0_sh3);

			//shift m0 by one position because hadd follows
			m3=_mm256_srli_si256(m0,2);
			m4=_mm256_and_si256(m0,mask3);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
			m0=_mm256_add_epi16(m3,m4);

			//shift m1 by one position because hadd follows
			m3=_mm256_srli_si256(m1,2);
			m4=_mm256_and_si256(m1,mask3);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
			m1=_mm256_add_epi16(m3,m4);

			//shift m2 by one position because hadd follows
			m3=_mm256_srli_si256(m2,2);
			m4=_mm256_and_si256(m2,mask3);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
			m2=_mm256_add_epi16(m3,m4);

			//make 16-bit values 32-bit (horizontal add)
			m0=_mm256_madd_epi16(m0,ones);
			m1=_mm256_madd_epi16(m1,ones);
			m2=_mm256_madd_epi16(m2,ones);

			//vertical add
			m0=_mm256_add_epi32(m0,m1);
			m0=_mm256_add_epi32(m0,m2);

			//m0 has 8 32bit values now.
			output4 = division_32(division_case,m0,f);

			//the 32-bit division generates 8-bit values that are stored into memory.

			//blend output1, output2, output3, output4 into one register
			output2 = _mm256_slli_si256(output2,1);
			output1 = _mm256_add_epi8(output1,output2);
			output3 = _mm256_slli_si256(output3,2);
			output1 = _mm256_add_epi8(output1,output3);
			output4 = _mm256_slli_si256(output4,3);
			output_row1 = _mm256_add_epi8(output1,output4);

		 //row==0
			//1st col iteration

			//multiply with the mask
			m0=_mm256_maddubs_epi16(r0,c1);
			m1=_mm256_maddubs_epi16(r1,c0);

			//make 16-bit values 32-bit (horizontal add)
			ones=_mm256_set1_epi16(1);
			m0=_mm256_madd_epi16(m0,ones);
			m1=_mm256_madd_epi16(m1,ones);

			//vertical add
			m0=_mm256_add_epi32(m0,m1);

			//m0 has 8 32bit values now.
			output1 = division_32(division_case,m0,f);


			//2nd col iteration
			//multiply with the mask
			m0=_mm256_maddubs_epi16(r0,c1_sh1);
			m1=_mm256_maddubs_epi16(r1,c0_sh1);

			//make 16-bit values 32-bit (horizontal add)
			m0=_mm256_madd_epi16(m0,ones);
			m1=_mm256_madd_epi16(m1,ones);

			//vertical add
			m0=_mm256_add_epi32(m0,m1);

			//m0 has 8 32bit values now.
			output2 = division_32(division_case,m0,f);


	         //3rd col iteration
			//multiply with the mask
			m0=_mm256_maddubs_epi16(r0,c1_sh2);
			m1=_mm256_maddubs_epi16(r1,c0_sh2);

			//shift m0 by one position because hadd follows
			m3=_mm256_srli_si256(m0,2);
			m4=_mm256_and_si256(m0,mask3);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
			m0=_mm256_add_epi16(m3,m4);

			//shift m1 by one position because hadd follows
			m3=_mm256_srli_si256(m1,2);
			m4=_mm256_and_si256(m1,mask3);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
			m1=_mm256_add_epi16(m3,m4);


			//make 16-bit values 32-bit (horizontal add)
			m0=_mm256_madd_epi16(m0,ones);
			m1=_mm256_madd_epi16(m1,ones);

			//vertical add
			m0=_mm256_add_epi32(m0,m1);

			//m0 has 8 32bit values now.
			output3 = division_32(division_case,m0,f);

			//4th col iteration

			//multiply with the mask
			m0=_mm256_maddubs_epi16(r0,c1_sh3);
			m1=_mm256_maddubs_epi16(r1,c0_sh3);

			//shift m0 by one position because hadd follows
			m3=_mm256_srli_si256(m0,2);
			m4=_mm256_and_si256(m0,mask3);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
			m0=_mm256_add_epi16(m3,m4);

			//shift m1 by one position because hadd follows
			m3=_mm256_srli_si256(m1,2);
			m4=_mm256_and_si256(m1,mask3);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
			m1=_mm256_add_epi16(m3,m4);


			//make 16-bit values 32-bit (horizontal add)
			m0=_mm256_madd_epi16(m0,ones);
			m1=_mm256_madd_epi16(m1,ones);

			//vertical add
			m0=_mm256_add_epi32(m0,m1);

			//m0 has 8 32bit values now.
			output4 = division_32(division_case,m0,f);

			//the 32-bit division generates 8-bit values that are stored into memory.

			//blend output1, output2, output3, output4 into one register
			output2 = _mm256_slli_si256(output2,1);
			output1 = _mm256_add_epi8(output1,output2);
			output3 = _mm256_slli_si256(output3,2);
			output1 = _mm256_add_epi8(output1,output3);
			output4 = _mm256_slli_si256(output4,3);
			output_row0 = _mm256_add_epi8(output1,output4);


 	switch (REMINDER_ITERATIONS){
	case 1:
		filt[1][col] = (unsigned char) _mm256_extract_epi8(output_row1,0);
		filt[0][col] = (unsigned char) _mm256_extract_epi8(output_row0,0);

	  break;
	case 2:
		for (i=0;i<=1;i++){
			filt[1][col+i] = (unsigned char) _mm256_extract_epi8(output_row1,i);
			filt[0][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 3:
		for (i=0;i<=2;i++){
			filt[1][col+i] = (unsigned char) _mm256_extract_epi8(output_row1,i);
			filt[0][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 4:
		for (i=0;i<=3;i++){
			filt[1][col+i] = (unsigned char) _mm256_extract_epi8(output_row1,i);
			filt[0][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 5:
		for (i=0;i<=4;i++){
			filt[1][col+i] = (unsigned char) _mm256_extract_epi8(output_row1,i);
			filt[0][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 6:
		for (i=0;i<=5;i++){
			filt[1][col+i] = (unsigned char) _mm256_extract_epi8(output_row1,i);
			filt[0][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 7:
		for (i=0;i<=6;i++){
			filt[1][col+i] = (unsigned char) _mm256_extract_epi8(output_row1,i);
			filt[0][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 8:
		for (i=0;i<=7;i++){
			filt[1][col+i] = (unsigned char) _mm256_extract_epi8(output_row1,i);
			filt[0][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 9:
		for (i=0;i<=8;i++){
			filt[1][col+i] = (unsigned char) _mm256_extract_epi8(output_row1,i);
			filt[0][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 10:
		for (i=0;i<=9;i++){
			filt[1][col+i] = (unsigned char) _mm256_extract_epi8(output_row1,i);
			filt[0][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 11:
		for (i=0;i<=10;i++){
			filt[1][col+i] = (unsigned char) _mm256_extract_epi8(output_row1,i);
			filt[0][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 12:
		for (i=0;i<=11;i++){
			filt[1][col+i] = (unsigned char) _mm256_extract_epi8(output_row1,i);
			filt[0][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 13:
		for (i=0;i<=12;i++){
			filt[1][col+i] = (unsigned char) _mm256_extract_epi8(output_row1,i);
			filt[0][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 14:
		for (i=0;i<=13;i++){
			filt[1][col+i] = (unsigned char) _mm256_extract_epi8(output_row1,i);
			filt[0][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 15:
		for (i=0;i<=14;i++){
			filt[1][col+i] = (unsigned char) _mm256_extract_epi8(output_row1,i);
			filt[0][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 16:
  	_mm_storeu_si128( (__m128i *) &filt[1][col],_mm256_extractf128_si256(output_row1, 0)); //store low 128bit - 16pixels
  	_mm_storeu_si128( (__m128i *) &filt[0][col],_mm256_extractf128_si256(output_row0, 0)); //store low 128bit - 16pixels

		  break;
	case 17:
  	_mm_storeu_si128( (__m128i *) &filt[1][col],_mm256_extractf128_si256(output_row1, 0)); //store low 128bit - 16pixels
  	_mm_storeu_si128( (__m128i *) &filt[0][col],_mm256_extractf128_si256(output_row0, 0)); //store low 128bit - 16pixels
	filt[1][col+16] = (unsigned char) _mm256_extract_epi8(output_row1,16);
	filt[0][col+16] = (unsigned char) _mm256_extract_epi8(output_row0,16);
		  break;
	case 18:
  	_mm_storeu_si128( (__m128i *) &filt[1][col],_mm256_extractf128_si256(output_row1, 0)); //store low 128bit - 16pixels
  	_mm_storeu_si128( (__m128i *) &filt[0][col],_mm256_extractf128_si256(output_row0, 0)); //store low 128bit - 16pixels

	for (i=16;i<=17;i++){
	 filt[1][col+i] = (unsigned char) _mm256_extract_epi8(output_row1,i);
	 filt[0][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
	}
		  break;
	case 19:
	  	_mm_storeu_si128( (__m128i *) &filt[1][col],_mm256_extractf128_si256(output_row1, 0)); //store low 128bit - 16pixels
	  	_mm_storeu_si128( (__m128i *) &filt[0][col],_mm256_extractf128_si256(output_row0, 0)); //store low 128bit - 16pixels

		for (i=16;i<=18;i++){
		 filt[1][col+i] = (unsigned char) _mm256_extract_epi8(output_row1,i);
		 filt[0][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 20:
	  	_mm_storeu_si128( (__m128i *) &filt[1][col],_mm256_extractf128_si256(output_row1, 0)); //store low 128bit - 16pixels
	  	_mm_storeu_si128( (__m128i *) &filt[0][col],_mm256_extractf128_si256(output_row0, 0)); //store low 128bit - 16pixels

		for (i=16;i<=19;i++){
		 filt[1][col+i] = (unsigned char) _mm256_extract_epi8(output_row1,i);
		 filt[0][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 21:
	  	_mm_storeu_si128( (__m128i *) &filt[1][col],_mm256_extractf128_si256(output_row1, 0)); //store low 128bit - 16pixels
	  	_mm_storeu_si128( (__m128i *) &filt[0][col],_mm256_extractf128_si256(output_row0, 0)); //store low 128bit - 16pixels

		for (i=16;i<=20;i++){
		 filt[1][col+i] = (unsigned char) _mm256_extract_epi8(output_row1,i);
		 filt[0][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 22:
	  	_mm_storeu_si128( (__m128i *) &filt[1][col],_mm256_extractf128_si256(output_row1, 0)); //store low 128bit - 16pixels
	  	_mm_storeu_si128( (__m128i *) &filt[0][col],_mm256_extractf128_si256(output_row0, 0)); //store low 128bit - 16pixels

		for (i=16;i<=21;i++){
		 filt[1][col+i] = (unsigned char) _mm256_extract_epi8(output_row1,i);
		 filt[0][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		break;
	case 23:
	  	_mm_storeu_si128( (__m128i *) &filt[1][col],_mm256_extractf128_si256(output_row1, 0)); //store low 128bit - 16pixels
	  	_mm_storeu_si128( (__m128i *) &filt[0][col],_mm256_extractf128_si256(output_row0, 0)); //store low 128bit - 16pixels

		for (i=16;i<=22;i++){
		 filt[1][col+i] = (unsigned char) _mm256_extract_epi8(output_row1,i);
		 filt[0][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 24:
	  	_mm_storeu_si128( (__m128i *) &filt[1][col],_mm256_extractf128_si256(output_row1, 0)); //store low 128bit - 16pixels
	  	_mm_storeu_si128( (__m128i *) &filt[0][col],_mm256_extractf128_si256(output_row0, 0)); //store low 128bit - 16pixels

		for (i=16;i<=23;i++){
		 filt[1][col+i] = (unsigned char) _mm256_extract_epi8(output_row1,i);
		 filt[0][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 25:
	  	_mm_storeu_si128( (__m128i *) &filt[1][col],_mm256_extractf128_si256(output_row1, 0)); //store low 128bit - 16pixels
	  	_mm_storeu_si128( (__m128i *) &filt[0][col],_mm256_extractf128_si256(output_row0, 0)); //store low 128bit - 16pixels

		for (i=16;i<=24;i++){
		 filt[1][col+i] = (unsigned char) _mm256_extract_epi8(output_row1,i);
		 filt[0][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 26:
	  	_mm_storeu_si128( (__m128i *) &filt[1][col],_mm256_extractf128_si256(output_row1, 0)); //store low 128bit - 16pixels
	  	_mm_storeu_si128( (__m128i *) &filt[0][col],_mm256_extractf128_si256(output_row0, 0)); //store low 128bit - 16pixels

		for (i=16;i<=25;i++){
		 filt[1][col+i] = (unsigned char) _mm256_extract_epi8(output_row1,i);
		 filt[0][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}

		  break;
	case 27:
	  	_mm_storeu_si128( (__m128i *) &filt[1][col],_mm256_extractf128_si256(output_row1, 0)); //store low 128bit - 16pixels
	  	_mm_storeu_si128( (__m128i *) &filt[0][col],_mm256_extractf128_si256(output_row0, 0)); //store low 128bit - 16pixels

		for (i=16;i<=26;i++){
		 filt[1][col+i] = (unsigned char) _mm256_extract_epi8(output_row1,i);
		 filt[0][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}

		  break;
	case 28:
	  	_mm_storeu_si128( (__m128i *) &filt[1][col],_mm256_extractf128_si256(output_row1, 0)); //store low 128bit - 16pixels
	  	_mm_storeu_si128( (__m128i *) &filt[0][col],_mm256_extractf128_si256(output_row0, 0)); //store low 128bit - 16pixels

		for (i=16;i<=27;i++){
		 filt[1][col+i] = (unsigned char) _mm256_extract_epi8(output_row1,i);
		 filt[0][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 29:
	  	_mm_storeu_si128( (__m128i *) &filt[1][col],_mm256_extractf128_si256(output_row1, 0)); //store low 128bit - 16pixels
	  	_mm_storeu_si128( (__m128i *) &filt[0][col],_mm256_extractf128_si256(output_row0, 0)); //store low 128bit - 16pixels

		filt[1][col+16] = (unsigned char) _mm256_extract_epi8(output_row1,16);
		filt[1][col+17] = (unsigned char) _mm256_extract_epi8(output_row1,17);
		filt[1][col+18] = (unsigned char) _mm256_extract_epi8(output_row1,18);
		filt[1][col+19] = (unsigned char) _mm256_extract_epi8(output_row1,19);
		filt[1][col+20] = (unsigned char) _mm256_extract_epi8(output_row1,20);
		filt[1][col+21] = (unsigned char) _mm256_extract_epi8(output_row1,21);
		filt[1][col+22] = (unsigned char) _mm256_extract_epi8(output_row1,22);
		filt[1][col+23] = (unsigned char) _mm256_extract_epi8(output_row1,23);
		filt[1][col+24] = (unsigned char) _mm256_extract_epi8(output_row1,24);
		filt[1][col+25] = (unsigned char) _mm256_extract_epi8(output_row1,25);
		filt[1][col+26] = (unsigned char) _mm256_extract_epi8(output_row1,26);
		filt[1][col+27] = (unsigned char) _mm256_extract_epi8(output_row1,27);
		filt[1][col+28] = (unsigned char) _mm256_extract_epi8(output_row1,28);

		filt[0][col+16] = (unsigned char) _mm256_extract_epi8(output_row0,16);
		filt[0][col+17] = (unsigned char) _mm256_extract_epi8(output_row0,17);
		filt[0][col+18] = (unsigned char) _mm256_extract_epi8(output_row0,18);
		filt[0][col+19] = (unsigned char) _mm256_extract_epi8(output_row0,19);
		filt[0][col+20] = (unsigned char) _mm256_extract_epi8(output_row0,20);
		filt[0][col+21] = (unsigned char) _mm256_extract_epi8(output_row0,21);
		filt[0][col+22] = (unsigned char) _mm256_extract_epi8(output_row0,22);
		filt[0][col+23] = (unsigned char) _mm256_extract_epi8(output_row0,23);
		filt[0][col+24] = (unsigned char) _mm256_extract_epi8(output_row0,24);
		filt[0][col+25] = (unsigned char) _mm256_extract_epi8(output_row0,25);
		filt[0][col+26] = (unsigned char) _mm256_extract_epi8(output_row0,26);
		filt[0][col+27] = (unsigned char) _mm256_extract_epi8(output_row0,27);
		filt[0][col+28] = (unsigned char) _mm256_extract_epi8(output_row0,28);

		  break;
	case 30:
	  	_mm_storeu_si128( (__m128i *) &filt[1][col],_mm256_extractf128_si256(output_row1, 0)); //store low 128bit - 16pixels
	  	_mm_storeu_si128( (__m128i *) &filt[0][col],_mm256_extractf128_si256(output_row0, 0)); //store low 128bit - 16pixels

		filt[1][col+16] = (unsigned char) _mm256_extract_epi8(output_row1,16);
		filt[1][col+17] = (unsigned char) _mm256_extract_epi8(output_row1,17);
		filt[1][col+18] = (unsigned char) _mm256_extract_epi8(output_row1,18);
		filt[1][col+19] = (unsigned char) _mm256_extract_epi8(output_row1,19);
		filt[1][col+20] = (unsigned char) _mm256_extract_epi8(output_row1,20);
		filt[1][col+21] = (unsigned char) _mm256_extract_epi8(output_row1,21);
		filt[1][col+22] = (unsigned char) _mm256_extract_epi8(output_row1,22);
		filt[1][col+23] = (unsigned char) _mm256_extract_epi8(output_row1,23);
		filt[1][col+24] = (unsigned char) _mm256_extract_epi8(output_row1,24);
		filt[1][col+25] = (unsigned char) _mm256_extract_epi8(output_row1,25);
		filt[1][col+26] = (unsigned char) _mm256_extract_epi8(output_row1,26);
		filt[1][col+27] = (unsigned char) _mm256_extract_epi8(output_row1,27);
		filt[1][col+28] = (unsigned char) _mm256_extract_epi8(output_row1,28);

		filt[0][col+16] = (unsigned char) _mm256_extract_epi8(output_row0,16);
		filt[0][col+17] = (unsigned char) _mm256_extract_epi8(output_row0,17);
		filt[0][col+18] = (unsigned char) _mm256_extract_epi8(output_row0,18);
		filt[0][col+19] = (unsigned char) _mm256_extract_epi8(output_row0,19);
		filt[0][col+20] = (unsigned char) _mm256_extract_epi8(output_row0,20);
		filt[0][col+21] = (unsigned char) _mm256_extract_epi8(output_row0,21);
		filt[0][col+22] = (unsigned char) _mm256_extract_epi8(output_row0,22);
		filt[0][col+23] = (unsigned char) _mm256_extract_epi8(output_row0,23);
		filt[0][col+24] = (unsigned char) _mm256_extract_epi8(output_row0,24);
		filt[0][col+25] = (unsigned char) _mm256_extract_epi8(output_row0,25);
		filt[0][col+26] = (unsigned char) _mm256_extract_epi8(output_row0,26);
		filt[0][col+27] = (unsigned char) _mm256_extract_epi8(output_row0,27);
		filt[0][col+28] = (unsigned char) _mm256_extract_epi8(output_row0,28);

	//the filt[row][col+29] is computed unvectorized
	newPixel = 0;
	newPixel += frame1[1-1][M-1-1] * filter[0][0];
	newPixel += frame1[1-1][M-1] * filter[0][1];

	newPixel += frame1[1][M-1-1] * filter[1][0];
	newPixel += frame1[1][M-1] * filter[1][1];

	newPixel += frame1[1+1][M-1-1] * filter[2][0];
	newPixel += frame1[1+1][M-1] * filter[2][1];

	filt[1][M-1] = (unsigned char) (newPixel / divisor);

	newPixel = 0;
	newPixel += frame1[1-1][M-1-1] * filter[1][0];
	newPixel += frame1[1-1][M-1] * filter[1][1];

	newPixel += frame1[1][M-1-1] * filter[2][0];
	newPixel += frame1[1][M-1] * filter[2][1];

	filt[0][M-1] = (unsigned char) (newPixel / divisor);

		  break;

	case 31:
	  	_mm_storeu_si128( (__m128i *) &filt[1][col],_mm256_extractf128_si256(output_row1, 0)); //store low 128bit - 16pixels
	  	_mm_storeu_si128( (__m128i *) &filt[0][col],_mm256_extractf128_si256(output_row0, 0)); //store low 128bit - 16pixels

		filt[1][col+16] = (unsigned char) _mm256_extract_epi8(output_row1,16);
		filt[1][col+17] = (unsigned char) _mm256_extract_epi8(output_row1,17);
		filt[1][col+18] = (unsigned char) _mm256_extract_epi8(output_row1,18);
		filt[1][col+19] = (unsigned char) _mm256_extract_epi8(output_row1,19);
		filt[1][col+20] = (unsigned char) _mm256_extract_epi8(output_row1,20);
		filt[1][col+21] = (unsigned char) _mm256_extract_epi8(output_row1,21);
		filt[1][col+22] = (unsigned char) _mm256_extract_epi8(output_row1,22);
		filt[1][col+23] = (unsigned char) _mm256_extract_epi8(output_row1,23);
		filt[1][col+24] = (unsigned char) _mm256_extract_epi8(output_row1,24);
		filt[1][col+25] = (unsigned char) _mm256_extract_epi8(output_row1,25);
		filt[1][col+26] = (unsigned char) _mm256_extract_epi8(output_row1,26);
		filt[1][col+27] = (unsigned char) _mm256_extract_epi8(output_row1,27);
		filt[1][col+28] = (unsigned char) _mm256_extract_epi8(output_row1,28);

		filt[0][col+16] = (unsigned char) _mm256_extract_epi8(output_row0,16);
		filt[0][col+17] = (unsigned char) _mm256_extract_epi8(output_row0,17);
		filt[0][col+18] = (unsigned char) _mm256_extract_epi8(output_row0,18);
		filt[0][col+19] = (unsigned char) _mm256_extract_epi8(output_row0,19);
		filt[0][col+20] = (unsigned char) _mm256_extract_epi8(output_row0,20);
		filt[0][col+21] = (unsigned char) _mm256_extract_epi8(output_row0,21);
		filt[0][col+22] = (unsigned char) _mm256_extract_epi8(output_row0,22);
		filt[0][col+23] = (unsigned char) _mm256_extract_epi8(output_row0,23);
		filt[0][col+24] = (unsigned char) _mm256_extract_epi8(output_row0,24);
		filt[0][col+25] = (unsigned char) _mm256_extract_epi8(output_row0,25);
		filt[0][col+26] = (unsigned char) _mm256_extract_epi8(output_row0,26);
		filt[0][col+27] = (unsigned char) _mm256_extract_epi8(output_row0,27);
		filt[0][col+28] = (unsigned char) _mm256_extract_epi8(output_row0,28);

	//the filt[1][col+29/30] are computed unvectorized
		newPixel = 0;
		newPixel += frame1[1-1][M-2-1] * filter[0][0];
		newPixel += frame1[1-1][M-2] * filter[0][1];
		newPixel += frame1[1-1][M-2+1] * filter[0][2];

		newPixel += frame1[1][M-2-1] * filter[1][0];
		newPixel += frame1[1][M-2] * filter[1][1];
		newPixel += frame1[1][M-2+1] * filter[1][2];

		newPixel += frame1[1+1][M-2-1] * filter[2][0];
		newPixel += frame1[1+1][M-2] * filter[2][1];
		newPixel += frame1[1+1][M-2+1] * filter[2][2];

		filt[1][M-2] = (unsigned char) (newPixel / divisor);

		newPixel = 0;
		newPixel += frame1[1-1][M-1-1] * filter[0][0];
		newPixel += frame1[1-1][M-1] * filter[0][1];

		newPixel += frame1[1][M-1-1] * filter[1][0];
		newPixel += frame1[1][M-1] * filter[1][1];

		newPixel += frame1[1+1][M-1-1] * filter[2][0];
		newPixel += frame1[1+1][M-1] * filter[2][1];

		filt[1][M-1] = (unsigned char) (newPixel / divisor);

		newPixel = 0;

		newPixel += frame1[0][M-2-1] * filter[1][0];
		newPixel += frame1[0][M-2] * filter[1][1];
		newPixel += frame1[0][M-2+1] * filter[1][2];

		newPixel += frame1[1][M-2-1] * filter[2][0];
		newPixel += frame1[1][M-2] * filter[2][1];
		newPixel += frame1[1][M-2+1] * filter[2][2];

		filt[0][M-2] = (unsigned char) (newPixel / divisor);

		newPixel = 0;

		newPixel += frame1[0][M-1-1] * filter[1][0];
		newPixel += frame1[0][M-1] * filter[1][1];

		newPixel += frame1[1][M-1-1] * filter[2][0];
		newPixel += frame1[1][M-1] * filter[2][1];

		filt[0][M-1] = (unsigned char) (newPixel / divisor);


		  break;

	default:
		printf("\nsomething went wrong");
		return -1;
 	}

return 0;

}





int loop_reminder_3x3_last_values_32(unsigned char **frame1,unsigned char **filt,const unsigned int M, const unsigned int N, const unsigned int col,const unsigned int REMINDER_ITERATIONS,const unsigned int division_case,const unsigned short int divisor,signed char **filter,const __m256i c0,const __m256i c1,const __m256i c0_sh1,const __m256i c1_sh1,const __m256i c0_sh2,const __m256i c1_sh2, const __m256i c0_sh3,const __m256i c1_sh3,const __m256i f){

register	__m256i r0,r1,r2,m0,m1,m2,m3,m4,output_row0,output_row1,output1,output2,output3,output4;
int newPixel;
unsigned int i;
const __m256i mask3  = _mm256_set_epi16(0,0,0,0,0,0,0,65535,0,0,0,0,0,0,0,0);


 __m256i reminder_mask1;

if (REMINDER_ITERATIONS == 0){
	return 0; //no loop reminder is needed
}

	reminder_mask1=_mm256_load_si256( (__m256i *) &reminder_msk1_3x3[REMINDER_ITERATIONS-1][0]);
	//reminder_mask2=_mm256_load_si256( (__m256i *) &reminder_msk2[REMINDER_ITERATIONS-1][0]);


	 //1st col iteration
	//load the 5 rows
	r0=_mm256_loadu_si256( (__m256i *) &frame1[N-3][col-1]);
	r1=_mm256_loadu_si256( (__m256i *) &frame1[N-2][col-1]);
	r2=_mm256_loadu_si256( (__m256i *) &frame1[N-1][col-1]);


	/*
	 * The above load operations load outside of the array bounds; these elements are filled with zeros just after
	 * Furthermore, I add one extra zero in the end to compute col=N-1. The last value of col is N-1, not N
	 */

	//AND r0-r4 with reminder_mask
	r0=_mm256_and_si256(r0,reminder_mask1);
	r1=_mm256_and_si256(r1,reminder_mask1);
	r2=_mm256_and_si256(r2,reminder_mask1);


	//col iteration computes output pixels of    1,5,9,13,17,21,25,29
	// col+1 iteration computes output pixels of 2,6,10,14,18,22,26,30
	// col+2 iteration computes output pixels of 3,7,11,15,19,23,27
	// col+3 iteration computes output pixels of 4,8,12,16,20,24,28
	//afterwards, col becomes 30 and repeat the above process

	 //row==N-2
			//1st col iteration

			//multiply with the mask
			m0=_mm256_maddubs_epi16(r0,c0);
			m1=_mm256_maddubs_epi16(r1,c1);
			m2=_mm256_maddubs_epi16(r2,c0);

			//make 16-bit values 32-bit (horizontal add)
			__m256i ones=_mm256_set1_epi16(1);
			m0=_mm256_madd_epi16(m0,ones);
			m1=_mm256_madd_epi16(m1,ones);
			m2=_mm256_madd_epi16(m2,ones);

			//vertical add
			m0=_mm256_add_epi32(m0,m1);
			m0=_mm256_add_epi32(m0,m2);

			//m0 has 8 32bit values now.
			output1 = division_32(division_case,m0,f);


			//2nd col iteration
			//multiply with the mask
			m0=_mm256_maddubs_epi16(r0,c0_sh1);
			m1=_mm256_maddubs_epi16(r1,c1_sh1);
			m2=_mm256_maddubs_epi16(r2,c0_sh1);

			//make 16-bit values 32-bit (horizontal add)
			m0=_mm256_madd_epi16(m0,ones);
			m1=_mm256_madd_epi16(m1,ones);
			m2=_mm256_madd_epi16(m2,ones);

			//vertical add
			m0=_mm256_add_epi32(m0,m1);
			m0=_mm256_add_epi32(m0,m2);

			//m0 has 8 32bit values now.
			output2 = division_32(division_case,m0,f);


	         //3rd col iteration
			//multiply with the mask
			m0=_mm256_maddubs_epi16(r0,c0_sh2);
			m1=_mm256_maddubs_epi16(r1,c1_sh2);
			m2=_mm256_maddubs_epi16(r2,c0_sh2);

			//shift m0 by one position because hadd follows
			m3=_mm256_srli_si256(m0,2);
			m4=_mm256_and_si256(m0,mask3);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
			m0=_mm256_add_epi16(m3,m4);

			//shift m1 by one position because hadd follows
			m3=_mm256_srli_si256(m1,2);
			m4=_mm256_and_si256(m1,mask3);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
			m1=_mm256_add_epi16(m3,m4);

			//shift m2 by one position because hadd follows
			m3=_mm256_srli_si256(m2,2);
			m4=_mm256_and_si256(m2,mask3);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
			m2=_mm256_add_epi16(m3,m4);

			//make 16-bit values 32-bit (horizontal add)
			m0=_mm256_madd_epi16(m0,ones);
			m1=_mm256_madd_epi16(m1,ones);
			m2=_mm256_madd_epi16(m2,ones);

			//vertical add
			m0=_mm256_add_epi32(m0,m1);
			m0=_mm256_add_epi32(m0,m2);

			//m0 has 8 32bit values now.
			output3 = division_32(division_case,m0,f);

			//4th col iteration

			//multiply with the mask
			m0=_mm256_maddubs_epi16(r0,c0_sh3);
			m1=_mm256_maddubs_epi16(r1,c1_sh3);
			m2=_mm256_maddubs_epi16(r2,c0_sh3);

			//shift m0 by one position because hadd follows
			m3=_mm256_srli_si256(m0,2);
			m4=_mm256_and_si256(m0,mask3);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
			m0=_mm256_add_epi16(m3,m4);

			//shift m1 by one position because hadd follows
			m3=_mm256_srli_si256(m1,2);
			m4=_mm256_and_si256(m1,mask3);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
			m1=_mm256_add_epi16(m3,m4);

			//shift m2 by one position because hadd follows
			m3=_mm256_srli_si256(m2,2);
			m4=_mm256_and_si256(m2,mask3);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
			m2=_mm256_add_epi16(m3,m4);

			//make 16-bit values 32-bit (horizontal add)
			m0=_mm256_madd_epi16(m0,ones);
			m1=_mm256_madd_epi16(m1,ones);
			m2=_mm256_madd_epi16(m2,ones);

			//vertical add
			m0=_mm256_add_epi32(m0,m1);
			m0=_mm256_add_epi32(m0,m2);

			//m0 has 8 32bit values now.
			output4 = division_32(division_case,m0,f);

			//the 32-bit division generates 8-bit values that are stored into memory.

			//blend output1, output2, output3, output4 into one register
			output2 = _mm256_slli_si256(output2,1);
			output1 = _mm256_add_epi8(output1,output2);
			output3 = _mm256_slli_si256(output3,2);
			output1 = _mm256_add_epi8(output1,output3);
			output4 = _mm256_slli_si256(output4,3);
			output_row1 = _mm256_add_epi8(output1,output4);


		 //row==N-1
			//1st col iteration

			//multiply with the mask
			m0=_mm256_maddubs_epi16(r1,c0);
			m1=_mm256_maddubs_epi16(r2,c1);

			//make 16-bit values 32-bit (horizontal add)
			ones=_mm256_set1_epi16(1);
			m0=_mm256_madd_epi16(m0,ones);
			m1=_mm256_madd_epi16(m1,ones);

			//vertical add
			m0=_mm256_add_epi32(m0,m1);

			//m0 has 8 32bit values now.
			output1 = division_32(division_case,m0,f);


			//2nd col iteration
			//multiply with the mask
			m0=_mm256_maddubs_epi16(r1,c0_sh1);
			m1=_mm256_maddubs_epi16(r2,c1_sh1);

			//make 16-bit values 32-bit (horizontal add)
			m0=_mm256_madd_epi16(m0,ones);
			m1=_mm256_madd_epi16(m1,ones);

			//vertical add
			m0=_mm256_add_epi32(m0,m1);

			//m0 has 8 32bit values now.
			output2 = division_32(division_case,m0,f);


	         //3rd col iteration
			//multiply with the mask
			m0=_mm256_maddubs_epi16(r1,c0_sh2);
			m1=_mm256_maddubs_epi16(r2,c1_sh2);

			//shift m0 by one position because hadd follows
			m3=_mm256_srli_si256(m0,2);
			m4=_mm256_and_si256(m0,mask3);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
			m0=_mm256_add_epi16(m3,m4);

			//shift m1 by one position because hadd follows
			m3=_mm256_srli_si256(m1,2);
			m4=_mm256_and_si256(m1,mask3);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
			m1=_mm256_add_epi16(m3,m4);


			//make 16-bit values 32-bit (horizontal add)
			m0=_mm256_madd_epi16(m0,ones);
			m1=_mm256_madd_epi16(m1,ones);

			//vertical add
			m0=_mm256_add_epi32(m0,m1);

			//m0 has 8 32bit values now.
			output3 = division_32(division_case,m0,f);

			//4th col iteration

			//multiply with the mask
			m0=_mm256_maddubs_epi16(r1,c0_sh3);
			m1=_mm256_maddubs_epi16(r2,c1_sh3);

			//shift m0 by one position because hadd follows
			m3=_mm256_srli_si256(m0,2);
			m4=_mm256_and_si256(m0,mask3);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
			m0=_mm256_add_epi16(m3,m4);

			//shift m1 by one position because hadd follows
			m3=_mm256_srli_si256(m1,2);
			m4=_mm256_and_si256(m1,mask3);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
			m1=_mm256_add_epi16(m3,m4);


			//make 16-bit values 32-bit (horizontal add)
			m0=_mm256_madd_epi16(m0,ones);
			m1=_mm256_madd_epi16(m1,ones);

			//vertical add
			m0=_mm256_add_epi32(m0,m1);

			//m0 has 8 32bit values now.
			output4 = division_32(division_case,m0,f);

			//the 32-bit division generates 8-bit values that are stored into memory.

			//blend output1, output2, output3, output4 into one register
			output2 = _mm256_slli_si256(output2,1);
			output1 = _mm256_add_epi8(output1,output2);
			output3 = _mm256_slli_si256(output3,2);
			output1 = _mm256_add_epi8(output1,output3);
			output4 = _mm256_slli_si256(output4,3);
			output_row0 = _mm256_add_epi8(output1,output4);




 	switch (REMINDER_ITERATIONS){
	case 1:
		filt[N-2][col] = (unsigned char) _mm256_extract_epi8(output_row1,0);
		filt[N-1][col] = (unsigned char) _mm256_extract_epi8(output_row0,0);

	  break;
	case 2:
		for (i=0;i<=1;i++){
			filt[N-2][col+i] = (unsigned char) _mm256_extract_epi8(output_row1,i);
			filt[N-1][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 3:
		for (i=0;i<=2;i++){
			filt[N-2][col+i] = (unsigned char) _mm256_extract_epi8(output_row1,i);
			filt[N-1][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 4:
		for (i=0;i<=3;i++){
			filt[N-2][col+i] = (unsigned char) _mm256_extract_epi8(output_row1,i);
			filt[N-1][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 5:
		for (i=0;i<=4;i++){
			filt[N-2][col+i] = (unsigned char) _mm256_extract_epi8(output_row1,i);
			filt[N-1][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 6:
		for (i=0;i<=5;i++){
			filt[N-2][col+i] = (unsigned char) _mm256_extract_epi8(output_row1,i);
			filt[N-1][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 7:
		for (i=0;i<=6;i++){
			filt[N-2][col+i] = (unsigned char) _mm256_extract_epi8(output_row1,i);
			filt[N-1][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 8:
		for (i=0;i<=7;i++){
			filt[N-2][col+i] = (unsigned char) _mm256_extract_epi8(output_row1,i);
			filt[N-1][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 9:
		for (i=0;i<=8;i++){
			filt[N-2][col+i] = (unsigned char) _mm256_extract_epi8(output_row1,i);
			filt[N-1][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 10:
		for (i=0;i<=9;i++){
			filt[N-2][col+i] = (unsigned char) _mm256_extract_epi8(output_row1,i);
			filt[N-1][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 11:
		for (i=0;i<=10;i++){
			filt[N-2][col+i] = (unsigned char) _mm256_extract_epi8(output_row1,i);
			filt[N-1][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 12:
		for (i=0;i<=11;i++){
			filt[N-2][col+i] = (unsigned char) _mm256_extract_epi8(output_row1,i);
			filt[N-1][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 13:
		for (i=0;i<=12;i++){
			filt[N-2][col+i] = (unsigned char) _mm256_extract_epi8(output_row1,i);
			filt[N-1][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 14:
		for (i=0;i<=13;i++){
			filt[N-2][col+i] = (unsigned char) _mm256_extract_epi8(output_row1,i);
			filt[N-1][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 15:
		for (i=0;i<=14;i++){
			filt[N-2][col+i] = (unsigned char) _mm256_extract_epi8(output_row1,i);
			filt[N-1][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 16:
  	_mm_storeu_si128( (__m128i *) &filt[N-2][col],_mm256_extractf128_si256(output_row1, 0)); //store low 128bit - 16pixels
  	_mm_storeu_si128( (__m128i *) &filt[N-1][col],_mm256_extractf128_si256(output_row0, 0)); //store low 128bit - 16pixels

		  break;
	case 17:
  	_mm_storeu_si128( (__m128i *) &filt[N-2][col],_mm256_extractf128_si256(output_row1, 0)); //store low 128bit - 16pixels
  	_mm_storeu_si128( (__m128i *) &filt[N-1][col],_mm256_extractf128_si256(output_row0, 0)); //store low 128bit - 16pixels
	filt[N-2][col+16] = (unsigned char) _mm256_extract_epi8(output_row1,16);
	filt[N-1][col+16] = (unsigned char) _mm256_extract_epi8(output_row0,16);
		  break;
	case 18:
  	_mm_storeu_si128( (__m128i *) &filt[N-2][col],_mm256_extractf128_si256(output_row1, 0)); //store low 128bit - 16pixels
  	_mm_storeu_si128( (__m128i *) &filt[N-1][col],_mm256_extractf128_si256(output_row0, 0)); //store low 128bit - 16pixels

	for (i=16;i<=17;i++){
	 filt[N-2][col+i] = (unsigned char) _mm256_extract_epi8(output_row1,i);
	 filt[N-1][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
	}
		  break;
	case 19:
	  	_mm_storeu_si128( (__m128i *) &filt[N-2][col],_mm256_extractf128_si256(output_row1, 0)); //store low 128bit - 16pixels
	  	_mm_storeu_si128( (__m128i *) &filt[N-1][col],_mm256_extractf128_si256(output_row0, 0)); //store low 128bit - 16pixels

		for (i=16;i<=18;i++){
		 filt[N-2][col+i] = (unsigned char) _mm256_extract_epi8(output_row1,i);
		 filt[N-1][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 20:
	  	_mm_storeu_si128( (__m128i *) &filt[N-2][col],_mm256_extractf128_si256(output_row1, 0)); //store low 128bit - 16pixels
	  	_mm_storeu_si128( (__m128i *) &filt[N-1][col],_mm256_extractf128_si256(output_row0, 0)); //store low 128bit - 16pixels

		for (i=16;i<=19;i++){
		 filt[N-2][col+i] = (unsigned char) _mm256_extract_epi8(output_row1,i);
		 filt[N-1][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 21:
	  	_mm_storeu_si128( (__m128i *) &filt[N-2][col],_mm256_extractf128_si256(output_row1, 0)); //store low 128bit - 16pixels
	  	_mm_storeu_si128( (__m128i *) &filt[N-1][col],_mm256_extractf128_si256(output_row0, 0)); //store low 128bit - 16pixels

		for (i=16;i<=20;i++){
		 filt[N-2][col+i] = (unsigned char) _mm256_extract_epi8(output_row1,i);
		 filt[N-1][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 22:
	  	_mm_storeu_si128( (__m128i *) &filt[N-2][col],_mm256_extractf128_si256(output_row1, 0)); //store low 128bit - 16pixels
	  	_mm_storeu_si128( (__m128i *) &filt[N-1][col],_mm256_extractf128_si256(output_row0, 0)); //store low 128bit - 16pixels

		for (i=16;i<=21;i++){
		 filt[N-2][col+i] = (unsigned char) _mm256_extract_epi8(output_row1,i);
		 filt[N-1][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		break;
	case 23:
	  	_mm_storeu_si128( (__m128i *) &filt[N-2][col],_mm256_extractf128_si256(output_row1, 0)); //store low 128bit - 16pixels
	  	_mm_storeu_si128( (__m128i *) &filt[N-1][col],_mm256_extractf128_si256(output_row0, 0)); //store low 128bit - 16pixels

		for (i=16;i<=22;i++){
		 filt[N-2][col+i] = (unsigned char) _mm256_extract_epi8(output_row1,i);
		 filt[N-1][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 24:
	  	_mm_storeu_si128( (__m128i *) &filt[N-2][col],_mm256_extractf128_si256(output_row1, 0)); //store low 128bit - 16pixels
	  	_mm_storeu_si128( (__m128i *) &filt[N-1][col],_mm256_extractf128_si256(output_row0, 0)); //store low 128bit - 16pixels

		for (i=16;i<=23;i++){
		 filt[N-2][col+i] = (unsigned char) _mm256_extract_epi8(output_row1,i);
		 filt[N-1][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 25:
	  	_mm_storeu_si128( (__m128i *) &filt[N-2][col],_mm256_extractf128_si256(output_row1, 0)); //store low 128bit - 16pixels
	  	_mm_storeu_si128( (__m128i *) &filt[N-1][col],_mm256_extractf128_si256(output_row0, 0)); //store low 128bit - 16pixels

		for (i=16;i<=24;i++){
		 filt[N-2][col+i] = (unsigned char) _mm256_extract_epi8(output_row1,i);
		 filt[N-1][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 26:
	  	_mm_storeu_si128( (__m128i *) &filt[N-2][col],_mm256_extractf128_si256(output_row1, 0)); //store low 128bit - 16pixels
	  	_mm_storeu_si128( (__m128i *) &filt[N-1][col],_mm256_extractf128_si256(output_row0, 0)); //store low 128bit - 16pixels

		for (i=16;i<=25;i++){
		 filt[N-2][col+i] = (unsigned char) _mm256_extract_epi8(output_row1,i);
		 filt[N-1][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}

		  break;
	case 27:
	  	_mm_storeu_si128( (__m128i *) &filt[N-2][col],_mm256_extractf128_si256(output_row1, 0)); //store low 128bit - 16pixels
	  	_mm_storeu_si128( (__m128i *) &filt[N-1][col],_mm256_extractf128_si256(output_row0, 0)); //store low 128bit - 16pixels

		for (i=16;i<=26;i++){
		 filt[N-2][col+i] = (unsigned char) _mm256_extract_epi8(output_row1,i);
		 filt[N-1][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}

		  break;
	case 28:
	  	_mm_storeu_si128( (__m128i *) &filt[N-2][col],_mm256_extractf128_si256(output_row1, 0)); //store low 128bit - 16pixels
	  	_mm_storeu_si128( (__m128i *) &filt[N-1][col],_mm256_extractf128_si256(output_row0, 0)); //store low 128bit - 16pixels

		for (i=16;i<=27;i++){
		 filt[N-2][col+i] = (unsigned char) _mm256_extract_epi8(output_row1,i);
		 filt[N-1][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 29:
	  	_mm_storeu_si128( (__m128i *) &filt[N-2][col],_mm256_extractf128_si256(output_row1, 0)); //store low 128bit - 16pixels
	  	_mm_storeu_si128( (__m128i *) &filt[N-1][col],_mm256_extractf128_si256(output_row0, 0)); //store low 128bit - 16pixels

		filt[N-2][col+16] = (unsigned char) _mm256_extract_epi8(output_row1,16);
		filt[N-2][col+17] = (unsigned char) _mm256_extract_epi8(output_row1,17);
		filt[N-2][col+18] = (unsigned char) _mm256_extract_epi8(output_row1,18);
		filt[N-2][col+19] = (unsigned char) _mm256_extract_epi8(output_row1,19);
		filt[N-2][col+20] = (unsigned char) _mm256_extract_epi8(output_row1,20);
		filt[N-2][col+21] = (unsigned char) _mm256_extract_epi8(output_row1,21);
		filt[N-2][col+22] = (unsigned char) _mm256_extract_epi8(output_row1,22);
		filt[N-2][col+23] = (unsigned char) _mm256_extract_epi8(output_row1,23);
		filt[N-2][col+24] = (unsigned char) _mm256_extract_epi8(output_row1,24);
		filt[N-2][col+25] = (unsigned char) _mm256_extract_epi8(output_row1,25);
		filt[N-2][col+26] = (unsigned char) _mm256_extract_epi8(output_row1,26);
		filt[N-2][col+27] = (unsigned char) _mm256_extract_epi8(output_row1,27);
		filt[N-2][col+28] = (unsigned char) _mm256_extract_epi8(output_row1,28);

		filt[N-1][col+16] = (unsigned char) _mm256_extract_epi8(output_row0,16);
		filt[N-1][col+17] = (unsigned char) _mm256_extract_epi8(output_row0,17);
		filt[N-1][col+18] = (unsigned char) _mm256_extract_epi8(output_row0,18);
		filt[N-1][col+19] = (unsigned char) _mm256_extract_epi8(output_row0,19);
		filt[N-1][col+20] = (unsigned char) _mm256_extract_epi8(output_row0,20);
		filt[N-1][col+21] = (unsigned char) _mm256_extract_epi8(output_row0,21);
		filt[N-1][col+22] = (unsigned char) _mm256_extract_epi8(output_row0,22);
		filt[N-1][col+23] = (unsigned char) _mm256_extract_epi8(output_row0,23);
		filt[N-1][col+24] = (unsigned char) _mm256_extract_epi8(output_row0,24);
		filt[N-1][col+25] = (unsigned char) _mm256_extract_epi8(output_row0,25);
		filt[N-1][col+26] = (unsigned char) _mm256_extract_epi8(output_row0,26);
		filt[N-1][col+27] = (unsigned char) _mm256_extract_epi8(output_row0,27);
		filt[N-1][col+28] = (unsigned char) _mm256_extract_epi8(output_row0,28);

		  break;
	case 30:
	  	_mm_storeu_si128( (__m128i *) &filt[N-2][col],_mm256_extractf128_si256(output_row1, 0)); //store low 128bit - 16pixels
	  	_mm_storeu_si128( (__m128i *) &filt[N-1][col],_mm256_extractf128_si256(output_row0, 0)); //store low 128bit - 16pixels

		filt[N-2][col+16] = (unsigned char) _mm256_extract_epi8(output_row1,16);
		filt[N-2][col+17] = (unsigned char) _mm256_extract_epi8(output_row1,17);
		filt[N-2][col+18] = (unsigned char) _mm256_extract_epi8(output_row1,18);
		filt[N-2][col+19] = (unsigned char) _mm256_extract_epi8(output_row1,19);
		filt[N-2][col+20] = (unsigned char) _mm256_extract_epi8(output_row1,20);
		filt[N-2][col+21] = (unsigned char) _mm256_extract_epi8(output_row1,21);
		filt[N-2][col+22] = (unsigned char) _mm256_extract_epi8(output_row1,22);
		filt[N-2][col+23] = (unsigned char) _mm256_extract_epi8(output_row1,23);
		filt[N-2][col+24] = (unsigned char) _mm256_extract_epi8(output_row1,24);
		filt[N-2][col+25] = (unsigned char) _mm256_extract_epi8(output_row1,25);
		filt[N-2][col+26] = (unsigned char) _mm256_extract_epi8(output_row1,26);
		filt[N-2][col+27] = (unsigned char) _mm256_extract_epi8(output_row1,27);
		filt[N-2][col+28] = (unsigned char) _mm256_extract_epi8(output_row1,28);

		filt[N-1][col+16] = (unsigned char) _mm256_extract_epi8(output_row0,16);
		filt[N-1][col+17] = (unsigned char) _mm256_extract_epi8(output_row0,17);
		filt[N-1][col+18] = (unsigned char) _mm256_extract_epi8(output_row0,18);
		filt[N-1][col+19] = (unsigned char) _mm256_extract_epi8(output_row0,19);
		filt[N-1][col+20] = (unsigned char) _mm256_extract_epi8(output_row0,20);
		filt[N-1][col+21] = (unsigned char) _mm256_extract_epi8(output_row0,21);
		filt[N-1][col+22] = (unsigned char) _mm256_extract_epi8(output_row0,22);
		filt[N-1][col+23] = (unsigned char) _mm256_extract_epi8(output_row0,23);
		filt[N-1][col+24] = (unsigned char) _mm256_extract_epi8(output_row0,24);
		filt[N-1][col+25] = (unsigned char) _mm256_extract_epi8(output_row0,25);
		filt[N-1][col+26] = (unsigned char) _mm256_extract_epi8(output_row0,26);
		filt[N-1][col+27] = (unsigned char) _mm256_extract_epi8(output_row0,27);
		filt[N-1][col+28] = (unsigned char) _mm256_extract_epi8(output_row0,28);

	//the filt[row][col+29] is computed unvectorized
	newPixel = 0;
	newPixel += frame1[N-2-1][M-1-1] * filter[0][0];
	newPixel += frame1[N-2-1][M-1] * filter[0][1];

	newPixel += frame1[N-2][M-1-1] * filter[1][0];
	newPixel += frame1[N-2][M-1] * filter[1][1];

	newPixel += frame1[N-2+1][M-1-1] * filter[2][0];
	newPixel += frame1[N-2+1][M-1] * filter[2][1];

	filt[N-2][M-1] = (unsigned char) (newPixel / divisor);

	newPixel = 0;
	newPixel += frame1[N-1-1][M-1-1] * filter[0][0];
	newPixel += frame1[N-1-1][M-1] * filter[0][1];

	newPixel += frame1[N-1][M-1-1] * filter[1][0];
	newPixel += frame1[N-1][M-1] * filter[1][1];

	filt[N-1][M-1] = (unsigned char) (newPixel / divisor);

		  break;

	case 31:
	  	_mm_storeu_si128( (__m128i *) &filt[N-2][col],_mm256_extractf128_si256(output_row1, 0)); //store low 128bit - 16pixels
	  	_mm_storeu_si128( (__m128i *) &filt[N-1][col],_mm256_extractf128_si256(output_row0, 0)); //store low 128bit - 16pixels

		filt[N-2][col+16] = (unsigned char) _mm256_extract_epi8(output_row1,16);
		filt[N-2][col+17] = (unsigned char) _mm256_extract_epi8(output_row1,17);
		filt[N-2][col+18] = (unsigned char) _mm256_extract_epi8(output_row1,18);
		filt[N-2][col+19] = (unsigned char) _mm256_extract_epi8(output_row1,19);
		filt[N-2][col+20] = (unsigned char) _mm256_extract_epi8(output_row1,20);
		filt[N-2][col+21] = (unsigned char) _mm256_extract_epi8(output_row1,21);
		filt[N-2][col+22] = (unsigned char) _mm256_extract_epi8(output_row1,22);
		filt[N-2][col+23] = (unsigned char) _mm256_extract_epi8(output_row1,23);
		filt[N-2][col+24] = (unsigned char) _mm256_extract_epi8(output_row1,24);
		filt[N-2][col+25] = (unsigned char) _mm256_extract_epi8(output_row1,25);
		filt[N-2][col+26] = (unsigned char) _mm256_extract_epi8(output_row1,26);
		filt[N-2][col+27] = (unsigned char) _mm256_extract_epi8(output_row1,27);
		filt[N-2][col+28] = (unsigned char) _mm256_extract_epi8(output_row1,28);

		filt[N-1][col+16] = (unsigned char) _mm256_extract_epi8(output_row0,16);
		filt[N-1][col+17] = (unsigned char) _mm256_extract_epi8(output_row0,17);
		filt[N-1][col+18] = (unsigned char) _mm256_extract_epi8(output_row0,18);
		filt[N-1][col+19] = (unsigned char) _mm256_extract_epi8(output_row0,19);
		filt[N-1][col+20] = (unsigned char) _mm256_extract_epi8(output_row0,20);
		filt[N-1][col+21] = (unsigned char) _mm256_extract_epi8(output_row0,21);
		filt[N-1][col+22] = (unsigned char) _mm256_extract_epi8(output_row0,22);
		filt[N-1][col+23] = (unsigned char) _mm256_extract_epi8(output_row0,23);
		filt[N-1][col+24] = (unsigned char) _mm256_extract_epi8(output_row0,24);
		filt[N-1][col+25] = (unsigned char) _mm256_extract_epi8(output_row0,25);
		filt[N-1][col+26] = (unsigned char) _mm256_extract_epi8(output_row0,26);
		filt[N-1][col+27] = (unsigned char) _mm256_extract_epi8(output_row0,27);
		filt[N-1][col+28] = (unsigned char) _mm256_extract_epi8(output_row0,28);

	//the filt[N-2][col+29/30] are computed unvectorized
		newPixel = 0;
		newPixel += frame1[N-2-1][M-2-1] * filter[0][0];
		newPixel += frame1[N-2-1][M-2] * filter[0][1];
		newPixel += frame1[N-2-1][M-2+1] * filter[0][2];

		newPixel += frame1[N-2][M-2-1] * filter[1][0];
		newPixel += frame1[N-2][M-2] * filter[1][1];
		newPixel += frame1[N-2][M-2+1] * filter[1][2];

		newPixel += frame1[N-2+1][M-2-1] * filter[2][0];
		newPixel += frame1[N-2+1][M-2] * filter[2][1];
		newPixel += frame1[N-2+1][M-2+1] * filter[2][2];

		filt[N-2][M-2] = (unsigned char) (newPixel / divisor);

		newPixel = 0;
		newPixel += frame1[N-2-1][M-1-1] * filter[0][0];
		newPixel += frame1[N-2-1][M-1] * filter[0][1];

		newPixel += frame1[N-2][M-1-1] * filter[1][0];
		newPixel += frame1[N-2][M-1] * filter[1][1];

		newPixel += frame1[N-2+1][M-1-1] * filter[2][0];
		newPixel += frame1[N-2+1][M-1] * filter[2][1];

		filt[N-2][M-1] = (unsigned char) (newPixel / divisor);

		newPixel = 0;

		newPixel += frame1[N-1-1][M-2-1] * filter[0][0];
		newPixel += frame1[N-1-1][M-2] * filter[0][1];
		newPixel += frame1[N-1-1][M-2+1] * filter[0][2];

		newPixel += frame1[N-1][M-2-1] * filter[1][0];
		newPixel += frame1[N-1][M-2] * filter[1][1];
		newPixel += frame1[N-1][M-2+1] * filter[1][2];

		filt[N-1][M-2] = (unsigned char) (newPixel / divisor);

		newPixel = 0;

		newPixel += frame1[N-1-1][M-1-1] * filter[0][0];
		newPixel += frame1[N-1-1][M-1] * filter[0][1];

		newPixel += frame1[N-1][M-1-1] * filter[1][0];
		newPixel += frame1[N-1][M-1] * filter[1][1];

		filt[N-1][M-1] = (unsigned char) (newPixel / divisor);


		  break;

	default:
		printf("\nsomething went wrong");
		return -1;
 	}

return 0;

}




void Gaussian_Blur_optimized_3x3_reg_blocking_32(unsigned char **frame1,unsigned char **filt,const unsigned int M, const unsigned int N, const unsigned short int divisor, signed char **filter){

const signed char f00=filter[0][0];
const signed char f01=filter[0][1];
const signed char f02=filter[0][2];

const signed char f10=filter[1][0];
const signed char f11=filter[1][1];
const signed char f12=filter[1][2];


	const __m256i c0=_mm256_set_epi8(0,f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00);
	const __m256i c1=_mm256_set_epi8(0,f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10);

	const __m256i c0_sh1=_mm256_set_epi8(f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00,0);
	const __m256i c1_sh1=_mm256_set_epi8(f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10,0);


	const __m256i c0_sh2=_mm256_set_epi8(0,0,0,f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00,0,0);
	const __m256i c1_sh2=_mm256_set_epi8(0,0,0,f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10,0,0);

	const __m256i c0_sh3=_mm256_set_epi8(0,0,f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00,0,0,0);
	const __m256i c1_sh3=_mm256_set_epi8(0,0,f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10,0,0,0);


	const __m256i mask3  = _mm256_set_epi16(0,0,0,0,0,0,0,65535,0,0,0,0,0,0,0,0);
	const __m256i mask_prelude  = _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0);




	const unsigned int REMINDER_ITERATIONS =  (M-((((M-32)/30)*30)+30)); //M-(last_col_value+30)
	//printf("\n%d",REMINDER_ITERATIONS);


	const unsigned int division_case=prepare_for_division(divisor); //determine which is the division case (A, B or C)
	const __m256i f = _mm256_load_si256( (__m256i *) &f_vector[0]);// initialize the division vector

//printf("\n%d %d",division_case,b);


#pragma omp parallel
{

int row,col;
register __m256i r0,r1,r2,r3,r4,m0,m1,m2,m3,m4;
//__m256i output_row0_even,output_row1_even,output_row0_odd,output_row1_odd; //these are for processing row #0 and #1 only.
__m256i output1,output2,output3,output4 ;
//__m256i m0_prelude_row0,m0_prelude_row1;


/*---------------------- Gaussian Blur ---------------------------------*/

    #pragma omp for schedule(static)
	for (row = -1; row < N; row+=3) {

	if (row==-1){//special case compute filt[0:1][:]

		for (col = 0; col <= M-32; col+=30){

		  if (col==0){

				//load the 5 rows
				r0=_mm256_loadu_si256( (__m256i *) &frame1[0][0]);
				r1=_mm256_loadu_si256( (__m256i *) &frame1[1][0]);
				r2=_mm256_loadu_si256( (__m256i *) &frame1[2][0]);


		  			//START - extra code needed for prelude
		  					m0=_mm256_slli_si256(r0,1);//shift 1 elements left - equivalent to filling with one zero inthe beginning
		  					m1=_mm256_slli_si256(r1,1);//shift 1 elements left - equivalent to filling with one zero inthe beginning
		  					m2=_mm256_slli_si256(r2,1);//shift 1 elements left - equivalent to filling with one zero inthe beginning

		  					//preserve the element being lost because of the shift above
		  					r0=_mm256_and_si256(r0,mask_prelude);
		  					r0=_mm256_permute2f128_si256(r0,r0,1);
		  					r0=_mm256_srli_si256(r0,15);//shift 15 elements
		  					r0=_mm256_add_epi16(m0,r0);

		  					r1=_mm256_and_si256(r1,mask_prelude);
		  					r1=_mm256_permute2f128_si256(r1,r1,1);
		  					r1=_mm256_srli_si256(r1,15);//shift 15 elements
		  					r1=_mm256_add_epi16(m1,r1);

		  					r2=_mm256_and_si256(r2,mask_prelude);
		  					r2=_mm256_permute2f128_si256(r2,r2,1);
		  					r2=_mm256_srli_si256(r2,15);//shift 15 elements
		  					r2=_mm256_add_epi16(m2,r2);

		  			//END - extra code needed for prelude
		  		}
		  else {
				//load the 5 rows
				r0=_mm256_loadu_si256( (__m256i *) &frame1[0][col-1]);
				r1=_mm256_loadu_si256( (__m256i *) &frame1[1][col-1]);
				r2=_mm256_loadu_si256( (__m256i *) &frame1[2][col-1]);
		  }

		//col iteration computes output pixels of    1,5,9,13,17,21,25,29
		// col+1 iteration computes output pixels of 2,6,10,14,18,22,26,30
		// col+2 iteration computes output pixels of 3,7,11,15,19,23,27
		// col+3 iteration computes output pixels of 4,8,12,16,20,24,28
		//afterwards, col becomes 30 and repeat the above process
			 //row==1
				//1st col iteration

				//multiply with the mask
				m0=_mm256_maddubs_epi16(r0,c0);
				m1=_mm256_maddubs_epi16(r1,c1);
				m2=_mm256_maddubs_epi16(r2,c0);

				//make 16-bit values 32-bit (horizontal add)
				__m256i ones=_mm256_set1_epi16(1);
				m0=_mm256_madd_epi16(m0,ones);
				m1=_mm256_madd_epi16(m1,ones);
				m2=_mm256_madd_epi16(m2,ones);

				//vertical add
				m0=_mm256_add_epi32(m0,m1);
				m0=_mm256_add_epi32(m0,m2);

				//m0 has 8 32bit values now.
				output1 = division_32(division_case,m0,f);


				//2nd col iteration
				//multiply with the mask
				m0=_mm256_maddubs_epi16(r0,c0_sh1);
				m1=_mm256_maddubs_epi16(r1,c1_sh1);
				m2=_mm256_maddubs_epi16(r2,c0_sh1);

				//make 16-bit values 32-bit (horizontal add)
				m0=_mm256_madd_epi16(m0,ones);
				m1=_mm256_madd_epi16(m1,ones);
				m2=_mm256_madd_epi16(m2,ones);

				//vertical add
				m0=_mm256_add_epi32(m0,m1);
				m0=_mm256_add_epi32(m0,m2);

				//m0 has 8 32bit values now.
				output2 = division_32(division_case,m0,f);


		         //3rd col iteration
				//multiply with the mask
				m0=_mm256_maddubs_epi16(r0,c0_sh2);
				m1=_mm256_maddubs_epi16(r1,c1_sh2);
				m2=_mm256_maddubs_epi16(r2,c0_sh2);

				//shift m0 by one position because hadd follows
				m3=_mm256_srli_si256(m0,2);
				m4=_mm256_and_si256(m0,mask3);
				m4=_mm256_permute2f128_si256(m4,m4,1);
				m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
				m0=_mm256_add_epi16(m3,m4);

				//shift m1 by one position because hadd follows
				m3=_mm256_srli_si256(m1,2);
				m4=_mm256_and_si256(m1,mask3);
				m4=_mm256_permute2f128_si256(m4,m4,1);
				m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
				m1=_mm256_add_epi16(m3,m4);

				//shift m2 by one position because hadd follows
				m3=_mm256_srli_si256(m2,2);
				m4=_mm256_and_si256(m2,mask3);
				m4=_mm256_permute2f128_si256(m4,m4,1);
				m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
				m2=_mm256_add_epi16(m3,m4);

				//make 16-bit values 32-bit (horizontal add)
				m0=_mm256_madd_epi16(m0,ones);
				m1=_mm256_madd_epi16(m1,ones);
				m2=_mm256_madd_epi16(m2,ones);

				//vertical add
				m0=_mm256_add_epi32(m0,m1);
				m0=_mm256_add_epi32(m0,m2);

				//m0 has 8 32bit values now.
				output3 = division_32(division_case,m0,f);

				//4th col iteration

				//multiply with the mask
				m0=_mm256_maddubs_epi16(r0,c0_sh3);
				m1=_mm256_maddubs_epi16(r1,c1_sh3);
				m2=_mm256_maddubs_epi16(r2,c0_sh3);

				//shift m0 by one position because hadd follows
				m3=_mm256_srli_si256(m0,2);
				m4=_mm256_and_si256(m0,mask3);
				m4=_mm256_permute2f128_si256(m4,m4,1);
				m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
				m0=_mm256_add_epi16(m3,m4);

				//shift m1 by one position because hadd follows
				m3=_mm256_srli_si256(m1,2);
				m4=_mm256_and_si256(m1,mask3);
				m4=_mm256_permute2f128_si256(m4,m4,1);
				m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
				m1=_mm256_add_epi16(m3,m4);

				//shift m2 by one position because hadd follows
				m3=_mm256_srli_si256(m2,2);
				m4=_mm256_and_si256(m2,mask3);
				m4=_mm256_permute2f128_si256(m4,m4,1);
				m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
				m2=_mm256_add_epi16(m3,m4);

				//make 16-bit values 32-bit (horizontal add)
				m0=_mm256_madd_epi16(m0,ones);
				m1=_mm256_madd_epi16(m1,ones);
				m2=_mm256_madd_epi16(m2,ones);

				//vertical add
				m0=_mm256_add_epi32(m0,m1);
				m0=_mm256_add_epi32(m0,m2);

				//m0 has 8 32bit values now.
				output4 = division_32(division_case,m0,f);

				//the 32-bit division generates 8-bit values that are stored into memory.

				//blend output1, output2, output3, output4 into one register
				output2 = _mm256_slli_si256(output2,1);
				output1 = _mm256_add_epi8(output1,output2);
				output3 = _mm256_slli_si256(output3,2);
				output1 = _mm256_add_epi8(output1,output3);
				output4 = _mm256_slli_si256(output4,3);
				output1 = _mm256_add_epi8(output1,output4);

				_mm256_storeu_si256( (__m256i *) &filt[1][col],output1 );

			 //row==0
				//1st col iteration

				//multiply with the mask
				m0=_mm256_maddubs_epi16(r0,c1);
				m1=_mm256_maddubs_epi16(r1,c0);

				//make 16-bit values 32-bit (horizontal add)
				ones=_mm256_set1_epi16(1);
				m0=_mm256_madd_epi16(m0,ones);
				m1=_mm256_madd_epi16(m1,ones);

				//vertical add
				m0=_mm256_add_epi32(m0,m1);

				//m0 has 8 32bit values now.
				output1 = division_32(division_case,m0,f);


				//2nd col iteration
				//multiply with the mask
				m0=_mm256_maddubs_epi16(r0,c1_sh1);
				m1=_mm256_maddubs_epi16(r1,c0_sh1);

				//make 16-bit values 32-bit (horizontal add)
				m0=_mm256_madd_epi16(m0,ones);
				m1=_mm256_madd_epi16(m1,ones);

				//vertical add
				m0=_mm256_add_epi32(m0,m1);

				//m0 has 8 32bit values now.
				output2 = division_32(division_case,m0,f);


		         //3rd col iteration
				//multiply with the mask
				m0=_mm256_maddubs_epi16(r0,c1_sh2);
				m1=_mm256_maddubs_epi16(r1,c0_sh2);

				//shift m0 by one position because hadd follows
				m3=_mm256_srli_si256(m0,2);
				m4=_mm256_and_si256(m0,mask3);
				m4=_mm256_permute2f128_si256(m4,m4,1);
				m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
				m0=_mm256_add_epi16(m3,m4);

				//shift m1 by one position because hadd follows
				m3=_mm256_srli_si256(m1,2);
				m4=_mm256_and_si256(m1,mask3);
				m4=_mm256_permute2f128_si256(m4,m4,1);
				m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
				m1=_mm256_add_epi16(m3,m4);


				//make 16-bit values 32-bit (horizontal add)
				m0=_mm256_madd_epi16(m0,ones);
				m1=_mm256_madd_epi16(m1,ones);

				//vertical add
				m0=_mm256_add_epi32(m0,m1);

				//m0 has 8 32bit values now.
				output3 = division_32(division_case,m0,f);

				//4th col iteration

				//multiply with the mask
				m0=_mm256_maddubs_epi16(r0,c1_sh3);
				m1=_mm256_maddubs_epi16(r1,c0_sh3);

				//shift m0 by one position because hadd follows
				m3=_mm256_srli_si256(m0,2);
				m4=_mm256_and_si256(m0,mask3);
				m4=_mm256_permute2f128_si256(m4,m4,1);
				m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
				m0=_mm256_add_epi16(m3,m4);

				//shift m1 by one position because hadd follows
				m3=_mm256_srli_si256(m1,2);
				m4=_mm256_and_si256(m1,mask3);
				m4=_mm256_permute2f128_si256(m4,m4,1);
				m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
				m1=_mm256_add_epi16(m3,m4);


				//make 16-bit values 32-bit (horizontal add)
				m0=_mm256_madd_epi16(m0,ones);
				m1=_mm256_madd_epi16(m1,ones);

				//vertical add
				m0=_mm256_add_epi32(m0,m1);

				//m0 has 8 32bit values now.
				output4 = division_32(division_case,m0,f);

				//the 32-bit division generates 8-bit values that are stored into memory.

				//blend output1, output2, output3, output4 into one register
				output2 = _mm256_slli_si256(output2,1);
				output1 = _mm256_add_epi8(output1,output2);
				output3 = _mm256_slli_si256(output3,2);
				output1 = _mm256_add_epi8(output1,output3);
				output4 = _mm256_slli_si256(output4,3);
				output1 = _mm256_add_epi8(output1,output4);

				_mm256_storeu_si256( (__m256i *) &filt[0][col],output1 );

		}
	loop_reminder_3x3_first_values_32(frame1,filt,M,N,col,REMINDER_ITERATIONS,division_case,divisor,filter,c0,c1,c0_sh1,c1_sh1,c0_sh2,c1_sh2,c0_sh3,c1_sh3,f);

	}

	else if (row==N-2){//special case compute filt[N-2:N-1][:]
		//printf("\nddd");
		for (col = 0; col <= M-32; col+=30){

		  if (col==0){

				//load the 5 rows
				r0=_mm256_loadu_si256( (__m256i *) &frame1[N-3][0]);
				r1=_mm256_loadu_si256( (__m256i *) &frame1[N-2][0]);
				r2=_mm256_loadu_si256( (__m256i *) &frame1[N-1][0]);


		  			//START - extra code needed for prelude
		  					m0=_mm256_slli_si256(r0,1);//shift 1 elements left - equivalent to filling with one zero inthe beginning
		  					m1=_mm256_slli_si256(r1,1);//shift 1 elements left - equivalent to filling with one zero inthe beginning
		  					m2=_mm256_slli_si256(r2,1);//shift 1 elements left - equivalent to filling with one zero inthe beginning

		  					//preserve the element being lost because of the shift above
		  					r0=_mm256_and_si256(r0,mask_prelude);
		  					r0=_mm256_permute2f128_si256(r0,r0,1);
		  					r0=_mm256_srli_si256(r0,15);//shift 15 elements
		  					r0=_mm256_add_epi16(m0,r0);

		  					r1=_mm256_and_si256(r1,mask_prelude);
		  					r1=_mm256_permute2f128_si256(r1,r1,1);
		  					r1=_mm256_srli_si256(r1,15);//shift 15 elements
		  					r1=_mm256_add_epi16(m1,r1);

		  					r2=_mm256_and_si256(r2,mask_prelude);
		  					r2=_mm256_permute2f128_si256(r2,r2,1);
		  					r2=_mm256_srli_si256(r2,15);//shift 15 elements
		  					r2=_mm256_add_epi16(m2,r2);

		  			//END - extra code needed for prelude
		  		}
		  else {
				//load the 5 rows
				r0=_mm256_loadu_si256( (__m256i *) &frame1[N-3][col-1]);
				r1=_mm256_loadu_si256( (__m256i *) &frame1[N-2][col-1]);
				r2=_mm256_loadu_si256( (__m256i *) &frame1[N-1][col-1]);
		  }

		//col iteration computes output pixels of    1,5,9,13,17,21,25,29
		// col+1 iteration computes output pixels of 2,6,10,14,18,22,26,30
		// col+2 iteration computes output pixels of 3,7,11,15,19,23,27
		// col+3 iteration computes output pixels of 4,8,12,16,20,24,28
		//afterwards, col becomes 30 and repeat the above process

		 //row==N-2
			//1st col iteration

				//multiply with the mask
				m0=_mm256_maddubs_epi16(r0,c0);
				m1=_mm256_maddubs_epi16(r1,c1);
				m2=_mm256_maddubs_epi16(r2,c0);

				//make 16-bit values 32-bit (horizontal add)
				__m256i ones=_mm256_set1_epi16(1);
				m0=_mm256_madd_epi16(m0,ones);
				m1=_mm256_madd_epi16(m1,ones);
				m2=_mm256_madd_epi16(m2,ones);

				//vertical add
				m0=_mm256_add_epi32(m0,m1);
				m0=_mm256_add_epi32(m0,m2);

				//m0 has 8 32bit values now.
				output1 = division_32(division_case,m0,f);


				//2nd col iteration
				//multiply with the mask
				m0=_mm256_maddubs_epi16(r0,c0_sh1);
				m1=_mm256_maddubs_epi16(r1,c1_sh1);
				m2=_mm256_maddubs_epi16(r2,c0_sh1);

				//make 16-bit values 32-bit (horizontal add)
				m0=_mm256_madd_epi16(m0,ones);
				m1=_mm256_madd_epi16(m1,ones);
				m2=_mm256_madd_epi16(m2,ones);

				//vertical add
				m0=_mm256_add_epi32(m0,m1);
				m0=_mm256_add_epi32(m0,m2);

				//m0 has 8 32bit values now.
				output2 = division_32(division_case,m0,f);


		         //3rd col iteration
				//multiply with the mask
				m0=_mm256_maddubs_epi16(r0,c0_sh2);
				m1=_mm256_maddubs_epi16(r1,c1_sh2);
				m2=_mm256_maddubs_epi16(r2,c0_sh2);

				//shift m0 by one position because hadd follows
				m3=_mm256_srli_si256(m0,2);
				m4=_mm256_and_si256(m0,mask3);
				m4=_mm256_permute2f128_si256(m4,m4,1);
				m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
				m0=_mm256_add_epi16(m3,m4);

				//shift m1 by one position because hadd follows
				m3=_mm256_srli_si256(m1,2);
				m4=_mm256_and_si256(m1,mask3);
				m4=_mm256_permute2f128_si256(m4,m4,1);
				m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
				m1=_mm256_add_epi16(m3,m4);

				//shift m2 by one position because hadd follows
				m3=_mm256_srli_si256(m2,2);
				m4=_mm256_and_si256(m2,mask3);
				m4=_mm256_permute2f128_si256(m4,m4,1);
				m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
				m2=_mm256_add_epi16(m3,m4);

				//make 16-bit values 32-bit (horizontal add)
				m0=_mm256_madd_epi16(m0,ones);
				m1=_mm256_madd_epi16(m1,ones);
				m2=_mm256_madd_epi16(m2,ones);

				//vertical add
				m0=_mm256_add_epi32(m0,m1);
				m0=_mm256_add_epi32(m0,m2);

				//m0 has 8 32bit values now.
				output3 = division_32(division_case,m0,f);

				//4th col iteration

				//multiply with the mask
				m0=_mm256_maddubs_epi16(r0,c0_sh3);
				m1=_mm256_maddubs_epi16(r1,c1_sh3);
				m2=_mm256_maddubs_epi16(r2,c0_sh3);

				//shift m0 by one position because hadd follows
				m3=_mm256_srli_si256(m0,2);
				m4=_mm256_and_si256(m0,mask3);
				m4=_mm256_permute2f128_si256(m4,m4,1);
				m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
				m0=_mm256_add_epi16(m3,m4);

				//shift m1 by one position because hadd follows
				m3=_mm256_srli_si256(m1,2);
				m4=_mm256_and_si256(m1,mask3);
				m4=_mm256_permute2f128_si256(m4,m4,1);
				m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
				m1=_mm256_add_epi16(m3,m4);

				//shift m2 by one position because hadd follows
				m3=_mm256_srli_si256(m2,2);
				m4=_mm256_and_si256(m2,mask3);
				m4=_mm256_permute2f128_si256(m4,m4,1);
				m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
				m2=_mm256_add_epi16(m3,m4);

				//make 16-bit values 32-bit (horizontal add)
				m0=_mm256_madd_epi16(m0,ones);
				m1=_mm256_madd_epi16(m1,ones);
				m2=_mm256_madd_epi16(m2,ones);

				//vertical add
				m0=_mm256_add_epi32(m0,m1);
				m0=_mm256_add_epi32(m0,m2);

				//m0 has 8 32bit values now.
				output4 = division_32(division_case,m0,f);

				//the 32-bit division generates 8-bit values that are stored into memory.

				//blend output1, output2, output3, output4 into one register
				output2 = _mm256_slli_si256(output2,1);
				output1 = _mm256_add_epi8(output1,output2);
				output3 = _mm256_slli_si256(output3,2);
				output1 = _mm256_add_epi8(output1,output3);
				output4 = _mm256_slli_si256(output4,3);
				output1 = _mm256_add_epi8(output1,output4);

				_mm256_storeu_si256( (__m256i *) &filt[N-2][col],output1 );

			 //row==N-1
				//1st col iteration

				//multiply with the mask
				m0=_mm256_maddubs_epi16(r1,c0);
				m1=_mm256_maddubs_epi16(r2,c1);

				//make 16-bit values 32-bit (horizontal add)
				ones=_mm256_set1_epi16(1);
				m0=_mm256_madd_epi16(m0,ones);
				m1=_mm256_madd_epi16(m1,ones);

				//vertical add
				m0=_mm256_add_epi32(m0,m1);

				//m0 has 8 32bit values now.
				output1 = division_32(division_case,m0,f);


				//2nd col iteration
				//multiply with the mask
				m0=_mm256_maddubs_epi16(r1,c0_sh1);
				m1=_mm256_maddubs_epi16(r2,c1_sh1);

				//make 16-bit values 32-bit (horizontal add)
				m0=_mm256_madd_epi16(m0,ones);
				m1=_mm256_madd_epi16(m1,ones);

				//vertical add
				m0=_mm256_add_epi32(m0,m1);

				//m0 has 8 32bit values now.
				output2 = division_32(division_case,m0,f);


		         //3rd col iteration
				//multiply with the mask
				m0=_mm256_maddubs_epi16(r1,c0_sh2);
				m1=_mm256_maddubs_epi16(r2,c1_sh2);

				//shift m0 by one position because hadd follows
				m3=_mm256_srli_si256(m0,2);
				m4=_mm256_and_si256(m0,mask3);
				m4=_mm256_permute2f128_si256(m4,m4,1);
				m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
				m0=_mm256_add_epi16(m3,m4);

				//shift m1 by one position because hadd follows
				m3=_mm256_srli_si256(m1,2);
				m4=_mm256_and_si256(m1,mask3);
				m4=_mm256_permute2f128_si256(m4,m4,1);
				m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
				m1=_mm256_add_epi16(m3,m4);


				//make 16-bit values 32-bit (horizontal add)
				m0=_mm256_madd_epi16(m0,ones);
				m1=_mm256_madd_epi16(m1,ones);

				//vertical add
				m0=_mm256_add_epi32(m0,m1);

				//m0 has 8 32bit values now.
				output3 = division_32(division_case,m0,f);

				//4th col iteration

				//multiply with the mask
				m0=_mm256_maddubs_epi16(r1,c0_sh3);
				m1=_mm256_maddubs_epi16(r2,c1_sh3);

				//shift m0 by one position because hadd follows
				m3=_mm256_srli_si256(m0,2);
				m4=_mm256_and_si256(m0,mask3);
				m4=_mm256_permute2f128_si256(m4,m4,1);
				m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
				m0=_mm256_add_epi16(m3,m4);

				//shift m1 by one position because hadd follows
				m3=_mm256_srli_si256(m1,2);
				m4=_mm256_and_si256(m1,mask3);
				m4=_mm256_permute2f128_si256(m4,m4,1);
				m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
				m1=_mm256_add_epi16(m3,m4);


				//make 16-bit values 32-bit (horizontal add)
				m0=_mm256_madd_epi16(m0,ones);
				m1=_mm256_madd_epi16(m1,ones);

				//vertical add
				m0=_mm256_add_epi32(m0,m1);

				//m0 has 8 32bit values now.
				output4 = division_32(division_case,m0,f);

				//the 32-bit division generates 8-bit values that are stored into memory.

				//blend output1, output2, output3, output4 into one register
				output2 = _mm256_slli_si256(output2,1);
				output1 = _mm256_add_epi8(output1,output2);
				output3 = _mm256_slli_si256(output3,2);
				output1 = _mm256_add_epi8(output1,output3);
				output4 = _mm256_slli_si256(output4,3);
				output1 = _mm256_add_epi8(output1,output4);

				_mm256_storeu_si256( (__m256i *) &filt[N-1][col],output1 );
		}
	loop_reminder_3x3_last_values_32(frame1,filt,M,N,col,REMINDER_ITERATIONS,division_case,divisor,filter,c0,c1,c0_sh1,c1_sh1,c0_sh2,c1_sh2,c0_sh3,c1_sh3,f);

	}

	else if (row==N-1){//special case compute filt[N-1][:]

		for (col = 0; col <= M-32; col+=30){

		  if (col==0){

				r1=_mm256_loadu_si256( (__m256i *) &frame1[N-2][0]);
				r2=_mm256_loadu_si256( (__m256i *) &frame1[N-1][0]);


		  			//START - extra code needed for prelude
		  					m1=_mm256_slli_si256(r1,1);//shift 1 elements left - equivalent to filling with one zero inthe beginning
		  					m2=_mm256_slli_si256(r2,1);//shift 1 elements left - equivalent to filling with one zero inthe beginning

		  					r1=_mm256_and_si256(r1,mask_prelude);
		  					r1=_mm256_permute2f128_si256(r1,r1,1);
		  					r1=_mm256_srli_si256(r1,15);//shift 15 elements
		  					r1=_mm256_add_epi16(m1,r1);

		  					r2=_mm256_and_si256(r2,mask_prelude);
		  					r2=_mm256_permute2f128_si256(r2,r2,1);
		  					r2=_mm256_srli_si256(r2,15);//shift 15 elements
		  					r2=_mm256_add_epi16(m2,r2);

		  			//END - extra code needed for prelude
		  		}
		  else {
				r1=_mm256_loadu_si256( (__m256i *) &frame1[N-2][col-1]);
				r2=_mm256_loadu_si256( (__m256i *) &frame1[N-1][col-1]);
		  }


		 //row==N-1
		//1st col iteration

			//1st col iteration

			//multiply with the mask
			m0=_mm256_maddubs_epi16(r1,c0);
			m1=_mm256_maddubs_epi16(r2,c1);

			//make 16-bit values 32-bit (horizontal add)
			__m256i ones=_mm256_set1_epi16(1);
			m0=_mm256_madd_epi16(m0,ones);
			m1=_mm256_madd_epi16(m1,ones);

			//vertical add
			m0=_mm256_add_epi32(m0,m1);

			//m0 has 8 32bit values now.
			output1 = division_32(division_case,m0,f);


			//2nd col iteration
			//multiply with the mask
			m0=_mm256_maddubs_epi16(r1,c0_sh1);
			m1=_mm256_maddubs_epi16(r2,c1_sh1);

			//make 16-bit values 32-bit (horizontal add)
			m0=_mm256_madd_epi16(m0,ones);
			m1=_mm256_madd_epi16(m1,ones);

			//vertical add
			m0=_mm256_add_epi32(m0,m1);

			//m0 has 8 32bit values now.
			output2 = division_32(division_case,m0,f);


	         //3rd col iteration
			//multiply with the mask
			m0=_mm256_maddubs_epi16(r1,c0_sh2);
			m1=_mm256_maddubs_epi16(r2,c1_sh2);

			//shift m0 by one position because hadd follows
			m3=_mm256_srli_si256(m0,2);
			m4=_mm256_and_si256(m0,mask3);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
			m0=_mm256_add_epi16(m3,m4);

			//shift m1 by one position because hadd follows
			m3=_mm256_srli_si256(m1,2);
			m4=_mm256_and_si256(m1,mask3);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
			m1=_mm256_add_epi16(m3,m4);


			//make 16-bit values 32-bit (horizontal add)
			m0=_mm256_madd_epi16(m0,ones);
			m1=_mm256_madd_epi16(m1,ones);

			//vertical add
			m0=_mm256_add_epi32(m0,m1);

			//m0 has 8 32bit values now.
			output3 = division_32(division_case,m0,f);

			//4th col iteration

			//multiply with the mask
			m0=_mm256_maddubs_epi16(r1,c0_sh3);
			m1=_mm256_maddubs_epi16(r2,c1_sh3);

			//shift m0 by one position because hadd follows
			m3=_mm256_srli_si256(m0,2);
			m4=_mm256_and_si256(m0,mask3);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
			m0=_mm256_add_epi16(m3,m4);

			//shift m1 by one position because hadd follows
			m3=_mm256_srli_si256(m1,2);
			m4=_mm256_and_si256(m1,mask3);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
			m1=_mm256_add_epi16(m3,m4);


			//make 16-bit values 32-bit (horizontal add)
			m0=_mm256_madd_epi16(m0,ones);
			m1=_mm256_madd_epi16(m1,ones);

			//vertical add
			m0=_mm256_add_epi32(m0,m1);

			//m0 has 8 32bit values now.
			output4 = division_32(division_case,m0,f);

			//the 32-bit division generates 8-bit values that are stored into memory.

			//blend output1, output2, output3, output4 into one register
			output2 = _mm256_slli_si256(output2,1);
			output1 = _mm256_add_epi8(output1,output2);
			output3 = _mm256_slli_si256(output3,2);
			output1 = _mm256_add_epi8(output1,output3);
			output4 = _mm256_slli_si256(output4,3);
			output1 = _mm256_add_epi8(output1,output4);

			_mm256_storeu_si256( (__m256i *) &filt[N-1][col],output1 );
		}
	loop_reminder_3x3_last_row_only_32(frame1,filt,M,N,col,REMINDER_ITERATIONS,division_case,divisor,filter,c0,c1,c0_sh1,c1_sh1,c0_sh2,c1_sh2,c0_sh3,c1_sh3,f);

	}

	else if (row==N-3){//special case compute filt[N-2:N-1][:] and filt[N-3][:]

		for (col = 0; col <= M-32; col+=30){

		  if (col==0){

				//load the 5 rows
				r0=_mm256_loadu_si256( (__m256i *) &frame1[N-4][0]);
				r1=_mm256_loadu_si256( (__m256i *) &frame1[N-3][0]);
				r2=_mm256_loadu_si256( (__m256i *) &frame1[N-2][0]);
				r3=_mm256_loadu_si256( (__m256i *) &frame1[N-1][0]);


		  			//START - extra code needed for prelude
		  					m0=_mm256_slli_si256(r0,1);//shift 1 elements left - equivalent to filling with one zero inthe beginning
		  					m1=_mm256_slli_si256(r1,1);//shift 1 elements left - equivalent to filling with one zero inthe beginning
		  					m2=_mm256_slli_si256(r2,1);//shift 1 elements left - equivalent to filling with one zero inthe beginning
		  					m3=_mm256_slli_si256(r3,1);//shift 1 elements left - equivalent to filling with one zero inthe beginning

		  					//preserve the element being lost because of the shift above
		  					r0=_mm256_and_si256(r0,mask_prelude);
		  					r0=_mm256_permute2f128_si256(r0,r0,1);
		  					r0=_mm256_srli_si256(r0,15);//shift 15 elements
		  					r0=_mm256_add_epi16(m0,r0);

		  					r1=_mm256_and_si256(r1,mask_prelude);
		  					r1=_mm256_permute2f128_si256(r1,r1,1);
		  					r1=_mm256_srli_si256(r1,15);//shift 15 elements
		  					r1=_mm256_add_epi16(m1,r1);

		  					r2=_mm256_and_si256(r2,mask_prelude);
		  					r2=_mm256_permute2f128_si256(r2,r2,1);
		  					r2=_mm256_srli_si256(r2,15);//shift 15 elements
		  					r2=_mm256_add_epi16(m2,r2);

		  					r3=_mm256_and_si256(r3,mask_prelude);
		  					r3=_mm256_permute2f128_si256(r3,r3,1);
		  					r3=_mm256_srli_si256(r3,15);//shift 15 elements
		  					r3=_mm256_add_epi16(m3,r3);

		  			//END - extra code needed for prelude
		  		}
		  else {
				r0=_mm256_loadu_si256( (__m256i *) &frame1[N-4][col-1]);
				r1=_mm256_loadu_si256( (__m256i *) &frame1[N-3][col-1]);
				r2=_mm256_loadu_si256( (__m256i *) &frame1[N-2][col-1]);
				r3=_mm256_loadu_si256( (__m256i *) &frame1[N-1][col-1]);
		  }

		//col iteration computes output pixels of    1,5,9,13,17,21,25,29
		// col+1 iteration computes output pixels of 2,6,10,14,18,22,26,30
		// col+2 iteration computes output pixels of 3,7,11,15,19,23,27
		// col+3 iteration computes output pixels of 4,8,12,16,20,24,28
		//afterwards, col becomes 30 and repeat the above process

		  //row=N-3
			 //row==N-2
				//1st col iteration

					//multiply with the mask
					m0=_mm256_maddubs_epi16(r0,c0);
					m1=_mm256_maddubs_epi16(r1,c1);
					m2=_mm256_maddubs_epi16(r2,c0);

					//make 16-bit values 32-bit (horizontal add)
					__m256i ones=_mm256_set1_epi16(1);
					m0=_mm256_madd_epi16(m0,ones);
					m1=_mm256_madd_epi16(m1,ones);
					m2=_mm256_madd_epi16(m2,ones);

					//vertical add
					m0=_mm256_add_epi32(m0,m1);
					m0=_mm256_add_epi32(m0,m2);

					//m0 has 8 32bit values now.
					output1 = division_32(division_case,m0,f);


					//2nd col iteration
					//multiply with the mask
					m0=_mm256_maddubs_epi16(r0,c0_sh1);
					m1=_mm256_maddubs_epi16(r1,c1_sh1);
					m2=_mm256_maddubs_epi16(r2,c0_sh1);

					//make 16-bit values 32-bit (horizontal add)
					m0=_mm256_madd_epi16(m0,ones);
					m1=_mm256_madd_epi16(m1,ones);
					m2=_mm256_madd_epi16(m2,ones);

					//vertical add
					m0=_mm256_add_epi32(m0,m1);
					m0=_mm256_add_epi32(m0,m2);

					//m0 has 8 32bit values now.
					output2 = division_32(division_case,m0,f);


			         //3rd col iteration
					//multiply with the mask
					m0=_mm256_maddubs_epi16(r0,c0_sh2);
					m1=_mm256_maddubs_epi16(r1,c1_sh2);
					m2=_mm256_maddubs_epi16(r2,c0_sh2);

					//shift m0 by one position because hadd follows
					m3=_mm256_srli_si256(m0,2);
					m4=_mm256_and_si256(m0,mask3);
					m4=_mm256_permute2f128_si256(m4,m4,1);
					m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
					m0=_mm256_add_epi16(m3,m4);

					//shift m1 by one position because hadd follows
					m3=_mm256_srli_si256(m1,2);
					m4=_mm256_and_si256(m1,mask3);
					m4=_mm256_permute2f128_si256(m4,m4,1);
					m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
					m1=_mm256_add_epi16(m3,m4);

					//shift m2 by one position because hadd follows
					m3=_mm256_srli_si256(m2,2);
					m4=_mm256_and_si256(m2,mask3);
					m4=_mm256_permute2f128_si256(m4,m4,1);
					m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
					m2=_mm256_add_epi16(m3,m4);

					//make 16-bit values 32-bit (horizontal add)
					m0=_mm256_madd_epi16(m0,ones);
					m1=_mm256_madd_epi16(m1,ones);
					m2=_mm256_madd_epi16(m2,ones);

					//vertical add
					m0=_mm256_add_epi32(m0,m1);
					m0=_mm256_add_epi32(m0,m2);

					//m0 has 8 32bit values now.
					output3 = division_32(division_case,m0,f);

					//4th col iteration

					//multiply with the mask
					m0=_mm256_maddubs_epi16(r0,c0_sh3);
					m1=_mm256_maddubs_epi16(r1,c1_sh3);
					m2=_mm256_maddubs_epi16(r2,c0_sh3);

					//shift m0 by one position because hadd follows
					m3=_mm256_srli_si256(m0,2);
					m4=_mm256_and_si256(m0,mask3);
					m4=_mm256_permute2f128_si256(m4,m4,1);
					m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
					m0=_mm256_add_epi16(m3,m4);

					//shift m1 by one position because hadd follows
					m3=_mm256_srli_si256(m1,2);
					m4=_mm256_and_si256(m1,mask3);
					m4=_mm256_permute2f128_si256(m4,m4,1);
					m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
					m1=_mm256_add_epi16(m3,m4);

					//shift m2 by one position because hadd follows
					m3=_mm256_srli_si256(m2,2);
					m4=_mm256_and_si256(m2,mask3);
					m4=_mm256_permute2f128_si256(m4,m4,1);
					m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
					m2=_mm256_add_epi16(m3,m4);

					//make 16-bit values 32-bit (horizontal add)
					m0=_mm256_madd_epi16(m0,ones);
					m1=_mm256_madd_epi16(m1,ones);
					m2=_mm256_madd_epi16(m2,ones);

					//vertical add
					m0=_mm256_add_epi32(m0,m1);
					m0=_mm256_add_epi32(m0,m2);

					//m0 has 8 32bit values now.
					output4 = division_32(division_case,m0,f);

					//the 32-bit division generates 8-bit values that are stored into memory.

					//blend output1, output2, output3, output4 into one register
					output2 = _mm256_slli_si256(output2,1);
					output1 = _mm256_add_epi8(output1,output2);
					output3 = _mm256_slli_si256(output3,2);
					output1 = _mm256_add_epi8(output1,output3);
					output4 = _mm256_slli_si256(output4,3);
					output1 = _mm256_add_epi8(output1,output4);

					_mm256_storeu_si256( (__m256i *) &filt[N-3][col],output1 );

			 //row==N-2
				//1st col iteration

					//multiply with the mask
					m0=_mm256_maddubs_epi16(r1,c0);
					m1=_mm256_maddubs_epi16(r2,c1);
					m2=_mm256_maddubs_epi16(r3,c0);

					//make 16-bit values 32-bit (horizontal add)
					ones=_mm256_set1_epi16(1);
					m0=_mm256_madd_epi16(m0,ones);
					m1=_mm256_madd_epi16(m1,ones);
					m2=_mm256_madd_epi16(m2,ones);

					//vertical add
					m0=_mm256_add_epi32(m0,m1);
					m0=_mm256_add_epi32(m0,m2);

					//m0 has 8 32bit values now.
					output1 = division_32(division_case,m0,f);


					//2nd col iteration
					//multiply with the mask
					m0=_mm256_maddubs_epi16(r1,c0_sh1);
					m1=_mm256_maddubs_epi16(r2,c1_sh1);
					m2=_mm256_maddubs_epi16(r3,c0_sh1);

					//make 16-bit values 32-bit (horizontal add)
					m0=_mm256_madd_epi16(m0,ones);
					m1=_mm256_madd_epi16(m1,ones);
					m2=_mm256_madd_epi16(m2,ones);

					//vertical add
					m0=_mm256_add_epi32(m0,m1);
					m0=_mm256_add_epi32(m0,m2);

					//m0 has 8 32bit values now.
					output2 = division_32(division_case,m0,f);


			         //3rd col iteration
					//multiply with the mask
					m0=_mm256_maddubs_epi16(r1,c0_sh2);
					m1=_mm256_maddubs_epi16(r2,c1_sh2);
					m2=_mm256_maddubs_epi16(r3,c0_sh2);

					//shift m0 by one position because hadd follows
					m3=_mm256_srli_si256(m0,2);
					m4=_mm256_and_si256(m0,mask3);
					m4=_mm256_permute2f128_si256(m4,m4,1);
					m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
					m0=_mm256_add_epi16(m3,m4);

					//shift m1 by one position because hadd follows
					m3=_mm256_srli_si256(m1,2);
					m4=_mm256_and_si256(m1,mask3);
					m4=_mm256_permute2f128_si256(m4,m4,1);
					m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
					m1=_mm256_add_epi16(m3,m4);

					//shift m2 by one position because hadd follows
					m3=_mm256_srli_si256(m2,2);
					m4=_mm256_and_si256(m2,mask3);
					m4=_mm256_permute2f128_si256(m4,m4,1);
					m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
					m2=_mm256_add_epi16(m3,m4);

					//make 16-bit values 32-bit (horizontal add)
					m0=_mm256_madd_epi16(m0,ones);
					m1=_mm256_madd_epi16(m1,ones);
					m2=_mm256_madd_epi16(m2,ones);

					//vertical add
					m0=_mm256_add_epi32(m0,m1);
					m0=_mm256_add_epi32(m0,m2);

					//m0 has 8 32bit values now.
					output3 = division_32(division_case,m0,f);

					//4th col iteration

					//multiply with the mask
					m0=_mm256_maddubs_epi16(r1,c0_sh3);
					m1=_mm256_maddubs_epi16(r2,c1_sh3);
					m2=_mm256_maddubs_epi16(r3,c0_sh3);

					//shift m0 by one position because hadd follows
					m3=_mm256_srli_si256(m0,2);
					m4=_mm256_and_si256(m0,mask3);
					m4=_mm256_permute2f128_si256(m4,m4,1);
					m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
					m0=_mm256_add_epi16(m3,m4);

					//shift m1 by one position because hadd follows
					m3=_mm256_srli_si256(m1,2);
					m4=_mm256_and_si256(m1,mask3);
					m4=_mm256_permute2f128_si256(m4,m4,1);
					m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
					m1=_mm256_add_epi16(m3,m4);

					//shift m2 by one position because hadd follows
					m3=_mm256_srli_si256(m2,2);
					m4=_mm256_and_si256(m2,mask3);
					m4=_mm256_permute2f128_si256(m4,m4,1);
					m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
					m2=_mm256_add_epi16(m3,m4);

					//make 16-bit values 32-bit (horizontal add)
					m0=_mm256_madd_epi16(m0,ones);
					m1=_mm256_madd_epi16(m1,ones);
					m2=_mm256_madd_epi16(m2,ones);

					//vertical add
					m0=_mm256_add_epi32(m0,m1);
					m0=_mm256_add_epi32(m0,m2);

					//m0 has 8 32bit values now.
					output4 = division_32(division_case,m0,f);

					//the 32-bit division generates 8-bit values that are stored into memory.

					//blend output1, output2, output3, output4 into one register
					output2 = _mm256_slli_si256(output2,1);
					output1 = _mm256_add_epi8(output1,output2);
					output3 = _mm256_slli_si256(output3,2);
					output1 = _mm256_add_epi8(output1,output3);
					output4 = _mm256_slli_si256(output4,3);
					output1 = _mm256_add_epi8(output1,output4);

					_mm256_storeu_si256( (__m256i *) &filt[N-2][col],output1 );

				 //row==N-1
					//1st col iteration

					//multiply with the mask
					m0=_mm256_maddubs_epi16(r2,c0);
					m1=_mm256_maddubs_epi16(r3,c1);

					//make 16-bit values 32-bit (horizontal add)
					ones=_mm256_set1_epi16(1);
					m0=_mm256_madd_epi16(m0,ones);
					m1=_mm256_madd_epi16(m1,ones);

					//vertical add
					m0=_mm256_add_epi32(m0,m1);

					//m0 has 8 32bit values now.
					output1 = division_32(division_case,m0,f);


					//2nd col iteration
					//multiply with the mask
					m0=_mm256_maddubs_epi16(r2,c0_sh1);
					m1=_mm256_maddubs_epi16(r3,c1_sh1);

					//make 16-bit values 32-bit (horizontal add)
					m0=_mm256_madd_epi16(m0,ones);
					m1=_mm256_madd_epi16(m1,ones);

					//vertical add
					m0=_mm256_add_epi32(m0,m1);

					//m0 has 8 32bit values now.
					output2 = division_32(division_case,m0,f);


			         //3rd col iteration
					//multiply with the mask
					m0=_mm256_maddubs_epi16(r2,c0_sh2);
					m1=_mm256_maddubs_epi16(r3,c1_sh2);

					//shift m0 by one position because hadd follows
					m3=_mm256_srli_si256(m0,2);
					m4=_mm256_and_si256(m0,mask3);
					m4=_mm256_permute2f128_si256(m4,m4,1);
					m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
					m0=_mm256_add_epi16(m3,m4);

					//shift m1 by one position because hadd follows
					m3=_mm256_srli_si256(m1,2);
					m4=_mm256_and_si256(m1,mask3);
					m4=_mm256_permute2f128_si256(m4,m4,1);
					m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
					m1=_mm256_add_epi16(m3,m4);


					//make 16-bit values 32-bit (horizontal add)
					m0=_mm256_madd_epi16(m0,ones);
					m1=_mm256_madd_epi16(m1,ones);

					//vertical add
					m0=_mm256_add_epi32(m0,m1);

					//m0 has 8 32bit values now.
					output3 = division_32(division_case,m0,f);

					//4th col iteration

					//multiply with the mask
					m0=_mm256_maddubs_epi16(r2,c0_sh3);
					m1=_mm256_maddubs_epi16(r3,c1_sh3);

					//shift m0 by one position because hadd follows
					m3=_mm256_srli_si256(m0,2);
					m4=_mm256_and_si256(m0,mask3);
					m4=_mm256_permute2f128_si256(m4,m4,1);
					m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
					m0=_mm256_add_epi16(m3,m4);

					//shift m1 by one position because hadd follows
					m3=_mm256_srli_si256(m1,2);
					m4=_mm256_and_si256(m1,mask3);
					m4=_mm256_permute2f128_si256(m4,m4,1);
					m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
					m1=_mm256_add_epi16(m3,m4);


					//make 16-bit values 32-bit (horizontal add)
					m0=_mm256_madd_epi16(m0,ones);
					m1=_mm256_madd_epi16(m1,ones);

					//vertical add
					m0=_mm256_add_epi32(m0,m1);

					//m0 has 8 32bit values now.
					output4 = division_32(division_case,m0,f);

					//the 32-bit division generates 8-bit values that are stored into memory.

					//blend output1, output2, output3, output4 into one register
					output2 = _mm256_slli_si256(output2,1);
					output1 = _mm256_add_epi8(output1,output2);
					output3 = _mm256_slli_si256(output3,2);
					output1 = _mm256_add_epi8(output1,output3);
					output4 = _mm256_slli_si256(output4,3);
					output1 = _mm256_add_epi8(output1,output4);

					_mm256_storeu_si256( (__m256i *) &filt[N-1][col],output1 );
			}
	loop_reminder_3x3_last_values_32(frame1,filt,M,N,col,REMINDER_ITERATIONS,division_case,divisor,filter,c0,c1,c0_sh1,c1_sh1,c0_sh2,c1_sh2,c0_sh3,c1_sh3,f);
	loop_reminder_3x3_32(frame1,filt,M,N,N-3,col,REMINDER_ITERATIONS,division_case,divisor,filter,c0,c1,c0_sh1,c1_sh1,c0_sh2,c1_sh2,c0_sh3,c1_sh3,f);

	}

	else {//main loop

	  for (col = 0; col <= M-32; col+=30){

		  if (col==0){

				//load the 5 rows
				r0=_mm256_loadu_si256( (__m256i *) &frame1[row-1][0]);
				r1=_mm256_loadu_si256( (__m256i *) &frame1[row][0]);
				r2=_mm256_loadu_si256( (__m256i *) &frame1[row+1][0]);
				r3=_mm256_loadu_si256( (__m256i *) &frame1[row+2][0]);
				r4=_mm256_loadu_si256( (__m256i *) &frame1[row+3][0]);



		  			//START - extra code needed for prelude
		  					m0=_mm256_slli_si256(r0,1);//shift 1 elements left - equivalent to filling with one zero inthe beginning
		  					m1=_mm256_slli_si256(r1,1);//shift 1 elements left - equivalent to filling with one zero inthe beginning
		  					m2=_mm256_slli_si256(r2,1);//shift 1 elements left - equivalent to filling with one zero inthe beginning
		  					m3=_mm256_slli_si256(r3,1);//shift 1 elements left - equivalent to filling with one zero inthe beginning
		  					m4=_mm256_slli_si256(r4,1);//shift 1 elements left - equivalent to filling with one zero inthe beginning


		  					//preserve the element being lost because of the shift above
		  					r0=_mm256_and_si256(r0,mask_prelude);
		  					r0=_mm256_permute2f128_si256(r0,r0,1);
		  					r0=_mm256_srli_si256(r0,15);//shift 15 elements
		  					r0=_mm256_add_epi16(m0,r0);

		  					r1=_mm256_and_si256(r1,mask_prelude);
		  					r1=_mm256_permute2f128_si256(r1,r1,1);
		  					r1=_mm256_srli_si256(r1,15);//shift 15 elements
		  					r1=_mm256_add_epi16(m1,r1);

		  					r2=_mm256_and_si256(r2,mask_prelude);
		  					r2=_mm256_permute2f128_si256(r2,r2,1);
		  					r2=_mm256_srli_si256(r2,15);//shift 15 elements
		  					r2=_mm256_add_epi16(m2,r2);

		  					r3=_mm256_and_si256(r3,mask_prelude);
		  					r3=_mm256_permute2f128_si256(r3,r3,1);
		  					r3=_mm256_srli_si256(r3,15);//shift 15 elements
		  					r3=_mm256_add_epi16(m3,r3);

		  					r4=_mm256_and_si256(r4,mask_prelude);
		  					r4=_mm256_permute2f128_si256(r4,r4,1);
		  					r4=_mm256_srli_si256(r4,15);//shift 15 elements
		  					r4=_mm256_add_epi16(m4,r4);

		  			//END - extra code needed for prelude
		  		}
		  else {
				//load the 5 rows
				r0=_mm256_loadu_si256( (__m256i *) &frame1[row-1][col-1]);
				r1=_mm256_loadu_si256( (__m256i *) &frame1[row][col-1]);
				r2=_mm256_loadu_si256( (__m256i *) &frame1[row+1][col-1]);
				r3=_mm256_loadu_si256( (__m256i *) &frame1[row+2][col-1]);
				r4=_mm256_loadu_si256( (__m256i *) &frame1[row+3][col-1]);

		  }

		//col iteration computes output pixels of    1,5,9,13,17,21,25,29
		// col+1 iteration computes output pixels of 2,6,10,14,18,22,26,30
		// col+2 iteration computes output pixels of 3,7,11,15,19,23,27
		// col+3 iteration computes output pixels of 4,8,12,16,20,24,28
		//afterwards, col becomes 30 and repeat the above process
			//1st col iteration

				//multiply with the mask
				m0=_mm256_maddubs_epi16(r0,c0);
				m1=_mm256_maddubs_epi16(r1,c1);
				m2=_mm256_maddubs_epi16(r2,c0);

				//make 16-bit values 32-bit (horizontal add)
				__m256i ones=_mm256_set1_epi16(1);
				m0=_mm256_madd_epi16(m0,ones);
				m1=_mm256_madd_epi16(m1,ones);
				m2=_mm256_madd_epi16(m2,ones);


				//vertical add
				m0=_mm256_add_epi32(m0,m1);
				m0=_mm256_add_epi32(m0,m2);


				//m0 has 8 32bit values now.
				output1 = division_32(division_case,m0,f);


				//2nd col iteration
				//multiply with the mask
				m0=_mm256_maddubs_epi16(r0,c0_sh1);
				m1=_mm256_maddubs_epi16(r1,c1_sh1);
				m2=_mm256_maddubs_epi16(r2,c0_sh1);

				//make 16-bit values 32-bit (horizontal add)
				m0=_mm256_madd_epi16(m0,ones);
				m1=_mm256_madd_epi16(m1,ones);
				m2=_mm256_madd_epi16(m2,ones);

				//vertical add
				m0=_mm256_add_epi32(m0,m1);
				m0=_mm256_add_epi32(m0,m2);

				//m0 has 8 32bit values now.
				output2 = division_32(division_case,m0,f);


		         //3rd col iteration
				//multiply with the mask
				m0=_mm256_maddubs_epi16(r0,c0_sh2);
				m1=_mm256_maddubs_epi16(r1,c1_sh2);
				m2=_mm256_maddubs_epi16(r2,c0_sh2);

				//shift m0 by one position because hadd follows
				m3=_mm256_srli_si256(m0,2);
				m4=_mm256_and_si256(m0,mask3);
				m4=_mm256_permute2f128_si256(m4,m4,1);
				m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
				m0=_mm256_add_epi16(m3,m4);

				//shift m1 by one position because hadd follows
				m3=_mm256_srli_si256(m1,2);
				m4=_mm256_and_si256(m1,mask3);
				m4=_mm256_permute2f128_si256(m4,m4,1);
				m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
				m1=_mm256_add_epi16(m3,m4);

				//shift m2 by one position because hadd follows
				m3=_mm256_srli_si256(m2,2);
				m4=_mm256_and_si256(m2,mask3);
				m4=_mm256_permute2f128_si256(m4,m4,1);
				m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
				m2=_mm256_add_epi16(m3,m4);

				//make 16-bit values 32-bit (horizontal add)
				m0=_mm256_madd_epi16(m0,ones);
				m1=_mm256_madd_epi16(m1,ones);
				m2=_mm256_madd_epi16(m2,ones);

				//vertical add
				m0=_mm256_add_epi32(m0,m1);
				m0=_mm256_add_epi32(m0,m2);

				//m0 has 8 32bit values now.
				output3 = division_32(division_case,m0,f);

				//4th col iteration

				//multiply with the mask
				m0=_mm256_maddubs_epi16(r0,c0_sh3);
				m1=_mm256_maddubs_epi16(r1,c1_sh3);
				m2=_mm256_maddubs_epi16(r2,c0_sh3);

				//shift m0 by one position because hadd follows
				m3=_mm256_srli_si256(m0,2);
				m4=_mm256_and_si256(m0,mask3);
				m4=_mm256_permute2f128_si256(m4,m4,1);
				m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
				m0=_mm256_add_epi16(m3,m4);

				//shift m1 by one position because hadd follows
				m3=_mm256_srli_si256(m1,2);
				m4=_mm256_and_si256(m1,mask3);
				m4=_mm256_permute2f128_si256(m4,m4,1);
				m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
				m1=_mm256_add_epi16(m3,m4);

				//shift m2 by one position because hadd follows
				m3=_mm256_srli_si256(m2,2);
				m4=_mm256_and_si256(m2,mask3);
				m4=_mm256_permute2f128_si256(m4,m4,1);
				m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
				m2=_mm256_add_epi16(m3,m4);

				//make 16-bit values 32-bit (horizontal add)
				m0=_mm256_madd_epi16(m0,ones);
				m1=_mm256_madd_epi16(m1,ones);
				m2=_mm256_madd_epi16(m2,ones);

				//vertical add
				m0=_mm256_add_epi32(m0,m1);
				m0=_mm256_add_epi32(m0,m2);

				//m0 has 8 32bit values now.
				output4 = division_32(division_case,m0,f);

				//the 32-bit division generates 8-bit values that are stored into memory.

				//blend output1, output2, output3, output4 into one register
				output2 = _mm256_slli_si256(output2,1);
				output1 = _mm256_add_epi8(output1,output2);
				output3 = _mm256_slli_si256(output3,2);
				output1 = _mm256_add_epi8(output1,output3);
				output4 = _mm256_slli_si256(output4,3);
				output1 = _mm256_add_epi8(output1,output4);

				_mm256_storeu_si256( (__m256i *) &filt[row][col],output1 );

				//2nd row
				//1st col iteration

					//multiply with the mask
					m0=_mm256_maddubs_epi16(r1,c0);
					m1=_mm256_maddubs_epi16(r2,c1);
					m2=_mm256_maddubs_epi16(r3,c0);

					//make 16-bit values 32-bit (horizontal add)
					ones=_mm256_set1_epi16(1);
					m0=_mm256_madd_epi16(m0,ones);
					m1=_mm256_madd_epi16(m1,ones);
					m2=_mm256_madd_epi16(m2,ones);


					//vertical add
					m0=_mm256_add_epi32(m0,m1);
					m0=_mm256_add_epi32(m0,m2);


					//m0 has 8 32bit values now.
					output1 = division_32(division_case,m0,f);


					//2nd col iteration
					//multiply with the mask
					m0=_mm256_maddubs_epi16(r1,c0_sh1);
					m1=_mm256_maddubs_epi16(r2,c1_sh1);
					m2=_mm256_maddubs_epi16(r3,c0_sh1);

					//make 16-bit values 32-bit (horizontal add)
					m0=_mm256_madd_epi16(m0,ones);
					m1=_mm256_madd_epi16(m1,ones);
					m2=_mm256_madd_epi16(m2,ones);

					//vertical add
					m0=_mm256_add_epi32(m0,m1);
					m0=_mm256_add_epi32(m0,m2);

					//m0 has 8 32bit values now.
					output2 = division_32(division_case,m0,f);


			         //3rd col iteration
					//multiply with the mask
					m0=_mm256_maddubs_epi16(r1,c0_sh2);
					m1=_mm256_maddubs_epi16(r2,c1_sh2);
					m2=_mm256_maddubs_epi16(r3,c0_sh2);

					//shift m0 by one position because hadd follows
					m3=_mm256_srli_si256(m0,2);
					m4=_mm256_and_si256(m0,mask3);
					m4=_mm256_permute2f128_si256(m4,m4,1);
					m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
					m0=_mm256_add_epi16(m3,m4);

					//shift m1 by one position because hadd follows
					m3=_mm256_srli_si256(m1,2);
					m4=_mm256_and_si256(m1,mask3);
					m4=_mm256_permute2f128_si256(m4,m4,1);
					m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
					m1=_mm256_add_epi16(m3,m4);

					//shift m2 by one position because hadd follows
					m3=_mm256_srli_si256(m2,2);
					m4=_mm256_and_si256(m2,mask3);
					m4=_mm256_permute2f128_si256(m4,m4,1);
					m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
					m2=_mm256_add_epi16(m3,m4);

					//make 16-bit values 32-bit (horizontal add)
					m0=_mm256_madd_epi16(m0,ones);
					m1=_mm256_madd_epi16(m1,ones);
					m2=_mm256_madd_epi16(m2,ones);

					//vertical add
					m0=_mm256_add_epi32(m0,m1);
					m0=_mm256_add_epi32(m0,m2);

					//m0 has 8 32bit values now.
					output3 = division_32(division_case,m0,f);

					//4th col iteration

					//multiply with the mask
					m0=_mm256_maddubs_epi16(r1,c0_sh3);
					m1=_mm256_maddubs_epi16(r2,c1_sh3);
					m2=_mm256_maddubs_epi16(r3,c0_sh3);

					//shift m0 by one position because hadd follows
					m3=_mm256_srli_si256(m0,2);
					m4=_mm256_and_si256(m0,mask3);
					m4=_mm256_permute2f128_si256(m4,m4,1);
					m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
					m0=_mm256_add_epi16(m3,m4);

					//shift m1 by one position because hadd follows
					m3=_mm256_srli_si256(m1,2);
					m4=_mm256_and_si256(m1,mask3);
					m4=_mm256_permute2f128_si256(m4,m4,1);
					m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
					m1=_mm256_add_epi16(m3,m4);

					//shift m2 by one position because hadd follows
					m3=_mm256_srli_si256(m2,2);
					m4=_mm256_and_si256(m2,mask3);
					m4=_mm256_permute2f128_si256(m4,m4,1);
					m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
					m2=_mm256_add_epi16(m3,m4);

					//make 16-bit values 32-bit (horizontal add)
					m0=_mm256_madd_epi16(m0,ones);
					m1=_mm256_madd_epi16(m1,ones);
					m2=_mm256_madd_epi16(m2,ones);

					//vertical add
					m0=_mm256_add_epi32(m0,m1);
					m0=_mm256_add_epi32(m0,m2);

					//m0 has 8 32bit values now.
					output4 = division_32(division_case,m0,f);

					//the 32-bit division generates 8-bit values that are stored into memory.

					//blend output1, output2, output3, output4 into one register
					output2 = _mm256_slli_si256(output2,1);
					output1 = _mm256_add_epi8(output1,output2);
					output3 = _mm256_slli_si256(output3,2);
					output1 = _mm256_add_epi8(output1,output3);
					output4 = _mm256_slli_si256(output4,3);
					output1 = _mm256_add_epi8(output1,output4);

					_mm256_storeu_si256( (__m256i *) &filt[row+1][col],output1 );

					//3rd row
					//1st col iteration

						//multiply with the mask
						m0=_mm256_maddubs_epi16(r2,c0);
						m1=_mm256_maddubs_epi16(r3,c1);
						m2=_mm256_maddubs_epi16(r4,c0);

						//make 16-bit values 32-bit (horizontal add)
						ones=_mm256_set1_epi16(1);
						m0=_mm256_madd_epi16(m0,ones);
						m1=_mm256_madd_epi16(m1,ones);
						m2=_mm256_madd_epi16(m2,ones);


						//vertical add
						m0=_mm256_add_epi32(m0,m1);
						m0=_mm256_add_epi32(m0,m2);


						//m0 has 8 32bit values now.
						output1 = division_32(division_case,m0,f);


						//2nd col iteration
						//multiply with the mask
						m0=_mm256_maddubs_epi16(r2,c0_sh1);
						m1=_mm256_maddubs_epi16(r3,c1_sh1);
						m2=_mm256_maddubs_epi16(r4,c0_sh1);

						//make 16-bit values 32-bit (horizontal add)
						m0=_mm256_madd_epi16(m0,ones);
						m1=_mm256_madd_epi16(m1,ones);
						m2=_mm256_madd_epi16(m2,ones);

						//vertical add
						m0=_mm256_add_epi32(m0,m1);
						m0=_mm256_add_epi32(m0,m2);

						//m0 has 8 32bit values now.
						output2 = division_32(division_case,m0,f);


				         //3rd col iteration
						//multiply with the mask
						m0=_mm256_maddubs_epi16(r2,c0_sh2);
						m1=_mm256_maddubs_epi16(r3,c1_sh2);
						m2=_mm256_maddubs_epi16(r4,c0_sh2);

						//shift m0 by one position because hadd follows
						m3=_mm256_srli_si256(m0,2);
						m4=_mm256_and_si256(m0,mask3);
						m4=_mm256_permute2f128_si256(m4,m4,1);
						m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
						m0=_mm256_add_epi16(m3,m4);

						//shift m1 by one position because hadd follows
						m3=_mm256_srli_si256(m1,2);
						m4=_mm256_and_si256(m1,mask3);
						m4=_mm256_permute2f128_si256(m4,m4,1);
						m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
						m1=_mm256_add_epi16(m3,m4);

						//shift m2 by one position because hadd follows
						m3=_mm256_srli_si256(m2,2);
						m4=_mm256_and_si256(m2,mask3);
						m4=_mm256_permute2f128_si256(m4,m4,1);
						m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
						m2=_mm256_add_epi16(m3,m4);

						//make 16-bit values 32-bit (horizontal add)
						m0=_mm256_madd_epi16(m0,ones);
						m1=_mm256_madd_epi16(m1,ones);
						m2=_mm256_madd_epi16(m2,ones);

						//vertical add
						m0=_mm256_add_epi32(m0,m1);
						m0=_mm256_add_epi32(m0,m2);

						//m0 has 8 32bit values now.
						output3 = division_32(division_case,m0,f);

						//4th col iteration

						//multiply with the mask
						m0=_mm256_maddubs_epi16(r2,c0_sh3);
						m1=_mm256_maddubs_epi16(r3,c1_sh3);
						m2=_mm256_maddubs_epi16(r4,c0_sh3);

						//shift m0 by one position because hadd follows
						m3=_mm256_srli_si256(m0,2);
						m4=_mm256_and_si256(m0,mask3);
						m4=_mm256_permute2f128_si256(m4,m4,1);
						m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
						m0=_mm256_add_epi16(m3,m4);

						//shift m1 by one position because hadd follows
						m3=_mm256_srli_si256(m1,2);
						m4=_mm256_and_si256(m1,mask3);
						m4=_mm256_permute2f128_si256(m4,m4,1);
						m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
						m1=_mm256_add_epi16(m3,m4);

						//shift m2 by one position because hadd follows
						m3=_mm256_srli_si256(m2,2);
						m4=_mm256_and_si256(m2,mask3);
						m4=_mm256_permute2f128_si256(m4,m4,1);
						m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
						m2=_mm256_add_epi16(m3,m4);

						//make 16-bit values 32-bit (horizontal add)
						m0=_mm256_madd_epi16(m0,ones);
						m1=_mm256_madd_epi16(m1,ones);
						m2=_mm256_madd_epi16(m2,ones);

						//vertical add
						m0=_mm256_add_epi32(m0,m1);
						m0=_mm256_add_epi32(m0,m2);

						//m0 has 8 32bit values now.
						output4 = division_32(division_case,m0,f);

						//the 32-bit division generates 8-bit values that are stored into memory.

						//blend output1, output2, output3, output4 into one register
						output2 = _mm256_slli_si256(output2,1);
						output1 = _mm256_add_epi8(output1,output2);
						output3 = _mm256_slli_si256(output3,2);
						output1 = _mm256_add_epi8(output1,output3);
						output4 = _mm256_slli_si256(output4,3);
						output1 = _mm256_add_epi8(output1,output4);

						_mm256_storeu_si256( (__m256i *) &filt[row+2][col],output1 );

		}
	  loop_reminder_3x3_32(frame1,filt,M,N,row,col,REMINDER_ITERATIONS,division_case,divisor,filter,c0,c1,c0_sh1,c1_sh1,c0_sh2,c1_sh2,c0_sh3,c1_sh3,f);
	  loop_reminder_3x3_32(frame1,filt,M,N,row+1,col,REMINDER_ITERATIONS,division_case,divisor,filter,c0,c1,c0_sh1,c1_sh1,c0_sh2,c1_sh2,c0_sh3,c1_sh3,f);
	  loop_reminder_3x3_32(frame1,filt,M,N,row+2,col,REMINDER_ITERATIONS,division_case,divisor,filter,c0,c1,c0_sh1,c1_sh1,c0_sh2,c1_sh2,c0_sh3,c1_sh3,f);

	}

}

}//end of parallel


}




int loop_reminder_3x3_last_row_only_32(unsigned char **frame1,unsigned char **filt,const unsigned int M, const unsigned int N, const unsigned int col,const unsigned int REMINDER_ITERATIONS,const unsigned int division_case,const unsigned short int divisor,signed char **filter,const __m256i c0,const __m256i c1,const __m256i c0_sh1,const __m256i c1_sh1,const __m256i c0_sh2,const __m256i c1_sh2, const __m256i c0_sh3,const __m256i c1_sh3,const __m256i f){

register	__m256i r1,r2,m0,m1,m3,m4,output1,output2,output3,output4,output_row0;
int newPixel;
unsigned int i;
const __m256i mask3  = _mm256_set_epi16(0,0,0,0,0,0,0,65535,0,0,0,0,0,0,0,0);


 __m256i reminder_mask1;

if (REMINDER_ITERATIONS == 0){
	return 0; //no loop reminder is needed
}

	reminder_mask1=_mm256_load_si256( (__m256i *) &reminder_msk1_3x3[REMINDER_ITERATIONS-1][0]);
	//reminder_mask2=_mm256_load_si256( (__m256i *) &reminder_msk2[REMINDER_ITERATIONS-1][0]);


	 //1st col iteration
	r1=_mm256_loadu_si256( (__m256i *) &frame1[N-2][col-1]);
	r2=_mm256_loadu_si256( (__m256i *) &frame1[N-1][col-1]);


	/*
	 * The above load operations load outside of the array bounds; these elements are filled with zeros just after
	 * Furthermore, I add one extra zero in the end to compute col=N-1. The last value of col is N-1, not N
	 */

	//AND r0-r4 with reminder_mask
	r1=_mm256_and_si256(r1,reminder_mask1);
	r2=_mm256_and_si256(r2,reminder_mask1);



	 //row==N-1
	 //row==N-1
	//1st col iteration
	//1st col iteration

	//multiply with the mask
	m0=_mm256_maddubs_epi16(r1,c0);
	m1=_mm256_maddubs_epi16(r2,c1);

	//make 16-bit values 32-bit (horizontal add)
	__m256i ones=_mm256_set1_epi16(1);
	m0=_mm256_madd_epi16(m0,ones);
	m1=_mm256_madd_epi16(m1,ones);

	//vertical add
	m0=_mm256_add_epi32(m0,m1);

	//m0 has 8 32bit values now.
	output1 = division_32(division_case,m0,f);


	//2nd col iteration
	//multiply with the mask
	m0=_mm256_maddubs_epi16(r1,c0_sh1);
	m1=_mm256_maddubs_epi16(r2,c1_sh1);

	//make 16-bit values 32-bit (horizontal add)
	m0=_mm256_madd_epi16(m0,ones);
	m1=_mm256_madd_epi16(m1,ones);

	//vertical add
	m0=_mm256_add_epi32(m0,m1);

	//m0 has 8 32bit values now.
	output2 = division_32(division_case,m0,f);


     //3rd col iteration
	//multiply with the mask
	m0=_mm256_maddubs_epi16(r1,c0_sh2);
	m1=_mm256_maddubs_epi16(r2,c1_sh2);

	//shift m0 by one position because hadd follows
	m3=_mm256_srli_si256(m0,2);
	m4=_mm256_and_si256(m0,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
	m0=_mm256_add_epi16(m3,m4);

	//shift m1 by one position because hadd follows
	m3=_mm256_srli_si256(m1,2);
	m4=_mm256_and_si256(m1,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
	m1=_mm256_add_epi16(m3,m4);


	//make 16-bit values 32-bit (horizontal add)
	m0=_mm256_madd_epi16(m0,ones);
	m1=_mm256_madd_epi16(m1,ones);

	//vertical add
	m0=_mm256_add_epi32(m0,m1);

	//m0 has 8 32bit values now.
	output3 = division_32(division_case,m0,f);

	//4th col iteration

	//multiply with the mask
	m0=_mm256_maddubs_epi16(r1,c0_sh3);
	m1=_mm256_maddubs_epi16(r2,c1_sh3);

	//shift m0 by one position because hadd follows
	m3=_mm256_srli_si256(m0,2);
	m4=_mm256_and_si256(m0,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
	m0=_mm256_add_epi16(m3,m4);

	//shift m1 by one position because hadd follows
	m3=_mm256_srli_si256(m1,2);
	m4=_mm256_and_si256(m1,mask3);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,14);//shift 7 short int positions or 14 char positions
	m1=_mm256_add_epi16(m3,m4);


	//make 16-bit values 32-bit (horizontal add)
	m0=_mm256_madd_epi16(m0,ones);
	m1=_mm256_madd_epi16(m1,ones);

	//vertical add
	m0=_mm256_add_epi32(m0,m1);

	//m0 has 8 32bit values now.
	output4 = division_32(division_case,m0,f);

	//the 32-bit division generates 8-bit values that are stored into memory.

	//blend output1, output2, output3, output4 into one register
	output2 = _mm256_slli_si256(output2,1);
	output1 = _mm256_add_epi8(output1,output2);
	output3 = _mm256_slli_si256(output3,2);
	output1 = _mm256_add_epi8(output1,output3);
	output4 = _mm256_slli_si256(output4,3);
	output_row0 = _mm256_add_epi8(output1,output4);

 	switch (REMINDER_ITERATIONS){
	case 1:
		filt[N-1][col] = (unsigned char) _mm256_extract_epi8(output_row0,0);

	  break;
	case 2:
		for (i=0;i<=1;i++){
			filt[N-1][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 3:
		for (i=0;i<=2;i++){
			filt[N-1][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 4:
		for (i=0;i<=3;i++){
			filt[N-1][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 5:
		for (i=0;i<=4;i++){
			filt[N-1][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 6:
		for (i=0;i<=5;i++){
			filt[N-1][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 7:
		for (i=0;i<=6;i++){
			filt[N-1][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 8:
		for (i=0;i<=7;i++){
			filt[N-1][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 9:
		for (i=0;i<=8;i++){
			filt[N-1][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 10:
		for (i=0;i<=9;i++){
			filt[N-1][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 11:
		for (i=0;i<=10;i++){
			filt[N-1][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 12:
		for (i=0;i<=11;i++){
			filt[N-1][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 13:
		for (i=0;i<=12;i++){
			filt[N-1][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 14:
		for (i=0;i<=13;i++){
			filt[N-1][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 15:
		for (i=0;i<=14;i++){
			filt[N-1][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 16:
  	_mm_storeu_si128( (__m128i *) &filt[N-1][col],_mm256_extractf128_si256(output_row0, 0)); //store low 128bit - 16pixels

		  break;
	case 17:
  	_mm_storeu_si128( (__m128i *) &filt[N-1][col],_mm256_extractf128_si256(output_row0, 0)); //store low 128bit - 16pixels
	filt[N-1][col+16] = (unsigned char) _mm256_extract_epi8(output_row0,16);
		  break;
	case 18:
  	_mm_storeu_si128( (__m128i *) &filt[N-1][col],_mm256_extractf128_si256(output_row0, 0)); //store low 128bit - 16pixels

	for (i=16;i<=17;i++){
	 filt[N-1][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
	}
		  break;
	case 19:
	  	_mm_storeu_si128( (__m128i *) &filt[N-1][col],_mm256_extractf128_si256(output_row0, 0)); //store low 128bit - 16pixels

		for (i=16;i<=18;i++){
		 filt[N-1][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 20:
	  	_mm_storeu_si128( (__m128i *) &filt[N-1][col],_mm256_extractf128_si256(output_row0, 0)); //store low 128bit - 16pixels

		for (i=16;i<=19;i++){
		 filt[N-1][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 21:
	  	_mm_storeu_si128( (__m128i *) &filt[N-1][col],_mm256_extractf128_si256(output_row0, 0)); //store low 128bit - 16pixels

		for (i=16;i<=20;i++){
		 filt[N-1][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 22:
	  	_mm_storeu_si128( (__m128i *) &filt[N-1][col],_mm256_extractf128_si256(output_row0, 0)); //store low 128bit - 16pixels

		for (i=16;i<=21;i++){
		 filt[N-1][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		break;
	case 23:
	  	_mm_storeu_si128( (__m128i *) &filt[N-1][col],_mm256_extractf128_si256(output_row0, 0)); //store low 128bit - 16pixels

		for (i=16;i<=22;i++){
		 filt[N-1][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 24:
	  	_mm_storeu_si128( (__m128i *) &filt[N-1][col],_mm256_extractf128_si256(output_row0, 0)); //store low 128bit - 16pixels

		for (i=16;i<=23;i++){
		 filt[N-1][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 25:
	  	_mm_storeu_si128( (__m128i *) &filt[N-1][col],_mm256_extractf128_si256(output_row0, 0)); //store low 128bit - 16pixels

		for (i=16;i<=24;i++){
		 filt[N-1][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 26:
	  	_mm_storeu_si128( (__m128i *) &filt[N-1][col],_mm256_extractf128_si256(output_row0, 0)); //store low 128bit - 16pixels

		for (i=16;i<=25;i++){
		 filt[N-1][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}

		  break;
	case 27:
	  	_mm_storeu_si128( (__m128i *) &filt[N-1][col],_mm256_extractf128_si256(output_row0, 0)); //store low 128bit - 16pixels

		for (i=16;i<=26;i++){
		 filt[N-1][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}

		  break;
	case 28:
	  	_mm_storeu_si128( (__m128i *) &filt[N-1][col],_mm256_extractf128_si256(output_row0, 0)); //store low 128bit - 16pixels

		for (i=16;i<=27;i++){
		 filt[N-1][col+i] = (unsigned char) _mm256_extract_epi8(output_row0,i);
		}
		  break;
	case 29:
	  	_mm_storeu_si128( (__m128i *) &filt[N-1][col],_mm256_extractf128_si256(output_row0, 0)); //store low 128bit - 16pixels

		filt[N-1][col+16] = (unsigned char) _mm256_extract_epi8(output_row0,16);
		filt[N-1][col+17] = (unsigned char) _mm256_extract_epi8(output_row0,17);
		filt[N-1][col+18] = (unsigned char) _mm256_extract_epi8(output_row0,18);
		filt[N-1][col+19] = (unsigned char) _mm256_extract_epi8(output_row0,19);
		filt[N-1][col+20] = (unsigned char) _mm256_extract_epi8(output_row0,20);
		filt[N-1][col+21] = (unsigned char) _mm256_extract_epi8(output_row0,21);
		filt[N-1][col+22] = (unsigned char) _mm256_extract_epi8(output_row0,22);
		filt[N-1][col+23] = (unsigned char) _mm256_extract_epi8(output_row0,23);
		filt[N-1][col+24] = (unsigned char) _mm256_extract_epi8(output_row0,24);
		filt[N-1][col+25] = (unsigned char) _mm256_extract_epi8(output_row0,25);
		filt[N-1][col+26] = (unsigned char) _mm256_extract_epi8(output_row0,26);
		filt[N-1][col+27] = (unsigned char) _mm256_extract_epi8(output_row0,27);
		filt[N-1][col+28] = (unsigned char) _mm256_extract_epi8(output_row0,28);

		  break;
	case 30:
	  	_mm_storeu_si128( (__m128i *) &filt[N-1][col],_mm256_extractf128_si256(output_row0, 0)); //store low 128bit - 16pixels

		filt[N-1][col+16] = (unsigned char) _mm256_extract_epi8(output_row0,16);
		filt[N-1][col+17] = (unsigned char) _mm256_extract_epi8(output_row0,17);
		filt[N-1][col+18] = (unsigned char) _mm256_extract_epi8(output_row0,18);
		filt[N-1][col+19] = (unsigned char) _mm256_extract_epi8(output_row0,19);
		filt[N-1][col+20] = (unsigned char) _mm256_extract_epi8(output_row0,20);
		filt[N-1][col+21] = (unsigned char) _mm256_extract_epi8(output_row0,21);
		filt[N-1][col+22] = (unsigned char) _mm256_extract_epi8(output_row0,22);
		filt[N-1][col+23] = (unsigned char) _mm256_extract_epi8(output_row0,23);
		filt[N-1][col+24] = (unsigned char) _mm256_extract_epi8(output_row0,24);
		filt[N-1][col+25] = (unsigned char) _mm256_extract_epi8(output_row0,25);
		filt[N-1][col+26] = (unsigned char) _mm256_extract_epi8(output_row0,26);
		filt[N-1][col+27] = (unsigned char) _mm256_extract_epi8(output_row0,27);
		filt[N-1][col+28] = (unsigned char) _mm256_extract_epi8(output_row0,28);

	//the filt[row][col+29] is computed unvectorized

	newPixel = 0;
	newPixel += frame1[N-1-1][M-1-1] * filter[0][0];
	newPixel += frame1[N-1-1][M-1] * filter[0][1];

	newPixel += frame1[N-1][M-1-1] * filter[1][0];
	newPixel += frame1[N-1][M-1] * filter[1][1];

	filt[N-1][M-1] = (unsigned char) (newPixel / divisor);

		  break;

	case 31:
	  	_mm_storeu_si128( (__m128i *) &filt[N-1][col],_mm256_extractf128_si256(output_row0, 0)); //store low 128bit - 16pixels


		filt[N-1][col+16] = (unsigned char) _mm256_extract_epi8(output_row0,16);
		filt[N-1][col+17] = (unsigned char) _mm256_extract_epi8(output_row0,17);
		filt[N-1][col+18] = (unsigned char) _mm256_extract_epi8(output_row0,18);
		filt[N-1][col+19] = (unsigned char) _mm256_extract_epi8(output_row0,19);
		filt[N-1][col+20] = (unsigned char) _mm256_extract_epi8(output_row0,20);
		filt[N-1][col+21] = (unsigned char) _mm256_extract_epi8(output_row0,21);
		filt[N-1][col+22] = (unsigned char) _mm256_extract_epi8(output_row0,22);
		filt[N-1][col+23] = (unsigned char) _mm256_extract_epi8(output_row0,23);
		filt[N-1][col+24] = (unsigned char) _mm256_extract_epi8(output_row0,24);
		filt[N-1][col+25] = (unsigned char) _mm256_extract_epi8(output_row0,25);
		filt[N-1][col+26] = (unsigned char) _mm256_extract_epi8(output_row0,26);
		filt[N-1][col+27] = (unsigned char) _mm256_extract_epi8(output_row0,27);
		filt[N-1][col+28] = (unsigned char) _mm256_extract_epi8(output_row0,28);

	//the filt[N-2][col+29/30] are computed unvectorized

		newPixel = 0;

		newPixel += frame1[N-1-1][M-2-1] * filter[0][0];
		newPixel += frame1[N-1-1][M-2] * filter[0][1];
		newPixel += frame1[N-1-1][M-2+1] * filter[0][2];

		newPixel += frame1[N-1][M-2-1] * filter[1][0];
		newPixel += frame1[N-1][M-2] * filter[1][1];
		newPixel += frame1[N-1][M-2+1] * filter[1][2];

		filt[N-1][M-2] = (unsigned char) (newPixel / divisor);

		newPixel = 0;

		newPixel += frame1[N-1-1][M-1-1] * filter[0][0];
		newPixel += frame1[N-1-1][M-1] * filter[0][1];

		newPixel += frame1[N-1][M-1-1] * filter[1][0];
		newPixel += frame1[N-1][M-1] * filter[1][1];

		filt[N-1][M-1] = (unsigned char) (newPixel / divisor);


		  break;

	default:
		printf("\nsomething went wrong");
		return -1;
 	}

return 0;

}







void Gaussian_Blur_7x7_32_separable_old(unsigned char **frame1,unsigned char **filt,const unsigned int M, const unsigned int N, signed char *kernel_y, signed char *kernel_x, const unsigned short int divisor_xy){

const signed char fx0=kernel_x[0];
const signed char fx1=kernel_x[1];
const signed char fx2=kernel_x[2];
const signed char fx3=kernel_x[3];
const signed char fx4=kernel_x[4];
const signed char fx5=kernel_x[5];
const signed char fx6=kernel_x[6];

const signed char fy0=kernel_y[0];
const signed char fy1=kernel_y[1];
const signed char fy2=kernel_y[2];
const signed char fy3=kernel_y[3];
const signed char fy4=kernel_y[4];
const signed char fy5=kernel_y[5];
const signed char fy6=kernel_y[6];



const signed char mask_vector_x[8][32] __attribute__((aligned(64))) ={
	{fx0,fx1,fx2,fx3,fx4,fx5,fx6,0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,0},
	{0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,0,fx0,fx1,fx2,fx3,fx4,fx5,fx6},
	{0,0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,0,0,0,0,0,0,0},
	{0,0,0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,0,0,0,0,0,0},
	{0,0,0,0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,0,0,0,0,0},
	{0,0,0,0,0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,0,0,0,0},
	{0,0,0,0,0,0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,0,0,0},
	{0,0,0,0,0,0,0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,0,0},
};

const signed char mask_vector_y[14][32] __attribute__((aligned(64))) ={
	{fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0},
	{fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0},
	{fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0},
	{fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0},
	{fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0},
	{fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0},
	{fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0},

	{0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0},
	{0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1},
	{0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2},
	{0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3},
	{0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4},
	{0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5},
	{0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6},
};

	const __m256i cx0=_mm256_load_si256( (__m256i *) &mask_vector_x[0]);
	const __m256i cx1=_mm256_load_si256( (__m256i *) &mask_vector_x[1]);
	const __m256i cx2=_mm256_load_si256( (__m256i *) &mask_vector_x[2]);
	const __m256i cx3=_mm256_load_si256( (__m256i *) &mask_vector_x[3]);
	const __m256i cx4=_mm256_load_si256( (__m256i *) &mask_vector_x[4]);
	const __m256i cx5=_mm256_load_si256( (__m256i *) &mask_vector_x[5]);
	const __m256i cx6=_mm256_load_si256( (__m256i *) &mask_vector_x[6]);
	const __m256i cx7=_mm256_load_si256( (__m256i *) &mask_vector_x[7]);


	const __m256i cy0=_mm256_load_si256( (__m256i *) &mask_vector_y[0]);
	const __m256i cy1=_mm256_load_si256( (__m256i *) &mask_vector_y[1]);
	const __m256i cy2=_mm256_load_si256( (__m256i *) &mask_vector_y[2]);
	const __m256i cy3=_mm256_load_si256( (__m256i *) &mask_vector_y[3]);
	const __m256i cy4=_mm256_load_si256( (__m256i *) &mask_vector_y[4]);
	const __m256i cy5=_mm256_load_si256( (__m256i *) &mask_vector_y[5]);
	const __m256i cy6=_mm256_load_si256( (__m256i *) &mask_vector_y[6]);

	const __m256i cy0_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[7]);
	const __m256i cy1_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[8]);
	const __m256i cy2_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[9]);
	const __m256i cy3_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[10]);
	const __m256i cy4_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[11]);
	const __m256i cy5_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[12]);
	const __m256i cy6_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[13]);


	const __m256i mask3  = _mm256_set_epi16(0,0,0,0,0,0,0,65535,0,0,0,0,0,0,0,0);
	const __m256i mask_prelude  = _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0);

	const __m256i mask_even_16  = _mm256_set_epi16(0,0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff);
	const __m256i mask_odd_16  = _mm256_set_epi16(0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff,0);

	const __m256i mask_even_32  = _mm256_set_epi32(0,0xffffffff,0,0xffffffff,0,0xffffffff,0,0xffffffff);
	const __m256i mask_odd_32  = _mm256_set_epi32(0xffffffff,0,0xffffffff,0,0xffffffff,0,0xffffffff,0);
	const __m256i mask4  = _mm256_set_epi32(0,0,0,0xffffffff,0,0,0,0);


	//REMINDER_ITERATIONS=[6,31]
	const unsigned int REMINDER_ITERATIONS =  (M-((((M-32)/26)*26)+26)); //M-(last_col_value+26)

	//printf("\nREMINDER_ITERATIONS=%d",REMINDER_ITERATIONS);


	const unsigned int division_case=prepare_for_division_32(divisor_xy); //determine which is the division case (A, B or C)
	const __m256i f = _mm256_load_si256( (__m256i *) &f_vector[0]);// initialize the division vector
	const __m256i ones=_mm256_set1_epi16(1);

//	const unsigned int division_case_x=prepare_for_division_32(divisor_x); //determine which is the division case (A, B or C)
//	const __m256i f_x = _mm256_load_si256( (__m256i *) &f_vector_x[0]);// initialize the division vector
//printf("\n%d %d",division_case,b);


#pragma omp parallel
{

unsigned int row,col;
__m256i r0,r1,r2,r3,r4,r5,r6,m0,m1,m2,m3,m4,m5,m6,even,odd;
__m256i output1,output2,output3,output4;



/*---------------------- Gaussian Blur ---------------------------------*/

    #pragma omp for schedule(static)
	for (row = 0; row < N; row++) {

		if (row<3){
			if (row==0){
				for (col = 0; col <= M-32; col+=26){
				r0=prelude_7x7_Ymask_1(0, col, frame1, mask_vector_y, division_case, f, mask_prelude, mask_even_16, mask_odd_16);
				prelude_7x7_Xmask(filt,row,col, r0, mask_vector_x,  division_case,f, mask_even_32, mask_odd_32);
				}
				loop_reminder_7x7_blur(frame1,filt,N,row, col, REMINDER_ITERATIONS, division_case, mask_vector_x, mask_vector_y, f, divisor_xy);

			}
			else if (row==1){
				for (col = 0; col <= M-32; col+=26){
				r0=prelude_7x7_Ymask_2(0, col, frame1, mask_vector_y, division_case, f, mask_prelude, mask_even_16, mask_odd_16);
				prelude_7x7_Xmask(filt,row,col, r0, mask_vector_x,  division_case,f, mask_even_32, mask_odd_32);
				}
				loop_reminder_7x7_blur(frame1,filt,N,row, col, REMINDER_ITERATIONS, division_case, mask_vector_x, mask_vector_y, f, divisor_xy);
			}
			else if (row==2){
				for (col = 0; col <= M-32; col+=26){
				r0=prelude_7x7_Ymask_3(0, col, frame1, mask_vector_y, division_case, f, mask_prelude, mask_even_16, mask_odd_16);
				prelude_7x7_Xmask(filt,row,col, r0, mask_vector_x,  division_case,f, mask_even_32, mask_odd_32);
				}
				loop_reminder_7x7_blur(frame1,filt,N,row, col, REMINDER_ITERATIONS, division_case, mask_vector_x, mask_vector_y, f, divisor_xy);
			}
		}

		else if (row>N-4){
			if (row==N-1){
				for (col = 0; col <= M-32; col+=26){
				r0=prelude_7x7_Ymask_1(N-4, col, frame1, mask_vector_y, division_case, f, mask_prelude, mask_even_16, mask_odd_16);
				prelude_7x7_Xmask(filt,row,col, r0, mask_vector_x,  division_case,f, mask_even_32, mask_odd_32);
			}
			loop_reminder_7x7_blur(frame1,filt,N,row, col, REMINDER_ITERATIONS, division_case, mask_vector_x, mask_vector_y, f, divisor_xy);

			}
			else if (row==N-2){
				for (col = 0; col <= M-32; col+=26){
				r0=prelude_7x7_Ymask_2(N-5, col, frame1, mask_vector_y, division_case, f, mask_prelude, mask_even_16, mask_odd_16);
				prelude_7x7_Xmask(filt,row,col, r0, mask_vector_x,  division_case,f, mask_even_32, mask_odd_32);
			}
			loop_reminder_7x7_blur(frame1,filt,N,row, col, REMINDER_ITERATIONS, division_case, mask_vector_x, mask_vector_y, f, divisor_xy);

			}
			else if (row==N-3){
				for (col = 0; col <= M-32; col+=26){
				r0=prelude_7x7_Ymask_3(N-6, col, frame1, mask_vector_y, division_case, f, mask_prelude, mask_even_16, mask_odd_16);
				prelude_7x7_Xmask(filt,row,col, r0, mask_vector_x,  division_case,f, mask_even_32, mask_odd_32);
			}
			loop_reminder_7x7_blur(frame1,filt,N,row, col, REMINDER_ITERATIONS, division_case, mask_vector_x, mask_vector_y, f, divisor_xy);

			}
		}

	  else { //main loop


	  for (col = 0; col <= M-32; col+=26){

		  if (col==0){

				//load the 7 rows
				r0=_mm256_loadu_si256( (__m256i *) &frame1[row-3][0]);
				r1=_mm256_loadu_si256( (__m256i *) &frame1[row-2][0]);
				r2=_mm256_loadu_si256( (__m256i *) &frame1[row-1][0]);
				r3=_mm256_loadu_si256( (__m256i *) &frame1[row][0]);
				r4=_mm256_loadu_si256( (__m256i *) &frame1[row+1][0]);
				r5=_mm256_loadu_si256( (__m256i *) &frame1[row+2][0]);
				r6=_mm256_loadu_si256( (__m256i *) &frame1[row+3][0]);


		  			//START - extra code needed for prelude
		  					m0=_mm256_slli_si256(r0,3);//shift 3 elements left - equivalent to filling with two zeros inthe beginning
		  					m1=_mm256_slli_si256(r1,3);//shift 3 elements left - equivalent to filling with two zeros inthe beginning
		  					m2=_mm256_slli_si256(r2,3);//shift 3 elements left - equivalent to filling with two zeros inthe beginning
		  					m3=_mm256_slli_si256(r3,3);//shift 3 elements left - equivalent to filling with two zeros inthe beginning
		  					m4=_mm256_slli_si256(r4,3);//shift 3 elements left - equivalent to filling with two zeros inthe beginning
		  					m5=_mm256_slli_si256(r5,3);//shift 3 elements left - equivalent to filling with two zeros inthe beginning
		  					m6=_mm256_slli_si256(r6,3);//shift 3 elements left - equivalent to filling with two zeros inthe beginning

		  					r0=_mm256_and_si256(r0,mask_prelude);
		  					r0=_mm256_permute2f128_si256(r0,r0,1);
		  					r0=_mm256_srli_si256(r0,13);//shift 14 elements
		  					r0=_mm256_add_epi16(m0,r0);

		  					r1=_mm256_and_si256(r1,mask_prelude);
		  					r1=_mm256_permute2f128_si256(r1,r1,1);
		  					r1=_mm256_srli_si256(r1,13);//shift 14 elements
		  					r1=_mm256_add_epi16(m1,r1);

		  					r2=_mm256_and_si256(r2,mask_prelude);
		  					r2=_mm256_permute2f128_si256(r2,r2,1);
		  					r2=_mm256_srli_si256(r2,13);//shift 14 elements
		  					r2=_mm256_add_epi16(m2,r2);

		  					r3=_mm256_and_si256(r3,mask_prelude);
		  					r3=_mm256_permute2f128_si256(r3,r3,1);
		  					r3=_mm256_srli_si256(r3,13);//shift 14 elements
		  					r3=_mm256_add_epi16(m3,r3);

		  					r4=_mm256_and_si256(r4,mask_prelude);
		  					r4=_mm256_permute2f128_si256(r4,r4,1);
		  					r4=_mm256_srli_si256(r4,13);//shift 14 elements
		  					r4=_mm256_add_epi16(m4,r4);

		  					r5=_mm256_and_si256(r5,mask_prelude);
		  					r5=_mm256_permute2f128_si256(r5,r5,1);
		  					r5=_mm256_srli_si256(r5,13);//shift 14 elements
		  					r5=_mm256_add_epi16(m5,r5);

		  					r6=_mm256_and_si256(r6,mask_prelude);
		  					r6=_mm256_permute2f128_si256(r6,r6,1);
		  					r6=_mm256_srli_si256(r6,13);//shift 14 elements
		  					r6=_mm256_add_epi16(m6,r6);

		  			//END - extra code needed for prelude
		  		}
		  else {
				//load the 7 rows
				r0=_mm256_loadu_si256( (__m256i *) &frame1[row-3][col-3]);
				r1=_mm256_loadu_si256( (__m256i *) &frame1[row-2][col-3]);
				r2=_mm256_loadu_si256( (__m256i *) &frame1[row-1][col-3]);
				r3=_mm256_loadu_si256( (__m256i *) &frame1[row][col-3]);
				r4=_mm256_loadu_si256( (__m256i *) &frame1[row+1][col-3]);
				r5=_mm256_loadu_si256( (__m256i *) &frame1[row+2][col-3]);
				r6=_mm256_loadu_si256( (__m256i *) &frame1[row+3][col-3]);

		  }

		  //--------------------y filter begins--------------------

			//multiply by the mask
			m0=_mm256_maddubs_epi16(r0,cy0);
			m1=_mm256_maddubs_epi16(r1,cy1);
			m2=_mm256_maddubs_epi16(r2,cy2);
			m3=_mm256_maddubs_epi16(r3,cy3);
			m4=_mm256_maddubs_epi16(r4,cy4);
			m5=_mm256_maddubs_epi16(r5,cy5);
			m6=_mm256_maddubs_epi16(r6,cy6);

			//discard odd
	__m256i m0_even=_mm256_and_si256(m0,mask_even_16);
	__m256i	m1_even=_mm256_and_si256(m1,mask_even_16);
	__m256i	m2_even=_mm256_and_si256(m2,mask_even_16);
	__m256i	m3_even=_mm256_and_si256(m3,mask_even_16);
	__m256i	m4_even=_mm256_and_si256(m4,mask_even_16);
	__m256i	m5_even=_mm256_and_si256(m5,mask_even_16);
	__m256i	m6_even=_mm256_and_si256(m6,mask_even_16);

	//make from 16-bit to 32bit
	  m0_even=_mm256_madd_epi16(m0_even,ones);
	  m1_even=_mm256_madd_epi16(m1_even,ones);
	  m2_even=_mm256_madd_epi16(m2_even,ones);
	  m3_even=_mm256_madd_epi16(m3_even,ones);
	  m4_even=_mm256_madd_epi16(m4_even,ones);
	  m5_even=_mm256_madd_epi16(m5_even,ones);
	  m6_even=_mm256_madd_epi16(m6_even,ones);

			//vertical 32-bit add
			m0_even=_mm256_add_epi32(m0_even,m1_even);
			m0_even=_mm256_add_epi32(m0_even,m2_even);
			m0_even=_mm256_add_epi32(m0_even,m3_even);
			m0_even=_mm256_add_epi32(m0_even,m4_even);
			m0_even=_mm256_add_epi32(m0_even,m5_even);
			m0_even=_mm256_add_epi32(m0_even,m6_even);

			//discard even
	__m256i	m0_odd=_mm256_and_si256(m0,mask_odd_16);
	__m256i	m1_odd=_mm256_and_si256(m1,mask_odd_16);
	__m256i	m2_odd=_mm256_and_si256(m2,mask_odd_16);
	__m256i	m3_odd=_mm256_and_si256(m3,mask_odd_16);
	__m256i	m4_odd=_mm256_and_si256(m4,mask_odd_16);
	__m256i	m5_odd=_mm256_and_si256(m5,mask_odd_16);
	__m256i	m6_odd=_mm256_and_si256(m6,mask_odd_16);

	//make from 16-bit to 32bit
			m0_odd=_mm256_madd_epi16(m0_odd,ones);
			m1_odd=_mm256_madd_epi16(m1_odd,ones);
			m2_odd=_mm256_madd_epi16(m2_odd,ones);
			m3_odd=_mm256_madd_epi16(m3_odd,ones);
			m4_odd=_mm256_madd_epi16(m4_odd,ones);
			m5_odd=_mm256_madd_epi16(m5_odd,ones);
			m6_odd=_mm256_madd_epi16(m6_odd,ones);

			//vertical 32-bit add
			m0_odd=_mm256_add_epi32(m0_odd,m1_odd);
			m0_odd=_mm256_add_epi32(m0_odd,m2_odd);
			m0_odd=_mm256_add_epi32(m0_odd,m3_odd);
			m0_odd=_mm256_add_epi32(m0_odd,m4_odd);
			m0_odd=_mm256_add_epi32(m0_odd,m5_odd);
			m0_odd=_mm256_add_epi32(m0_odd,m6_odd);

			m0_even=division_32(division_case,m0_even,f);//even results
			m0_odd=division_32(division_case,m0_odd,f);//even results

			m0_odd=_mm256_slli_si256(m0_odd,2); //shift two position right
			even=_mm256_add_epi8(m0_even,m0_odd); //add the odd with the even

			//-----------------multiply by the second mask----------
			m0=_mm256_maddubs_epi16(r0,cy0_sh1);
			m1=_mm256_maddubs_epi16(r1,cy1_sh1);
			m2=_mm256_maddubs_epi16(r2,cy2_sh1);
			m3=_mm256_maddubs_epi16(r3,cy3_sh1);
			m4=_mm256_maddubs_epi16(r4,cy4_sh1);
			m5=_mm256_maddubs_epi16(r5,cy5_sh1);
			m6=_mm256_maddubs_epi16(r6,cy6_sh1);


			m0_even=_mm256_and_si256(m0,mask_even_16);
			m1_even=_mm256_and_si256(m1,mask_even_16);
			m2_even=_mm256_and_si256(m2,mask_even_16);
			m3_even=_mm256_and_si256(m3,mask_even_16);
			m4_even=_mm256_and_si256(m4,mask_even_16);
			m5_even=_mm256_and_si256(m5,mask_even_16);
			m6_even=_mm256_and_si256(m6,mask_even_16);

			//make from 16-bit to 32bit
			  m0_even=_mm256_madd_epi16(m0_even,ones);
			  m1_even=_mm256_madd_epi16(m1_even,ones);
			  m2_even=_mm256_madd_epi16(m2_even,ones);
			  m3_even=_mm256_madd_epi16(m3_even,ones);
			  m4_even=_mm256_madd_epi16(m4_even,ones);
			  m5_even=_mm256_madd_epi16(m5_even,ones);
			  m6_even=_mm256_madd_epi16(m6_even,ones);

			//vertical 32-bit add
			m0_even=_mm256_add_epi32(m0_even,m1_even);
			m0_even=_mm256_add_epi32(m0_even,m2_even);
			m0_even=_mm256_add_epi32(m0_even,m3_even);
			m0_even=_mm256_add_epi32(m0_even,m4_even);
			m0_even=_mm256_add_epi32(m0_even,m5_even);
			m0_even=_mm256_add_epi32(m0_even,m6_even);

			 m0_odd=_mm256_and_si256(m0,mask_odd_16);
			 m1_odd=_mm256_and_si256(m1,mask_odd_16);
			 m2_odd=_mm256_and_si256(m2,mask_odd_16);
			 m3_odd=_mm256_and_si256(m3,mask_odd_16);
			 m4_odd=_mm256_and_si256(m4,mask_odd_16);
			 m5_odd=_mm256_and_si256(m5,mask_odd_16);
			 m6_odd=_mm256_and_si256(m6,mask_odd_16);

			 //make from 16-bit to 32bit
				m0_odd=_mm256_madd_epi16(m0_odd,ones);
				m1_odd=_mm256_madd_epi16(m1_odd,ones);
				m2_odd=_mm256_madd_epi16(m2_odd,ones);
				m3_odd=_mm256_madd_epi16(m3_odd,ones);
				m4_odd=_mm256_madd_epi16(m4_odd,ones);
				m5_odd=_mm256_madd_epi16(m5_odd,ones);
				m6_odd=_mm256_madd_epi16(m6_odd,ones);


			//vertical 32-bit add
			m0_odd=_mm256_add_epi32(m0_odd,m1_odd);
			m0_odd=_mm256_add_epi32(m0_odd,m2_odd);
			m0_odd=_mm256_add_epi32(m0_odd,m3_odd);
			m0_odd=_mm256_add_epi32(m0_odd,m4_odd);
			m0_odd=_mm256_add_epi32(m0_odd,m5_odd);
			m0_odd=_mm256_add_epi32(m0_odd,m6_odd);

			m0_even=division_32(division_case,m0_even,f);//even results
			m0_odd=division_32(division_case,m0_odd,f);//even results

			m0_odd=_mm256_slli_si256(m0_odd,2); //shift two position right
			odd=_mm256_add_epi8(m0_even,m0_odd); //add the odd with the even


			//MERGE AND pack to one register
			odd=_mm256_slli_si256(odd,1); //shift one position right
			r0=_mm256_add_epi8(even,odd); //add the odd with the even

			//y filter ends - r0 has now the data to be processed by x filter

		// col iteration computes output pixels of   0,8,16,24
		// col+1 iteration computes output pixels of 1,9,17,25
		// col+2 iteration computes output pixels of 2,10,18
		// col+3 iteration computes output pixels of 3,11,19
		// col+4 iteration computes output pixels of 4,12,20
		// col+5 iteration computes output pixels of 5,13,21
		// col+6 iteration computes output pixels of 6,14,22
		// col+7 iteration computes output pixels of 7,15,23
		//afterwards, col becomes 26 and repeat the above process

			//1st col iteration

			//multiply by the mask
			m0=_mm256_maddubs_epi16(r0,cx0);

			//make 16-bit values 32-bit (horizontal add)
			m0=_mm256_madd_epi16(m0,ones);

			//last horizontal add
			m1=_mm256_slli_si256(m0,4);//shift by one integer
			m0=_mm256_add_epi32(m0,m1);//add
			m1=_mm256_srli_si256(m0,4);//shift by one integer
			output1=_mm256_and_si256(m1,mask_even_32);//keep only 0,2,4,6 and discard 1,3,5,7


			//2nd col iteration

			//multiply by the mask
			m0=_mm256_maddubs_epi16(r0,cx1);

			//make 16-bit values 32-bit (horizontal add)
			m0=_mm256_madd_epi16(m0,ones);

			//last horizontal add
			m1=_mm256_slli_si256(m0,4);//shift by one integer
			m0=_mm256_add_epi32(m0,m1);//add
			m1=_mm256_srli_si256(m0,4);//shift by one integer
			output2=_mm256_and_si256(m1,mask_even_32);//keep only 0,2,4,6 and discard 1,3,5,7

			//3rd col iteration

			//multiply by the mask
			m0=_mm256_maddubs_epi16(r0,cx2);

			//shift m0-m6 by one position because hadd follows
			m0=my_rshift16_by_one(m0,mask3);

			//make 16-bit values 32-bit (horizontal add)
			m0=_mm256_madd_epi16(m0,ones);


			//last horizontal add
			m1=_mm256_slli_si256(m0,4);//shift by one integer
			m0=_mm256_add_epi32(m0,m1);//add
			m1=_mm256_srli_si256(m0,4);//shift by one integer
			output3=_mm256_and_si256(m1,mask_even_32);//keep only 0,2,4,6 and discard 1,3,5,7


			//4th col iteration

			//multiply by the mask
			m0=_mm256_maddubs_epi16(r0,cx3);

			//shift m0-m6 by one position because hadd follows
			m0=my_rshift16_by_one(m0,mask3);

			//make 16-bit values 32-bit (horizontal add)
			m0=_mm256_madd_epi16(m0,ones);


			//last horizontal add
			m1=_mm256_slli_si256(m0,4);//shift by one integer
			m0=_mm256_add_epi32(m0,m1);//add
			m1=_mm256_srli_si256(m0,4);//shift by one integer
			output4=_mm256_and_si256(m1,mask_even_32);//keep only 0,2,4,6 and discard 1,3,5,7


			//5th col iteration

			//multiply by the mask
			m0=_mm256_maddubs_epi16(r0,cx4);

			//No need for shift m0-m6 now

			//make 16-bit values 32-bit (horizontal add)
			m0=_mm256_madd_epi16(m0,ones);


			//last horizontal add
			m3=_mm256_srli_si256(m0,4);
			m4=_mm256_and_si256(m0,mask4);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_slli_si256(m4,12);//shift 3 int positions
			m3=_mm256_add_epi32(m3,m4);
			m0=_mm256_add_epi32(m0,m3);
			m0=_mm256_and_si256(m0,mask_odd_32);//keep only 1,3,5,7 and discard 0,2,4,6

			//MERGE output1 with output5 and DIVIDE
			output1=_mm256_add_epi32(m0,output1);
			output1 = division_32(division_case,output1,f);


			//6th col iteration

			//multiply by the mask
			m0=_mm256_maddubs_epi16(r0,cx5);

			//No need for shift m0-m6 now

			//make 16-bit values 32-bit (horizontal add)
			m0=_mm256_madd_epi16(m0,ones);


			//last horizontal add
			m3=_mm256_srli_si256(m0,4);
			m4=_mm256_and_si256(m0,mask4);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_slli_si256(m4,12);//shift 3 int positions
			m3=_mm256_add_epi32(m3,m4);
			m0=_mm256_add_epi32(m0,m3);
			m0=_mm256_and_si256(m0,mask_odd_32);//keep only 1,3,5,7 and discard 0,2,4,6

			//MERGE output1 with output5 and DIVIDE
			output2=_mm256_add_epi32(m0,output2);
			output2 = division_32(division_case,output2,f);


			//7th col iteration

			//multiply by the mask
			m0=_mm256_maddubs_epi16(r0,cx6);


			//shift m0-m6 by one position because hadd follows
			m0=my_rshift16_by_one(m0,mask3);

			//make 16-bit values 32-bit (horizontal add)
			m0=_mm256_madd_epi16(m0,ones);

			//last horizontal add
			m3=_mm256_srli_si256(m0,4);
			m4=_mm256_and_si256(m0,mask4);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_slli_si256(m4,12);//shift 3 int positions
			m3=_mm256_add_epi32(m3,m4);
			m0=_mm256_add_epi32(m0,m3);
			m0=_mm256_and_si256(m0,mask_odd_32);//keep only 1,3,5,7 and discard 0,2,4,6

			//MERGE output1 with output5 and DIVIDE
			output3=_mm256_add_epi32(m0,output3);
			output3 = division_32(division_case,output3,f);

			//8th col iteration

			//multiply by the mask
			m0=_mm256_maddubs_epi16(r0,cx7);

			//shift m0-m6 by one position because hadd follows
			m0=my_rshift16_by_one(m0,mask3);

			//make 16-bit values 32-bit (horizontal add)
			m0=_mm256_madd_epi16(m0,ones);

			//last horizontal add
			m3=_mm256_srli_si256(m0,4);
			m4=_mm256_and_si256(m0,mask4);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_slli_si256(m4,12);//shift 3 int positions
			m3=_mm256_add_epi32(m3,m4);
			m0=_mm256_add_epi32(m0,m3);
			m0=_mm256_and_si256(m0,mask_odd_32);//keep only 1,3,5,7 and discard 0,2,4,6

			//MERGE output1 with output5 and DIVIDE
			output4=_mm256_add_epi32(m0,output4);
			output4 = division_32(division_case,output4,f);



			//blend output1, output2, output3, output4 into one register
			output2 = _mm256_slli_si256(output2,1);
			output1 = _mm256_add_epi8(output1,output2);
			output3 = _mm256_slli_si256(output3,2);
			output1 = _mm256_add_epi8(output1,output3);
			output4 = _mm256_slli_si256(output4,3);
			output1 = _mm256_add_epi8(output1,output4);

			_mm256_storeu_si256( (__m256i *) &filt[row][col],output1 );
		}
	  loop_reminder_7x7_blur(frame1,filt,N,row, col, REMINDER_ITERATIONS, division_case, mask_vector_x, mask_vector_y, f, divisor_xy);


		}
}

}//end of parallel


}




//this function is computing the row=2 and row=N-3 only
//for row=2 input prelude_7x7_Ymask_3(0,col,...
//for row=N-3 input prelude_7x7_Ymask_3(N-6,col,...
__m256i prelude_7x7_Ymask_3(const int row, const int col, unsigned char **frame1, const signed char mask_vector_y[][32],  const unsigned int division_case, const __m256i f, const __m256i mask_prelude, const __m256i mask_even_16, const __m256i mask_odd_16){

	__m256i r0,r1,r2,r3,r4,r5,m0,m1,m2,m3,m4,m5,even,odd;
	__m256i ones=_mm256_set1_epi16(1);
	 __m256i cy0,cy1,cy2,cy3,cy4,cy5,cy0_sh1,cy1_sh1,cy2_sh1,cy3_sh1,cy4_sh1,cy5_sh1;
	if (row!=0){ //if row=N-6
	  cy0=_mm256_load_si256( (__m256i *) &mask_vector_y[0]);
	  cy1=_mm256_load_si256( (__m256i *) &mask_vector_y[1]);
	  cy2=_mm256_load_si256( (__m256i *) &mask_vector_y[2]);
	  cy3=_mm256_load_si256( (__m256i *) &mask_vector_y[3]);
	  cy4=_mm256_load_si256( (__m256i *) &mask_vector_y[4]);
	  cy5=_mm256_load_si256( (__m256i *) &mask_vector_y[5]);
	 // cy6=_mm256_load_si256( (__m256i *) &mask_vector_y[6]);

	  cy0_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[7]);
	  cy1_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[8]);
	  cy2_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[9]);
	  cy3_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[10]);
	  cy4_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[11]);
	  cy5_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[12]);
	 // cy6_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[13]);
	}
	else {
		  cy0=_mm256_load_si256( (__m256i *) &mask_vector_y[1]);
		  cy1=_mm256_load_si256( (__m256i *) &mask_vector_y[2]);
		  cy2=_mm256_load_si256( (__m256i *) &mask_vector_y[3]);
		  cy3=_mm256_load_si256( (__m256i *) &mask_vector_y[4]);
		  cy4=_mm256_load_si256( (__m256i *) &mask_vector_y[5]);
		  cy5=_mm256_load_si256( (__m256i *) &mask_vector_y[6]);
		 // cy6=_mm256_load_si256( (__m256i *) &mask_vector_y[6]);

		  cy0_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[8]);
		  cy1_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[9]);
		  cy2_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[10]);
		  cy3_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[11]);
		  cy4_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[12]);
		  cy5_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[13]);
		//  cy6_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[13]);
	}

	/*
	 row=N-3:
		 r1xc0
		 r2xc1
		 ...
		 r6xc5
	 */
	/*
	 row=2:
		 r0xc1
		 r1xc2
		 ...
		 r5xc6
	 */


	  if (col==0){

			r0=_mm256_loadu_si256( (__m256i *) &frame1[row][0]);
			r1=_mm256_loadu_si256( (__m256i *) &frame1[row+1][0]);
			r2=_mm256_loadu_si256( (__m256i *) &frame1[row+2][0]);
			r3=_mm256_loadu_si256( (__m256i *) &frame1[row+3][0]);
			r4=_mm256_loadu_si256( (__m256i *) &frame1[row+4][0]);
			r5=_mm256_loadu_si256( (__m256i *) &frame1[row+5][0]);


	  			//START - extra code needed for prelude
	  					m0=_mm256_slli_si256(r0,3);//shift 3 elements left - equivalent to filling with two zeros inthe beginning
	  					m1=_mm256_slli_si256(r1,3);//shift 3 elements left - equivalent to filling with two zeros inthe beginning
	  					m2=_mm256_slli_si256(r2,3);//shift 3 elements left - equivalent to filling with two zeros inthe beginning
	  					m3=_mm256_slli_si256(r3,3);//shift 3 elements left - equivalent to filling with two zeros inthe beginning
	  					m4=_mm256_slli_si256(r4,3);//shift 3 elements left - equivalent to filling with two zeros inthe beginning
	  					m5=_mm256_slli_si256(r5,3);//shift 3 elements left - equivalent to filling with two zeros inthe beginning

	  					r0=_mm256_and_si256(r0,mask_prelude);
	  					r0=_mm256_permute2f128_si256(r0,r0,1);
	  					r0=_mm256_srli_si256(r0,13);//shift 14 elements
	  					r0=_mm256_add_epi16(m0,r0);

	  					r1=_mm256_and_si256(r1,mask_prelude);
	  					r1=_mm256_permute2f128_si256(r1,r1,1);
	  					r1=_mm256_srli_si256(r1,13);//shift 14 elements
	  					r1=_mm256_add_epi16(m1,r1);

	  					r2=_mm256_and_si256(r2,mask_prelude);
	  					r2=_mm256_permute2f128_si256(r2,r2,1);
	  					r2=_mm256_srli_si256(r2,13);//shift 14 elements
	  					r2=_mm256_add_epi16(m2,r2);

	  					r3=_mm256_and_si256(r3,mask_prelude);
	  					r3=_mm256_permute2f128_si256(r3,r3,1);
	  					r3=_mm256_srli_si256(r3,13);//shift 14 elements
	  					r3=_mm256_add_epi16(m3,r3);

	  					r4=_mm256_and_si256(r4,mask_prelude);
	  					r4=_mm256_permute2f128_si256(r4,r4,1);
	  					r4=_mm256_srli_si256(r4,13);//shift 14 elements
	  					r4=_mm256_add_epi16(m4,r4);

	  					r5=_mm256_and_si256(r5,mask_prelude);
	  					r5=_mm256_permute2f128_si256(r5,r5,1);
	  					r5=_mm256_srli_si256(r5,13);//shift 14 elements
	  					r5=_mm256_add_epi16(m5,r5);


	  			//END - extra code needed for prelude
	  		}
	  else {
			//load the 5 rows
			r0=_mm256_loadu_si256( (__m256i *) &frame1[row][col-3]);
			r1=_mm256_loadu_si256( (__m256i *) &frame1[row+1][col-3]);
			r2=_mm256_loadu_si256( (__m256i *) &frame1[row+2][col-3]);
			r3=_mm256_loadu_si256( (__m256i *) &frame1[row+3][col-3]);
			r4=_mm256_loadu_si256( (__m256i *) &frame1[row+4][col-3]);
			r5=_mm256_loadu_si256( (__m256i *) &frame1[row+5][col-3]);

	  }

	  //--------------------y filter begins--------------------

		//multiply by the mask
		m0=_mm256_maddubs_epi16(r0,cy0);
		m1=_mm256_maddubs_epi16(r1,cy1);
		m2=_mm256_maddubs_epi16(r2,cy2);
		m3=_mm256_maddubs_epi16(r3,cy3);
		m4=_mm256_maddubs_epi16(r4,cy4);
		m5=_mm256_maddubs_epi16(r5,cy5);

		//discard odd
__m256i m0_even=_mm256_and_si256(m0,mask_even_16);
__m256i	m1_even=_mm256_and_si256(m1,mask_even_16);
__m256i	m2_even=_mm256_and_si256(m2,mask_even_16);
__m256i	m3_even=_mm256_and_si256(m3,mask_even_16);
__m256i	m4_even=_mm256_and_si256(m4,mask_even_16);
__m256i	m5_even=_mm256_and_si256(m5,mask_even_16);

//make from 16-bit to 32bit
m0_even=_mm256_madd_epi16(m0_even,ones);
m1_even=_mm256_madd_epi16(m1_even,ones);
m2_even=_mm256_madd_epi16(m2_even,ones);
m3_even=_mm256_madd_epi16(m3_even,ones);
m4_even=_mm256_madd_epi16(m4_even,ones);
m5_even=_mm256_madd_epi16(m5_even,ones);

		//vertical 32-bit add
		m0_even=_mm256_add_epi32(m0_even,m1_even);
		m0_even=_mm256_add_epi32(m0_even,m2_even);
		m0_even=_mm256_add_epi32(m0_even,m3_even);
		m0_even=_mm256_add_epi32(m0_even,m4_even);
		m0_even=_mm256_add_epi32(m0_even,m5_even);

		//discard even
__m256i	m0_odd=_mm256_and_si256(m0,mask_odd_16);
__m256i	m1_odd=_mm256_and_si256(m1,mask_odd_16);
__m256i	m2_odd=_mm256_and_si256(m2,mask_odd_16);
__m256i	m3_odd=_mm256_and_si256(m3,mask_odd_16);
__m256i	m4_odd=_mm256_and_si256(m4,mask_odd_16);
__m256i	m5_odd=_mm256_and_si256(m5,mask_odd_16);

//make from 16-bit to 32bit
		m0_odd=_mm256_madd_epi16(m0_odd,ones);
		m1_odd=_mm256_madd_epi16(m1_odd,ones);
		m2_odd=_mm256_madd_epi16(m2_odd,ones);
		m3_odd=_mm256_madd_epi16(m3_odd,ones);
		m4_odd=_mm256_madd_epi16(m4_odd,ones);
		m5_odd=_mm256_madd_epi16(m5_odd,ones);

		//vertical 32-bit add
		m0_odd=_mm256_add_epi32(m0_odd,m1_odd);
		m0_odd=_mm256_add_epi32(m0_odd,m2_odd);
		m0_odd=_mm256_add_epi32(m0_odd,m3_odd);
		m0_odd=_mm256_add_epi32(m0_odd,m4_odd);
		m0_odd=_mm256_add_epi32(m0_odd,m5_odd);

		m0_even=division_32(division_case,m0_even,f);//even results
		m0_odd=division_32(division_case,m0_odd,f);//even results

		m0_odd=_mm256_slli_si256(m0_odd,2); //shift two position right
		even=_mm256_add_epi8(m0_even,m0_odd); //add the odd with the even

		//-----------------multiply by the second mask----------
		m0=_mm256_maddubs_epi16(r0,cy0_sh1);
		m1=_mm256_maddubs_epi16(r1,cy1_sh1);
		m2=_mm256_maddubs_epi16(r2,cy2_sh1);
		m3=_mm256_maddubs_epi16(r3,cy3_sh1);
		m4=_mm256_maddubs_epi16(r4,cy4_sh1);
		m5=_mm256_maddubs_epi16(r5,cy5_sh1);


		m0_even=_mm256_and_si256(m0,mask_even_16);
		m1_even=_mm256_and_si256(m1,mask_even_16);
		m2_even=_mm256_and_si256(m2,mask_even_16);
		m3_even=_mm256_and_si256(m3,mask_even_16);
		m4_even=_mm256_and_si256(m4,mask_even_16);
		m5_even=_mm256_and_si256(m5,mask_even_16);

		//make from 16-bit to 32bit
		  m0_even=_mm256_madd_epi16(m0_even,ones);
		  m1_even=_mm256_madd_epi16(m1_even,ones);
		  m2_even=_mm256_madd_epi16(m2_even,ones);
		  m3_even=_mm256_madd_epi16(m3_even,ones);
		  m4_even=_mm256_madd_epi16(m4_even,ones);
		  m5_even=_mm256_madd_epi16(m5_even,ones);

		//vertical 32-bit add
		m0_even=_mm256_add_epi32(m0_even,m1_even);
		m0_even=_mm256_add_epi32(m0_even,m2_even);
		m0_even=_mm256_add_epi32(m0_even,m3_even);
		m0_even=_mm256_add_epi32(m0_even,m4_even);
		m0_even=_mm256_add_epi32(m0_even,m5_even);

		 m0_odd=_mm256_and_si256(m0,mask_odd_16);
		 m1_odd=_mm256_and_si256(m1,mask_odd_16);
		 m2_odd=_mm256_and_si256(m2,mask_odd_16);
		 m3_odd=_mm256_and_si256(m3,mask_odd_16);
		 m4_odd=_mm256_and_si256(m4,mask_odd_16);
		 m5_odd=_mm256_and_si256(m5,mask_odd_16);

		 //make from 16-bit to 32bit
			m0_odd=_mm256_madd_epi16(m0_odd,ones);
			m1_odd=_mm256_madd_epi16(m1_odd,ones);
			m2_odd=_mm256_madd_epi16(m2_odd,ones);
			m3_odd=_mm256_madd_epi16(m3_odd,ones);
			m4_odd=_mm256_madd_epi16(m4_odd,ones);
			m5_odd=_mm256_madd_epi16(m5_odd,ones);


		//vertical 32-bit add
		m0_odd=_mm256_add_epi32(m0_odd,m1_odd);
		m0_odd=_mm256_add_epi32(m0_odd,m2_odd);
		m0_odd=_mm256_add_epi32(m0_odd,m3_odd);
		m0_odd=_mm256_add_epi32(m0_odd,m4_odd);
		m0_odd=_mm256_add_epi32(m0_odd,m5_odd);

		m0_even=division_32(division_case,m0_even,f);//even results
		m0_odd=division_32(division_case,m0_odd,f);//even results

		m0_odd=_mm256_slli_si256(m0_odd,2); //shift two position right
		odd=_mm256_add_epi8(m0_even,m0_odd); //add the odd with the even


		//MERGE AND pack to one register
		odd=_mm256_slli_si256(odd,1); //shift one position right
		return _mm256_add_epi8(even,odd); //add the odd with the even

		//y filter ends - r0 has now the data to be processed by x filter
}

//this function is computing the row=1 and row=N-2 only
//for row=1 input prelude_7x7_Ymask_3(0,col,...
//for row=N-2 input prelude_7x7_Ymask_3(N-5,col,...
__m256i prelude_7x7_Ymask_2(const int row, const int col, unsigned char **frame1, const signed char mask_vector_y[][32],  const unsigned int division_case, const __m256i f, const __m256i mask_prelude, const __m256i mask_even_16, const __m256i mask_odd_16){

	__m256i r0,r1,r2,r3,r4,m0,m1,m2,m3,m4,even,odd;
	__m256i ones=_mm256_set1_epi16(1);

	 __m256i cy0,cy1,cy2,cy3,cy4,cy0_sh1,cy1_sh1,cy2_sh1,cy3_sh1,cy4_sh1;
	if (row!=0){//if row=N-5
	  cy0=_mm256_load_si256( (__m256i *) &mask_vector_y[0]);
	  cy1=_mm256_load_si256( (__m256i *) &mask_vector_y[1]);
	  cy2=_mm256_load_si256( (__m256i *) &mask_vector_y[2]);
	  cy3=_mm256_load_si256( (__m256i *) &mask_vector_y[3]);
	  cy4=_mm256_load_si256( (__m256i *) &mask_vector_y[4]);
	 // cy5=_mm256_load_si256( (__m256i *) &mask_vector_y[5]);
	 // cy6=_mm256_load_si256( (__m256i *) &mask_vector_y[6]);

	  cy0_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[7]);
	  cy1_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[8]);
	  cy2_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[9]);
	  cy3_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[10]);
	  cy4_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[11]);
	//  cy5_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[12]);
	 // cy6_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[13]);
	}
	else {
		  cy0=_mm256_load_si256( (__m256i *) &mask_vector_y[2]);
		  cy1=_mm256_load_si256( (__m256i *) &mask_vector_y[3]);
		  cy2=_mm256_load_si256( (__m256i *) &mask_vector_y[4]);
		  cy3=_mm256_load_si256( (__m256i *) &mask_vector_y[5]);
		  cy4=_mm256_load_si256( (__m256i *) &mask_vector_y[6]);
		//  cy5=_mm256_load_si256( (__m256i *) &mask_vector_y[6]);
		 // cy6=_mm256_load_si256( (__m256i *) &mask_vector_y[6]);

		  cy0_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[9]);
		  cy1_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[10]);
		  cy2_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[11]);
		  cy3_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[12]);
		  cy4_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[13]);
		//  cy5_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[13]);
		//  cy6_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[13]);
	}

	/*
	 row=N-2:
		 r2xc0
		 ...
		 r6xc4
	 */
	/*
	 row=1:
		 r0xc2
		 r1xc2
		 ...
		 r4xc6
	 */

	  if (col==0){

			r0=_mm256_loadu_si256( (__m256i *) &frame1[row][0]);
			r1=_mm256_loadu_si256( (__m256i *) &frame1[row+1][0]);
			r2=_mm256_loadu_si256( (__m256i *) &frame1[row+2][0]);
			r3=_mm256_loadu_si256( (__m256i *) &frame1[row+3][0]);
			r4=_mm256_loadu_si256( (__m256i *) &frame1[row+4][0]);


	  			//START - extra code needed for prelude
	  					m0=_mm256_slli_si256(r0,3);//shift 3 elements left - equivalent to filling with two zeros inthe beginning
	  					m1=_mm256_slli_si256(r1,3);//shift 3 elements left - equivalent to filling with two zeros inthe beginning
	  					m2=_mm256_slli_si256(r2,3);//shift 3 elements left - equivalent to filling with two zeros inthe beginning
	  					m3=_mm256_slli_si256(r3,3);//shift 3 elements left - equivalent to filling with two zeros inthe beginning
	  					m4=_mm256_slli_si256(r4,3);//shift 3 elements left - equivalent to filling with two zeros inthe beginning

	  					r0=_mm256_and_si256(r0,mask_prelude);
	  					r0=_mm256_permute2f128_si256(r0,r0,1);
	  					r0=_mm256_srli_si256(r0,13);//shift 14 elements
	  					r0=_mm256_add_epi16(m0,r0);

	  					r1=_mm256_and_si256(r1,mask_prelude);
	  					r1=_mm256_permute2f128_si256(r1,r1,1);
	  					r1=_mm256_srli_si256(r1,13);//shift 14 elements
	  					r1=_mm256_add_epi16(m1,r1);

	  					r2=_mm256_and_si256(r2,mask_prelude);
	  					r2=_mm256_permute2f128_si256(r2,r2,1);
	  					r2=_mm256_srli_si256(r2,13);//shift 14 elements
	  					r2=_mm256_add_epi16(m2,r2);

	  					r3=_mm256_and_si256(r3,mask_prelude);
	  					r3=_mm256_permute2f128_si256(r3,r3,1);
	  					r3=_mm256_srli_si256(r3,13);//shift 14 elements
	  					r3=_mm256_add_epi16(m3,r3);

	  					r4=_mm256_and_si256(r4,mask_prelude);
	  					r4=_mm256_permute2f128_si256(r4,r4,1);
	  					r4=_mm256_srli_si256(r4,13);//shift 14 elements
	  					r4=_mm256_add_epi16(m4,r4);


	  			//END - extra code needed for prelude
	  		}
	  else {
			//load the 5 rows
			r0=_mm256_loadu_si256( (__m256i *) &frame1[row][col-3]);
			r1=_mm256_loadu_si256( (__m256i *) &frame1[row+1][col-3]);
			r2=_mm256_loadu_si256( (__m256i *) &frame1[row+2][col-3]);
			r3=_mm256_loadu_si256( (__m256i *) &frame1[row+3][col-3]);
			r4=_mm256_loadu_si256( (__m256i *) &frame1[row+4][col-3]);
	  }

	  //--------------------y filter begins--------------------

		//multiply by the mask
		m0=_mm256_maddubs_epi16(r0,cy0);
		m1=_mm256_maddubs_epi16(r1,cy1);
		m2=_mm256_maddubs_epi16(r2,cy2);
		m3=_mm256_maddubs_epi16(r3,cy3);
		m4=_mm256_maddubs_epi16(r4,cy4);

		//discard odd
		__m256i m0_even=_mm256_and_si256(m0,mask_even_16);
		__m256i	m1_even=_mm256_and_si256(m1,mask_even_16);
		__m256i	m2_even=_mm256_and_si256(m2,mask_even_16);
		__m256i	m3_even=_mm256_and_si256(m3,mask_even_16);
		__m256i	m4_even=_mm256_and_si256(m4,mask_even_16);

		//make from 16-bit to 32bit
		m0_even=_mm256_madd_epi16(m0_even,ones);
		m1_even=_mm256_madd_epi16(m1_even,ones);
		m2_even=_mm256_madd_epi16(m2_even,ones);
		m3_even=_mm256_madd_epi16(m3_even,ones);
		m4_even=_mm256_madd_epi16(m4_even,ones);

		//vertical 32-bit add
		m0_even=_mm256_add_epi32(m0_even,m1_even);
		m0_even=_mm256_add_epi32(m0_even,m2_even);
		m0_even=_mm256_add_epi32(m0_even,m3_even);
		m0_even=_mm256_add_epi32(m0_even,m4_even);

		//discard even
		__m256i	m0_odd=_mm256_and_si256(m0,mask_odd_16);
		__m256i	m1_odd=_mm256_and_si256(m1,mask_odd_16);
		__m256i	m2_odd=_mm256_and_si256(m2,mask_odd_16);
		__m256i	m3_odd=_mm256_and_si256(m3,mask_odd_16);
		__m256i	m4_odd=_mm256_and_si256(m4,mask_odd_16);

//make from 16-bit to 32bit
		m0_odd=_mm256_madd_epi16(m0_odd,ones);
		m1_odd=_mm256_madd_epi16(m1_odd,ones);
		m2_odd=_mm256_madd_epi16(m2_odd,ones);
		m3_odd=_mm256_madd_epi16(m3_odd,ones);
		m4_odd=_mm256_madd_epi16(m4_odd,ones);

		//vertical 32-bit add
		m0_odd=_mm256_add_epi32(m0_odd,m1_odd);
		m0_odd=_mm256_add_epi32(m0_odd,m2_odd);
		m0_odd=_mm256_add_epi32(m0_odd,m3_odd);
		m0_odd=_mm256_add_epi32(m0_odd,m4_odd);

		m0_even=division_32(division_case,m0_even,f);//even results
		m0_odd=division_32(division_case,m0_odd,f);//even results

		m0_odd=_mm256_slli_si256(m0_odd,2); //shift two position right
		even=_mm256_add_epi8(m0_even,m0_odd); //add the odd with the even

		//-----------------multiply by the second mask----------
		m0=_mm256_maddubs_epi16(r0,cy0_sh1);
		m1=_mm256_maddubs_epi16(r1,cy1_sh1);
		m2=_mm256_maddubs_epi16(r2,cy2_sh1);
		m3=_mm256_maddubs_epi16(r3,cy3_sh1);
		m4=_mm256_maddubs_epi16(r4,cy4_sh1);

		m0_even=_mm256_and_si256(m0,mask_even_16);
		m1_even=_mm256_and_si256(m1,mask_even_16);
		m2_even=_mm256_and_si256(m2,mask_even_16);
		m3_even=_mm256_and_si256(m3,mask_even_16);
		m4_even=_mm256_and_si256(m4,mask_even_16);

		//make from 16-bit to 32bit
		  m0_even=_mm256_madd_epi16(m0_even,ones);
		  m1_even=_mm256_madd_epi16(m1_even,ones);
		  m2_even=_mm256_madd_epi16(m2_even,ones);
		  m3_even=_mm256_madd_epi16(m3_even,ones);
		  m4_even=_mm256_madd_epi16(m4_even,ones);

		//vertical 32-bit add
		m0_even=_mm256_add_epi32(m0_even,m1_even);
		m0_even=_mm256_add_epi32(m0_even,m2_even);
		m0_even=_mm256_add_epi32(m0_even,m3_even);
		m0_even=_mm256_add_epi32(m0_even,m4_even);

		 m0_odd=_mm256_and_si256(m0,mask_odd_16);
		 m1_odd=_mm256_and_si256(m1,mask_odd_16);
		 m2_odd=_mm256_and_si256(m2,mask_odd_16);
		 m3_odd=_mm256_and_si256(m3,mask_odd_16);
		 m4_odd=_mm256_and_si256(m4,mask_odd_16);

		 //make from 16-bit to 32bit
			m0_odd=_mm256_madd_epi16(m0_odd,ones);
			m1_odd=_mm256_madd_epi16(m1_odd,ones);
			m2_odd=_mm256_madd_epi16(m2_odd,ones);
			m3_odd=_mm256_madd_epi16(m3_odd,ones);
			m4_odd=_mm256_madd_epi16(m4_odd,ones);


		//vertical 32-bit add
		m0_odd=_mm256_add_epi32(m0_odd,m1_odd);
		m0_odd=_mm256_add_epi32(m0_odd,m2_odd);
		m0_odd=_mm256_add_epi32(m0_odd,m3_odd);
		m0_odd=_mm256_add_epi32(m0_odd,m4_odd);

		m0_even=division_32(division_case,m0_even,f);//even results
		m0_odd=division_32(division_case,m0_odd,f);//even results

		m0_odd=_mm256_slli_si256(m0_odd,2); //shift two position right
		odd=_mm256_add_epi8(m0_even,m0_odd); //add the odd with the even


		//MERGE AND pack to one register
		odd=_mm256_slli_si256(odd,1); //shift one position right
		return _mm256_add_epi8(even,odd); //add the odd with the even

		//y filter ends - r0 has now the data to be processed by x filter
}


//this function is computing the row=0 and row=N-1 only
//for row=0 input prelude_7x7_Ymask_3(0,col,...
//for row=N-1 input prelude_7x7_Ymask_3(N-4,col,...
__m256i prelude_7x7_Ymask_1(const int row, const int col, unsigned char **frame1, const signed char mask_vector_y[][32],  const unsigned int division_case, const __m256i f, const __m256i mask_prelude, const __m256i mask_even_16, const __m256i mask_odd_16){

	__m256i r0,r1,r2,r3,m0,m1,m2,m3,even,odd;
	__m256i ones=_mm256_set1_epi16(1);

	 __m256i cy0,cy1,cy2,cy3,cy0_sh1,cy1_sh1,cy2_sh1,cy3_sh1;
	if (row!=0){//if row=N-4
	  cy0=_mm256_load_si256( (__m256i *) &mask_vector_y[0]);
	  cy1=_mm256_load_si256( (__m256i *) &mask_vector_y[1]);
	  cy2=_mm256_load_si256( (__m256i *) &mask_vector_y[2]);
	  cy3=_mm256_load_si256( (__m256i *) &mask_vector_y[3]);
	//  cy4=_mm256_load_si256( (__m256i *) &mask_vector_y[4]);
	 // cy5=_mm256_load_si256( (__m256i *) &mask_vector_y[5]);
	 // cy6=_mm256_load_si256( (__m256i *) &mask_vector_y[6]);

	  cy0_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[7]);
	  cy1_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[8]);
	  cy2_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[9]);
	  cy3_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[10]);
	//  cy4_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[11]);
	//  cy5_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[12]);
	 // cy6_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[13]);
	}
	else {
		  cy0=_mm256_load_si256( (__m256i *) &mask_vector_y[3]);
		  cy1=_mm256_load_si256( (__m256i *) &mask_vector_y[4]);
		  cy2=_mm256_load_si256( (__m256i *) &mask_vector_y[5]);
		  cy3=_mm256_load_si256( (__m256i *) &mask_vector_y[6]);
		//  cy4=_mm256_load_si256( (__m256i *) &mask_vector_y[6]);
		//  cy5=_mm256_load_si256( (__m256i *) &mask_vector_y[6]);
		 // cy6=_mm256_load_si256( (__m256i *) &mask_vector_y[6]);

		  cy0_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[10]);
		  cy1_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[11]);
		  cy2_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[12]);
		  cy3_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[13]);
		//  cy4_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[13]);
		//  cy5_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[13]);
		//  cy6_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[13]);
	}


	/*
	 row=N-1:
		 r3xc0
		 ...
		 r6xc3
	 */
	/*
	 row=0:
		 r0xc3
		 ...
		 r3xc6
	 */


	  if (col==0){

			r0=_mm256_loadu_si256( (__m256i *) &frame1[row][0]);
			r1=_mm256_loadu_si256( (__m256i *) &frame1[row+1][0]);
			r2=_mm256_loadu_si256( (__m256i *) &frame1[row+2][0]);
			r3=_mm256_loadu_si256( (__m256i *) &frame1[row+3][0]);


	  			//START - extra code needed for prelude
	  					m0=_mm256_slli_si256(r0,3);//shift 3 elements left - equivalent to filling with two zeros inthe beginning
	  					m1=_mm256_slli_si256(r1,3);//shift 3 elements left - equivalent to filling with two zeros inthe beginning
	  					m2=_mm256_slli_si256(r2,3);//shift 3 elements left - equivalent to filling with two zeros inthe beginning
	  					m3=_mm256_slli_si256(r3,3);//shift 3 elements left - equivalent to filling with two zeros inthe beginning

	  					r0=_mm256_and_si256(r0,mask_prelude);
	  					r0=_mm256_permute2f128_si256(r0,r0,1);
	  					r0=_mm256_srli_si256(r0,13);//shift 14 elements
	  					r0=_mm256_add_epi16(m0,r0);

	  					r1=_mm256_and_si256(r1,mask_prelude);
	  					r1=_mm256_permute2f128_si256(r1,r1,1);
	  					r1=_mm256_srli_si256(r1,13);//shift 14 elements
	  					r1=_mm256_add_epi16(m1,r1);

	  					r2=_mm256_and_si256(r2,mask_prelude);
	  					r2=_mm256_permute2f128_si256(r2,r2,1);
	  					r2=_mm256_srli_si256(r2,13);//shift 14 elements
	  					r2=_mm256_add_epi16(m2,r2);

	  					r3=_mm256_and_si256(r3,mask_prelude);
	  					r3=_mm256_permute2f128_si256(r3,r3,1);
	  					r3=_mm256_srli_si256(r3,13);//shift 14 elements
	  					r3=_mm256_add_epi16(m3,r3);


	  			//END - extra code needed for prelude
	  		}
	  else {
			//load the 5 rows
			r0=_mm256_loadu_si256( (__m256i *) &frame1[row][col-3]);
			r1=_mm256_loadu_si256( (__m256i *) &frame1[row+1][col-3]);
			r2=_mm256_loadu_si256( (__m256i *) &frame1[row+2][col-3]);
			r3=_mm256_loadu_si256( (__m256i *) &frame1[row+3][col-3]);
	  }

	  //--------------------y filter begins--------------------

		//multiply by the mask
		m0=_mm256_maddubs_epi16(r0,cy0);
		m1=_mm256_maddubs_epi16(r1,cy1);
		m2=_mm256_maddubs_epi16(r2,cy2);
		m3=_mm256_maddubs_epi16(r3,cy3);

		//discard odd
__m256i m0_even=_mm256_and_si256(m0,mask_even_16);
__m256i	m1_even=_mm256_and_si256(m1,mask_even_16);
__m256i	m2_even=_mm256_and_si256(m2,mask_even_16);
__m256i	m3_even=_mm256_and_si256(m3,mask_even_16);

//make from 16-bit to 32bit
m0_even=_mm256_madd_epi16(m0_even,ones);
m1_even=_mm256_madd_epi16(m1_even,ones);
m2_even=_mm256_madd_epi16(m2_even,ones);
m3_even=_mm256_madd_epi16(m3_even,ones);

		//vertical 32-bit add
		m0_even=_mm256_add_epi32(m0_even,m1_even);
		m0_even=_mm256_add_epi32(m0_even,m2_even);
		m0_even=_mm256_add_epi32(m0_even,m3_even);

		//discard even
__m256i	m0_odd=_mm256_and_si256(m0,mask_odd_16);
__m256i	m1_odd=_mm256_and_si256(m1,mask_odd_16);
__m256i	m2_odd=_mm256_and_si256(m2,mask_odd_16);
__m256i	m3_odd=_mm256_and_si256(m3,mask_odd_16);

//make from 16-bit to 32bit
		m0_odd=_mm256_madd_epi16(m0_odd,ones);
		m1_odd=_mm256_madd_epi16(m1_odd,ones);
		m2_odd=_mm256_madd_epi16(m2_odd,ones);
		m3_odd=_mm256_madd_epi16(m3_odd,ones);

		//vertical 32-bit add
		m0_odd=_mm256_add_epi32(m0_odd,m1_odd);
		m0_odd=_mm256_add_epi32(m0_odd,m2_odd);
		m0_odd=_mm256_add_epi32(m0_odd,m3_odd);

		m0_even=division_32(division_case,m0_even,f);//even results
		m0_odd=division_32(division_case,m0_odd,f);//even results

		m0_odd=_mm256_slli_si256(m0_odd,2); //shift two position right
		even=_mm256_add_epi8(m0_even,m0_odd); //add the odd with the even

		//-----------------multiply by the second mask----------
		m0=_mm256_maddubs_epi16(r0,cy0_sh1);
		m1=_mm256_maddubs_epi16(r1,cy1_sh1);
		m2=_mm256_maddubs_epi16(r2,cy2_sh1);
		m3=_mm256_maddubs_epi16(r3,cy3_sh1);

		m0_even=_mm256_and_si256(m0,mask_even_16);
		m1_even=_mm256_and_si256(m1,mask_even_16);
		m2_even=_mm256_and_si256(m2,mask_even_16);
		m3_even=_mm256_and_si256(m3,mask_even_16);

		//make from 16-bit to 32bit
		  m0_even=_mm256_madd_epi16(m0_even,ones);
		  m1_even=_mm256_madd_epi16(m1_even,ones);
		  m2_even=_mm256_madd_epi16(m2_even,ones);
		  m3_even=_mm256_madd_epi16(m3_even,ones);

		//vertical 32-bit add
		m0_even=_mm256_add_epi32(m0_even,m1_even);
		m0_even=_mm256_add_epi32(m0_even,m2_even);
		m0_even=_mm256_add_epi32(m0_even,m3_even);

		 m0_odd=_mm256_and_si256(m0,mask_odd_16);
		 m1_odd=_mm256_and_si256(m1,mask_odd_16);
		 m2_odd=_mm256_and_si256(m2,mask_odd_16);
		 m3_odd=_mm256_and_si256(m3,mask_odd_16);

		 //make from 16-bit to 32bit
			m0_odd=_mm256_madd_epi16(m0_odd,ones);
			m1_odd=_mm256_madd_epi16(m1_odd,ones);
			m2_odd=_mm256_madd_epi16(m2_odd,ones);
			m3_odd=_mm256_madd_epi16(m3_odd,ones);


		//vertical 32-bit add
		m0_odd=_mm256_add_epi32(m0_odd,m1_odd);
		m0_odd=_mm256_add_epi32(m0_odd,m2_odd);
		m0_odd=_mm256_add_epi32(m0_odd,m3_odd);

		m0_even=division_32(division_case,m0_even,f);//even results
		m0_odd=division_32(division_case,m0_odd,f);//even results

		m0_odd=_mm256_slli_si256(m0_odd,2); //shift two position right
		odd=_mm256_add_epi8(m0_even,m0_odd); //add the odd with the even


		//MERGE AND pack to one register
		odd=_mm256_slli_si256(odd,1); //shift one position right
		return _mm256_add_epi8(even,odd); //add the odd with the even

		//y filter ends - r0 has now the data to be processed by x filter
}

void prelude_7x7_Xmask(unsigned char **filt,const int row, const int col, const __m256i r0, const signed char mask_vector_x[][32],  const unsigned int division_case, const __m256i f, const __m256i mask_even_32, const __m256i mask_odd_32){

	__m256i m0,m1,m3,m4,output1,output2,output3,output4;
	__m256i ones=_mm256_set1_epi16(1);

	const __m256i mask3  = _mm256_set_epi16(0,0,0,0,0,0,0,65535,0,0,0,0,0,0,0,0);
	const __m256i mask4  = _mm256_set_epi32(0,0,0,0xffffffff,0,0,0,0);

	const __m256i cx0=_mm256_load_si256( (__m256i *) &mask_vector_x[0]);
	const __m256i cx1=_mm256_load_si256( (__m256i *) &mask_vector_x[1]);
	const __m256i cx2=_mm256_load_si256( (__m256i *) &mask_vector_x[2]);
	const __m256i cx3=_mm256_load_si256( (__m256i *) &mask_vector_x[3]);
	const __m256i cx4=_mm256_load_si256( (__m256i *) &mask_vector_x[4]);
	const __m256i cx5=_mm256_load_si256( (__m256i *) &mask_vector_x[5]);
	const __m256i cx6=_mm256_load_si256( (__m256i *) &mask_vector_x[6]);
	const __m256i cx7=_mm256_load_si256( (__m256i *) &mask_vector_x[7]);

	//1st col iteration

	//multiply by the mask
	m0=_mm256_maddubs_epi16(r0,cx0);

	//make 16-bit values 32-bit (horizontal add)
	m0=_mm256_madd_epi16(m0,ones);

	//last horizontal add
	m1=_mm256_slli_si256(m0,4);//shift by one integer
	m0=_mm256_add_epi32(m0,m1);//add
	m1=_mm256_srli_si256(m0,4);//shift by one integer
	output1=_mm256_and_si256(m1,mask_even_32);//keep only 0,2,4,6 and discard 1,3,5,7


	//2nd col iteration

	//multiply by the mask
	m0=_mm256_maddubs_epi16(r0,cx1);

	//make 16-bit values 32-bit (horizontal add)
	m0=_mm256_madd_epi16(m0,ones);

	//last horizontal add
	m1=_mm256_slli_si256(m0,4);//shift by one integer
	m0=_mm256_add_epi32(m0,m1);//add
	m1=_mm256_srli_si256(m0,4);//shift by one integer
	output2=_mm256_and_si256(m1,mask_even_32);//keep only 0,2,4,6 and discard 1,3,5,7

	//3rd col iteration

	//multiply by the mask
	m0=_mm256_maddubs_epi16(r0,cx2);

	//shift m0-m6 by one position because hadd follows
	m0=my_rshift16_by_one(m0,mask3);

	//make 16-bit values 32-bit (horizontal add)
	m0=_mm256_madd_epi16(m0,ones);


	//last horizontal add
	m1=_mm256_slli_si256(m0,4);//shift by one integer
	m0=_mm256_add_epi32(m0,m1);//add
	m1=_mm256_srli_si256(m0,4);//shift by one integer
	output3=_mm256_and_si256(m1,mask_even_32);//keep only 0,2,4,6 and discard 1,3,5,7


	//4th col iteration

	//multiply by the mask
	m0=_mm256_maddubs_epi16(r0,cx3);

	//shift m0-m6 by one position because hadd follows
	m0=my_rshift16_by_one(m0,mask3);

	//make 16-bit values 32-bit (horizontal add)
	m0=_mm256_madd_epi16(m0,ones);


	//last horizontal add
	m1=_mm256_slli_si256(m0,4);//shift by one integer
	m0=_mm256_add_epi32(m0,m1);//add
	m1=_mm256_srli_si256(m0,4);//shift by one integer
	output4=_mm256_and_si256(m1,mask_even_32);//keep only 0,2,4,6 and discard 1,3,5,7


	//5th col iteration

	//multiply by the mask
	m0=_mm256_maddubs_epi16(r0,cx4);

	//No need for shift m0-m6 now

	//make 16-bit values 32-bit (horizontal add)
	m0=_mm256_madd_epi16(m0,ones);


	//last horizontal add
	m3=_mm256_srli_si256(m0,4);
	m4=_mm256_and_si256(m0,mask4);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,12);//shift 3 int positions
	m3=_mm256_add_epi32(m3,m4);
	m0=_mm256_add_epi32(m0,m3);
	m0=_mm256_and_si256(m0,mask_odd_32);//keep only 1,3,5,7 and discard 0,2,4,6

	//MERGE output1 with output5 and DIVIDE
	output1=_mm256_add_epi32(m0,output1);
	output1 = division_32(division_case,output1,f);


	//6th col iteration

	//multiply by the mask
	m0=_mm256_maddubs_epi16(r0,cx5);

	//No need for shift m0-m6 now

	//make 16-bit values 32-bit (horizontal add)
	m0=_mm256_madd_epi16(m0,ones);


	//last horizontal add
	m3=_mm256_srli_si256(m0,4);
	m4=_mm256_and_si256(m0,mask4);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,12);//shift 3 int positions
	m3=_mm256_add_epi32(m3,m4);
	m0=_mm256_add_epi32(m0,m3);
	m0=_mm256_and_si256(m0,mask_odd_32);//keep only 1,3,5,7 and discard 0,2,4,6

	//MERGE output1 with output5 and DIVIDE
	output2=_mm256_add_epi32(m0,output2);
	output2 = division_32(division_case,output2,f);


	//7th col iteration

	//multiply by the mask
	m0=_mm256_maddubs_epi16(r0,cx6);


	//shift m0-m6 by one position because hadd follows
	m0=my_rshift16_by_one(m0,mask3);

	//make 16-bit values 32-bit (horizontal add)
	m0=_mm256_madd_epi16(m0,ones);

	//last horizontal add
	m3=_mm256_srli_si256(m0,4);
	m4=_mm256_and_si256(m0,mask4);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,12);//shift 3 int positions
	m3=_mm256_add_epi32(m3,m4);
	m0=_mm256_add_epi32(m0,m3);
	m0=_mm256_and_si256(m0,mask_odd_32);//keep only 1,3,5,7 and discard 0,2,4,6

	//MERGE output1 with output5 and DIVIDE
	output3=_mm256_add_epi32(m0,output3);
	output3 = division_32(division_case,output3,f);

	//8th col iteration

	//multiply by the mask
	m0=_mm256_maddubs_epi16(r0,cx7);

	//shift m0-m6 by one position because hadd follows
	m0=my_rshift16_by_one(m0,mask3);

	//make 16-bit values 32-bit (horizontal add)
	m0=_mm256_madd_epi16(m0,ones);

	//last horizontal add
	m3=_mm256_srli_si256(m0,4);
	m4=_mm256_and_si256(m0,mask4);
	m4=_mm256_permute2f128_si256(m4,m4,1);
	m4=_mm256_slli_si256(m4,12);//shift 3 int positions
	m3=_mm256_add_epi32(m3,m4);
	m0=_mm256_add_epi32(m0,m3);
	m0=_mm256_and_si256(m0,mask_odd_32);//keep only 1,3,5,7 and discard 0,2,4,6

	//MERGE output1 with output5 and DIVIDE
	output4=_mm256_add_epi32(m0,output4);
	output4 = division_32(division_case,output4,f);



	//blend output1, output2, output3, output4 into one register
	output2 = _mm256_slli_si256(output2,1);
	output1 = _mm256_add_epi8(output1,output2);
	output3 = _mm256_slli_si256(output3,2);
	output1 = _mm256_add_epi8(output1,output3);
	output4 = _mm256_slli_si256(output4,3);
	output1 = _mm256_add_epi8(output1,output4);

	_mm256_storeu_si256( (__m256i *) &filt[row][col],output1 );
}


void loop_reminder_7x7_blur(unsigned char **frame1,unsigned char **filt,const unsigned int N,const unsigned int row, const unsigned int col,const unsigned int REMINDER_ITERATIONS,const unsigned int division_case,const signed char mask_vector_x[][32],const signed char mask_vector_y[][32], const __m256i f, const unsigned short int divisor_xy){

	const __m256i cx0=_mm256_load_si256( (__m256i *) &mask_vector_x[0]);
	const __m256i cx1=_mm256_load_si256( (__m256i *) &mask_vector_x[1]);
	const __m256i cx2=_mm256_load_si256( (__m256i *) &mask_vector_x[2]);
	const __m256i cx3=_mm256_load_si256( (__m256i *) &mask_vector_x[3]);
	const __m256i cx4=_mm256_load_si256( (__m256i *) &mask_vector_x[4]);
	const __m256i cx5=_mm256_load_si256( (__m256i *) &mask_vector_x[5]);
	const __m256i cx6=_mm256_load_si256( (__m256i *) &mask_vector_x[6]);
	const __m256i cx7=_mm256_load_si256( (__m256i *) &mask_vector_x[7]);

	__m256i cy0,cy1,cy2,cy3,cy4,cy5,cy6,cy0_sh1,cy1_sh1,cy2_sh1,cy3_sh1,cy4_sh1,cy5_sh1,cy6_sh1;

	const __m256i mask3  = _mm256_set_epi16(0,0,0,0,0,0,0,65535,0,0,0,0,0,0,0,0);

	const __m256i mask_even_16  = _mm256_set_epi16(0,0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff);
	const __m256i mask_odd_16  = _mm256_set_epi16(0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff,0);

	const __m256i mask_even_32  = _mm256_set_epi32(0,0xffffffff,0,0xffffffff,0,0xffffffff,0,0xffffffff);
	const __m256i mask_odd_32  = _mm256_set_epi32(0xffffffff,0,0xffffffff,0,0xffffffff,0,0xffffffff,0);
	const __m256i mask4  = _mm256_set_epi32(0,0,0,0xffffffff,0,0,0,0);
	const __m256i ones=_mm256_set1_epi16(1);

	__m256i r0,r1,r2,r3,r4,r5,r6,m0,m1,m2,m3,m4,m5,m6,even,odd,output1,output2,output3,output4,reminder_mask;

	if ((row>2) && (row<N-3)){//main case
		r0=_mm256_loadu_si256( (__m256i *) &frame1[row-3][col-3]);
		r1=_mm256_loadu_si256( (__m256i *) &frame1[row-2][col-3]);
		r2=_mm256_loadu_si256( (__m256i *) &frame1[row-1][col-3]);
		r3=_mm256_loadu_si256( (__m256i *) &frame1[row][col-3]);
		r4=_mm256_loadu_si256( (__m256i *) &frame1[row+1][col-3]);
		r5=_mm256_loadu_si256( (__m256i *) &frame1[row+2][col-3]);
		r6=_mm256_loadu_si256( (__m256i *) &frame1[row+3][col-3]);

		cy0=_mm256_load_si256( (__m256i *) &mask_vector_y[0]);
		cy1=_mm256_load_si256( (__m256i *) &mask_vector_y[1]);
		cy2=_mm256_load_si256( (__m256i *) &mask_vector_y[2]);
		cy3=_mm256_load_si256( (__m256i *) &mask_vector_y[3]);
		cy4=_mm256_load_si256( (__m256i *) &mask_vector_y[4]);
		cy5=_mm256_load_si256( (__m256i *) &mask_vector_y[5]);
		cy6=_mm256_load_si256( (__m256i *) &mask_vector_y[6]);

		cy0_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[7]);
		cy1_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[8]);
		cy2_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[9]);
		cy3_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[10]);
		cy4_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[11]);
		cy5_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[12]);
		cy6_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[13]);
	}
	else {
		if (row==0){
			r0=_mm256_loadu_si256( (__m256i *) &frame1[0][col-3]);
			r1=_mm256_loadu_si256( (__m256i *) &frame1[1][col-3]);
			r2=_mm256_loadu_si256( (__m256i *) &frame1[2][col-3]);
			r3=_mm256_loadu_si256( (__m256i *) &frame1[3][col-3]);
			r4=_mm256_setzero_si256();
			r5=_mm256_setzero_si256();
			r6=_mm256_setzero_si256();

			cy0=_mm256_load_si256( (__m256i *) &mask_vector_y[3]);
			cy1=_mm256_load_si256( (__m256i *) &mask_vector_y[4]);
			cy2=_mm256_load_si256( (__m256i *) &mask_vector_y[5]);
			cy3=_mm256_load_si256( (__m256i *) &mask_vector_y[6]);
			cy4=_mm256_setzero_si256();
			cy5=_mm256_setzero_si256();
			cy6=_mm256_setzero_si256();

			cy0_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[10]);
			cy1_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[11]);
			cy2_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[12]);
			cy3_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[13]);
			cy4_sh1=_mm256_setzero_si256();
			cy5_sh1=_mm256_setzero_si256();
			cy6_sh1=_mm256_setzero_si256();
		}
		else if (row==1){
			r0=_mm256_loadu_si256( (__m256i *) &frame1[0][col-3]);
			r1=_mm256_loadu_si256( (__m256i *) &frame1[1][col-3]);
			r2=_mm256_loadu_si256( (__m256i *) &frame1[2][col-3]);
			r3=_mm256_loadu_si256( (__m256i *) &frame1[3][col-3]);
			r4=_mm256_loadu_si256( (__m256i *) &frame1[4][col-3]);
			r5=_mm256_setzero_si256();
			r6=_mm256_setzero_si256();

			cy0=_mm256_load_si256( (__m256i *) &mask_vector_y[2]);
			cy1=_mm256_load_si256( (__m256i *) &mask_vector_y[3]);
			cy2=_mm256_load_si256( (__m256i *) &mask_vector_y[4]);
			cy3=_mm256_load_si256( (__m256i *) &mask_vector_y[5]);
			cy4=_mm256_load_si256( (__m256i *) &mask_vector_y[6]);
			cy5=_mm256_setzero_si256();
			cy6=_mm256_setzero_si256();

			cy0_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[9]);
			cy1_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[10]);
			cy2_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[11]);
			cy3_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[12]);
			cy4_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[13]);
			cy5_sh1=_mm256_setzero_si256();
			cy6_sh1=_mm256_setzero_si256();
		}
		else if (row==2){
			r0=_mm256_loadu_si256( (__m256i *) &frame1[0][col-3]);
			r1=_mm256_loadu_si256( (__m256i *) &frame1[1][col-3]);
			r2=_mm256_loadu_si256( (__m256i *) &frame1[2][col-3]);
			r3=_mm256_loadu_si256( (__m256i *) &frame1[3][col-3]);
			r4=_mm256_loadu_si256( (__m256i *) &frame1[4][col-3]);
			r5=_mm256_loadu_si256( (__m256i *) &frame1[5][col-3]);
			r6=_mm256_setzero_si256();

			cy0=_mm256_load_si256( (__m256i *) &mask_vector_y[1]);
			cy1=_mm256_load_si256( (__m256i *) &mask_vector_y[2]);
			cy2=_mm256_load_si256( (__m256i *) &mask_vector_y[3]);
			cy3=_mm256_load_si256( (__m256i *) &mask_vector_y[4]);
			cy4=_mm256_load_si256( (__m256i *) &mask_vector_y[5]);
			cy5=_mm256_load_si256( (__m256i *) &mask_vector_y[6]);
			cy6=_mm256_setzero_si256();

			cy0_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[8]);
			cy1_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[9]);
			cy2_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[10]);
			cy3_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[11]);
			cy4_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[12]);
			cy5_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[13]);
			cy6_sh1=_mm256_setzero_si256();
		}
		else if (row==N-3){
			r0=_mm256_setzero_si256();
			r1=_mm256_loadu_si256( (__m256i *) &frame1[N-6][col-3]);
			r2=_mm256_loadu_si256( (__m256i *) &frame1[N-5][col-3]);
			r3=_mm256_loadu_si256( (__m256i *) &frame1[N-4][col-3]);
			r4=_mm256_loadu_si256( (__m256i *) &frame1[N-3][col-3]);
			r5=_mm256_loadu_si256( (__m256i *) &frame1[N-2][col-3]);
			r6=_mm256_loadu_si256( (__m256i *) &frame1[N-1][col-3]);

			cy0=_mm256_setzero_si256();
			cy1=_mm256_load_si256( (__m256i *) &mask_vector_y[0]);
			cy2=_mm256_load_si256( (__m256i *) &mask_vector_y[1]);
			cy3=_mm256_load_si256( (__m256i *) &mask_vector_y[2]);
			cy4=_mm256_load_si256( (__m256i *) &mask_vector_y[3]);
			cy5=_mm256_load_si256( (__m256i *) &mask_vector_y[4]);
			cy6=_mm256_load_si256( (__m256i *) &mask_vector_y[5]);

			cy0_sh1=_mm256_setzero_si256();
			cy1_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[7]);
			cy2_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[8]);
			cy3_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[9]);
			cy4_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[10]);
			cy5_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[11]);
			cy6_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[12]);
		}
		else if (row==N-2){
			r0=_mm256_setzero_si256();
			r1=_mm256_setzero_si256();
			r2=_mm256_loadu_si256( (__m256i *) &frame1[N-5][col-3]);
			r3=_mm256_loadu_si256( (__m256i *) &frame1[N-4][col-3]);
			r4=_mm256_loadu_si256( (__m256i *) &frame1[N-3][col-3]);
			r5=_mm256_loadu_si256( (__m256i *) &frame1[N-2][col-3]);
			r6=_mm256_loadu_si256( (__m256i *) &frame1[N-1][col-3]);

			cy0=_mm256_setzero_si256();
			cy1=_mm256_setzero_si256();
			cy2=_mm256_load_si256( (__m256i *) &mask_vector_y[0]);
			cy3=_mm256_load_si256( (__m256i *) &mask_vector_y[1]);
			cy4=_mm256_load_si256( (__m256i *) &mask_vector_y[2]);
			cy5=_mm256_load_si256( (__m256i *) &mask_vector_y[3]);
			cy6=_mm256_load_si256( (__m256i *) &mask_vector_y[4]);

			cy0_sh1=_mm256_setzero_si256();
			cy1_sh1=_mm256_setzero_si256();
			cy2_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[7]);
			cy3_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[8]);
			cy4_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[9]);
			cy5_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[10]);
			cy6_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[11]);
		}
		else if (row==N-1){
			r0=_mm256_setzero_si256();
			r1=_mm256_setzero_si256();
			r2=_mm256_setzero_si256();
			r3=_mm256_loadu_si256( (__m256i *) &frame1[N-4][col-3]);
			r4=_mm256_loadu_si256( (__m256i *) &frame1[N-3][col-3]);
			r5=_mm256_loadu_si256( (__m256i *) &frame1[N-2][col-3]);
			r6=_mm256_loadu_si256( (__m256i *) &frame1[N-1][col-3]);

			cy0=_mm256_setzero_si256();
			cy1=_mm256_setzero_si256();
			cy2=_mm256_setzero_si256();
			cy3=_mm256_load_si256( (__m256i *) &mask_vector_y[0]);
			cy4=_mm256_load_si256( (__m256i *) &mask_vector_y[1]);
			cy5=_mm256_load_si256( (__m256i *) &mask_vector_y[2]);
			cy6=_mm256_load_si256( (__m256i *) &mask_vector_y[3]);

			cy0_sh1=_mm256_setzero_si256();
			cy1_sh1=_mm256_setzero_si256();
			cy2_sh1=_mm256_setzero_si256();
			cy3_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[7]);
			cy4_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[8]);
			cy5_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[9]);
			cy6_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[10]);
		}
		else {
			printf("\nsomething went wrong");
			exit(EXIT_FAILURE);
		}
	}


	/*
	 * The above load operations load outside of the array bounds; these elements are filled with zeros just after
	 */
	reminder_mask=_mm256_load_si256( (__m256i *) &reminder_mask_7x7[REMINDER_ITERATIONS-1][0]);

	//AND r0-r4 with reminder_mask
	r0=_mm256_and_si256(r0,reminder_mask);
	r1=_mm256_and_si256(r1,reminder_mask);
	r2=_mm256_and_si256(r2,reminder_mask);
	r3=_mm256_and_si256(r3,reminder_mask);
	r4=_mm256_and_si256(r4,reminder_mask);
	r5=_mm256_and_si256(r5,reminder_mask);
	r6=_mm256_and_si256(r6,reminder_mask);

//--------------------y filter begins--------------------

	//multiply by the mask
	m0=_mm256_maddubs_epi16(r0,cy0);
	m1=_mm256_maddubs_epi16(r1,cy1);
	m2=_mm256_maddubs_epi16(r2,cy2);
	m3=_mm256_maddubs_epi16(r3,cy3);
	m4=_mm256_maddubs_epi16(r4,cy4);
	m5=_mm256_maddubs_epi16(r5,cy5);
	m6=_mm256_maddubs_epi16(r6,cy6);

	//discard odd
	__m256i m0_even=_mm256_and_si256(m0,mask_even_16);
	__m256i	m1_even=_mm256_and_si256(m1,mask_even_16);
	__m256i	m2_even=_mm256_and_si256(m2,mask_even_16);
	__m256i	m3_even=_mm256_and_si256(m3,mask_even_16);
	__m256i	m4_even=_mm256_and_si256(m4,mask_even_16);
	__m256i	m5_even=_mm256_and_si256(m5,mask_even_16);
	__m256i	m6_even=_mm256_and_si256(m6,mask_even_16);

	//make from 16-bit to 32bit
	m0_even=_mm256_madd_epi16(m0_even,ones);
	m1_even=_mm256_madd_epi16(m1_even,ones);
	m2_even=_mm256_madd_epi16(m2_even,ones);
	m3_even=_mm256_madd_epi16(m3_even,ones);
	m4_even=_mm256_madd_epi16(m4_even,ones);
	m5_even=_mm256_madd_epi16(m5_even,ones);
	m6_even=_mm256_madd_epi16(m6_even,ones);

	//vertical 32-bit add
	m0_even=_mm256_add_epi32(m0_even,m1_even);
	m0_even=_mm256_add_epi32(m0_even,m2_even);
	m0_even=_mm256_add_epi32(m0_even,m3_even);
	m0_even=_mm256_add_epi32(m0_even,m4_even);
	m0_even=_mm256_add_epi32(m0_even,m5_even);
	m0_even=_mm256_add_epi32(m0_even,m6_even);

	//discard even
	__m256i	m0_odd=_mm256_and_si256(m0,mask_odd_16);
	__m256i	m1_odd=_mm256_and_si256(m1,mask_odd_16);
	__m256i	m2_odd=_mm256_and_si256(m2,mask_odd_16);
	__m256i	m3_odd=_mm256_and_si256(m3,mask_odd_16);
	__m256i	m4_odd=_mm256_and_si256(m4,mask_odd_16);
	__m256i	m5_odd=_mm256_and_si256(m5,mask_odd_16);
	__m256i	m6_odd=_mm256_and_si256(m6,mask_odd_16);

	//make from 16-bit to 32bit
	m0_odd=_mm256_madd_epi16(m0_odd,ones);
	m1_odd=_mm256_madd_epi16(m1_odd,ones);
	m2_odd=_mm256_madd_epi16(m2_odd,ones);
	m3_odd=_mm256_madd_epi16(m3_odd,ones);
	m4_odd=_mm256_madd_epi16(m4_odd,ones);
	m5_odd=_mm256_madd_epi16(m5_odd,ones);
	m6_odd=_mm256_madd_epi16(m6_odd,ones);

	//vertical 32-bit add
	m0_odd=_mm256_add_epi32(m0_odd,m1_odd);
	m0_odd=_mm256_add_epi32(m0_odd,m2_odd);
	m0_odd=_mm256_add_epi32(m0_odd,m3_odd);
	m0_odd=_mm256_add_epi32(m0_odd,m4_odd);
	m0_odd=_mm256_add_epi32(m0_odd,m5_odd);
	m0_odd=_mm256_add_epi32(m0_odd,m6_odd);

	m0_even=division_32(division_case,m0_even,f);//even results
	m0_odd=division_32(division_case,m0_odd,f);//even results

	m0_odd=_mm256_slli_si256(m0_odd,2); //shift two position right
	even=_mm256_add_epi8(m0_even,m0_odd); //add the odd with the even

	//-----------------multiply by the second mask----------
	m0=_mm256_maddubs_epi16(r0,cy0_sh1);
	m1=_mm256_maddubs_epi16(r1,cy1_sh1);
	m2=_mm256_maddubs_epi16(r2,cy2_sh1);
	m3=_mm256_maddubs_epi16(r3,cy3_sh1);
	m4=_mm256_maddubs_epi16(r4,cy4_sh1);
	m5=_mm256_maddubs_epi16(r5,cy5_sh1);
	m6=_mm256_maddubs_epi16(r6,cy6_sh1);


	m0_even=_mm256_and_si256(m0,mask_even_16);
	m1_even=_mm256_and_si256(m1,mask_even_16);
	m2_even=_mm256_and_si256(m2,mask_even_16);
	m3_even=_mm256_and_si256(m3,mask_even_16);
	m4_even=_mm256_and_si256(m4,mask_even_16);
	m5_even=_mm256_and_si256(m5,mask_even_16);
	m6_even=_mm256_and_si256(m6,mask_even_16);

	//make from 16-bit to 32bit
	  m0_even=_mm256_madd_epi16(m0_even,ones);
	  m1_even=_mm256_madd_epi16(m1_even,ones);
	  m2_even=_mm256_madd_epi16(m2_even,ones);
	  m3_even=_mm256_madd_epi16(m3_even,ones);
	  m4_even=_mm256_madd_epi16(m4_even,ones);
	  m5_even=_mm256_madd_epi16(m5_even,ones);
	  m6_even=_mm256_madd_epi16(m6_even,ones);

	//vertical 32-bit add
	m0_even=_mm256_add_epi32(m0_even,m1_even);
	m0_even=_mm256_add_epi32(m0_even,m2_even);
	m0_even=_mm256_add_epi32(m0_even,m3_even);
	m0_even=_mm256_add_epi32(m0_even,m4_even);
	m0_even=_mm256_add_epi32(m0_even,m5_even);
	m0_even=_mm256_add_epi32(m0_even,m6_even);

	 m0_odd=_mm256_and_si256(m0,mask_odd_16);
	 m1_odd=_mm256_and_si256(m1,mask_odd_16);
	 m2_odd=_mm256_and_si256(m2,mask_odd_16);
	 m3_odd=_mm256_and_si256(m3,mask_odd_16);
	 m4_odd=_mm256_and_si256(m4,mask_odd_16);
	 m5_odd=_mm256_and_si256(m5,mask_odd_16);
	 m6_odd=_mm256_and_si256(m6,mask_odd_16);

 //make from 16-bit to 32bit
	m0_odd=_mm256_madd_epi16(m0_odd,ones);
	m1_odd=_mm256_madd_epi16(m1_odd,ones);
	m2_odd=_mm256_madd_epi16(m2_odd,ones);
	m3_odd=_mm256_madd_epi16(m3_odd,ones);
	m4_odd=_mm256_madd_epi16(m4_odd,ones);
	m5_odd=_mm256_madd_epi16(m5_odd,ones);
	m6_odd=_mm256_madd_epi16(m6_odd,ones);


		//vertical 32-bit add
		m0_odd=_mm256_add_epi32(m0_odd,m1_odd);
		m0_odd=_mm256_add_epi32(m0_odd,m2_odd);
		m0_odd=_mm256_add_epi32(m0_odd,m3_odd);
		m0_odd=_mm256_add_epi32(m0_odd,m4_odd);
		m0_odd=_mm256_add_epi32(m0_odd,m5_odd);
		m0_odd=_mm256_add_epi32(m0_odd,m6_odd);

		m0_even=division_32(division_case,m0_even,f);//even results
		m0_odd=division_32(division_case,m0_odd,f);//even results

		m0_odd=_mm256_slli_si256(m0_odd,2); //shift two position right
		odd=_mm256_add_epi8(m0_even,m0_odd); //add the odd with the even


		//MERGE AND pack to one register
		odd=_mm256_slli_si256(odd,1); //shift one position right
		r0=_mm256_add_epi8(even,odd); //add the odd with the even

		//y filter ends - r0 has now the data to be processed by x filter

		// col iteration computes output pixels of   0,8,16,24
		// col+1 iteration computes output pixels of 1,9,17,25
		// col+2 iteration computes output pixels of 2,10,18
		// col+3 iteration computes output pixels of 3,11,19
		// col+4 iteration computes output pixels of 4,12,20
		// col+5 iteration computes output pixels of 5,13,21
		// col+6 iteration computes output pixels of 6,14,22
		// col+7 iteration computes output pixels of 7,15,23
		//afterwards, col becomes 26 and repeat the above process

		//1st col iteration

		//multiply by the mask
		m0=_mm256_maddubs_epi16(r0,cx0);

		//make 16-bit values 32-bit (horizontal add)
		m0=_mm256_madd_epi16(m0,ones);

		//last horizontal add
		m1=_mm256_slli_si256(m0,4);//shift by one integer
		m0=_mm256_add_epi32(m0,m1);//add
		m1=_mm256_srli_si256(m0,4);//shift by one integer
		output1=_mm256_and_si256(m1,mask_even_32);//keep only 0,2,4,6 and discard 1,3,5,7


		//2nd col iteration

		//multiply by the mask
		m0=_mm256_maddubs_epi16(r0,cx1);

		//make 16-bit values 32-bit (horizontal add)
		m0=_mm256_madd_epi16(m0,ones);

		//last horizontal add
		m1=_mm256_slli_si256(m0,4);//shift by one integer
		m0=_mm256_add_epi32(m0,m1);//add
		m1=_mm256_srli_si256(m0,4);//shift by one integer
		output2=_mm256_and_si256(m1,mask_even_32);//keep only 0,2,4,6 and discard 1,3,5,7

		//3rd col iteration

		//multiply by the mask
		m0=_mm256_maddubs_epi16(r0,cx2);

		//shift m0-m6 by one position because hadd follows
		m0=my_rshift16_by_one(m0,mask3);

		//make 16-bit values 32-bit (horizontal add)
		m0=_mm256_madd_epi16(m0,ones);


		//last horizontal add
		m1=_mm256_slli_si256(m0,4);//shift by one integer
		m0=_mm256_add_epi32(m0,m1);//add
		m1=_mm256_srli_si256(m0,4);//shift by one integer
		output3=_mm256_and_si256(m1,mask_even_32);//keep only 0,2,4,6 and discard 1,3,5,7


		//4th col iteration

		//multiply by the mask
		m0=_mm256_maddubs_epi16(r0,cx3);

		//shift m0-m6 by one position because hadd follows
		m0=my_rshift16_by_one(m0,mask3);

		//make 16-bit values 32-bit (horizontal add)
		m0=_mm256_madd_epi16(m0,ones);


		//last horizontal add
		m1=_mm256_slli_si256(m0,4);//shift by one integer
		m0=_mm256_add_epi32(m0,m1);//add
		m1=_mm256_srli_si256(m0,4);//shift by one integer
		output4=_mm256_and_si256(m1,mask_even_32);//keep only 0,2,4,6 and discard 1,3,5,7


		//5th col iteration

		//multiply by the mask
		m0=_mm256_maddubs_epi16(r0,cx4);

		//No need for shift m0-m6 now

		//make 16-bit values 32-bit (horizontal add)
		m0=_mm256_madd_epi16(m0,ones);


		//last horizontal add
		m3=_mm256_srli_si256(m0,4);
		m4=_mm256_and_si256(m0,mask4);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,12);//shift 3 int positions
		m3=_mm256_add_epi32(m3,m4);
		m0=_mm256_add_epi32(m0,m3);
		m0=_mm256_and_si256(m0,mask_odd_32);//keep only 1,3,5,7 and discard 0,2,4,6

		//MERGE output1 with output5 and DIVIDE
		output1=_mm256_add_epi32(m0,output1);
		output1 = division_32(division_case,output1,f);


		//6th col iteration

		//multiply by the mask
		m0=_mm256_maddubs_epi16(r0,cx5);

		//No need for shift m0-m6 now

		//make 16-bit values 32-bit (horizontal add)
		m0=_mm256_madd_epi16(m0,ones);


		//last horizontal add
		m3=_mm256_srli_si256(m0,4);
		m4=_mm256_and_si256(m0,mask4);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,12);//shift 3 int positions
		m3=_mm256_add_epi32(m3,m4);
		m0=_mm256_add_epi32(m0,m3);
		m0=_mm256_and_si256(m0,mask_odd_32);//keep only 1,3,5,7 and discard 0,2,4,6

		//MERGE output1 with output5 and DIVIDE
		output2=_mm256_add_epi32(m0,output2);
		output2 = division_32(division_case,output2,f);


		//7th col iteration

		//multiply by the mask
		m0=_mm256_maddubs_epi16(r0,cx6);


		//shift m0-m6 by one position because hadd follows
		m0=my_rshift16_by_one(m0,mask3);

		//make 16-bit values 32-bit (horizontal add)
		m0=_mm256_madd_epi16(m0,ones);

		//last horizontal add
		m3=_mm256_srli_si256(m0,4);
		m4=_mm256_and_si256(m0,mask4);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,12);//shift 3 int positions
		m3=_mm256_add_epi32(m3,m4);
		m0=_mm256_add_epi32(m0,m3);
		m0=_mm256_and_si256(m0,mask_odd_32);//keep only 1,3,5,7 and discard 0,2,4,6

		//MERGE output1 with output5 and DIVIDE
		output3=_mm256_add_epi32(m0,output3);
		output3 = division_32(division_case,output3,f);

		//8th col iteration

		//multiply by the mask
		m0=_mm256_maddubs_epi16(r0,cx7);

		//shift m0-m6 by one position because hadd follows
		m0=my_rshift16_by_one(m0,mask3);

		//make 16-bit values 32-bit (horizontal add)
		m0=_mm256_madd_epi16(m0,ones);

		//last horizontal add
		m3=_mm256_srli_si256(m0,4);
		m4=_mm256_and_si256(m0,mask4);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,12);//shift 3 int positions
		m3=_mm256_add_epi32(m3,m4);
		m0=_mm256_add_epi32(m0,m3);
		m0=_mm256_and_si256(m0,mask_odd_32);//keep only 1,3,5,7 and discard 0,2,4,6

		//MERGE output1 with output5 and DIVIDE
		output4=_mm256_add_epi32(m0,output4);
		output4 = division_32(division_case,output4,f);


		//blend output1, output2, output3, output4 into one register
		output2 = _mm256_slli_si256(output2,1);
		output1 = _mm256_add_epi8(output1,output2);
		output3 = _mm256_slli_si256(output3,2);
		output1 = _mm256_add_epi8(output1,output3);
		output4 = _mm256_slli_si256(output4,3);
		output1 = _mm256_add_epi8(output1,output4);

	switch (REMINDER_ITERATIONS){
	case 1:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		  break;
	case 2:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		  break;
	case 3:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		  break;
	case 4:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output1,3);
		  break;
	case 5:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output1,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output1,4);
		  break;
	case 6:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output1,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output1,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output1,5);
		  break;
	case 7:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output1,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output1,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output1,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output1,6);
		  break;
	case 8:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output1,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output1,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output1,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output1,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output1,7);
		  break;
	case 9:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output1,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output1,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output1,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output1,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output1,7);
		filt[row][col+8] = (unsigned char) _mm256_extract_epi8(output1,8);
		  break;
	case 10:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output1,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output1,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output1,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output1,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output1,7);
		filt[row][col+8] = (unsigned char) _mm256_extract_epi8(output1,8);
		filt[row][col+9] = (unsigned char) _mm256_extract_epi8(output1,9);
		  break;
	case 11:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output1,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output1,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output1,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output1,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output1,7);
		filt[row][col+8] = (unsigned char) _mm256_extract_epi8(output1,8);
		filt[row][col+9] = (unsigned char) _mm256_extract_epi8(output1,9);
		filt[row][col+10] = (unsigned char) _mm256_extract_epi8(output1,10);
		  break;
	case 12:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output1,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output1,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output1,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output1,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output1,7);
		filt[row][col+8] = (unsigned char) _mm256_extract_epi8(output1,8);
		filt[row][col+9] = (unsigned char) _mm256_extract_epi8(output1,9);
		filt[row][col+10] = (unsigned char) _mm256_extract_epi8(output1,10);
		filt[row][col+11] = (unsigned char) _mm256_extract_epi8(output1,11);
		  break;
	case 13:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output1,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output1,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output1,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output1,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output1,7);
		filt[row][col+8] = (unsigned char) _mm256_extract_epi8(output1,8);
		filt[row][col+9] = (unsigned char) _mm256_extract_epi8(output1,9);
		filt[row][col+10] = (unsigned char) _mm256_extract_epi8(output1,10);
		filt[row][col+11] = (unsigned char) _mm256_extract_epi8(output1,11);
		filt[row][col+12] = (unsigned char) _mm256_extract_epi8(output1,12);
		  break;
	case 14:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output1,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output1,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output1,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output1,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output1,7);
		filt[row][col+8] = (unsigned char) _mm256_extract_epi8(output1,8);
		filt[row][col+9] = (unsigned char) _mm256_extract_epi8(output1,9);
		filt[row][col+10] = (unsigned char) _mm256_extract_epi8(output1,10);
		filt[row][col+11] = (unsigned char) _mm256_extract_epi8(output1,11);
		filt[row][col+12] = (unsigned char) _mm256_extract_epi8(output1,12);
		filt[row][col+13] = (unsigned char) _mm256_extract_epi8(output1,13);
		  break;
	case 15:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output1,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output1,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output1,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output1,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output1,7);
		filt[row][col+8] = (unsigned char) _mm256_extract_epi8(output1,8);
		filt[row][col+9] = (unsigned char) _mm256_extract_epi8(output1,9);
		filt[row][col+10] = (unsigned char) _mm256_extract_epi8(output1,10);
		filt[row][col+11] = (unsigned char) _mm256_extract_epi8(output1,11);
		filt[row][col+12] = (unsigned char) _mm256_extract_epi8(output1,12);
		filt[row][col+13] = (unsigned char) _mm256_extract_epi8(output1,13);
		filt[row][col+14] = (unsigned char) _mm256_extract_epi8(output1,14);
		  break;
	case 16:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
		  break;
	case 17:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
		  break;
	case 18:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
		  break;
	case 19:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
		  break;
	case 20:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output1,19);
		  break;
	case 21:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output1,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output1,20);
		  break;
	case 22:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output1,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output1,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output1,21);
		break;
	case 23:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output1,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output1,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output1,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output1,22);
		  break;
	case 24:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output1,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output1,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output1,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output1,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output1,23);
		  break;
	case 25:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output1,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output1,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output1,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output1,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output1,23);
	filt[row][col+24] = (unsigned char) _mm256_extract_epi8(output1,24);
		  break;
	case 26:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output1,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output1,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output1,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output1,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output1,23);
	filt[row][col+24] = (unsigned char) _mm256_extract_epi8(output1,24);
	filt[row][col+25] = (unsigned char) _mm256_extract_epi8(output1,25);
		  break;

	default: //REMINDER IS EITHER 27,28,29,30 OR 31
	  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
		filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
		filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
		filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
		filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output1,19);
		filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output1,20);
		filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output1,21);
		filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output1,22);
		filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output1,23);
		filt[row][col+24] = (unsigned char) _mm256_extract_epi8(output1,24);
		filt[row][col+25] = (unsigned char) _mm256_extract_epi8(output1,25);

		loop_reminder_7x7_blur(frame1,filt,N,row, col+26, (REMINDER_ITERATIONS%26), division_case, mask_vector_x, mask_vector_y, f, divisor_xy);

 	}


}



void Gaussian_Blur_7x7_32_separable_blocking_old(unsigned char **frame1,unsigned char **filt,const unsigned int M, const unsigned int N, signed char *kernel_y, signed char *kernel_x, const unsigned short int divisor_xy){

const signed char fx0=kernel_x[0];
const signed char fx1=kernel_x[1];
const signed char fx2=kernel_x[2];
const signed char fx3=kernel_x[3];
const signed char fx4=kernel_x[4];
const signed char fx5=kernel_x[5];
const signed char fx6=kernel_x[6];

const signed char fy0=kernel_y[0];
const signed char fy1=kernel_y[1];
const signed char fy2=kernel_y[2];
const signed char fy3=kernel_y[3];
const signed char fy4=kernel_y[4];
const signed char fy5=kernel_y[5];
const signed char fy6=kernel_y[6];



const signed char mask_vector_x[8][32] __attribute__((aligned(64))) ={
	{fx0,fx1,fx2,fx3,fx4,fx5,fx6,0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,0},
	{0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,0,fx0,fx1,fx2,fx3,fx4,fx5,fx6},
	{0,0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,0,0,0,0,0,0,0},
	{0,0,0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,0,0,0,0,0,0},
	{0,0,0,0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,0,0,0,0,0},
	{0,0,0,0,0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,0,0,0,0},
	{0,0,0,0,0,0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,0,0,0},
	{0,0,0,0,0,0,0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,0,0},
};

const signed char mask_vector_y[14][32] __attribute__((aligned(64))) ={
	{fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0},
	{fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0},
	{fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0},
	{fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0},
	{fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0},
	{fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0},
	{fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0},

	{0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0},
	{0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1},
	{0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2},
	{0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3},
	{0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4},
	{0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5},
	{0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6},
};

	const __m256i cx0=_mm256_load_si256( (__m256i *) &mask_vector_x[0]);
	const __m256i cx1=_mm256_load_si256( (__m256i *) &mask_vector_x[1]);
	const __m256i cx2=_mm256_load_si256( (__m256i *) &mask_vector_x[2]);
	const __m256i cx3=_mm256_load_si256( (__m256i *) &mask_vector_x[3]);
	const __m256i cx4=_mm256_load_si256( (__m256i *) &mask_vector_x[4]);
	const __m256i cx5=_mm256_load_si256( (__m256i *) &mask_vector_x[5]);
	const __m256i cx6=_mm256_load_si256( (__m256i *) &mask_vector_x[6]);
	const __m256i cx7=_mm256_load_si256( (__m256i *) &mask_vector_x[7]);


	const __m256i cy0=_mm256_load_si256( (__m256i *) &mask_vector_y[0]);
	const __m256i cy1=_mm256_load_si256( (__m256i *) &mask_vector_y[1]);
	const __m256i cy2=_mm256_load_si256( (__m256i *) &mask_vector_y[2]);
	const __m256i cy3=_mm256_load_si256( (__m256i *) &mask_vector_y[3]);
	const __m256i cy4=_mm256_load_si256( (__m256i *) &mask_vector_y[4]);
	const __m256i cy5=_mm256_load_si256( (__m256i *) &mask_vector_y[5]);
	const __m256i cy6=_mm256_load_si256( (__m256i *) &mask_vector_y[6]);

	const __m256i cy0_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[7]);
	const __m256i cy1_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[8]);
	const __m256i cy2_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[9]);
	const __m256i cy3_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[10]);
	const __m256i cy4_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[11]);
	const __m256i cy5_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[12]);
	const __m256i cy6_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[13]);


	const __m256i mask3  = _mm256_set_epi16(0,0,0,0,0,0,0,65535,0,0,0,0,0,0,0,0);
	const __m256i mask_prelude  = _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0);

	const __m256i mask_even_16  = _mm256_set_epi16(0,0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff);
	const __m256i mask_odd_16  = _mm256_set_epi16(0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff,0);

	const __m256i mask_even_32  = _mm256_set_epi32(0,0xffffffff,0,0xffffffff,0,0xffffffff,0,0xffffffff);
	const __m256i mask_odd_32  = _mm256_set_epi32(0xffffffff,0,0xffffffff,0,0xffffffff,0,0xffffffff,0);
	const __m256i mask4  = _mm256_set_epi32(0,0,0,0xffffffff,0,0,0,0);


	//REMINDER_ITERATIONS=[6,31]
	const unsigned int REMINDER_ITERATIONS =  (M-((((M-32)/26)*26)+26)); //M-(last_col_value+26)

	//printf("\nREMINDER_ITERATIONS=%d",REMINDER_ITERATIONS);


	const unsigned int division_case=prepare_for_division_32(divisor_xy); //determine which is the division case (A, B or C)
	const __m256i f = _mm256_load_si256( (__m256i *) &f_vector[0]);// initialize the division vector
	const __m256i ones=_mm256_set1_epi16(1);

//	const unsigned int division_case_x=prepare_for_division_32(divisor_x); //determine which is the division case (A, B or C)
//	const __m256i f_x = _mm256_load_si256( (__m256i *) &f_vector_x[0]);// initialize the division vector
//printf("\n%d %d",division_case,b);


#pragma omp parallel
{

unsigned int row,col;
__m256i r0,r1,r2,r3,r4,r5,r6,r7,m0,m1,m2,m3,m4,m5,m6,m7,even,odd;
__m256i output1,output2,output3,output4;



/*---------------------- Gaussian Blur ---------------------------------*/

    #pragma omp for schedule(static)
	for (row = 1; row < N-2; row+=2) {

		if (row==1){

				for (col = 0; col <= M-32; col+=26){
				r0=prelude_7x7_Ymask_1(0, col, frame1, mask_vector_y, division_case, f, mask_prelude, mask_even_16, mask_odd_16);
				prelude_7x7_Xmask(filt,0,col, r0, mask_vector_x,  division_case,f, mask_even_32, mask_odd_32);
				}
				loop_reminder_7x7_blur(frame1,filt,N,0, col, REMINDER_ITERATIONS, division_case, mask_vector_x, mask_vector_y, f, divisor_xy);

				for (col = 0; col <= M-32; col+=26){
				r0=prelude_7x7_Ymask_2(0, col, frame1, mask_vector_y, division_case, f, mask_prelude, mask_even_16, mask_odd_16);
				prelude_7x7_Xmask(filt,1,col, r0, mask_vector_x,  division_case,f, mask_even_32, mask_odd_32);
				}
				loop_reminder_7x7_blur(frame1,filt,N,1, col, REMINDER_ITERATIONS, division_case, mask_vector_x, mask_vector_y, f, divisor_xy);

				for (col = 0; col <= M-32; col+=26){
				r0=prelude_7x7_Ymask_3(0, col, frame1, mask_vector_y, division_case, f, mask_prelude, mask_even_16, mask_odd_16);
				prelude_7x7_Xmask(filt,2,col, r0, mask_vector_x,  division_case,f, mask_even_32, mask_odd_32);
				}
				loop_reminder_7x7_blur(frame1,filt,N,2, col, REMINDER_ITERATIONS, division_case, mask_vector_x, mask_vector_y, f, divisor_xy);

		}

		else if (row==N-3){

			for (col = 0; col <= M-32; col+=26){
				r0=prelude_7x7_Ymask_3(N-6, col, frame1, mask_vector_y, division_case, f, mask_prelude, mask_even_16, mask_odd_16);
				prelude_7x7_Xmask(filt,N-3,col, r0, mask_vector_x,  division_case,f, mask_even_32, mask_odd_32);
			}
			loop_reminder_7x7_blur(frame1,filt,N,N-3, col, REMINDER_ITERATIONS, division_case, mask_vector_x, mask_vector_y, f, divisor_xy);

			for (col = 0; col <= M-32; col+=26){
				r0=prelude_7x7_Ymask_2(N-5, col, frame1, mask_vector_y, division_case, f, mask_prelude, mask_even_16, mask_odd_16);
				prelude_7x7_Xmask(filt,N-2,col, r0, mask_vector_x,  division_case,f, mask_even_32, mask_odd_32);
			}
			loop_reminder_7x7_blur(frame1,filt,N,N-2, col, REMINDER_ITERATIONS, division_case, mask_vector_x, mask_vector_y, f, divisor_xy);

			for (col = 0; col <= M-32; col+=26){
			r0=prelude_7x7_Ymask_1(N-4, col, frame1, mask_vector_y, division_case, f, mask_prelude, mask_even_16, mask_odd_16);
			prelude_7x7_Xmask(filt,N-1,col, r0, mask_vector_x,  division_case,f, mask_even_32, mask_odd_32);
			}
		  loop_reminder_7x7_blur(frame1,filt,N,N-1, col, REMINDER_ITERATIONS, division_case, mask_vector_x, mask_vector_y, f, divisor_xy);

		}
		else if (row==N-4){

			  for (col = 0; col <= M-32; col+=26){

				  if (col==0){

						//load the 7 rows
						r0=_mm256_loadu_si256( (__m256i *) &frame1[row-3][0]);
						r1=_mm256_loadu_si256( (__m256i *) &frame1[row-2][0]);
						r2=_mm256_loadu_si256( (__m256i *) &frame1[row-1][0]);
						r3=_mm256_loadu_si256( (__m256i *) &frame1[row][0]);
						r4=_mm256_loadu_si256( (__m256i *) &frame1[row+1][0]);
						r5=_mm256_loadu_si256( (__m256i *) &frame1[row+2][0]);
						r6=_mm256_loadu_si256( (__m256i *) &frame1[row+3][0]);


				  			//START - extra code needed for prelude
				  					m0=_mm256_slli_si256(r0,3);//shift 3 elements left - equivalent to filling with two zeros inthe beginning
				  					m1=_mm256_slli_si256(r1,3);//shift 3 elements left - equivalent to filling with two zeros inthe beginning
				  					m2=_mm256_slli_si256(r2,3);//shift 3 elements left - equivalent to filling with two zeros inthe beginning
				  					m3=_mm256_slli_si256(r3,3);//shift 3 elements left - equivalent to filling with two zeros inthe beginning
				  					m4=_mm256_slli_si256(r4,3);//shift 3 elements left - equivalent to filling with two zeros inthe beginning
				  					m5=_mm256_slli_si256(r5,3);//shift 3 elements left - equivalent to filling with two zeros inthe beginning
				  					m6=_mm256_slli_si256(r6,3);//shift 3 elements left - equivalent to filling with two zeros inthe beginning

				  					r0=_mm256_and_si256(r0,mask_prelude);
				  					r0=_mm256_permute2f128_si256(r0,r0,1);
				  					r0=_mm256_srli_si256(r0,13);//shift 14 elements
				  					r0=_mm256_add_epi16(m0,r0);

				  					r1=_mm256_and_si256(r1,mask_prelude);
				  					r1=_mm256_permute2f128_si256(r1,r1,1);
				  					r1=_mm256_srli_si256(r1,13);//shift 14 elements
				  					r1=_mm256_add_epi16(m1,r1);

				  					r2=_mm256_and_si256(r2,mask_prelude);
				  					r2=_mm256_permute2f128_si256(r2,r2,1);
				  					r2=_mm256_srli_si256(r2,13);//shift 14 elements
				  					r2=_mm256_add_epi16(m2,r2);

				  					r3=_mm256_and_si256(r3,mask_prelude);
				  					r3=_mm256_permute2f128_si256(r3,r3,1);
				  					r3=_mm256_srli_si256(r3,13);//shift 14 elements
				  					r3=_mm256_add_epi16(m3,r3);

				  					r4=_mm256_and_si256(r4,mask_prelude);
				  					r4=_mm256_permute2f128_si256(r4,r4,1);
				  					r4=_mm256_srli_si256(r4,13);//shift 14 elements
				  					r4=_mm256_add_epi16(m4,r4);

				  					r5=_mm256_and_si256(r5,mask_prelude);
				  					r5=_mm256_permute2f128_si256(r5,r5,1);
				  					r5=_mm256_srli_si256(r5,13);//shift 14 elements
				  					r5=_mm256_add_epi16(m5,r5);

				  					r6=_mm256_and_si256(r6,mask_prelude);
				  					r6=_mm256_permute2f128_si256(r6,r6,1);
				  					r6=_mm256_srli_si256(r6,13);//shift 14 elements
				  					r6=_mm256_add_epi16(m6,r6);

				  			//END - extra code needed for prelude
				  		}
				  else {
						//load the 7 rows
						r0=_mm256_loadu_si256( (__m256i *) &frame1[row-3][col-3]);
						r1=_mm256_loadu_si256( (__m256i *) &frame1[row-2][col-3]);
						r2=_mm256_loadu_si256( (__m256i *) &frame1[row-1][col-3]);
						r3=_mm256_loadu_si256( (__m256i *) &frame1[row][col-3]);
						r4=_mm256_loadu_si256( (__m256i *) &frame1[row+1][col-3]);
						r5=_mm256_loadu_si256( (__m256i *) &frame1[row+2][col-3]);
						r6=_mm256_loadu_si256( (__m256i *) &frame1[row+3][col-3]);

				  }

				  //--------------------y filter begins--------------------

					//multiply by the mask
					m0=_mm256_maddubs_epi16(r0,cy0);
					m1=_mm256_maddubs_epi16(r1,cy1);
					m2=_mm256_maddubs_epi16(r2,cy2);
					m3=_mm256_maddubs_epi16(r3,cy3);
					m4=_mm256_maddubs_epi16(r4,cy4);
					m5=_mm256_maddubs_epi16(r5,cy5);
					m6=_mm256_maddubs_epi16(r6,cy6);

					//discard odd
			__m256i m0_even=_mm256_and_si256(m0,mask_even_16);
			__m256i	m1_even=_mm256_and_si256(m1,mask_even_16);
			__m256i	m2_even=_mm256_and_si256(m2,mask_even_16);
			__m256i	m3_even=_mm256_and_si256(m3,mask_even_16);
			__m256i	m4_even=_mm256_and_si256(m4,mask_even_16);
			__m256i	m5_even=_mm256_and_si256(m5,mask_even_16);
			__m256i	m6_even=_mm256_and_si256(m6,mask_even_16);

			//make from 16-bit to 32bit
			  m0_even=_mm256_madd_epi16(m0_even,ones);
			  m1_even=_mm256_madd_epi16(m1_even,ones);
			  m2_even=_mm256_madd_epi16(m2_even,ones);
			  m3_even=_mm256_madd_epi16(m3_even,ones);
			  m4_even=_mm256_madd_epi16(m4_even,ones);
			  m5_even=_mm256_madd_epi16(m5_even,ones);
			  m6_even=_mm256_madd_epi16(m6_even,ones);

					//vertical 32-bit add
					m0_even=_mm256_add_epi32(m0_even,m1_even);
					m0_even=_mm256_add_epi32(m0_even,m2_even);
					m0_even=_mm256_add_epi32(m0_even,m3_even);
					m0_even=_mm256_add_epi32(m0_even,m4_even);
					m0_even=_mm256_add_epi32(m0_even,m5_even);
					m0_even=_mm256_add_epi32(m0_even,m6_even);

					//discard even
			__m256i	m0_odd=_mm256_and_si256(m0,mask_odd_16);
			__m256i	m1_odd=_mm256_and_si256(m1,mask_odd_16);
			__m256i	m2_odd=_mm256_and_si256(m2,mask_odd_16);
			__m256i	m3_odd=_mm256_and_si256(m3,mask_odd_16);
			__m256i	m4_odd=_mm256_and_si256(m4,mask_odd_16);
			__m256i	m5_odd=_mm256_and_si256(m5,mask_odd_16);
			__m256i	m6_odd=_mm256_and_si256(m6,mask_odd_16);

			//make from 16-bit to 32bit
					m0_odd=_mm256_madd_epi16(m0_odd,ones);
					m1_odd=_mm256_madd_epi16(m1_odd,ones);
					m2_odd=_mm256_madd_epi16(m2_odd,ones);
					m3_odd=_mm256_madd_epi16(m3_odd,ones);
					m4_odd=_mm256_madd_epi16(m4_odd,ones);
					m5_odd=_mm256_madd_epi16(m5_odd,ones);
					m6_odd=_mm256_madd_epi16(m6_odd,ones);

					//vertical 32-bit add
					m0_odd=_mm256_add_epi32(m0_odd,m1_odd);
					m0_odd=_mm256_add_epi32(m0_odd,m2_odd);
					m0_odd=_mm256_add_epi32(m0_odd,m3_odd);
					m0_odd=_mm256_add_epi32(m0_odd,m4_odd);
					m0_odd=_mm256_add_epi32(m0_odd,m5_odd);
					m0_odd=_mm256_add_epi32(m0_odd,m6_odd);

					m0_even=division_32(division_case,m0_even,f);//even results
					m0_odd=division_32(division_case,m0_odd,f);//even results

					m0_odd=_mm256_slli_si256(m0_odd,2); //shift two position right
					even=_mm256_add_epi8(m0_even,m0_odd); //add the odd with the even

					//-----------------multiply by the second mask----------
					m0=_mm256_maddubs_epi16(r0,cy0_sh1);
					m1=_mm256_maddubs_epi16(r1,cy1_sh1);
					m2=_mm256_maddubs_epi16(r2,cy2_sh1);
					m3=_mm256_maddubs_epi16(r3,cy3_sh1);
					m4=_mm256_maddubs_epi16(r4,cy4_sh1);
					m5=_mm256_maddubs_epi16(r5,cy5_sh1);
					m6=_mm256_maddubs_epi16(r6,cy6_sh1);


					m0_even=_mm256_and_si256(m0,mask_even_16);
					m1_even=_mm256_and_si256(m1,mask_even_16);
					m2_even=_mm256_and_si256(m2,mask_even_16);
					m3_even=_mm256_and_si256(m3,mask_even_16);
					m4_even=_mm256_and_si256(m4,mask_even_16);
					m5_even=_mm256_and_si256(m5,mask_even_16);
					m6_even=_mm256_and_si256(m6,mask_even_16);

					//make from 16-bit to 32bit
					  m0_even=_mm256_madd_epi16(m0_even,ones);
					  m1_even=_mm256_madd_epi16(m1_even,ones);
					  m2_even=_mm256_madd_epi16(m2_even,ones);
					  m3_even=_mm256_madd_epi16(m3_even,ones);
					  m4_even=_mm256_madd_epi16(m4_even,ones);
					  m5_even=_mm256_madd_epi16(m5_even,ones);
					  m6_even=_mm256_madd_epi16(m6_even,ones);

					//vertical 32-bit add
					m0_even=_mm256_add_epi32(m0_even,m1_even);
					m0_even=_mm256_add_epi32(m0_even,m2_even);
					m0_even=_mm256_add_epi32(m0_even,m3_even);
					m0_even=_mm256_add_epi32(m0_even,m4_even);
					m0_even=_mm256_add_epi32(m0_even,m5_even);
					m0_even=_mm256_add_epi32(m0_even,m6_even);

					 m0_odd=_mm256_and_si256(m0,mask_odd_16);
					 m1_odd=_mm256_and_si256(m1,mask_odd_16);
					 m2_odd=_mm256_and_si256(m2,mask_odd_16);
					 m3_odd=_mm256_and_si256(m3,mask_odd_16);
					 m4_odd=_mm256_and_si256(m4,mask_odd_16);
					 m5_odd=_mm256_and_si256(m5,mask_odd_16);
					 m6_odd=_mm256_and_si256(m6,mask_odd_16);

					 //make from 16-bit to 32bit
						m0_odd=_mm256_madd_epi16(m0_odd,ones);
						m1_odd=_mm256_madd_epi16(m1_odd,ones);
						m2_odd=_mm256_madd_epi16(m2_odd,ones);
						m3_odd=_mm256_madd_epi16(m3_odd,ones);
						m4_odd=_mm256_madd_epi16(m4_odd,ones);
						m5_odd=_mm256_madd_epi16(m5_odd,ones);
						m6_odd=_mm256_madd_epi16(m6_odd,ones);


					//vertical 32-bit add
					m0_odd=_mm256_add_epi32(m0_odd,m1_odd);
					m0_odd=_mm256_add_epi32(m0_odd,m2_odd);
					m0_odd=_mm256_add_epi32(m0_odd,m3_odd);
					m0_odd=_mm256_add_epi32(m0_odd,m4_odd);
					m0_odd=_mm256_add_epi32(m0_odd,m5_odd);
					m0_odd=_mm256_add_epi32(m0_odd,m6_odd);

					m0_even=division_32(division_case,m0_even,f);//even results
					m0_odd=division_32(division_case,m0_odd,f);//even results

					m0_odd=_mm256_slli_si256(m0_odd,2); //shift two position right
					odd=_mm256_add_epi8(m0_even,m0_odd); //add the odd with the even


					//MERGE AND pack to one register
					odd=_mm256_slli_si256(odd,1); //shift one position right
					r0=_mm256_add_epi8(even,odd); //add the odd with the even

					//y filter ends - r0 has now the data to be processed by x filter

				// col iteration computes output pixels of   0,8,16,24
				// col+1 iteration computes output pixels of 1,9,17,25
				// col+2 iteration computes output pixels of 2,10,18
				// col+3 iteration computes output pixels of 3,11,19
				// col+4 iteration computes output pixels of 4,12,20
				// col+5 iteration computes output pixels of 5,13,21
				// col+6 iteration computes output pixels of 6,14,22
				// col+7 iteration computes output pixels of 7,15,23
				//afterwards, col becomes 26 and repeat the above process

					//1st col iteration

					//multiply by the mask
					m0=_mm256_maddubs_epi16(r0,cx0);

					//make 16-bit values 32-bit (horizontal add)
					m0=_mm256_madd_epi16(m0,ones);

					//last horizontal add
					m1=_mm256_slli_si256(m0,4);//shift by one integer
					m0=_mm256_add_epi32(m0,m1);//add
					m1=_mm256_srli_si256(m0,4);//shift by one integer
					output1=_mm256_and_si256(m1,mask_even_32);//keep only 0,2,4,6 and discard 1,3,5,7


					//2nd col iteration

					//multiply by the mask
					m0=_mm256_maddubs_epi16(r0,cx1);

					//make 16-bit values 32-bit (horizontal add)
					m0=_mm256_madd_epi16(m0,ones);

					//last horizontal add
					m1=_mm256_slli_si256(m0,4);//shift by one integer
					m0=_mm256_add_epi32(m0,m1);//add
					m1=_mm256_srli_si256(m0,4);//shift by one integer
					output2=_mm256_and_si256(m1,mask_even_32);//keep only 0,2,4,6 and discard 1,3,5,7

					//3rd col iteration

					//multiply by the mask
					m0=_mm256_maddubs_epi16(r0,cx2);

					//shift m0-m6 by one position because hadd follows
					m0=my_rshift16_by_one(m0,mask3);

					//make 16-bit values 32-bit (horizontal add)
					m0=_mm256_madd_epi16(m0,ones);


					//last horizontal add
					m1=_mm256_slli_si256(m0,4);//shift by one integer
					m0=_mm256_add_epi32(m0,m1);//add
					m1=_mm256_srli_si256(m0,4);//shift by one integer
					output3=_mm256_and_si256(m1,mask_even_32);//keep only 0,2,4,6 and discard 1,3,5,7


					//4th col iteration

					//multiply by the mask
					m0=_mm256_maddubs_epi16(r0,cx3);

					//shift m0-m6 by one position because hadd follows
					m0=my_rshift16_by_one(m0,mask3);

					//make 16-bit values 32-bit (horizontal add)
					m0=_mm256_madd_epi16(m0,ones);


					//last horizontal add
					m1=_mm256_slli_si256(m0,4);//shift by one integer
					m0=_mm256_add_epi32(m0,m1);//add
					m1=_mm256_srli_si256(m0,4);//shift by one integer
					output4=_mm256_and_si256(m1,mask_even_32);//keep only 0,2,4,6 and discard 1,3,5,7


					//5th col iteration

					//multiply by the mask
					m0=_mm256_maddubs_epi16(r0,cx4);

					//No need for shift m0-m6 now

					//make 16-bit values 32-bit (horizontal add)
					m0=_mm256_madd_epi16(m0,ones);


					//last horizontal add
					m3=_mm256_srli_si256(m0,4);
					m4=_mm256_and_si256(m0,mask4);
					m4=_mm256_permute2f128_si256(m4,m4,1);
					m4=_mm256_slli_si256(m4,12);//shift 3 int positions
					m3=_mm256_add_epi32(m3,m4);
					m0=_mm256_add_epi32(m0,m3);
					m0=_mm256_and_si256(m0,mask_odd_32);//keep only 1,3,5,7 and discard 0,2,4,6

					//MERGE output1 with output5 and DIVIDE
					output1=_mm256_add_epi32(m0,output1);
					output1 = division_32(division_case,output1,f);


					//6th col iteration

					//multiply by the mask
					m0=_mm256_maddubs_epi16(r0,cx5);

					//No need for shift m0-m6 now

					//make 16-bit values 32-bit (horizontal add)
					m0=_mm256_madd_epi16(m0,ones);


					//last horizontal add
					m3=_mm256_srli_si256(m0,4);
					m4=_mm256_and_si256(m0,mask4);
					m4=_mm256_permute2f128_si256(m4,m4,1);
					m4=_mm256_slli_si256(m4,12);//shift 3 int positions
					m3=_mm256_add_epi32(m3,m4);
					m0=_mm256_add_epi32(m0,m3);
					m0=_mm256_and_si256(m0,mask_odd_32);//keep only 1,3,5,7 and discard 0,2,4,6

					//MERGE output1 with output5 and DIVIDE
					output2=_mm256_add_epi32(m0,output2);
					output2 = division_32(division_case,output2,f);


					//7th col iteration

					//multiply by the mask
					m0=_mm256_maddubs_epi16(r0,cx6);


					//shift m0-m6 by one position because hadd follows
					m0=my_rshift16_by_one(m0,mask3);

					//make 16-bit values 32-bit (horizontal add)
					m0=_mm256_madd_epi16(m0,ones);

					//last horizontal add
					m3=_mm256_srli_si256(m0,4);
					m4=_mm256_and_si256(m0,mask4);
					m4=_mm256_permute2f128_si256(m4,m4,1);
					m4=_mm256_slli_si256(m4,12);//shift 3 int positions
					m3=_mm256_add_epi32(m3,m4);
					m0=_mm256_add_epi32(m0,m3);
					m0=_mm256_and_si256(m0,mask_odd_32);//keep only 1,3,5,7 and discard 0,2,4,6

					//MERGE output1 with output5 and DIVIDE
					output3=_mm256_add_epi32(m0,output3);
					output3 = division_32(division_case,output3,f);

					//8th col iteration

					//multiply by the mask
					m0=_mm256_maddubs_epi16(r0,cx7);

					//shift m0-m6 by one position because hadd follows
					m0=my_rshift16_by_one(m0,mask3);

					//make 16-bit values 32-bit (horizontal add)
					m0=_mm256_madd_epi16(m0,ones);

					//last horizontal add
					m3=_mm256_srli_si256(m0,4);
					m4=_mm256_and_si256(m0,mask4);
					m4=_mm256_permute2f128_si256(m4,m4,1);
					m4=_mm256_slli_si256(m4,12);//shift 3 int positions
					m3=_mm256_add_epi32(m3,m4);
					m0=_mm256_add_epi32(m0,m3);
					m0=_mm256_and_si256(m0,mask_odd_32);//keep only 1,3,5,7 and discard 0,2,4,6

					//MERGE output1 with output5 and DIVIDE
					output4=_mm256_add_epi32(m0,output4);
					output4 = division_32(division_case,output4,f);



					//blend output1, output2, output3, output4 into one register
					output2 = _mm256_slli_si256(output2,1);
					output1 = _mm256_add_epi8(output1,output2);
					output3 = _mm256_slli_si256(output3,2);
					output1 = _mm256_add_epi8(output1,output3);
					output4 = _mm256_slli_si256(output4,3);
					output1 = _mm256_add_epi8(output1,output4);

					_mm256_storeu_si256( (__m256i *) &filt[row][col],output1 );
				}
			  loop_reminder_7x7_blur(frame1,filt,N,row, col, REMINDER_ITERATIONS, division_case, mask_vector_x, mask_vector_y, f, divisor_xy);

			for (col = 0; col <= M-32; col+=26){
				r0=prelude_7x7_Ymask_3(N-6, col, frame1, mask_vector_y, division_case, f, mask_prelude, mask_even_16, mask_odd_16);
				prelude_7x7_Xmask(filt,N-3,col, r0, mask_vector_x,  division_case,f, mask_even_32, mask_odd_32);
			}
			loop_reminder_7x7_blur(frame1,filt,N,N-3, col, REMINDER_ITERATIONS, division_case, mask_vector_x, mask_vector_y, f, divisor_xy);

			for (col = 0; col <= M-32; col+=26){
				r0=prelude_7x7_Ymask_2(N-5, col, frame1, mask_vector_y, division_case, f, mask_prelude, mask_even_16, mask_odd_16);
				prelude_7x7_Xmask(filt,N-2,col, r0, mask_vector_x,  division_case,f, mask_even_32, mask_odd_32);
			}
			loop_reminder_7x7_blur(frame1,filt,N,N-2, col, REMINDER_ITERATIONS, division_case, mask_vector_x, mask_vector_y, f, divisor_xy);

			for (col = 0; col <= M-32; col+=26){
			r0=prelude_7x7_Ymask_1(N-4, col, frame1, mask_vector_y, division_case, f, mask_prelude, mask_even_16, mask_odd_16);
			prelude_7x7_Xmask(filt,N-1,col, r0, mask_vector_x,  division_case,f, mask_even_32, mask_odd_32);
			}
		  loop_reminder_7x7_blur(frame1,filt,N,N-1, col, REMINDER_ITERATIONS, division_case, mask_vector_x, mask_vector_y, f, divisor_xy);

		}

	  else { //main loop


	  for (col = 0; col <= M-32; col+=26){

		  if (col==0){

				//load the 7 rows
				r0=_mm256_loadu_si256( (__m256i *) &frame1[row-3][0]);
				r1=_mm256_loadu_si256( (__m256i *) &frame1[row-2][0]);
				r2=_mm256_loadu_si256( (__m256i *) &frame1[row-1][0]);
				r3=_mm256_loadu_si256( (__m256i *) &frame1[row][0]);
				r4=_mm256_loadu_si256( (__m256i *) &frame1[row+1][0]);
				r5=_mm256_loadu_si256( (__m256i *) &frame1[row+2][0]);
				r6=_mm256_loadu_si256( (__m256i *) &frame1[row+3][0]);
				r7=_mm256_loadu_si256( (__m256i *) &frame1[row+4][0]);


		  			//START - extra code needed for prelude
		  					m0=_mm256_slli_si256(r0,3);//shift 3 elements left - equivalent to filling with two zeros inthe beginning
		  					m1=_mm256_slli_si256(r1,3);//shift 3 elements left - equivalent to filling with two zeros inthe beginning
		  					m2=_mm256_slli_si256(r2,3);//shift 3 elements left - equivalent to filling with two zeros inthe beginning
		  					m3=_mm256_slli_si256(r3,3);//shift 3 elements left - equivalent to filling with two zeros inthe beginning
		  					m4=_mm256_slli_si256(r4,3);//shift 3 elements left - equivalent to filling with two zeros inthe beginning
		  					m5=_mm256_slli_si256(r5,3);//shift 3 elements left - equivalent to filling with two zeros inthe beginning
		  					m6=_mm256_slli_si256(r6,3);//shift 3 elements left - equivalent to filling with two zeros inthe beginning
		  					m7=_mm256_slli_si256(r7,3);//shift 3 elements left - equivalent to filling with two zeros inthe beginning

		  					r0=_mm256_and_si256(r0,mask_prelude);
		  					r0=_mm256_permute2f128_si256(r0,r0,1);
		  					r0=_mm256_srli_si256(r0,13);//shift 14 elements
		  					r0=_mm256_add_epi16(m0,r0);

		  					r1=_mm256_and_si256(r1,mask_prelude);
		  					r1=_mm256_permute2f128_si256(r1,r1,1);
		  					r1=_mm256_srli_si256(r1,13);//shift 14 elements
		  					r1=_mm256_add_epi16(m1,r1);

		  					r2=_mm256_and_si256(r2,mask_prelude);
		  					r2=_mm256_permute2f128_si256(r2,r2,1);
		  					r2=_mm256_srli_si256(r2,13);//shift 14 elements
		  					r2=_mm256_add_epi16(m2,r2);

		  					r3=_mm256_and_si256(r3,mask_prelude);
		  					r3=_mm256_permute2f128_si256(r3,r3,1);
		  					r3=_mm256_srli_si256(r3,13);//shift 14 elements
		  					r3=_mm256_add_epi16(m3,r3);

		  					r4=_mm256_and_si256(r4,mask_prelude);
		  					r4=_mm256_permute2f128_si256(r4,r4,1);
		  					r4=_mm256_srli_si256(r4,13);//shift 14 elements
		  					r4=_mm256_add_epi16(m4,r4);

		  					r5=_mm256_and_si256(r5,mask_prelude);
		  					r5=_mm256_permute2f128_si256(r5,r5,1);
		  					r5=_mm256_srli_si256(r5,13);//shift 14 elements
		  					r5=_mm256_add_epi16(m5,r5);

		  					r6=_mm256_and_si256(r6,mask_prelude);
		  					r6=_mm256_permute2f128_si256(r6,r6,1);
		  					r6=_mm256_srli_si256(r6,13);//shift 14 elements
		  					r6=_mm256_add_epi16(m6,r6);

		  					r7=_mm256_and_si256(r7,mask_prelude);
		  					r7=_mm256_permute2f128_si256(r7,r7,1);
		  					r7=_mm256_srli_si256(r7,13);//shift 14 elements
		  					r7=_mm256_add_epi16(m7,r7);

		  			//END - extra code needed for prelude
		  		}
		  else {
				//load the 8 rows
				r0=_mm256_loadu_si256( (__m256i *) &frame1[row-3][col-3]);
				r1=_mm256_loadu_si256( (__m256i *) &frame1[row-2][col-3]);
				r2=_mm256_loadu_si256( (__m256i *) &frame1[row-1][col-3]);
				r3=_mm256_loadu_si256( (__m256i *) &frame1[row][col-3]);
				r4=_mm256_loadu_si256( (__m256i *) &frame1[row+1][col-3]);
				r5=_mm256_loadu_si256( (__m256i *) &frame1[row+2][col-3]);
				r6=_mm256_loadu_si256( (__m256i *) &frame1[row+3][col-3]);
				r7=_mm256_loadu_si256( (__m256i *) &frame1[row+4][col-3]);

		  }

		  //--------------------y filter for row begins--------------------

			//multiply by the mask
			m0=_mm256_maddubs_epi16(r0,cy0);
			m1=_mm256_maddubs_epi16(r1,cy1);
			m2=_mm256_maddubs_epi16(r2,cy2);
			m3=_mm256_maddubs_epi16(r3,cy3);
			m4=_mm256_maddubs_epi16(r4,cy4);
			m5=_mm256_maddubs_epi16(r5,cy5);
			m6=_mm256_maddubs_epi16(r6,cy6);

			//discard odd
	__m256i m0_even=_mm256_and_si256(m0,mask_even_16);
	__m256i	m1_even=_mm256_and_si256(m1,mask_even_16);
	__m256i	m2_even=_mm256_and_si256(m2,mask_even_16);
	__m256i	m3_even=_mm256_and_si256(m3,mask_even_16);
	__m256i	m4_even=_mm256_and_si256(m4,mask_even_16);
	__m256i	m5_even=_mm256_and_si256(m5,mask_even_16);
	__m256i	m6_even=_mm256_and_si256(m6,mask_even_16);

	//make from 16-bit to 32bit
	  m0_even=_mm256_madd_epi16(m0_even,ones);
	  m1_even=_mm256_madd_epi16(m1_even,ones);
	  m2_even=_mm256_madd_epi16(m2_even,ones);
	  m3_even=_mm256_madd_epi16(m3_even,ones);
	  m4_even=_mm256_madd_epi16(m4_even,ones);
	  m5_even=_mm256_madd_epi16(m5_even,ones);
	  m6_even=_mm256_madd_epi16(m6_even,ones);

			//vertical 32-bit add
			m0_even=_mm256_add_epi32(m0_even,m1_even);
			m0_even=_mm256_add_epi32(m0_even,m2_even);
			m0_even=_mm256_add_epi32(m0_even,m3_even);
			m0_even=_mm256_add_epi32(m0_even,m4_even);
			m0_even=_mm256_add_epi32(m0_even,m5_even);
			m0_even=_mm256_add_epi32(m0_even,m6_even);

			//discard even
	__m256i	m0_odd=_mm256_and_si256(m0,mask_odd_16);
	__m256i	m1_odd=_mm256_and_si256(m1,mask_odd_16);
	__m256i	m2_odd=_mm256_and_si256(m2,mask_odd_16);
	__m256i	m3_odd=_mm256_and_si256(m3,mask_odd_16);
	__m256i	m4_odd=_mm256_and_si256(m4,mask_odd_16);
	__m256i	m5_odd=_mm256_and_si256(m5,mask_odd_16);
	__m256i	m6_odd=_mm256_and_si256(m6,mask_odd_16);

	//make from 16-bit to 32bit
			m0_odd=_mm256_madd_epi16(m0_odd,ones);
			m1_odd=_mm256_madd_epi16(m1_odd,ones);
			m2_odd=_mm256_madd_epi16(m2_odd,ones);
			m3_odd=_mm256_madd_epi16(m3_odd,ones);
			m4_odd=_mm256_madd_epi16(m4_odd,ones);
			m5_odd=_mm256_madd_epi16(m5_odd,ones);
			m6_odd=_mm256_madd_epi16(m6_odd,ones);

			//vertical 32-bit add
			m0_odd=_mm256_add_epi32(m0_odd,m1_odd);
			m0_odd=_mm256_add_epi32(m0_odd,m2_odd);
			m0_odd=_mm256_add_epi32(m0_odd,m3_odd);
			m0_odd=_mm256_add_epi32(m0_odd,m4_odd);
			m0_odd=_mm256_add_epi32(m0_odd,m5_odd);
			m0_odd=_mm256_add_epi32(m0_odd,m6_odd);

			m0_even=division_32(division_case,m0_even,f);//even results
			m0_odd=division_32(division_case,m0_odd,f);//even results

			m0_odd=_mm256_slli_si256(m0_odd,2); //shift two position right
			even=_mm256_add_epi8(m0_even,m0_odd); //add the odd with the even

			//-----------------multiply by the second mask----------
			m0=_mm256_maddubs_epi16(r0,cy0_sh1);
			m1=_mm256_maddubs_epi16(r1,cy1_sh1);
			m2=_mm256_maddubs_epi16(r2,cy2_sh1);
			m3=_mm256_maddubs_epi16(r3,cy3_sh1);
			m4=_mm256_maddubs_epi16(r4,cy4_sh1);
			m5=_mm256_maddubs_epi16(r5,cy5_sh1);
			m6=_mm256_maddubs_epi16(r6,cy6_sh1);


			m0_even=_mm256_and_si256(m0,mask_even_16);
			m1_even=_mm256_and_si256(m1,mask_even_16);
			m2_even=_mm256_and_si256(m2,mask_even_16);
			m3_even=_mm256_and_si256(m3,mask_even_16);
			m4_even=_mm256_and_si256(m4,mask_even_16);
			m5_even=_mm256_and_si256(m5,mask_even_16);
			m6_even=_mm256_and_si256(m6,mask_even_16);

			//make from 16-bit to 32bit
			  m0_even=_mm256_madd_epi16(m0_even,ones);
			  m1_even=_mm256_madd_epi16(m1_even,ones);
			  m2_even=_mm256_madd_epi16(m2_even,ones);
			  m3_even=_mm256_madd_epi16(m3_even,ones);
			  m4_even=_mm256_madd_epi16(m4_even,ones);
			  m5_even=_mm256_madd_epi16(m5_even,ones);
			  m6_even=_mm256_madd_epi16(m6_even,ones);

			//vertical 32-bit add
			m0_even=_mm256_add_epi32(m0_even,m1_even);
			m0_even=_mm256_add_epi32(m0_even,m2_even);
			m0_even=_mm256_add_epi32(m0_even,m3_even);
			m0_even=_mm256_add_epi32(m0_even,m4_even);
			m0_even=_mm256_add_epi32(m0_even,m5_even);
			m0_even=_mm256_add_epi32(m0_even,m6_even);

			 m0_odd=_mm256_and_si256(m0,mask_odd_16);
			 m1_odd=_mm256_and_si256(m1,mask_odd_16);
			 m2_odd=_mm256_and_si256(m2,mask_odd_16);
			 m3_odd=_mm256_and_si256(m3,mask_odd_16);
			 m4_odd=_mm256_and_si256(m4,mask_odd_16);
			 m5_odd=_mm256_and_si256(m5,mask_odd_16);
			 m6_odd=_mm256_and_si256(m6,mask_odd_16);

			 //make from 16-bit to 32bit
				m0_odd=_mm256_madd_epi16(m0_odd,ones);
				m1_odd=_mm256_madd_epi16(m1_odd,ones);
				m2_odd=_mm256_madd_epi16(m2_odd,ones);
				m3_odd=_mm256_madd_epi16(m3_odd,ones);
				m4_odd=_mm256_madd_epi16(m4_odd,ones);
				m5_odd=_mm256_madd_epi16(m5_odd,ones);
				m6_odd=_mm256_madd_epi16(m6_odd,ones);


			//vertical 32-bit add
			m0_odd=_mm256_add_epi32(m0_odd,m1_odd);
			m0_odd=_mm256_add_epi32(m0_odd,m2_odd);
			m0_odd=_mm256_add_epi32(m0_odd,m3_odd);
			m0_odd=_mm256_add_epi32(m0_odd,m4_odd);
			m0_odd=_mm256_add_epi32(m0_odd,m5_odd);
			m0_odd=_mm256_add_epi32(m0_odd,m6_odd);

			m0_even=division_32(division_case,m0_even,f);//even results
			m0_odd=division_32(division_case,m0_odd,f);//even results

			m0_odd=_mm256_slli_si256(m0_odd,2); //shift two position right
			odd=_mm256_add_epi8(m0_even,m0_odd); //add the odd with the even


			//MERGE AND pack to one register
			odd=_mm256_slli_si256(odd,1); //shift one position right
			r0=_mm256_add_epi8(even,odd); //add the odd with the even

			//y filter ends - r0 has now the data to be processed by x filter

			  //--------------------y filter for row+1 begins--------------------

				//multiply by the mask
				m0=_mm256_maddubs_epi16(r1,cy0);
				m1=_mm256_maddubs_epi16(r2,cy1);
				m2=_mm256_maddubs_epi16(r3,cy2);
				m3=_mm256_maddubs_epi16(r4,cy3);
				m4=_mm256_maddubs_epi16(r5,cy4);
				m5=_mm256_maddubs_epi16(r6,cy5);
				m6=_mm256_maddubs_epi16(r7,cy6);

				//discard odd
		    m0_even=_mm256_and_si256(m0,mask_even_16);
		 	m1_even=_mm256_and_si256(m1,mask_even_16);
		 	m2_even=_mm256_and_si256(m2,mask_even_16);
		 	m3_even=_mm256_and_si256(m3,mask_even_16);
		 	m4_even=_mm256_and_si256(m4,mask_even_16);
		  	m5_even=_mm256_and_si256(m5,mask_even_16);
		 	m6_even=_mm256_and_si256(m6,mask_even_16);

		//make from 16-bit to 32bit
		  m0_even=_mm256_madd_epi16(m0_even,ones);
		  m1_even=_mm256_madd_epi16(m1_even,ones);
		  m2_even=_mm256_madd_epi16(m2_even,ones);
		  m3_even=_mm256_madd_epi16(m3_even,ones);
		  m4_even=_mm256_madd_epi16(m4_even,ones);
		  m5_even=_mm256_madd_epi16(m5_even,ones);
		  m6_even=_mm256_madd_epi16(m6_even,ones);

				//vertical 32-bit add
				m0_even=_mm256_add_epi32(m0_even,m1_even);
				m0_even=_mm256_add_epi32(m0_even,m2_even);
				m0_even=_mm256_add_epi32(m0_even,m3_even);
				m0_even=_mm256_add_epi32(m0_even,m4_even);
				m0_even=_mm256_add_epi32(m0_even,m5_even);
				m0_even=_mm256_add_epi32(m0_even,m6_even);

				//discard even
		    m0_odd=_mm256_and_si256(m0,mask_odd_16);
		 	m1_odd=_mm256_and_si256(m1,mask_odd_16);
		 	m2_odd=_mm256_and_si256(m2,mask_odd_16);
		 	m3_odd=_mm256_and_si256(m3,mask_odd_16);
		 	m4_odd=_mm256_and_si256(m4,mask_odd_16);
		 	m5_odd=_mm256_and_si256(m5,mask_odd_16);
		 	m6_odd=_mm256_and_si256(m6,mask_odd_16);

		//make from 16-bit to 32bit
				m0_odd=_mm256_madd_epi16(m0_odd,ones);
				m1_odd=_mm256_madd_epi16(m1_odd,ones);
				m2_odd=_mm256_madd_epi16(m2_odd,ones);
				m3_odd=_mm256_madd_epi16(m3_odd,ones);
				m4_odd=_mm256_madd_epi16(m4_odd,ones);
				m5_odd=_mm256_madd_epi16(m5_odd,ones);
				m6_odd=_mm256_madd_epi16(m6_odd,ones);

				//vertical 32-bit add
				m0_odd=_mm256_add_epi32(m0_odd,m1_odd);
				m0_odd=_mm256_add_epi32(m0_odd,m2_odd);
				m0_odd=_mm256_add_epi32(m0_odd,m3_odd);
				m0_odd=_mm256_add_epi32(m0_odd,m4_odd);
				m0_odd=_mm256_add_epi32(m0_odd,m5_odd);
				m0_odd=_mm256_add_epi32(m0_odd,m6_odd);

				m0_even=division_32(division_case,m0_even,f);//even results
				m0_odd=division_32(division_case,m0_odd,f);//even results

				m0_odd=_mm256_slli_si256(m0_odd,2); //shift two position right
				even=_mm256_add_epi8(m0_even,m0_odd); //add the odd with the even

				//-----------------multiply by the second mask----------
				m0=_mm256_maddubs_epi16(r1,cy0_sh1);
				m1=_mm256_maddubs_epi16(r2,cy1_sh1);
				m2=_mm256_maddubs_epi16(r3,cy2_sh1);
				m3=_mm256_maddubs_epi16(r4,cy3_sh1);
				m4=_mm256_maddubs_epi16(r5,cy4_sh1);
				m5=_mm256_maddubs_epi16(r6,cy5_sh1);
				m6=_mm256_maddubs_epi16(r7,cy6_sh1);


				m0_even=_mm256_and_si256(m0,mask_even_16);
				m1_even=_mm256_and_si256(m1,mask_even_16);
				m2_even=_mm256_and_si256(m2,mask_even_16);
				m3_even=_mm256_and_si256(m3,mask_even_16);
				m4_even=_mm256_and_si256(m4,mask_even_16);
				m5_even=_mm256_and_si256(m5,mask_even_16);
				m6_even=_mm256_and_si256(m6,mask_even_16);

				//make from 16-bit to 32bit
				  m0_even=_mm256_madd_epi16(m0_even,ones);
				  m1_even=_mm256_madd_epi16(m1_even,ones);
				  m2_even=_mm256_madd_epi16(m2_even,ones);
				  m3_even=_mm256_madd_epi16(m3_even,ones);
				  m4_even=_mm256_madd_epi16(m4_even,ones);
				  m5_even=_mm256_madd_epi16(m5_even,ones);
				  m6_even=_mm256_madd_epi16(m6_even,ones);

				//vertical 32-bit add
				m0_even=_mm256_add_epi32(m0_even,m1_even);
				m0_even=_mm256_add_epi32(m0_even,m2_even);
				m0_even=_mm256_add_epi32(m0_even,m3_even);
				m0_even=_mm256_add_epi32(m0_even,m4_even);
				m0_even=_mm256_add_epi32(m0_even,m5_even);
				m0_even=_mm256_add_epi32(m0_even,m6_even);

				 m0_odd=_mm256_and_si256(m0,mask_odd_16);
				 m1_odd=_mm256_and_si256(m1,mask_odd_16);
				 m2_odd=_mm256_and_si256(m2,mask_odd_16);
				 m3_odd=_mm256_and_si256(m3,mask_odd_16);
				 m4_odd=_mm256_and_si256(m4,mask_odd_16);
				 m5_odd=_mm256_and_si256(m5,mask_odd_16);
				 m6_odd=_mm256_and_si256(m6,mask_odd_16);

				 //make from 16-bit to 32bit
					m0_odd=_mm256_madd_epi16(m0_odd,ones);
					m1_odd=_mm256_madd_epi16(m1_odd,ones);
					m2_odd=_mm256_madd_epi16(m2_odd,ones);
					m3_odd=_mm256_madd_epi16(m3_odd,ones);
					m4_odd=_mm256_madd_epi16(m4_odd,ones);
					m5_odd=_mm256_madd_epi16(m5_odd,ones);
					m6_odd=_mm256_madd_epi16(m6_odd,ones);


				//vertical 32-bit add
				m0_odd=_mm256_add_epi32(m0_odd,m1_odd);
				m0_odd=_mm256_add_epi32(m0_odd,m2_odd);
				m0_odd=_mm256_add_epi32(m0_odd,m3_odd);
				m0_odd=_mm256_add_epi32(m0_odd,m4_odd);
				m0_odd=_mm256_add_epi32(m0_odd,m5_odd);
				m0_odd=_mm256_add_epi32(m0_odd,m6_odd);

				m0_even=division_32(division_case,m0_even,f);//even results
				m0_odd=division_32(division_case,m0_odd,f);//even results

				m0_odd=_mm256_slli_si256(m0_odd,2); //shift two position right
				odd=_mm256_add_epi8(m0_even,m0_odd); //add the odd with the even


				//MERGE AND pack to one register
				odd=_mm256_slli_si256(odd,1); //shift one position right
				r1=_mm256_add_epi8(even,odd); //add the odd with the even

				//y filter ends - r1 has now the data to be processed by x filter

		// col iteration computes output pixels of   0,8,16,24
		// col+1 iteration computes output pixels of 1,9,17,25
		// col+2 iteration computes output pixels of 2,10,18
		// col+3 iteration computes output pixels of 3,11,19
		// col+4 iteration computes output pixels of 4,12,20
		// col+5 iteration computes output pixels of 5,13,21
		// col+6 iteration computes output pixels of 6,14,22
		// col+7 iteration computes output pixels of 7,15,23
		//afterwards, col becomes 26 and repeat the above process

			//1st col iteration

			//multiply by the mask
			m0=_mm256_maddubs_epi16(r0,cx0);

			//make 16-bit values 32-bit (horizontal add)
			m0=_mm256_madd_epi16(m0,ones);

			//last horizontal add
			m1=_mm256_slli_si256(m0,4);//shift by one integer
			m0=_mm256_add_epi32(m0,m1);//add
			m1=_mm256_srli_si256(m0,4);//shift by one integer
			output1=_mm256_and_si256(m1,mask_even_32);//keep only 0,2,4,6 and discard 1,3,5,7


			//2nd col iteration

			//multiply by the mask
			m0=_mm256_maddubs_epi16(r0,cx1);

			//make 16-bit values 32-bit (horizontal add)
			m0=_mm256_madd_epi16(m0,ones);

			//last horizontal add
			m1=_mm256_slli_si256(m0,4);//shift by one integer
			m0=_mm256_add_epi32(m0,m1);//add
			m1=_mm256_srli_si256(m0,4);//shift by one integer
			output2=_mm256_and_si256(m1,mask_even_32);//keep only 0,2,4,6 and discard 1,3,5,7

			//3rd col iteration

			//multiply by the mask
			m0=_mm256_maddubs_epi16(r0,cx2);

			//shift m0-m6 by one position because hadd follows
			m0=my_rshift16_by_one(m0,mask3);

			//make 16-bit values 32-bit (horizontal add)
			m0=_mm256_madd_epi16(m0,ones);


			//last horizontal add
			m1=_mm256_slli_si256(m0,4);//shift by one integer
			m0=_mm256_add_epi32(m0,m1);//add
			m1=_mm256_srli_si256(m0,4);//shift by one integer
			output3=_mm256_and_si256(m1,mask_even_32);//keep only 0,2,4,6 and discard 1,3,5,7


			//4th col iteration

			//multiply by the mask
			m0=_mm256_maddubs_epi16(r0,cx3);

			//shift m0-m6 by one position because hadd follows
			m0=my_rshift16_by_one(m0,mask3);

			//make 16-bit values 32-bit (horizontal add)
			m0=_mm256_madd_epi16(m0,ones);


			//last horizontal add
			m1=_mm256_slli_si256(m0,4);//shift by one integer
			m0=_mm256_add_epi32(m0,m1);//add
			m1=_mm256_srli_si256(m0,4);//shift by one integer
			output4=_mm256_and_si256(m1,mask_even_32);//keep only 0,2,4,6 and discard 1,3,5,7


			//5th col iteration

			//multiply by the mask
			m0=_mm256_maddubs_epi16(r0,cx4);

			//No need for shift m0-m6 now

			//make 16-bit values 32-bit (horizontal add)
			m0=_mm256_madd_epi16(m0,ones);


			//last horizontal add
			m3=_mm256_srli_si256(m0,4);
			m4=_mm256_and_si256(m0,mask4);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_slli_si256(m4,12);//shift 3 int positions
			m3=_mm256_add_epi32(m3,m4);
			m0=_mm256_add_epi32(m0,m3);
			m0=_mm256_and_si256(m0,mask_odd_32);//keep only 1,3,5,7 and discard 0,2,4,6

			//MERGE output1 with output5 and DIVIDE
			output1=_mm256_add_epi32(m0,output1);
			output1 = division_32(division_case,output1,f);


			//6th col iteration

			//multiply by the mask
			m0=_mm256_maddubs_epi16(r0,cx5);

			//No need for shift m0-m6 now

			//make 16-bit values 32-bit (horizontal add)
			m0=_mm256_madd_epi16(m0,ones);


			//last horizontal add
			m3=_mm256_srli_si256(m0,4);
			m4=_mm256_and_si256(m0,mask4);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_slli_si256(m4,12);//shift 3 int positions
			m3=_mm256_add_epi32(m3,m4);
			m0=_mm256_add_epi32(m0,m3);
			m0=_mm256_and_si256(m0,mask_odd_32);//keep only 1,3,5,7 and discard 0,2,4,6

			//MERGE output1 with output5 and DIVIDE
			output2=_mm256_add_epi32(m0,output2);
			output2 = division_32(division_case,output2,f);


			//7th col iteration

			//multiply by the mask
			m0=_mm256_maddubs_epi16(r0,cx6);


			//shift m0-m6 by one position because hadd follows
			m0=my_rshift16_by_one(m0,mask3);

			//make 16-bit values 32-bit (horizontal add)
			m0=_mm256_madd_epi16(m0,ones);

			//last horizontal add
			m3=_mm256_srli_si256(m0,4);
			m4=_mm256_and_si256(m0,mask4);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_slli_si256(m4,12);//shift 3 int positions
			m3=_mm256_add_epi32(m3,m4);
			m0=_mm256_add_epi32(m0,m3);
			m0=_mm256_and_si256(m0,mask_odd_32);//keep only 1,3,5,7 and discard 0,2,4,6

			//MERGE output1 with output5 and DIVIDE
			output3=_mm256_add_epi32(m0,output3);
			output3 = division_32(division_case,output3,f);

			//8th col iteration

			//multiply by the mask
			m0=_mm256_maddubs_epi16(r0,cx7);

			//shift m0-m6 by one position because hadd follows
			m0=my_rshift16_by_one(m0,mask3);

			//make 16-bit values 32-bit (horizontal add)
			m0=_mm256_madd_epi16(m0,ones);

			//last horizontal add
			m3=_mm256_srli_si256(m0,4);
			m4=_mm256_and_si256(m0,mask4);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_slli_si256(m4,12);//shift 3 int positions
			m3=_mm256_add_epi32(m3,m4);
			m0=_mm256_add_epi32(m0,m3);
			m0=_mm256_and_si256(m0,mask_odd_32);//keep only 1,3,5,7 and discard 0,2,4,6

			//MERGE output1 with output5 and DIVIDE
			output4=_mm256_add_epi32(m0,output4);
			output4 = division_32(division_case,output4,f);



			//blend output1, output2, output3, output4 into one register
			output2 = _mm256_slli_si256(output2,1);
			output1 = _mm256_add_epi8(output1,output2);
			output3 = _mm256_slli_si256(output3,2);
			output1 = _mm256_add_epi8(output1,output3);
			output4 = _mm256_slli_si256(output4,3);
			output1 = _mm256_add_epi8(output1,output4);

			_mm256_storeu_si256( (__m256i *) &filt[row][col],output1 );

			//row+1

			//1st col iteration

			//multiply by the mask
			m0=_mm256_maddubs_epi16(r1,cx0);

			//make 16-bit values 32-bit (horizontal add)
			m0=_mm256_madd_epi16(m0,ones);

			//last horizontal add
			m1=_mm256_slli_si256(m0,4);//shift by one integer
			m0=_mm256_add_epi32(m0,m1);//add
			m1=_mm256_srli_si256(m0,4);//shift by one integer
			output1=_mm256_and_si256(m1,mask_even_32);//keep only 0,2,4,6 and discard 1,3,5,7


			//2nd col iteration

			//multiply by the mask
			m0=_mm256_maddubs_epi16(r1,cx1);

			//make 16-bit values 32-bit (horizontal add)
			m0=_mm256_madd_epi16(m0,ones);

			//last horizontal add
			m1=_mm256_slli_si256(m0,4);//shift by one integer
			m0=_mm256_add_epi32(m0,m1);//add
			m1=_mm256_srli_si256(m0,4);//shift by one integer
			output2=_mm256_and_si256(m1,mask_even_32);//keep only 0,2,4,6 and discard 1,3,5,7

			//3rd col iteration

			//multiply by the mask
			m0=_mm256_maddubs_epi16(r1,cx2);

			//shift m0-m6 by one position because hadd follows
			m0=my_rshift16_by_one(m0,mask3);

			//make 16-bit values 32-bit (horizontal add)
			m0=_mm256_madd_epi16(m0,ones);


			//last horizontal add
			m1=_mm256_slli_si256(m0,4);//shift by one integer
			m0=_mm256_add_epi32(m0,m1);//add
			m1=_mm256_srli_si256(m0,4);//shift by one integer
			output3=_mm256_and_si256(m1,mask_even_32);//keep only 0,2,4,6 and discard 1,3,5,7


			//4th col iteration

			//multiply by the mask
			m0=_mm256_maddubs_epi16(r1,cx3);

			//shift m0-m6 by one position because hadd follows
			m0=my_rshift16_by_one(m0,mask3);

			//make 16-bit values 32-bit (horizontal add)
			m0=_mm256_madd_epi16(m0,ones);


			//last horizontal add
			m1=_mm256_slli_si256(m0,4);//shift by one integer
			m0=_mm256_add_epi32(m0,m1);//add
			m1=_mm256_srli_si256(m0,4);//shift by one integer
			output4=_mm256_and_si256(m1,mask_even_32);//keep only 0,2,4,6 and discard 1,3,5,7


			//5th col iteration

			//multiply by the mask
			m0=_mm256_maddubs_epi16(r1,cx4);

			//No need for shift m0-m6 now

			//make 16-bit values 32-bit (horizontal add)
			m0=_mm256_madd_epi16(m0,ones);


			//last horizontal add
			m3=_mm256_srli_si256(m0,4);
			m4=_mm256_and_si256(m0,mask4);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_slli_si256(m4,12);//shift 3 int positions
			m3=_mm256_add_epi32(m3,m4);
			m0=_mm256_add_epi32(m0,m3);
			m0=_mm256_and_si256(m0,mask_odd_32);//keep only 1,3,5,7 and discard 0,2,4,6

			//MERGE output1 with output5 and DIVIDE
			output1=_mm256_add_epi32(m0,output1);
			output1 = division_32(division_case,output1,f);


			//6th col iteration

			//multiply by the mask
			m0=_mm256_maddubs_epi16(r1,cx5);

			//No need for shift m0-m6 now

			//make 16-bit values 32-bit (horizontal add)
			m0=_mm256_madd_epi16(m0,ones);


			//last horizontal add
			m3=_mm256_srli_si256(m0,4);
			m4=_mm256_and_si256(m0,mask4);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_slli_si256(m4,12);//shift 3 int positions
			m3=_mm256_add_epi32(m3,m4);
			m0=_mm256_add_epi32(m0,m3);
			m0=_mm256_and_si256(m0,mask_odd_32);//keep only 1,3,5,7 and discard 0,2,4,6

			//MERGE output1 with output5 and DIVIDE
			output2=_mm256_add_epi32(m0,output2);
			output2 = division_32(division_case,output2,f);


			//7th col iteration

			//multiply by the mask
			m0=_mm256_maddubs_epi16(r1,cx6);


			//shift m0-m6 by one position because hadd follows
			m0=my_rshift16_by_one(m0,mask3);

			//make 16-bit values 32-bit (horizontal add)
			m0=_mm256_madd_epi16(m0,ones);

			//last horizontal add
			m3=_mm256_srli_si256(m0,4);
			m4=_mm256_and_si256(m0,mask4);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_slli_si256(m4,12);//shift 3 int positions
			m3=_mm256_add_epi32(m3,m4);
			m0=_mm256_add_epi32(m0,m3);
			m0=_mm256_and_si256(m0,mask_odd_32);//keep only 1,3,5,7 and discard 0,2,4,6

			//MERGE output1 with output5 and DIVIDE
			output3=_mm256_add_epi32(m0,output3);
			output3 = division_32(division_case,output3,f);

			//8th col iteration

			//multiply by the mask
			m0=_mm256_maddubs_epi16(r1,cx7);

			//shift m0-m6 by one position because hadd follows
			m0=my_rshift16_by_one(m0,mask3);

			//make 16-bit values 32-bit (horizontal add)
			m0=_mm256_madd_epi16(m0,ones);

			//last horizontal add
			m3=_mm256_srli_si256(m0,4);
			m4=_mm256_and_si256(m0,mask4);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_slli_si256(m4,12);//shift 3 int positions
			m3=_mm256_add_epi32(m3,m4);
			m0=_mm256_add_epi32(m0,m3);
			m0=_mm256_and_si256(m0,mask_odd_32);//keep only 1,3,5,7 and discard 0,2,4,6

			//MERGE output1 with output5 and DIVIDE
			output4=_mm256_add_epi32(m0,output4);
			output4 = division_32(division_case,output4,f);



			//blend output1, output2, output3, output4 into one register
			output2 = _mm256_slli_si256(output2,1);
			output1 = _mm256_add_epi8(output1,output2);
			output3 = _mm256_slli_si256(output3,2);
			output1 = _mm256_add_epi8(output1,output3);
			output4 = _mm256_slli_si256(output4,3);
			output1 = _mm256_add_epi8(output1,output4);

			_mm256_storeu_si256( (__m256i *) &filt[row+1][col],output1 );
		}
	  loop_reminder_7x7_blur(frame1,filt,N,row, col, REMINDER_ITERATIONS, division_case, mask_vector_x, mask_vector_y, f, divisor_xy);
	  loop_reminder_7x7_blur(frame1,filt,N,row+1, col, REMINDER_ITERATIONS, division_case, mask_vector_x, mask_vector_y, f, divisor_xy);


		}
}

}//end of parallel


}



void Gaussian_Blur_7x7_16_separable_old(unsigned char **frame1,unsigned char **filt,const unsigned int M, const unsigned int N, signed char *kernel_y, signed char *kernel_x, const unsigned short int divisor_xy){

const signed char fx0=kernel_x[0];
const signed char fx1=kernel_x[1];
const signed char fx2=kernel_x[2];
const signed char fx3=kernel_x[3];
const signed char fx4=kernel_x[4];
const signed char fx5=kernel_x[5];
const signed char fx6=kernel_x[6];

const signed char fy0=kernel_y[0];
const signed char fy1=kernel_y[1];
const signed char fy2=kernel_y[2];
const signed char fy3=kernel_y[3];
const signed char fy4=kernel_y[4];
const signed char fy5=kernel_y[5];
const signed char fy6=kernel_y[6];



const signed char mask_vector_x[8][32] __attribute__((aligned(64))) ={
	{fx0,fx1,fx2,fx3,fx4,fx5,fx6,0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,0},
	{0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,0,fx0,fx1,fx2,fx3,fx4,fx5,fx6},
	{0,0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,0,0,0,0,0,0,0},
	{0,0,0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,0,0,0,0,0,0},
	{0,0,0,0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,0,0,0,0,0},
	{0,0,0,0,0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,0,0,0,0},
	{0,0,0,0,0,0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,0,0,0},
	{0,0,0,0,0,0,0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,0,0},
};

const signed char mask_vector_y[14][32] __attribute__((aligned(64))) ={
	{fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0},
	{fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0},
	{fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0},
	{fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0},
	{fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0},
	{fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0},
	{fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0},

	{0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0},
	{0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1},
	{0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2},
	{0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3},
	{0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4},
	{0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5},
	{0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6},
};

	const __m256i cx0=_mm256_load_si256( (__m256i *) &mask_vector_x[0]);
	const __m256i cx1=_mm256_load_si256( (__m256i *) &mask_vector_x[1]);
	const __m256i cx2=_mm256_load_si256( (__m256i *) &mask_vector_x[2]);
	const __m256i cx3=_mm256_load_si256( (__m256i *) &mask_vector_x[3]);
	const __m256i cx4=_mm256_load_si256( (__m256i *) &mask_vector_x[4]);
	const __m256i cx5=_mm256_load_si256( (__m256i *) &mask_vector_x[5]);
	const __m256i cx6=_mm256_load_si256( (__m256i *) &mask_vector_x[6]);
	const __m256i cx7=_mm256_load_si256( (__m256i *) &mask_vector_x[7]);


	const __m256i cy0=_mm256_load_si256( (__m256i *) &mask_vector_y[0]);
	const __m256i cy1=_mm256_load_si256( (__m256i *) &mask_vector_y[1]);
	const __m256i cy2=_mm256_load_si256( (__m256i *) &mask_vector_y[2]);
	const __m256i cy3=_mm256_load_si256( (__m256i *) &mask_vector_y[3]);
	const __m256i cy4=_mm256_load_si256( (__m256i *) &mask_vector_y[4]);
	const __m256i cy5=_mm256_load_si256( (__m256i *) &mask_vector_y[5]);
	const __m256i cy6=_mm256_load_si256( (__m256i *) &mask_vector_y[6]);

	const __m256i cy0_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[7]);
	const __m256i cy1_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[8]);
	const __m256i cy2_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[9]);
	const __m256i cy3_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[10]);
	const __m256i cy4_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[11]);
	const __m256i cy5_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[12]);
	const __m256i cy6_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[13]);


	const __m256i mask3  = _mm256_set_epi16(0,0,0,0,0,0,0,65535,0,0,0,0,0,0,0,0);
	const __m256i mask_prelude  = _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0);

//	const __m256i mask_even_16  = _mm256_set_epi16(0,0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff);
//	const __m256i mask_odd_16  = _mm256_set_epi16(0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff,0);
	const __m256i mask_16_1  = _mm256_set_epi16(0,0,0,0xffff,0,0,0,0xffff,0,0,0,0xffff,0,0,0,0xffff);
	const __m256i mask_16_2  = _mm256_set_epi16(0,0,0,0,0,0,0xffff,0,0,0,0xffff,0,0,0,0xffff,0);
	const __m256i mask_16_3  = _mm256_set_epi16(0,0,0,0,0,0xffff,0,0,0,0xffff,0,0,0,0xffff,0,0);
	const __m256i mask_16_4  = _mm256_set_epi16(0,0,0,0,0,0xffff,0xffff,0xffff,0,0,0,0,0,0,0,0);


//	const __m256i mask_even_32  = _mm256_set_epi32(0,0xffffffff,0,0xffffffff,0,0xffffffff,0,0xffffffff);
//	const __m256i mask_odd_32  = _mm256_set_epi32(0xffffffff,0,0xffffffff,0,0xffffffff,0,0xffffffff,0);
	//const __m256i mask4  = _mm256_set_epi32(0,0,0,0xffffffff,0,0,0,0);


	//REMINDER_ITERATIONS=[6,31]
	const unsigned int REMINDER_ITERATIONS =  (M-((((M-32)/26)*26)+26)); //M-(last_col_value+26)

	//printf("\nREMINDER_ITERATIONS=%d",REMINDER_ITERATIONS);


	const unsigned int division_case=prepare_for_division(divisor_xy); //determine which is the division case (A, B or C)
	const __m256i f = _mm256_load_si256( (__m256i *) &f_vector[0]);// initialize the division vector
	//const __m256i ones=_mm256_set1_epi16(1);

//	const unsigned int division_case_x=prepare_for_division_32(divisor_x); //determine which is the division case (A, B or C)
//	const __m256i f_x = _mm256_load_si256( (__m256i *) &f_vector_x[0]);// initialize the division vector
//printf("\n%d %d",division_case,b);


#pragma omp parallel
{

unsigned int row,col;
__m256i r0,r1,r2,r3,r4,r5,r6,m0,m1,m2,m3,m4,m5,m6,even,odd;
__m256i output1,output2,output3,output4;



/*---------------------- Gaussian Blur ---------------------------------*/

    #pragma omp for schedule(static)
	for (row = 0; row < N; row++) {

		if (row<3){
			if (row==0){
				for (col = 0; col <= M-32; col+=26){
				r0=prelude_7x7_16_Ymask_1(0, col, frame1, mask_vector_y, division_case, f, mask_prelude);
				prelude_7x7_16_Xmask(filt,row,col, r0, mask_vector_x,  division_case,f);
				}
				loop_reminder_7x7_16_blur(frame1,filt,N,row, col, REMINDER_ITERATIONS, division_case, mask_vector_x, mask_vector_y, f, divisor_xy);

			}
			else if (row==1){
				for (col = 0; col <= M-32; col+=26){
				r0=prelude_7x7_16_Ymask_2(0, col, frame1, mask_vector_y, division_case, f, mask_prelude);
				prelude_7x7_16_Xmask(filt,row,col, r0, mask_vector_x,  division_case,f);
				}
				loop_reminder_7x7_16_blur(frame1,filt,N,row, col, REMINDER_ITERATIONS, division_case, mask_vector_x, mask_vector_y, f, divisor_xy);
			}
			else if (row==2){
				for (col = 0; col <= M-32; col+=26){
				r0=prelude_7x7_16_Ymask_3(0, col, frame1, mask_vector_y, division_case, f, mask_prelude);
				prelude_7x7_16_Xmask(filt,row,col, r0, mask_vector_x,  division_case,f);
				}
				loop_reminder_7x7_16_blur(frame1,filt,N,row, col, REMINDER_ITERATIONS, division_case, mask_vector_x, mask_vector_y, f, divisor_xy);
			}
		}

		else if (row>N-4){
			if (row==N-1){
				for (col = 0; col <= M-32; col+=26){
				r0=prelude_7x7_16_Ymask_1(N-4, col, frame1, mask_vector_y, division_case, f, mask_prelude);
				prelude_7x7_16_Xmask(filt,row,col, r0, mask_vector_x,  division_case,f);
			}
			loop_reminder_7x7_16_blur(frame1,filt,N,row, col, REMINDER_ITERATIONS, division_case, mask_vector_x, mask_vector_y, f, divisor_xy);

			}
			else if (row==N-2){
				for (col = 0; col <= M-32; col+=26){
				r0=prelude_7x7_16_Ymask_2(N-5, col, frame1, mask_vector_y, division_case, f, mask_prelude);
				prelude_7x7_16_Xmask(filt,row,col, r0, mask_vector_x,  division_case,f);
			}
			loop_reminder_7x7_16_blur(frame1,filt,N,row, col, REMINDER_ITERATIONS, division_case, mask_vector_x, mask_vector_y, f, divisor_xy);

			}
			else if (row==N-3){
				for (col = 0; col <= M-32; col+=26){
				r0=prelude_7x7_16_Ymask_3(N-6, col, frame1, mask_vector_y, division_case, f, mask_prelude);
				prelude_7x7_16_Xmask(filt,row,col, r0, mask_vector_x,  division_case,f);
			}
			loop_reminder_7x7_16_blur(frame1,filt,N,row, col, REMINDER_ITERATIONS, division_case, mask_vector_x, mask_vector_y, f, divisor_xy);

			}
		}

	  else { //main loop


	  for (col = 0; col <= M-32; col+=26){

		  if (col==0){

				//load the 7 rows
				r0=_mm256_loadu_si256( (__m256i *) &frame1[row-3][0]);
				r1=_mm256_loadu_si256( (__m256i *) &frame1[row-2][0]);
				r2=_mm256_loadu_si256( (__m256i *) &frame1[row-1][0]);
				r3=_mm256_loadu_si256( (__m256i *) &frame1[row][0]);
				r4=_mm256_loadu_si256( (__m256i *) &frame1[row+1][0]);
				r5=_mm256_loadu_si256( (__m256i *) &frame1[row+2][0]);
				r6=_mm256_loadu_si256( (__m256i *) &frame1[row+3][0]);


		  			//START - extra code needed for prelude
		  					m0=_mm256_slli_si256(r0,3);//shift 3 elements left - equivalent to filling with two zeros inthe beginning
		  					m1=_mm256_slli_si256(r1,3);//shift 3 elements left - equivalent to filling with two zeros inthe beginning
		  					m2=_mm256_slli_si256(r2,3);//shift 3 elements left - equivalent to filling with two zeros inthe beginning
		  					m3=_mm256_slli_si256(r3,3);//shift 3 elements left - equivalent to filling with two zeros inthe beginning
		  					m4=_mm256_slli_si256(r4,3);//shift 3 elements left - equivalent to filling with two zeros inthe beginning
		  					m5=_mm256_slli_si256(r5,3);//shift 3 elements left - equivalent to filling with two zeros inthe beginning
		  					m6=_mm256_slli_si256(r6,3);//shift 3 elements left - equivalent to filling with two zeros inthe beginning

		  					r0=_mm256_and_si256(r0,mask_prelude);
		  					r0=_mm256_permute2f128_si256(r0,r0,1);
		  					r0=_mm256_srli_si256(r0,13);//shift 14 elements
		  					r0=_mm256_add_epi16(m0,r0);

		  					r1=_mm256_and_si256(r1,mask_prelude);
		  					r1=_mm256_permute2f128_si256(r1,r1,1);
		  					r1=_mm256_srli_si256(r1,13);//shift 14 elements
		  					r1=_mm256_add_epi16(m1,r1);

		  					r2=_mm256_and_si256(r2,mask_prelude);
		  					r2=_mm256_permute2f128_si256(r2,r2,1);
		  					r2=_mm256_srli_si256(r2,13);//shift 14 elements
		  					r2=_mm256_add_epi16(m2,r2);

		  					r3=_mm256_and_si256(r3,mask_prelude);
		  					r3=_mm256_permute2f128_si256(r3,r3,1);
		  					r3=_mm256_srli_si256(r3,13);//shift 14 elements
		  					r3=_mm256_add_epi16(m3,r3);

		  					r4=_mm256_and_si256(r4,mask_prelude);
		  					r4=_mm256_permute2f128_si256(r4,r4,1);
		  					r4=_mm256_srli_si256(r4,13);//shift 14 elements
		  					r4=_mm256_add_epi16(m4,r4);

		  					r5=_mm256_and_si256(r5,mask_prelude);
		  					r5=_mm256_permute2f128_si256(r5,r5,1);
		  					r5=_mm256_srli_si256(r5,13);//shift 14 elements
		  					r5=_mm256_add_epi16(m5,r5);

		  					r6=_mm256_and_si256(r6,mask_prelude);
		  					r6=_mm256_permute2f128_si256(r6,r6,1);
		  					r6=_mm256_srli_si256(r6,13);//shift 14 elements
		  					r6=_mm256_add_epi16(m6,r6);

		  			//END - extra code needed for prelude
		  		}
		  else {
				//load the 7 rows
				r0=_mm256_loadu_si256( (__m256i *) &frame1[row-3][col-3]);
				r1=_mm256_loadu_si256( (__m256i *) &frame1[row-2][col-3]);
				r2=_mm256_loadu_si256( (__m256i *) &frame1[row-1][col-3]);
				r3=_mm256_loadu_si256( (__m256i *) &frame1[row][col-3]);
				r4=_mm256_loadu_si256( (__m256i *) &frame1[row+1][col-3]);
				r5=_mm256_loadu_si256( (__m256i *) &frame1[row+2][col-3]);
				r6=_mm256_loadu_si256( (__m256i *) &frame1[row+3][col-3]);

		  }

		  //--------------------y filter begins--------------------

			//multiply with the mask
				m0=_mm256_maddubs_epi16(r0,cy0);
				m1=_mm256_maddubs_epi16(r1,cy1);
				m2=_mm256_maddubs_epi16(r2,cy2);
				m3=_mm256_maddubs_epi16(r3,cy3);
				m4=_mm256_maddubs_epi16(r4,cy4);
				m5=_mm256_maddubs_epi16(r5,cy5);
				m6=_mm256_maddubs_epi16(r6,cy6);

				//vertical add
				m0=_mm256_add_epi16(m0,m1);
				m0=_mm256_add_epi16(m0,m2);
				m0=_mm256_add_epi16(m0,m3);
				m0=_mm256_add_epi16(m0,m4);
				m0=_mm256_add_epi16(m0,m5);
				m0=_mm256_add_epi16(m0,m6);

				even=division(division_case,m0,f);//even results

				//multiply with the mask
				m0=_mm256_maddubs_epi16(r0,cy0_sh1);
				m1=_mm256_maddubs_epi16(r1,cy1_sh1);
				m2=_mm256_maddubs_epi16(r2,cy2_sh1);
				m3=_mm256_maddubs_epi16(r3,cy3_sh1);
				m4=_mm256_maddubs_epi16(r4,cy4_sh1);
				m5=_mm256_maddubs_epi16(r5,cy5_sh1);
				m6=_mm256_maddubs_epi16(r6,cy6_sh1);

				//vertical add
				m0=_mm256_add_epi16(m0,m1);
				m0=_mm256_add_epi16(m0,m2);
				m0=_mm256_add_epi16(m0,m3);
				m0=_mm256_add_epi16(m0,m4);
				m0=_mm256_add_epi16(m0,m5);
				m0=_mm256_add_epi16(m0,m6);


				odd=division(division_case,m0,f);//odd results

				//pack to one register
				odd=_mm256_slli_si256(odd,1); //shift one position right
				r0=_mm256_add_epi8(even,odd); //add the odd with the even

			//y filter ends - r0 has now the data to be processed by x filter

		// col iteration computes output pixels of   0,8,16,24
		// col+1 iteration computes output pixels of 1,9,17,25
		// col+2 iteration computes output pixels of 2,10,18
		// col+3 iteration computes output pixels of 3,11,19
		// col+4 iteration computes output pixels of 4,12,20
		// col+5 iteration computes output pixels of 5,13,21
		// col+6 iteration computes output pixels of 6,14,22
		// col+7 iteration computes output pixels of 7,15,23
		//afterwards, col becomes 26 and repeat the above process

			//1st col iteration

			//multiply by the mask
			m0=_mm256_maddubs_epi16(r0,cx0);

			//first horizontal add
			m1=_mm256_srli_si256(m0,2);
			m0=_mm256_add_epi16(m0,m1);//add

			//second horizontal add
			m1=_mm256_srli_si256(m0,4);
			m0=_mm256_add_epi16(m0,m1);//add

			output1=_mm256_and_si256(m0,mask_16_1);//in 16-bit format keep only 0,4,8,12 and discard the others


			//2nd col iteration

			//multiply by the mask
			m0=_mm256_maddubs_epi16(r0,cx1);

			//first horizontal add
			m1=_mm256_srli_si256(m0,2);
			m0=_mm256_add_epi16(m0,m1);//add

			//second horizontal add
			m1=_mm256_srli_si256(m0,4);
			m0=_mm256_add_epi16(m0,m1);//add

			output2=_mm256_and_si256(m0,mask_16_1);//in 16-bit format keep only 0,4,8,12 and discard the others


			//3rd col iteration

			//multiply by the mask
			m0=_mm256_maddubs_epi16(r0,cx2);

			//first horizontal add (complex shift is needed)
			m3=_mm256_srli_si256(m0,2);
			m4=_mm256_and_si256(m0,mask3);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_slli_si256(m4,14);
			m3=_mm256_add_epi16(m3,m4);

			m0=_mm256_add_epi16(m0,m3);//add

			//second horizontal add
			m1=_mm256_srli_si256(m0,4);
			m0=_mm256_add_epi16(m0,m1);//add

			output3=_mm256_and_si256(m0,mask_16_2);//in 16-bit format keep only 1,5,9 and discard the others


			//4th col iteration

			//multiply by the mask
			m0=_mm256_maddubs_epi16(r0,cx3);

			//first horizontal add (complex shift is needed)
			m3=_mm256_srli_si256(m0,2);
			m4=_mm256_and_si256(m0,mask3);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_slli_si256(m4,14);
			m3=_mm256_add_epi16(m3,m4);

			m0=_mm256_add_epi16(m0,m3);//add

			//second horizontal add
			m1=_mm256_srli_si256(m0,4);
			m0=_mm256_add_epi16(m0,m1);//add

			output4=_mm256_and_si256(m0,mask_16_2);//in 16-bit format keep only 1,5,9 and discard the others


			//5th col iteration

			//multiply by the mask
			m0=_mm256_maddubs_epi16(r0,cx4);

			//first horizontal add
			m1=_mm256_srli_si256(m0,2);
			m0=_mm256_add_epi16(m0,m1);//add

			//second horizontal add (complex shift is needed)
			m3=_mm256_srli_si256(m0,4);
			m4=_mm256_and_si256(m0,mask3);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_slli_si256(m4,12);
			m3=_mm256_add_epi16(m3,m4);

			m0=_mm256_add_epi16(m0,m3);//add
			m0=_mm256_and_si256(m0,mask_16_3);//in 16-bit format keep only 2,6,10 and discard the others


			//MERGE output1 with output5 and DIVIDE
			output1=_mm256_add_epi16(m0,output1);
			output1 = division(division_case,output1,f);


			//6th col iteration

			//multiply by the mask
			m0=_mm256_maddubs_epi16(r0,cx5);

			//first horizontal add
			m1=_mm256_srli_si256(m0,2);
			m0=_mm256_add_epi16(m0,m1);//add

			//second horizontal add (complex shift is needed)
			m3=_mm256_srli_si256(m0,4);
			m4=_mm256_and_si256(m0,mask3);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_slli_si256(m4,12);
			m3=_mm256_add_epi16(m3,m4);

			m0=_mm256_add_epi16(m0,m3);//add
			m0=_mm256_and_si256(m0,mask_16_3);//in 16-bit format keep only 2,6,10 and discard the others

			//MERGE output1 with output5 and DIVIDE
			output2=_mm256_add_epi16(m0,output2);
			output2 = division(division_case,output2,f);


			//7th col iteration

			//multiply by the mask
			m0=_mm256_maddubs_epi16(r0,cx6);

			//shift so as the 1st useful element is on position zero (complex shift is needed)
			m3=_mm256_srli_si256(m0,6);
			m4=_mm256_and_si256(m0,mask_16_4);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_slli_si256(m4,10);
			m0=_mm256_add_epi16(m3,m4);

			//first horizontal add
			m1=_mm256_srli_si256(m0,2);
			m0=_mm256_add_epi16(m0,m1);//add

			//second horizontal add
			m1=_mm256_srli_si256(m0,4);
			m0=_mm256_add_epi16(m0,m1);//add

			m0=_mm256_and_si256(m0,mask_16_1);//in 16-bit format keep only 0,4,8,12 and discard the others
			m0=_mm256_slli_si256(m0,6);//in 16-bit output is on 3,7,11

			//MERGE output1 with output5 and DIVIDE
			output3=_mm256_add_epi16(m0,output3);
			output3 = division(division_case,output3,f);

			//8th col iteration

			//multiply by the mask
			m0=_mm256_maddubs_epi16(r0,cx7);

			//shift so as the 1st useful element is on position zero (complex shift is needed)
			m3=_mm256_srli_si256(m0,6);
			m4=_mm256_and_si256(m0,mask_16_4);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_slli_si256(m4,10);
			m0=_mm256_add_epi16(m3,m4);

			//first horizontal add
			m1=_mm256_srli_si256(m0,2);
			m0=_mm256_add_epi16(m0,m1);//add

			//second horizontal add
			m1=_mm256_srli_si256(m0,4);
			m0=_mm256_add_epi16(m0,m1);//add

			m0=_mm256_and_si256(m0,mask_16_1);//in 16-bit format keep only 0,4,8,12 and discard the others
			m0=_mm256_slli_si256(m0,6);//in 16-bit output is on 3,7,11

			//MERGE output1 with output5 and DIVIDE
			output4=_mm256_add_epi16(m0,output4);
			output4 = division(division_case,output4,f);

			//blend output1, output2, output3, output4 into one register
			output2 = _mm256_slli_si256(output2,1);
			output1 = _mm256_add_epi8(output1,output2);

			output4 = _mm256_slli_si256(output4,1);
			output3 = _mm256_add_epi8(output3,output4);

			output1 = _mm256_add_epi8(output1,output3);

			_mm256_storeu_si256( (__m256i *) &filt[row][col],output1 );
		}
	  loop_reminder_7x7_16_blur(frame1,filt,N,row, col, REMINDER_ITERATIONS, division_case, mask_vector_x, mask_vector_y, f, divisor_xy);


		}
}

}//end of parallel


}



//this function is computing the row=2 and row=N-3 only
//for row=2 input prelude_7x7_Ymask_3(0,col,...
//for row=N-3 input prelude_7x7_Ymask_3(N-6,col,...
__m256i prelude_7x7_16_Ymask_3(const int row, const int col, unsigned char **frame1, const signed char mask_vector_y[][32],  const unsigned int division_case, const __m256i f, const __m256i mask_prelude){

	__m256i r0,r1,r2,r3,r4,r5,m0,m1,m2,m3,m4,m5,even,odd;
	//__m256i ones=_mm256_set1_epi16(1);
	 __m256i cy0,cy1,cy2,cy3,cy4,cy5,cy0_sh1,cy1_sh1,cy2_sh1,cy3_sh1,cy4_sh1,cy5_sh1;

	if (row!=0){ //if row=N-6
	  cy0=_mm256_load_si256( (__m256i *) &mask_vector_y[0]);
	  cy1=_mm256_load_si256( (__m256i *) &mask_vector_y[1]);
	  cy2=_mm256_load_si256( (__m256i *) &mask_vector_y[2]);
	  cy3=_mm256_load_si256( (__m256i *) &mask_vector_y[3]);
	  cy4=_mm256_load_si256( (__m256i *) &mask_vector_y[4]);
	  cy5=_mm256_load_si256( (__m256i *) &mask_vector_y[5]);
	 // cy6=_mm256_load_si256( (__m256i *) &mask_vector_y[6]);

	  cy0_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[7]);
	  cy1_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[8]);
	  cy2_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[9]);
	  cy3_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[10]);
	  cy4_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[11]);
	  cy5_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[12]);
	 // cy6_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[13]);
	}
	else {
		  cy0=_mm256_load_si256( (__m256i *) &mask_vector_y[1]);
		  cy1=_mm256_load_si256( (__m256i *) &mask_vector_y[2]);
		  cy2=_mm256_load_si256( (__m256i *) &mask_vector_y[3]);
		  cy3=_mm256_load_si256( (__m256i *) &mask_vector_y[4]);
		  cy4=_mm256_load_si256( (__m256i *) &mask_vector_y[5]);
		  cy5=_mm256_load_si256( (__m256i *) &mask_vector_y[6]);
		 // cy6=_mm256_load_si256( (__m256i *) &mask_vector_y[6]);

		  cy0_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[8]);
		  cy1_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[9]);
		  cy2_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[10]);
		  cy3_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[11]);
		  cy4_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[12]);
		  cy5_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[13]);
		//  cy6_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[13]);
	}

	/*
	 row=N-3:
		 r1xc0
		 r2xc1
		 ...
		 r6xc5
	 */
	/*
	 row=2:
		 r0xc1
		 r1xc2
		 ...
		 r5xc6
	 */


	  if (col==0){

			r0=_mm256_loadu_si256( (__m256i *) &frame1[row][0]);
			r1=_mm256_loadu_si256( (__m256i *) &frame1[row+1][0]);
			r2=_mm256_loadu_si256( (__m256i *) &frame1[row+2][0]);
			r3=_mm256_loadu_si256( (__m256i *) &frame1[row+3][0]);
			r4=_mm256_loadu_si256( (__m256i *) &frame1[row+4][0]);
			r5=_mm256_loadu_si256( (__m256i *) &frame1[row+5][0]);


	  			//START - extra code needed for prelude
	  					m0=_mm256_slli_si256(r0,3);//shift 3 elements left - equivalent to filling with two zeros inthe beginning
	  					m1=_mm256_slli_si256(r1,3);//shift 3 elements left - equivalent to filling with two zeros inthe beginning
	  					m2=_mm256_slli_si256(r2,3);//shift 3 elements left - equivalent to filling with two zeros inthe beginning
	  					m3=_mm256_slli_si256(r3,3);//shift 3 elements left - equivalent to filling with two zeros inthe beginning
	  					m4=_mm256_slli_si256(r4,3);//shift 3 elements left - equivalent to filling with two zeros inthe beginning
	  					m5=_mm256_slli_si256(r5,3);//shift 3 elements left - equivalent to filling with two zeros inthe beginning

	  					r0=_mm256_and_si256(r0,mask_prelude);
	  					r0=_mm256_permute2f128_si256(r0,r0,1);
	  					r0=_mm256_srli_si256(r0,13);//shift 14 elements
	  					r0=_mm256_add_epi16(m0,r0);

	  					r1=_mm256_and_si256(r1,mask_prelude);
	  					r1=_mm256_permute2f128_si256(r1,r1,1);
	  					r1=_mm256_srli_si256(r1,13);//shift 14 elements
	  					r1=_mm256_add_epi16(m1,r1);

	  					r2=_mm256_and_si256(r2,mask_prelude);
	  					r2=_mm256_permute2f128_si256(r2,r2,1);
	  					r2=_mm256_srli_si256(r2,13);//shift 14 elements
	  					r2=_mm256_add_epi16(m2,r2);

	  					r3=_mm256_and_si256(r3,mask_prelude);
	  					r3=_mm256_permute2f128_si256(r3,r3,1);
	  					r3=_mm256_srli_si256(r3,13);//shift 14 elements
	  					r3=_mm256_add_epi16(m3,r3);

	  					r4=_mm256_and_si256(r4,mask_prelude);
	  					r4=_mm256_permute2f128_si256(r4,r4,1);
	  					r4=_mm256_srli_si256(r4,13);//shift 14 elements
	  					r4=_mm256_add_epi16(m4,r4);

	  					r5=_mm256_and_si256(r5,mask_prelude);
	  					r5=_mm256_permute2f128_si256(r5,r5,1);
	  					r5=_mm256_srli_si256(r5,13);//shift 14 elements
	  					r5=_mm256_add_epi16(m5,r5);


	  			//END - extra code needed for prelude
	  		}
	  else {
			//load the 5 rows
			r0=_mm256_loadu_si256( (__m256i *) &frame1[row][col-3]);
			r1=_mm256_loadu_si256( (__m256i *) &frame1[row+1][col-3]);
			r2=_mm256_loadu_si256( (__m256i *) &frame1[row+2][col-3]);
			r3=_mm256_loadu_si256( (__m256i *) &frame1[row+3][col-3]);
			r4=_mm256_loadu_si256( (__m256i *) &frame1[row+4][col-3]);
			r5=_mm256_loadu_si256( (__m256i *) &frame1[row+5][col-3]);

	  }

	  //--------------------y filter begins--------------------

		//multiply with the mask
			m0=_mm256_maddubs_epi16(r0,cy0);
			m1=_mm256_maddubs_epi16(r1,cy1);
			m2=_mm256_maddubs_epi16(r2,cy2);
			m3=_mm256_maddubs_epi16(r3,cy3);
			m4=_mm256_maddubs_epi16(r4,cy4);
			m5=_mm256_maddubs_epi16(r5,cy5);

			//vertical add
			m0=_mm256_add_epi16(m0,m1);
			m0=_mm256_add_epi16(m0,m2);
			m0=_mm256_add_epi16(m0,m3);
			m0=_mm256_add_epi16(m0,m4);
			m0=_mm256_add_epi16(m0,m5);

			even=division(division_case,m0,f);//even results

			//multiply with the mask
			m0=_mm256_maddubs_epi16(r0,cy0_sh1);
			m1=_mm256_maddubs_epi16(r1,cy1_sh1);
			m2=_mm256_maddubs_epi16(r2,cy2_sh1);
			m3=_mm256_maddubs_epi16(r3,cy3_sh1);
			m4=_mm256_maddubs_epi16(r4,cy4_sh1);
			m5=_mm256_maddubs_epi16(r5,cy5_sh1);

			//vertical add
			m0=_mm256_add_epi16(m0,m1);
			m0=_mm256_add_epi16(m0,m2);
			m0=_mm256_add_epi16(m0,m3);
			m0=_mm256_add_epi16(m0,m4);
			m0=_mm256_add_epi16(m0,m5);


			odd=division(division_case,m0,f);//odd results

			//pack to one register
			odd=_mm256_slli_si256(odd,1); //shift one position right
			return _mm256_add_epi8(even,odd); //add the odd with the even

		//y filter ends - r0 has now the data to be processed by x filter
}


//this function is computing the row=1 and row=N-2 only
//for row=1 input prelude_7x7_Ymask_3(0,col,...
//for row=N-2 input prelude_7x7_Ymask_3(N-5,col,...
__m256i prelude_7x7_16_Ymask_2(const int row, const int col, unsigned char **frame1, const signed char mask_vector_y[][32],  const unsigned int division_case, const __m256i f, const __m256i mask_prelude){

	__m256i r0,r1,r2,r3,r4,m0,m1,m2,m3,m4,even,odd;
	//__m256i ones=_mm256_set1_epi16(1);

	 __m256i cy0,cy1,cy2,cy3,cy4,cy0_sh1,cy1_sh1,cy2_sh1,cy3_sh1,cy4_sh1;
	if (row!=0){//if row=N-5
	  cy0=_mm256_load_si256( (__m256i *) &mask_vector_y[0]);
	  cy1=_mm256_load_si256( (__m256i *) &mask_vector_y[1]);
	  cy2=_mm256_load_si256( (__m256i *) &mask_vector_y[2]);
	  cy3=_mm256_load_si256( (__m256i *) &mask_vector_y[3]);
	  cy4=_mm256_load_si256( (__m256i *) &mask_vector_y[4]);
	 // cy5=_mm256_load_si256( (__m256i *) &mask_vector_y[5]);
	 // cy6=_mm256_load_si256( (__m256i *) &mask_vector_y[6]);

	  cy0_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[7]);
	  cy1_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[8]);
	  cy2_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[9]);
	  cy3_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[10]);
	  cy4_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[11]);
	//  cy5_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[12]);
	 // cy6_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[13]);
	}
	else {
		  cy0=_mm256_load_si256( (__m256i *) &mask_vector_y[2]);
		  cy1=_mm256_load_si256( (__m256i *) &mask_vector_y[3]);
		  cy2=_mm256_load_si256( (__m256i *) &mask_vector_y[4]);
		  cy3=_mm256_load_si256( (__m256i *) &mask_vector_y[5]);
		  cy4=_mm256_load_si256( (__m256i *) &mask_vector_y[6]);
		//  cy5=_mm256_load_si256( (__m256i *) &mask_vector_y[6]);
		 // cy6=_mm256_load_si256( (__m256i *) &mask_vector_y[6]);

		  cy0_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[9]);
		  cy1_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[10]);
		  cy2_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[11]);
		  cy3_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[12]);
		  cy4_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[13]);
		//  cy5_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[13]);
		//  cy6_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[13]);
	}

	/*
	 row=N-2:
		 r2xc0
		 ...
		 r6xc4
	 */
	/*
	 row=1:
		 r0xc2
		 r1xc2
		 ...
		 r4xc6
	 */

	  if (col==0){

			r0=_mm256_loadu_si256( (__m256i *) &frame1[row][0]);
			r1=_mm256_loadu_si256( (__m256i *) &frame1[row+1][0]);
			r2=_mm256_loadu_si256( (__m256i *) &frame1[row+2][0]);
			r3=_mm256_loadu_si256( (__m256i *) &frame1[row+3][0]);
			r4=_mm256_loadu_si256( (__m256i *) &frame1[row+4][0]);


	  			//START - extra code needed for prelude
	  					m0=_mm256_slli_si256(r0,3);//shift 3 elements left - equivalent to filling with two zeros inthe beginning
	  					m1=_mm256_slli_si256(r1,3);//shift 3 elements left - equivalent to filling with two zeros inthe beginning
	  					m2=_mm256_slli_si256(r2,3);//shift 3 elements left - equivalent to filling with two zeros inthe beginning
	  					m3=_mm256_slli_si256(r3,3);//shift 3 elements left - equivalent to filling with two zeros inthe beginning
	  					m4=_mm256_slli_si256(r4,3);//shift 3 elements left - equivalent to filling with two zeros inthe beginning

	  					r0=_mm256_and_si256(r0,mask_prelude);
	  					r0=_mm256_permute2f128_si256(r0,r0,1);
	  					r0=_mm256_srli_si256(r0,13);//shift 14 elements
	  					r0=_mm256_add_epi16(m0,r0);

	  					r1=_mm256_and_si256(r1,mask_prelude);
	  					r1=_mm256_permute2f128_si256(r1,r1,1);
	  					r1=_mm256_srli_si256(r1,13);//shift 14 elements
	  					r1=_mm256_add_epi16(m1,r1);

	  					r2=_mm256_and_si256(r2,mask_prelude);
	  					r2=_mm256_permute2f128_si256(r2,r2,1);
	  					r2=_mm256_srli_si256(r2,13);//shift 14 elements
	  					r2=_mm256_add_epi16(m2,r2);

	  					r3=_mm256_and_si256(r3,mask_prelude);
	  					r3=_mm256_permute2f128_si256(r3,r3,1);
	  					r3=_mm256_srli_si256(r3,13);//shift 14 elements
	  					r3=_mm256_add_epi16(m3,r3);

	  					r4=_mm256_and_si256(r4,mask_prelude);
	  					r4=_mm256_permute2f128_si256(r4,r4,1);
	  					r4=_mm256_srli_si256(r4,13);//shift 14 elements
	  					r4=_mm256_add_epi16(m4,r4);


	  			//END - extra code needed for prelude
	  		}
	  else {
			//load the 5 rows
			r0=_mm256_loadu_si256( (__m256i *) &frame1[row][col-3]);
			r1=_mm256_loadu_si256( (__m256i *) &frame1[row+1][col-3]);
			r2=_mm256_loadu_si256( (__m256i *) &frame1[row+2][col-3]);
			r3=_mm256_loadu_si256( (__m256i *) &frame1[row+3][col-3]);
			r4=_mm256_loadu_si256( (__m256i *) &frame1[row+4][col-3]);
	  }

	  //--------------------y filter begins--------------------

		//multiply with the mask
			m0=_mm256_maddubs_epi16(r0,cy0);
			m1=_mm256_maddubs_epi16(r1,cy1);
			m2=_mm256_maddubs_epi16(r2,cy2);
			m3=_mm256_maddubs_epi16(r3,cy3);
			m4=_mm256_maddubs_epi16(r4,cy4);

			//vertical add
			m0=_mm256_add_epi16(m0,m1);
			m0=_mm256_add_epi16(m0,m2);
			m0=_mm256_add_epi16(m0,m3);
			m0=_mm256_add_epi16(m0,m4);

			even=division(division_case,m0,f);//even results

			//multiply with the mask
			m0=_mm256_maddubs_epi16(r0,cy0_sh1);
			m1=_mm256_maddubs_epi16(r1,cy1_sh1);
			m2=_mm256_maddubs_epi16(r2,cy2_sh1);
			m3=_mm256_maddubs_epi16(r3,cy3_sh1);
			m4=_mm256_maddubs_epi16(r4,cy4_sh1);

			//vertical add
			m0=_mm256_add_epi16(m0,m1);
			m0=_mm256_add_epi16(m0,m2);
			m0=_mm256_add_epi16(m0,m3);
			m0=_mm256_add_epi16(m0,m4);


			odd=division(division_case,m0,f);//odd results

			//pack to one register
			odd=_mm256_slli_si256(odd,1); //shift one position right
			return _mm256_add_epi8(even,odd); //add the odd with the even
		//y filter ends - r0 has now the data to be processed by x filter
}



//this function is computing the row=0 and row=N-1 only
//for row=0 input prelude_7x7_Ymask_3(0,col,...
//for row=N-1 input prelude_7x7_Ymask_3(N-4,col,...
__m256i prelude_7x7_16_Ymask_1(const int row, const int col, unsigned char **frame1, const signed char mask_vector_y[][32],  const unsigned int division_case, const __m256i f, const __m256i mask_prelude){

	__m256i r0,r1,r2,r3,m0,m1,m2,m3,even,odd;
	//__m256i ones=_mm256_set1_epi16(1);

	 __m256i cy0,cy1,cy2,cy3,cy0_sh1,cy1_sh1,cy2_sh1,cy3_sh1;
	if (row!=0){//if row=N-4
	  cy0=_mm256_load_si256( (__m256i *) &mask_vector_y[0]);
	  cy1=_mm256_load_si256( (__m256i *) &mask_vector_y[1]);
	  cy2=_mm256_load_si256( (__m256i *) &mask_vector_y[2]);
	  cy3=_mm256_load_si256( (__m256i *) &mask_vector_y[3]);
	//  cy4=_mm256_load_si256( (__m256i *) &mask_vector_y[4]);
	 // cy5=_mm256_load_si256( (__m256i *) &mask_vector_y[5]);
	 // cy6=_mm256_load_si256( (__m256i *) &mask_vector_y[6]);

	  cy0_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[7]);
	  cy1_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[8]);
	  cy2_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[9]);
	  cy3_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[10]);
	//  cy4_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[11]);
	//  cy5_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[12]);
	 // cy6_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[13]);
	}
	else {
		  cy0=_mm256_load_si256( (__m256i *) &mask_vector_y[3]);
		  cy1=_mm256_load_si256( (__m256i *) &mask_vector_y[4]);
		  cy2=_mm256_load_si256( (__m256i *) &mask_vector_y[5]);
		  cy3=_mm256_load_si256( (__m256i *) &mask_vector_y[6]);
		//  cy4=_mm256_load_si256( (__m256i *) &mask_vector_y[6]);
		//  cy5=_mm256_load_si256( (__m256i *) &mask_vector_y[6]);
		 // cy6=_mm256_load_si256( (__m256i *) &mask_vector_y[6]);

		  cy0_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[10]);
		  cy1_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[11]);
		  cy2_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[12]);
		  cy3_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[13]);
		//  cy4_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[13]);
		//  cy5_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[13]);
		//  cy6_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[13]);
	}


	/*
	 row=N-1:
		 r3xc0
		 ...
		 r6xc3
	 */
	/*
	 row=0:
		 r0xc3
		 ...
		 r3xc6
	 */


	  if (col==0){

			r0=_mm256_loadu_si256( (__m256i *) &frame1[row][0]);
			r1=_mm256_loadu_si256( (__m256i *) &frame1[row+1][0]);
			r2=_mm256_loadu_si256( (__m256i *) &frame1[row+2][0]);
			r3=_mm256_loadu_si256( (__m256i *) &frame1[row+3][0]);


	  			//START - extra code needed for prelude
	  					m0=_mm256_slli_si256(r0,3);//shift 3 elements left - equivalent to filling with two zeros inthe beginning
	  					m1=_mm256_slli_si256(r1,3);//shift 3 elements left - equivalent to filling with two zeros inthe beginning
	  					m2=_mm256_slli_si256(r2,3);//shift 3 elements left - equivalent to filling with two zeros inthe beginning
	  					m3=_mm256_slli_si256(r3,3);//shift 3 elements left - equivalent to filling with two zeros inthe beginning

	  					r0=_mm256_and_si256(r0,mask_prelude);
	  					r0=_mm256_permute2f128_si256(r0,r0,1);
	  					r0=_mm256_srli_si256(r0,13);//shift 14 elements
	  					r0=_mm256_add_epi16(m0,r0);

	  					r1=_mm256_and_si256(r1,mask_prelude);
	  					r1=_mm256_permute2f128_si256(r1,r1,1);
	  					r1=_mm256_srli_si256(r1,13);//shift 14 elements
	  					r1=_mm256_add_epi16(m1,r1);

	  					r2=_mm256_and_si256(r2,mask_prelude);
	  					r2=_mm256_permute2f128_si256(r2,r2,1);
	  					r2=_mm256_srli_si256(r2,13);//shift 14 elements
	  					r2=_mm256_add_epi16(m2,r2);

	  					r3=_mm256_and_si256(r3,mask_prelude);
	  					r3=_mm256_permute2f128_si256(r3,r3,1);
	  					r3=_mm256_srli_si256(r3,13);//shift 14 elements
	  					r3=_mm256_add_epi16(m3,r3);


	  			//END - extra code needed for prelude
	  		}
	  else {
			//load the 5 rows
			r0=_mm256_loadu_si256( (__m256i *) &frame1[row][col-3]);
			r1=_mm256_loadu_si256( (__m256i *) &frame1[row+1][col-3]);
			r2=_mm256_loadu_si256( (__m256i *) &frame1[row+2][col-3]);
			r3=_mm256_loadu_si256( (__m256i *) &frame1[row+3][col-3]);
	  }

	  //--------------------y filter begins--------------------

		//multiply with the mask
			m0=_mm256_maddubs_epi16(r0,cy0);
			m1=_mm256_maddubs_epi16(r1,cy1);
			m2=_mm256_maddubs_epi16(r2,cy2);
			m3=_mm256_maddubs_epi16(r3,cy3);

			//vertical add
			m0=_mm256_add_epi16(m0,m1);
			m0=_mm256_add_epi16(m0,m2);
			m0=_mm256_add_epi16(m0,m3);

			even=division(division_case,m0,f);//even results

			//multiply with the mask
			m0=_mm256_maddubs_epi16(r0,cy0_sh1);
			m1=_mm256_maddubs_epi16(r1,cy1_sh1);
			m2=_mm256_maddubs_epi16(r2,cy2_sh1);
			m3=_mm256_maddubs_epi16(r3,cy3_sh1);

			//vertical add
			m0=_mm256_add_epi16(m0,m1);
			m0=_mm256_add_epi16(m0,m2);
			m0=_mm256_add_epi16(m0,m3);


			odd=division(division_case,m0,f);//odd results

			//pack to one register
			odd=_mm256_slli_si256(odd,1); //shift one position right
			return _mm256_add_epi8(even,odd); //add the odd with the even

		//y filter ends - r0 has now the data to be processed by x filter
}


void prelude_7x7_16_Xmask(unsigned char **filt,const int row, const int col, const __m256i r0, const signed char mask_vector_x[][32],  const unsigned int division_case, const __m256i f){

	__m256i m0,m1,m3,m4,output1,output2,output3,output4;
	//__m256i ones=_mm256_set1_epi16(1);

	const __m256i mask3  = _mm256_set_epi16(0,0,0,0,0,0,0,65535,0,0,0,0,0,0,0,0);
	//const __m256i mask4  = _mm256_set_epi32(0,0,0,0xffffffff,0,0,0,0);
	const __m256i mask_16_1  = _mm256_set_epi16(0,0,0,0xffff,0,0,0,0xffff,0,0,0,0xffff,0,0,0,0xffff);
	const __m256i mask_16_2  = _mm256_set_epi16(0,0,0,0,0,0,0xffff,0,0,0,0xffff,0,0,0,0xffff,0);
	const __m256i mask_16_3  = _mm256_set_epi16(0,0,0,0,0,0xffff,0,0,0,0xffff,0,0,0,0xffff,0,0);
	const __m256i mask_16_4  = _mm256_set_epi16(0,0,0,0,0,0xffff,0xffff,0xffff,0,0,0,0,0,0,0,0);

	const __m256i cx0=_mm256_load_si256( (__m256i *) &mask_vector_x[0]);
	const __m256i cx1=_mm256_load_si256( (__m256i *) &mask_vector_x[1]);
	const __m256i cx2=_mm256_load_si256( (__m256i *) &mask_vector_x[2]);
	const __m256i cx3=_mm256_load_si256( (__m256i *) &mask_vector_x[3]);
	const __m256i cx4=_mm256_load_si256( (__m256i *) &mask_vector_x[4]);
	const __m256i cx5=_mm256_load_si256( (__m256i *) &mask_vector_x[5]);
	const __m256i cx6=_mm256_load_si256( (__m256i *) &mask_vector_x[6]);
	const __m256i cx7=_mm256_load_si256( (__m256i *) &mask_vector_x[7]);

	//1st col iteration

		//multiply by the mask
		m0=_mm256_maddubs_epi16(r0,cx0);

		//first horizontal add
		m1=_mm256_srli_si256(m0,2);
		m0=_mm256_add_epi16(m0,m1);//add

		//second horizontal add
		m1=_mm256_srli_si256(m0,4);
		m0=_mm256_add_epi16(m0,m1);//add

		output1=_mm256_and_si256(m0,mask_16_1);//in 16-bit format keep only 0,4,8,12 and discard the others


		//2nd col iteration

		//multiply by the mask
		m0=_mm256_maddubs_epi16(r0,cx1);

		//first horizontal add
		m1=_mm256_srli_si256(m0,2);
		m0=_mm256_add_epi16(m0,m1);//add

		//second horizontal add
		m1=_mm256_srli_si256(m0,4);
		m0=_mm256_add_epi16(m0,m1);//add

		output2=_mm256_and_si256(m0,mask_16_1);//in 16-bit format keep only 0,4,8,12 and discard the others


		//3rd col iteration

		//multiply by the mask
		m0=_mm256_maddubs_epi16(r0,cx2);

		//first horizontal add (complex shift is needed)
		m3=_mm256_srli_si256(m0,2);
		m4=_mm256_and_si256(m0,mask3);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,14);
		m3=_mm256_add_epi16(m3,m4);

		m0=_mm256_add_epi16(m0,m3);//add

		//second horizontal add
		m1=_mm256_srli_si256(m0,4);
		m0=_mm256_add_epi16(m0,m1);//add

		output3=_mm256_and_si256(m0,mask_16_2);//in 16-bit format keep only 1,5,9 and discard the others


		//4th col iteration

		//multiply by the mask
		m0=_mm256_maddubs_epi16(r0,cx3);

		//first horizontal add (complex shift is needed)
		m3=_mm256_srli_si256(m0,2);
		m4=_mm256_and_si256(m0,mask3);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,14);
		m3=_mm256_add_epi16(m3,m4);

		m0=_mm256_add_epi16(m0,m3);//add

		//second horizontal add
		m1=_mm256_srli_si256(m0,4);
		m0=_mm256_add_epi16(m0,m1);//add

		output4=_mm256_and_si256(m0,mask_16_2);//in 16-bit format keep only 1,5,9 and discard the others


		//5th col iteration

		//multiply by the mask
		m0=_mm256_maddubs_epi16(r0,cx4);

		//first horizontal add
		m1=_mm256_srli_si256(m0,2);
		m0=_mm256_add_epi16(m0,m1);//add

		//second horizontal add (complex shift is needed)
		m3=_mm256_srli_si256(m0,4);
		m4=_mm256_and_si256(m0,mask3);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,12);
		m3=_mm256_add_epi16(m3,m4);

		m0=_mm256_add_epi16(m0,m3);//add
		m0=_mm256_and_si256(m0,mask_16_3);//in 16-bit format keep only 2,6,10 and discard the others


		//MERGE output1 with output5 and DIVIDE
		output1=_mm256_add_epi16(m0,output1);
		output1 = division(division_case,output1,f);


		//6th col iteration

		//multiply by the mask
		m0=_mm256_maddubs_epi16(r0,cx5);

		//first horizontal add
		m1=_mm256_srli_si256(m0,2);
		m0=_mm256_add_epi16(m0,m1);//add

		//second horizontal add (complex shift is needed)
		m3=_mm256_srli_si256(m0,4);
		m4=_mm256_and_si256(m0,mask3);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,12);
		m3=_mm256_add_epi16(m3,m4);

		m0=_mm256_add_epi16(m0,m3);//add
		m0=_mm256_and_si256(m0,mask_16_3);//in 16-bit format keep only 2,6,10 and discard the others

		//MERGE output1 with output5 and DIVIDE
		output2=_mm256_add_epi16(m0,output2);
		output2 = division(division_case,output2,f);


		//7th col iteration

		//multiply by the mask
		m0=_mm256_maddubs_epi16(r0,cx6);

		//shift so as the 1st useful element is on position zero (complex shift is needed)
		m3=_mm256_srli_si256(m0,6);
		m4=_mm256_and_si256(m0,mask_16_4);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,10);
		m0=_mm256_add_epi16(m3,m4);

		//first horizontal add
		m1=_mm256_srli_si256(m0,2);
		m0=_mm256_add_epi16(m0,m1);//add

		//second horizontal add
		m1=_mm256_srli_si256(m0,4);
		m0=_mm256_add_epi16(m0,m1);//add

		m0=_mm256_and_si256(m0,mask_16_1);//in 16-bit format keep only 0,4,8,12 and discard the others
		m0=_mm256_slli_si256(m0,6);//in 16-bit output is on 3,7,11

		//MERGE output1 with output5 and DIVIDE
		output3=_mm256_add_epi16(m0,output3);
		output3 = division(division_case,output3,f);

		//8th col iteration

		//multiply by the mask
		m0=_mm256_maddubs_epi16(r0,cx7);

		//shift so as the 1st useful element is on position zero (complex shift is needed)
		m3=_mm256_srli_si256(m0,6);
		m4=_mm256_and_si256(m0,mask_16_4);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,10);
		m0=_mm256_add_epi16(m3,m4);

		//first horizontal add
		m1=_mm256_srli_si256(m0,2);
		m0=_mm256_add_epi16(m0,m1);//add

		//second horizontal add
		m1=_mm256_srli_si256(m0,4);
		m0=_mm256_add_epi16(m0,m1);//add

		m0=_mm256_and_si256(m0,mask_16_1);//in 16-bit format keep only 0,4,8,12 and discard the others
		m0=_mm256_slli_si256(m0,6);//in 16-bit output is on 3,7,11

		//MERGE output1 with output5 and DIVIDE
		output4=_mm256_add_epi16(m0,output4);
		output4 = division(division_case,output4,f);

		//blend output1, output2, output3, output4 into one register
		output2 = _mm256_slli_si256(output2,1);
		output1 = _mm256_add_epi8(output1,output2);

		output4 = _mm256_slli_si256(output4,1);
		output3 = _mm256_add_epi8(output3,output4);

		output1 = _mm256_add_epi8(output1,output3);

		_mm256_storeu_si256( (__m256i *) &filt[row][col],output1 );

}



void loop_reminder_7x7_16_blur(unsigned char **frame1,unsigned char **filt,const unsigned int N,const unsigned int row, const unsigned int col,const unsigned int REMINDER_ITERATIONS,const unsigned int division_case,const signed char mask_vector_x[][32],const signed char mask_vector_y[][32], const __m256i f, const unsigned short int divisor_xy){

	const __m256i cx0=_mm256_load_si256( (__m256i *) &mask_vector_x[0]);
	const __m256i cx1=_mm256_load_si256( (__m256i *) &mask_vector_x[1]);
	const __m256i cx2=_mm256_load_si256( (__m256i *) &mask_vector_x[2]);
	const __m256i cx3=_mm256_load_si256( (__m256i *) &mask_vector_x[3]);
	const __m256i cx4=_mm256_load_si256( (__m256i *) &mask_vector_x[4]);
	const __m256i cx5=_mm256_load_si256( (__m256i *) &mask_vector_x[5]);
	const __m256i cx6=_mm256_load_si256( (__m256i *) &mask_vector_x[6]);
	const __m256i cx7=_mm256_load_si256( (__m256i *) &mask_vector_x[7]);

	__m256i cy0,cy1,cy2,cy3,cy4,cy5,cy6,cy0_sh1,cy1_sh1,cy2_sh1,cy3_sh1,cy4_sh1,cy5_sh1,cy6_sh1;

	const __m256i mask3  = _mm256_set_epi16(0,0,0,0,0,0,0,65535,0,0,0,0,0,0,0,0);

	const __m256i mask_16_1  = _mm256_set_epi16(0,0,0,0xffff,0,0,0,0xffff,0,0,0,0xffff,0,0,0,0xffff);
	const __m256i mask_16_2  = _mm256_set_epi16(0,0,0,0,0,0,0xffff,0,0,0,0xffff,0,0,0,0xffff,0);
	const __m256i mask_16_3  = _mm256_set_epi16(0,0,0,0,0,0xffff,0,0,0,0xffff,0,0,0,0xffff,0,0);
	const __m256i mask_16_4  = _mm256_set_epi16(0,0,0,0,0,0xffff,0xffff,0xffff,0,0,0,0,0,0,0,0);
	__m256i r0,r1,r2,r3,r4,r5,r6,m0,m1,m2,m3,m4,m5,m6,even,odd,output1,output2,output3,output4,reminder_mask;

	if ((row>2) && (row<N-3)){//main case
		r0=_mm256_loadu_si256( (__m256i *) &frame1[row-3][col-3]);
		r1=_mm256_loadu_si256( (__m256i *) &frame1[row-2][col-3]);
		r2=_mm256_loadu_si256( (__m256i *) &frame1[row-1][col-3]);
		r3=_mm256_loadu_si256( (__m256i *) &frame1[row][col-3]);
		r4=_mm256_loadu_si256( (__m256i *) &frame1[row+1][col-3]);
		r5=_mm256_loadu_si256( (__m256i *) &frame1[row+2][col-3]);
		r6=_mm256_loadu_si256( (__m256i *) &frame1[row+3][col-3]);

		cy0=_mm256_load_si256( (__m256i *) &mask_vector_y[0]);
		cy1=_mm256_load_si256( (__m256i *) &mask_vector_y[1]);
		cy2=_mm256_load_si256( (__m256i *) &mask_vector_y[2]);
		cy3=_mm256_load_si256( (__m256i *) &mask_vector_y[3]);
		cy4=_mm256_load_si256( (__m256i *) &mask_vector_y[4]);
		cy5=_mm256_load_si256( (__m256i *) &mask_vector_y[5]);
		cy6=_mm256_load_si256( (__m256i *) &mask_vector_y[6]);

		cy0_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[7]);
		cy1_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[8]);
		cy2_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[9]);
		cy3_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[10]);
		cy4_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[11]);
		cy5_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[12]);
		cy6_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[13]);
	}
	else {
		if (row==0){
			r0=_mm256_loadu_si256( (__m256i *) &frame1[0][col-3]);
			r1=_mm256_loadu_si256( (__m256i *) &frame1[1][col-3]);
			r2=_mm256_loadu_si256( (__m256i *) &frame1[2][col-3]);
			r3=_mm256_loadu_si256( (__m256i *) &frame1[3][col-3]);
			r4=_mm256_setzero_si256();
			r5=_mm256_setzero_si256();
			r6=_mm256_setzero_si256();

			cy0=_mm256_load_si256( (__m256i *) &mask_vector_y[3]);
			cy1=_mm256_load_si256( (__m256i *) &mask_vector_y[4]);
			cy2=_mm256_load_si256( (__m256i *) &mask_vector_y[5]);
			cy3=_mm256_load_si256( (__m256i *) &mask_vector_y[6]);
			cy4=_mm256_setzero_si256();
			cy5=_mm256_setzero_si256();
			cy6=_mm256_setzero_si256();

			cy0_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[10]);
			cy1_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[11]);
			cy2_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[12]);
			cy3_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[13]);
			cy4_sh1=_mm256_setzero_si256();
			cy5_sh1=_mm256_setzero_si256();
			cy6_sh1=_mm256_setzero_si256();
		}
		else if (row==1){
			r0=_mm256_loadu_si256( (__m256i *) &frame1[0][col-3]);
			r1=_mm256_loadu_si256( (__m256i *) &frame1[1][col-3]);
			r2=_mm256_loadu_si256( (__m256i *) &frame1[2][col-3]);
			r3=_mm256_loadu_si256( (__m256i *) &frame1[3][col-3]);
			r4=_mm256_loadu_si256( (__m256i *) &frame1[4][col-3]);
			r5=_mm256_setzero_si256();
			r6=_mm256_setzero_si256();

			cy0=_mm256_load_si256( (__m256i *) &mask_vector_y[2]);
			cy1=_mm256_load_si256( (__m256i *) &mask_vector_y[3]);
			cy2=_mm256_load_si256( (__m256i *) &mask_vector_y[4]);
			cy3=_mm256_load_si256( (__m256i *) &mask_vector_y[5]);
			cy4=_mm256_load_si256( (__m256i *) &mask_vector_y[6]);
			cy5=_mm256_setzero_si256();
			cy6=_mm256_setzero_si256();

			cy0_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[9]);
			cy1_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[10]);
			cy2_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[11]);
			cy3_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[12]);
			cy4_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[13]);
			cy5_sh1=_mm256_setzero_si256();
			cy6_sh1=_mm256_setzero_si256();
		}
		else if (row==2){
			r0=_mm256_loadu_si256( (__m256i *) &frame1[0][col-3]);
			r1=_mm256_loadu_si256( (__m256i *) &frame1[1][col-3]);
			r2=_mm256_loadu_si256( (__m256i *) &frame1[2][col-3]);
			r3=_mm256_loadu_si256( (__m256i *) &frame1[3][col-3]);
			r4=_mm256_loadu_si256( (__m256i *) &frame1[4][col-3]);
			r5=_mm256_loadu_si256( (__m256i *) &frame1[5][col-3]);
			r6=_mm256_setzero_si256();

			cy0=_mm256_load_si256( (__m256i *) &mask_vector_y[1]);
			cy1=_mm256_load_si256( (__m256i *) &mask_vector_y[2]);
			cy2=_mm256_load_si256( (__m256i *) &mask_vector_y[3]);
			cy3=_mm256_load_si256( (__m256i *) &mask_vector_y[4]);
			cy4=_mm256_load_si256( (__m256i *) &mask_vector_y[5]);
			cy5=_mm256_load_si256( (__m256i *) &mask_vector_y[6]);
			cy6=_mm256_setzero_si256();

			cy0_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[8]);
			cy1_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[9]);
			cy2_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[10]);
			cy3_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[11]);
			cy4_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[12]);
			cy5_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[13]);
			cy6_sh1=_mm256_setzero_si256();
		}
		else if (row==N-3){
			r0=_mm256_setzero_si256();
			r1=_mm256_loadu_si256( (__m256i *) &frame1[N-6][col-3]);
			r2=_mm256_loadu_si256( (__m256i *) &frame1[N-5][col-3]);
			r3=_mm256_loadu_si256( (__m256i *) &frame1[N-4][col-3]);
			r4=_mm256_loadu_si256( (__m256i *) &frame1[N-3][col-3]);
			r5=_mm256_loadu_si256( (__m256i *) &frame1[N-2][col-3]);
			r6=_mm256_loadu_si256( (__m256i *) &frame1[N-1][col-3]);

			cy0=_mm256_setzero_si256();
			cy1=_mm256_load_si256( (__m256i *) &mask_vector_y[0]);
			cy2=_mm256_load_si256( (__m256i *) &mask_vector_y[1]);
			cy3=_mm256_load_si256( (__m256i *) &mask_vector_y[2]);
			cy4=_mm256_load_si256( (__m256i *) &mask_vector_y[3]);
			cy5=_mm256_load_si256( (__m256i *) &mask_vector_y[4]);
			cy6=_mm256_load_si256( (__m256i *) &mask_vector_y[5]);

			cy0_sh1=_mm256_setzero_si256();
			cy1_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[7]);
			cy2_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[8]);
			cy3_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[9]);
			cy4_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[10]);
			cy5_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[11]);
			cy6_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[12]);
		}
		else if (row==N-2){
			r0=_mm256_setzero_si256();
			r1=_mm256_setzero_si256();
			r2=_mm256_loadu_si256( (__m256i *) &frame1[N-5][col-3]);
			r3=_mm256_loadu_si256( (__m256i *) &frame1[N-4][col-3]);
			r4=_mm256_loadu_si256( (__m256i *) &frame1[N-3][col-3]);
			r5=_mm256_loadu_si256( (__m256i *) &frame1[N-2][col-3]);
			r6=_mm256_loadu_si256( (__m256i *) &frame1[N-1][col-3]);

			cy0=_mm256_setzero_si256();
			cy1=_mm256_setzero_si256();
			cy2=_mm256_load_si256( (__m256i *) &mask_vector_y[0]);
			cy3=_mm256_load_si256( (__m256i *) &mask_vector_y[1]);
			cy4=_mm256_load_si256( (__m256i *) &mask_vector_y[2]);
			cy5=_mm256_load_si256( (__m256i *) &mask_vector_y[3]);
			cy6=_mm256_load_si256( (__m256i *) &mask_vector_y[4]);

			cy0_sh1=_mm256_setzero_si256();
			cy1_sh1=_mm256_setzero_si256();
			cy2_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[7]);
			cy3_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[8]);
			cy4_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[9]);
			cy5_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[10]);
			cy6_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[11]);
		}
		else if (row==N-1){
			r0=_mm256_setzero_si256();
			r1=_mm256_setzero_si256();
			r2=_mm256_setzero_si256();
			r3=_mm256_loadu_si256( (__m256i *) &frame1[N-4][col-3]);
			r4=_mm256_loadu_si256( (__m256i *) &frame1[N-3][col-3]);
			r5=_mm256_loadu_si256( (__m256i *) &frame1[N-2][col-3]);
			r6=_mm256_loadu_si256( (__m256i *) &frame1[N-1][col-3]);

			cy0=_mm256_setzero_si256();
			cy1=_mm256_setzero_si256();
			cy2=_mm256_setzero_si256();
			cy3=_mm256_load_si256( (__m256i *) &mask_vector_y[0]);
			cy4=_mm256_load_si256( (__m256i *) &mask_vector_y[1]);
			cy5=_mm256_load_si256( (__m256i *) &mask_vector_y[2]);
			cy6=_mm256_load_si256( (__m256i *) &mask_vector_y[3]);

			cy0_sh1=_mm256_setzero_si256();
			cy1_sh1=_mm256_setzero_si256();
			cy2_sh1=_mm256_setzero_si256();
			cy3_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[7]);
			cy4_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[8]);
			cy5_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[9]);
			cy6_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[10]);
		}
		else {
			printf("\nsomething went wrong");
			exit(EXIT_FAILURE);
		}
	}


	/*
	 * The above load operations load outside of the array bounds; these elements are filled with zeros just after
	 */
	reminder_mask=_mm256_load_si256( (__m256i *) &reminder_mask_7x7[REMINDER_ITERATIONS-1][0]);

	//AND r0-r4 with reminder_mask
	r0=_mm256_and_si256(r0,reminder_mask);
	r1=_mm256_and_si256(r1,reminder_mask);
	r2=_mm256_and_si256(r2,reminder_mask);
	r3=_mm256_and_si256(r3,reminder_mask);
	r4=_mm256_and_si256(r4,reminder_mask);
	r5=_mm256_and_si256(r5,reminder_mask);
	r6=_mm256_and_si256(r6,reminder_mask);

//--------------------y filter begins--------------------

	//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,cy0);
		m1=_mm256_maddubs_epi16(r1,cy1);
		m2=_mm256_maddubs_epi16(r2,cy2);
		m3=_mm256_maddubs_epi16(r3,cy3);
		m4=_mm256_maddubs_epi16(r4,cy4);
		m5=_mm256_maddubs_epi16(r5,cy5);
		m6=_mm256_maddubs_epi16(r6,cy6);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m0=_mm256_add_epi16(m0,m2);
		m0=_mm256_add_epi16(m0,m3);
		m0=_mm256_add_epi16(m0,m4);
		m0=_mm256_add_epi16(m0,m5);
		m0=_mm256_add_epi16(m0,m6);

		even=division(division_case,m0,f);//even results

		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,cy0_sh1);
		m1=_mm256_maddubs_epi16(r1,cy1_sh1);
		m2=_mm256_maddubs_epi16(r2,cy2_sh1);
		m3=_mm256_maddubs_epi16(r3,cy3_sh1);
		m4=_mm256_maddubs_epi16(r4,cy4_sh1);
		m5=_mm256_maddubs_epi16(r5,cy5_sh1);
		m6=_mm256_maddubs_epi16(r6,cy6_sh1);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m0=_mm256_add_epi16(m0,m2);
		m0=_mm256_add_epi16(m0,m3);
		m0=_mm256_add_epi16(m0,m4);
		m0=_mm256_add_epi16(m0,m5);
		m0=_mm256_add_epi16(m0,m6);


		odd=division(division_case,m0,f);//odd results

		//pack to one register
		odd=_mm256_slli_si256(odd,1); //shift one position right
		r0=_mm256_add_epi8(even,odd); //add the odd with the even

		//y filter ends - r0 has now the data to be processed by x filter

		// col iteration computes output pixels of   0,8,16,24
		// col+1 iteration computes output pixels of 1,9,17,25
		// col+2 iteration computes output pixels of 2,10,18
		// col+3 iteration computes output pixels of 3,11,19
		// col+4 iteration computes output pixels of 4,12,20
		// col+5 iteration computes output pixels of 5,13,21
		// col+6 iteration computes output pixels of 6,14,22
		// col+7 iteration computes output pixels of 7,15,23
		//afterwards, col becomes 26 and repeat the above process

		//1st col iteration

			//multiply by the mask
			m0=_mm256_maddubs_epi16(r0,cx0);

			//first horizontal add
			m1=_mm256_srli_si256(m0,2);
			m0=_mm256_add_epi16(m0,m1);//add

			//second horizontal add
			m1=_mm256_srli_si256(m0,4);
			m0=_mm256_add_epi16(m0,m1);//add

			output1=_mm256_and_si256(m0,mask_16_1);//in 16-bit format keep only 0,4,8,12 and discard the others


			//2nd col iteration

			//multiply by the mask
			m0=_mm256_maddubs_epi16(r0,cx1);

			//first horizontal add
			m1=_mm256_srli_si256(m0,2);
			m0=_mm256_add_epi16(m0,m1);//add

			//second horizontal add
			m1=_mm256_srli_si256(m0,4);
			m0=_mm256_add_epi16(m0,m1);//add

			output2=_mm256_and_si256(m0,mask_16_1);//in 16-bit format keep only 0,4,8,12 and discard the others


			//3rd col iteration

			//multiply by the mask
			m0=_mm256_maddubs_epi16(r0,cx2);

			//first horizontal add (complex shift is needed)
			m3=_mm256_srli_si256(m0,2);
			m4=_mm256_and_si256(m0,mask3);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_slli_si256(m4,14);
			m3=_mm256_add_epi16(m3,m4);

			m0=_mm256_add_epi16(m0,m3);//add

			//second horizontal add
			m1=_mm256_srli_si256(m0,4);
			m0=_mm256_add_epi16(m0,m1);//add

			output3=_mm256_and_si256(m0,mask_16_2);//in 16-bit format keep only 1,5,9 and discard the others


			//4th col iteration

			//multiply by the mask
			m0=_mm256_maddubs_epi16(r0,cx3);

			//first horizontal add (complex shift is needed)
			m3=_mm256_srli_si256(m0,2);
			m4=_mm256_and_si256(m0,mask3);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_slli_si256(m4,14);
			m3=_mm256_add_epi16(m3,m4);

			m0=_mm256_add_epi16(m0,m3);//add

			//second horizontal add
			m1=_mm256_srli_si256(m0,4);
			m0=_mm256_add_epi16(m0,m1);//add

			output4=_mm256_and_si256(m0,mask_16_2);//in 16-bit format keep only 1,5,9 and discard the others


			//5th col iteration

			//multiply by the mask
			m0=_mm256_maddubs_epi16(r0,cx4);

			//first horizontal add
			m1=_mm256_srli_si256(m0,2);
			m0=_mm256_add_epi16(m0,m1);//add

			//second horizontal add (complex shift is needed)
			m3=_mm256_srli_si256(m0,4);
			m4=_mm256_and_si256(m0,mask3);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_slli_si256(m4,12);
			m3=_mm256_add_epi16(m3,m4);

			m0=_mm256_add_epi16(m0,m3);//add
			m0=_mm256_and_si256(m0,mask_16_3);//in 16-bit format keep only 2,6,10 and discard the others


			//MERGE output1 with output5 and DIVIDE
			output1=_mm256_add_epi16(m0,output1);
			output1 = division(division_case,output1,f);


			//6th col iteration

			//multiply by the mask
			m0=_mm256_maddubs_epi16(r0,cx5);

			//first horizontal add
			m1=_mm256_srli_si256(m0,2);
			m0=_mm256_add_epi16(m0,m1);//add

			//second horizontal add (complex shift is needed)
			m3=_mm256_srli_si256(m0,4);
			m4=_mm256_and_si256(m0,mask3);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_slli_si256(m4,12);
			m3=_mm256_add_epi16(m3,m4);

			m0=_mm256_add_epi16(m0,m3);//add
			m0=_mm256_and_si256(m0,mask_16_3);//in 16-bit format keep only 2,6,10 and discard the others

			//MERGE output1 with output5 and DIVIDE
			output2=_mm256_add_epi16(m0,output2);
			output2 = division(division_case,output2,f);


			//7th col iteration

			//multiply by the mask
			m0=_mm256_maddubs_epi16(r0,cx6);

			//shift so as the 1st useful element is on position zero (complex shift is needed)
			m3=_mm256_srli_si256(m0,6);
			m4=_mm256_and_si256(m0,mask_16_4);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_slli_si256(m4,10);
			m0=_mm256_add_epi16(m3,m4);

			//first horizontal add
			m1=_mm256_srli_si256(m0,2);
			m0=_mm256_add_epi16(m0,m1);//add

			//second horizontal add
			m1=_mm256_srli_si256(m0,4);
			m0=_mm256_add_epi16(m0,m1);//add

			m0=_mm256_and_si256(m0,mask_16_1);//in 16-bit format keep only 0,4,8,12 and discard the others
			m0=_mm256_slli_si256(m0,6);//in 16-bit output is on 3,7,11

			//MERGE output1 with output5 and DIVIDE
			output3=_mm256_add_epi16(m0,output3);
			output3 = division(division_case,output3,f);

			//8th col iteration

			//multiply by the mask
			m0=_mm256_maddubs_epi16(r0,cx7);

			//shift so as the 1st useful element is on position zero (complex shift is needed)
			m3=_mm256_srli_si256(m0,6);
			m4=_mm256_and_si256(m0,mask_16_4);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_slli_si256(m4,10);
			m0=_mm256_add_epi16(m3,m4);

			//first horizontal add
			m1=_mm256_srli_si256(m0,2);
			m0=_mm256_add_epi16(m0,m1);//add

			//second horizontal add
			m1=_mm256_srli_si256(m0,4);
			m0=_mm256_add_epi16(m0,m1);//add

			m0=_mm256_and_si256(m0,mask_16_1);//in 16-bit format keep only 0,4,8,12 and discard the others
			m0=_mm256_slli_si256(m0,6);//in 16-bit output is on 3,7,11

			//MERGE output1 with output5 and DIVIDE
			output4=_mm256_add_epi16(m0,output4);
			output4 = division(division_case,output4,f);

			//blend output1, output2, output3, output4 into one register
			output2 = _mm256_slli_si256(output2,1);
			output1 = _mm256_add_epi8(output1,output2);

			output4 = _mm256_slli_si256(output4,1);
			output3 = _mm256_add_epi8(output3,output4);

			output1 = _mm256_add_epi8(output1,output3);

	switch (REMINDER_ITERATIONS){
	case 1:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		  break;
	case 2:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		  break;
	case 3:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		  break;
	case 4:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output1,3);
		  break;
	case 5:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output1,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output1,4);
		  break;
	case 6:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output1,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output1,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output1,5);
		  break;
	case 7:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output1,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output1,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output1,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output1,6);
		  break;
	case 8:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output1,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output1,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output1,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output1,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output1,7);
		  break;
	case 9:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output1,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output1,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output1,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output1,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output1,7);
		filt[row][col+8] = (unsigned char) _mm256_extract_epi8(output1,8);
		  break;
	case 10:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output1,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output1,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output1,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output1,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output1,7);
		filt[row][col+8] = (unsigned char) _mm256_extract_epi8(output1,8);
		filt[row][col+9] = (unsigned char) _mm256_extract_epi8(output1,9);
		  break;
	case 11:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output1,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output1,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output1,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output1,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output1,7);
		filt[row][col+8] = (unsigned char) _mm256_extract_epi8(output1,8);
		filt[row][col+9] = (unsigned char) _mm256_extract_epi8(output1,9);
		filt[row][col+10] = (unsigned char) _mm256_extract_epi8(output1,10);
		  break;
	case 12:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output1,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output1,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output1,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output1,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output1,7);
		filt[row][col+8] = (unsigned char) _mm256_extract_epi8(output1,8);
		filt[row][col+9] = (unsigned char) _mm256_extract_epi8(output1,9);
		filt[row][col+10] = (unsigned char) _mm256_extract_epi8(output1,10);
		filt[row][col+11] = (unsigned char) _mm256_extract_epi8(output1,11);
		  break;
	case 13:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output1,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output1,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output1,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output1,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output1,7);
		filt[row][col+8] = (unsigned char) _mm256_extract_epi8(output1,8);
		filt[row][col+9] = (unsigned char) _mm256_extract_epi8(output1,9);
		filt[row][col+10] = (unsigned char) _mm256_extract_epi8(output1,10);
		filt[row][col+11] = (unsigned char) _mm256_extract_epi8(output1,11);
		filt[row][col+12] = (unsigned char) _mm256_extract_epi8(output1,12);
		  break;
	case 14:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output1,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output1,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output1,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output1,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output1,7);
		filt[row][col+8] = (unsigned char) _mm256_extract_epi8(output1,8);
		filt[row][col+9] = (unsigned char) _mm256_extract_epi8(output1,9);
		filt[row][col+10] = (unsigned char) _mm256_extract_epi8(output1,10);
		filt[row][col+11] = (unsigned char) _mm256_extract_epi8(output1,11);
		filt[row][col+12] = (unsigned char) _mm256_extract_epi8(output1,12);
		filt[row][col+13] = (unsigned char) _mm256_extract_epi8(output1,13);
		  break;
	case 15:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output1,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output1,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output1,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output1,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output1,7);
		filt[row][col+8] = (unsigned char) _mm256_extract_epi8(output1,8);
		filt[row][col+9] = (unsigned char) _mm256_extract_epi8(output1,9);
		filt[row][col+10] = (unsigned char) _mm256_extract_epi8(output1,10);
		filt[row][col+11] = (unsigned char) _mm256_extract_epi8(output1,11);
		filt[row][col+12] = (unsigned char) _mm256_extract_epi8(output1,12);
		filt[row][col+13] = (unsigned char) _mm256_extract_epi8(output1,13);
		filt[row][col+14] = (unsigned char) _mm256_extract_epi8(output1,14);
		  break;
	case 16:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
		  break;
	case 17:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
		  break;
	case 18:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
		  break;
	case 19:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
		  break;
	case 20:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output1,19);
		  break;
	case 21:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output1,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output1,20);
		  break;
	case 22:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output1,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output1,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output1,21);
		break;
	case 23:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output1,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output1,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output1,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output1,22);
		  break;
	case 24:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output1,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output1,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output1,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output1,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output1,23);
		  break;
	case 25:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output1,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output1,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output1,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output1,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output1,23);
	filt[row][col+24] = (unsigned char) _mm256_extract_epi8(output1,24);
		  break;
	case 26:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output1,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output1,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output1,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output1,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output1,23);
	filt[row][col+24] = (unsigned char) _mm256_extract_epi8(output1,24);
	filt[row][col+25] = (unsigned char) _mm256_extract_epi8(output1,25);
		  break;

	default: //REMINDER IS EITHER 27,28,29,30 OR 31
	  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
		filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
		filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
		filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
		filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output1,19);
		filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output1,20);
		filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output1,21);
		filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output1,22);
		filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output1,23);
		filt[row][col+24] = (unsigned char) _mm256_extract_epi8(output1,24);
		filt[row][col+25] = (unsigned char) _mm256_extract_epi8(output1,25);

		loop_reminder_7x7_16_blur(frame1,filt,N,row, col+26, (REMINDER_ITERATIONS%26), division_case, mask_vector_x, mask_vector_y, f, divisor_xy);

 	}


}





void Gaussian_Blur_7x7_16_separable_blocking_old(unsigned char **frame1,unsigned char **filt,const unsigned int M, const unsigned int N, signed char *kernel_y, signed char *kernel_x, const unsigned short int divisor_xy){

const signed char fx0=kernel_x[0];
const signed char fx1=kernel_x[1];
const signed char fx2=kernel_x[2];
const signed char fx3=kernel_x[3];
const signed char fx4=kernel_x[4];
const signed char fx5=kernel_x[5];
const signed char fx6=kernel_x[6];

const signed char fy0=kernel_y[0];
const signed char fy1=kernel_y[1];
const signed char fy2=kernel_y[2];
const signed char fy3=kernel_y[3];
const signed char fy4=kernel_y[4];
const signed char fy5=kernel_y[5];
const signed char fy6=kernel_y[6];



const signed char mask_vector_x[8][32] __attribute__((aligned(64))) ={
	{fx0,fx1,fx2,fx3,fx4,fx5,fx6,0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,0},
	{0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,0,fx0,fx1,fx2,fx3,fx4,fx5,fx6},
	{0,0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,0,0,0,0,0,0,0},
	{0,0,0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,0,0,0,0,0,0},
	{0,0,0,0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,0,0,0,0,0},
	{0,0,0,0,0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,0,0,0,0},
	{0,0,0,0,0,0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,0,0,0},
	{0,0,0,0,0,0,0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,0,0},
};

const signed char mask_vector_y[14][32] __attribute__((aligned(64))) ={
	{fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0},
	{fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0},
	{fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0},
	{fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0},
	{fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0},
	{fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0},
	{fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0},

	{0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0},
	{0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1},
	{0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2},
	{0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3},
	{0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4},
	{0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5},
	{0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6},
};

	const __m256i cx0=_mm256_load_si256( (__m256i *) &mask_vector_x[0]);
	const __m256i cx1=_mm256_load_si256( (__m256i *) &mask_vector_x[1]);
	const __m256i cx2=_mm256_load_si256( (__m256i *) &mask_vector_x[2]);
	const __m256i cx3=_mm256_load_si256( (__m256i *) &mask_vector_x[3]);
	const __m256i cx4=_mm256_load_si256( (__m256i *) &mask_vector_x[4]);
	const __m256i cx5=_mm256_load_si256( (__m256i *) &mask_vector_x[5]);
	const __m256i cx6=_mm256_load_si256( (__m256i *) &mask_vector_x[6]);
	const __m256i cx7=_mm256_load_si256( (__m256i *) &mask_vector_x[7]);


	const __m256i cy0=_mm256_load_si256( (__m256i *) &mask_vector_y[0]);
	const __m256i cy1=_mm256_load_si256( (__m256i *) &mask_vector_y[1]);
	const __m256i cy2=_mm256_load_si256( (__m256i *) &mask_vector_y[2]);
	const __m256i cy3=_mm256_load_si256( (__m256i *) &mask_vector_y[3]);
	const __m256i cy4=_mm256_load_si256( (__m256i *) &mask_vector_y[4]);
	const __m256i cy5=_mm256_load_si256( (__m256i *) &mask_vector_y[5]);
	const __m256i cy6=_mm256_load_si256( (__m256i *) &mask_vector_y[6]);

	const __m256i cy0_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[7]);
	const __m256i cy1_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[8]);
	const __m256i cy2_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[9]);
	const __m256i cy3_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[10]);
	const __m256i cy4_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[11]);
	const __m256i cy5_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[12]);
	const __m256i cy6_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[13]);


	const __m256i mask3  = _mm256_set_epi16(0,0,0,0,0,0,0,65535,0,0,0,0,0,0,0,0);
	const __m256i mask_prelude  = _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0);

//	const __m256i mask_even_16  = _mm256_set_epi16(0,0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff);
//	const __m256i mask_odd_16  = _mm256_set_epi16(0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff,0);
	const __m256i mask_16_1  = _mm256_set_epi16(0,0,0,0xffff,0,0,0,0xffff,0,0,0,0xffff,0,0,0,0xffff);
	const __m256i mask_16_2  = _mm256_set_epi16(0,0,0,0,0,0,0xffff,0,0,0,0xffff,0,0,0,0xffff,0);
	const __m256i mask_16_3  = _mm256_set_epi16(0,0,0,0,0,0xffff,0,0,0,0xffff,0,0,0,0xffff,0,0);
	const __m256i mask_16_4  = _mm256_set_epi16(0,0,0,0,0,0xffff,0xffff,0xffff,0,0,0,0,0,0,0,0);


//	const __m256i mask_even_32  = _mm256_set_epi32(0,0xffffffff,0,0xffffffff,0,0xffffffff,0,0xffffffff);
//	const __m256i mask_odd_32  = _mm256_set_epi32(0xffffffff,0,0xffffffff,0,0xffffffff,0,0xffffffff,0);
	//const __m256i mask4  = _mm256_set_epi32(0,0,0,0xffffffff,0,0,0,0);


	//REMINDER_ITERATIONS=[6,31]
	const unsigned int REMINDER_ITERATIONS =  (M-((((M-32)/26)*26)+26)); //M-(last_col_value+26)

	//printf("\nREMINDER_ITERATIONS=%d",REMINDER_ITERATIONS);


	const unsigned int division_case=prepare_for_division(divisor_xy); //determine which is the division case (A, B or C)
	const __m256i f = _mm256_load_si256( (__m256i *) &f_vector[0]);// initialize the division vector
	//const __m256i ones=_mm256_set1_epi16(1);

//	const unsigned int division_case_x=prepare_for_division_32(divisor_x); //determine which is the division case (A, B or C)
//	const __m256i f_x = _mm256_load_si256( (__m256i *) &f_vector_x[0]);// initialize the division vector
//printf("\n%d %d",division_case,b);


#pragma omp parallel
{

unsigned int row,col;
__m256i r0,r1,r2,r3,r4,r5,r6,r7,m0,m1,m2,m3,m4,m5,m6,m7,even,odd;
__m256i output1,output2,output3,output4;



/*---------------------- Gaussian Blur ---------------------------------*/

    #pragma omp for schedule(static)
	for (row = 1; row < N-2; row+=2) {

		if (row==1){
				for (col = 0; col <= M-32; col+=26){
				r0=prelude_7x7_16_Ymask_1(0, col, frame1, mask_vector_y, division_case, f, mask_prelude);
				prelude_7x7_16_Xmask(filt,0,col, r0, mask_vector_x,  division_case,f);
				}
				loop_reminder_7x7_16_blur(frame1,filt,N,0, col, REMINDER_ITERATIONS, division_case, mask_vector_x, mask_vector_y, f, divisor_xy);

				for (col = 0; col <= M-32; col+=26){
				r0=prelude_7x7_16_Ymask_2(0, col, frame1, mask_vector_y, division_case, f, mask_prelude);
				prelude_7x7_16_Xmask(filt,1,col, r0, mask_vector_x,  division_case,f);
				}
				loop_reminder_7x7_16_blur(frame1,filt,N,1, col, REMINDER_ITERATIONS, division_case, mask_vector_x, mask_vector_y, f, divisor_xy);

				for (col = 0; col <= M-32; col+=26){
				r0=prelude_7x7_16_Ymask_3(0, col, frame1, mask_vector_y, division_case, f, mask_prelude);
				prelude_7x7_16_Xmask(filt,2,col, r0, mask_vector_x,  division_case,f);
				}
				loop_reminder_7x7_16_blur(frame1,filt,N,2, col, REMINDER_ITERATIONS, division_case, mask_vector_x, mask_vector_y, f, divisor_xy);
		}

		else if (row==N-3){


			for (col = 0; col <= M-32; col+=26){
				r0=prelude_7x7_16_Ymask_3(N-6, col, frame1, mask_vector_y, division_case, f, mask_prelude);
				prelude_7x7_16_Xmask(filt,N-3,col, r0, mask_vector_x,  division_case,f);
			}
			loop_reminder_7x7_16_blur(frame1,filt,N,N-3, col, REMINDER_ITERATIONS, division_case, mask_vector_x, mask_vector_y, f, divisor_xy);

			for (col = 0; col <= M-32; col+=26){
				r0=prelude_7x7_16_Ymask_2(N-5, col, frame1, mask_vector_y, division_case, f, mask_prelude);
				prelude_7x7_16_Xmask(filt,N-2,col, r0, mask_vector_x,  division_case,f);
			}
			loop_reminder_7x7_16_blur(frame1,filt,N,N-2, col, REMINDER_ITERATIONS, division_case, mask_vector_x, mask_vector_y, f, divisor_xy);

			for (col = 0; col <= M-32; col+=26){
				r0=prelude_7x7_16_Ymask_1(N-4, col, frame1, mask_vector_y, division_case, f, mask_prelude);
				prelude_7x7_16_Xmask(filt,N-1,col, r0, mask_vector_x,  division_case,f);
			}
			loop_reminder_7x7_16_blur(frame1,filt,N,N-1, col, REMINDER_ITERATIONS, division_case, mask_vector_x, mask_vector_y, f, divisor_xy);

		}

		else if (row==N-4){

			  for (col = 0; col <= M-32; col+=26){

				  if (col==0){

						//load the 7 rows
						r0=_mm256_loadu_si256( (__m256i *) &frame1[row-3][0]);
						r1=_mm256_loadu_si256( (__m256i *) &frame1[row-2][0]);
						r2=_mm256_loadu_si256( (__m256i *) &frame1[row-1][0]);
						r3=_mm256_loadu_si256( (__m256i *) &frame1[row][0]);
						r4=_mm256_loadu_si256( (__m256i *) &frame1[row+1][0]);
						r5=_mm256_loadu_si256( (__m256i *) &frame1[row+2][0]);
						r6=_mm256_loadu_si256( (__m256i *) &frame1[row+3][0]);


				  			//START - extra code needed for prelude
				  					m0=_mm256_slli_si256(r0,3);//shift 3 elements left - equivalent to filling with two zeros inthe beginning
				  					m1=_mm256_slli_si256(r1,3);//shift 3 elements left - equivalent to filling with two zeros inthe beginning
				  					m2=_mm256_slli_si256(r2,3);//shift 3 elements left - equivalent to filling with two zeros inthe beginning
				  					m3=_mm256_slli_si256(r3,3);//shift 3 elements left - equivalent to filling with two zeros inthe beginning
				  					m4=_mm256_slli_si256(r4,3);//shift 3 elements left - equivalent to filling with two zeros inthe beginning
				  					m5=_mm256_slli_si256(r5,3);//shift 3 elements left - equivalent to filling with two zeros inthe beginning
				  					m6=_mm256_slli_si256(r6,3);//shift 3 elements left - equivalent to filling with two zeros inthe beginning

				  					r0=_mm256_and_si256(r0,mask_prelude);
				  					r0=_mm256_permute2f128_si256(r0,r0,1);
				  					r0=_mm256_srli_si256(r0,13);//shift 14 elements
				  					r0=_mm256_add_epi16(m0,r0);

				  					r1=_mm256_and_si256(r1,mask_prelude);
				  					r1=_mm256_permute2f128_si256(r1,r1,1);
				  					r1=_mm256_srli_si256(r1,13);//shift 14 elements
				  					r1=_mm256_add_epi16(m1,r1);

				  					r2=_mm256_and_si256(r2,mask_prelude);
				  					r2=_mm256_permute2f128_si256(r2,r2,1);
				  					r2=_mm256_srli_si256(r2,13);//shift 14 elements
				  					r2=_mm256_add_epi16(m2,r2);

				  					r3=_mm256_and_si256(r3,mask_prelude);
				  					r3=_mm256_permute2f128_si256(r3,r3,1);
				  					r3=_mm256_srli_si256(r3,13);//shift 14 elements
				  					r3=_mm256_add_epi16(m3,r3);

				  					r4=_mm256_and_si256(r4,mask_prelude);
				  					r4=_mm256_permute2f128_si256(r4,r4,1);
				  					r4=_mm256_srli_si256(r4,13);//shift 14 elements
				  					r4=_mm256_add_epi16(m4,r4);

				  					r5=_mm256_and_si256(r5,mask_prelude);
				  					r5=_mm256_permute2f128_si256(r5,r5,1);
				  					r5=_mm256_srli_si256(r5,13);//shift 14 elements
				  					r5=_mm256_add_epi16(m5,r5);

				  					r6=_mm256_and_si256(r6,mask_prelude);
				  					r6=_mm256_permute2f128_si256(r6,r6,1);
				  					r6=_mm256_srli_si256(r6,13);//shift 14 elements
				  					r6=_mm256_add_epi16(m6,r6);

				  			//END - extra code needed for prelude
				  		}
				  else {
						//load the 7 rows
						r0=_mm256_loadu_si256( (__m256i *) &frame1[row-3][col-3]);
						r1=_mm256_loadu_si256( (__m256i *) &frame1[row-2][col-3]);
						r2=_mm256_loadu_si256( (__m256i *) &frame1[row-1][col-3]);
						r3=_mm256_loadu_si256( (__m256i *) &frame1[row][col-3]);
						r4=_mm256_loadu_si256( (__m256i *) &frame1[row+1][col-3]);
						r5=_mm256_loadu_si256( (__m256i *) &frame1[row+2][col-3]);
						r6=_mm256_loadu_si256( (__m256i *) &frame1[row+3][col-3]);

				  }

				  //--------------------y filter begins--------------------

					//multiply with the mask
						m0=_mm256_maddubs_epi16(r0,cy0);
						m1=_mm256_maddubs_epi16(r1,cy1);
						m2=_mm256_maddubs_epi16(r2,cy2);
						m3=_mm256_maddubs_epi16(r3,cy3);
						m4=_mm256_maddubs_epi16(r4,cy4);
						m5=_mm256_maddubs_epi16(r5,cy5);
						m6=_mm256_maddubs_epi16(r6,cy6);

						//vertical add
						m0=_mm256_add_epi16(m0,m1);
						m0=_mm256_add_epi16(m0,m2);
						m0=_mm256_add_epi16(m0,m3);
						m0=_mm256_add_epi16(m0,m4);
						m0=_mm256_add_epi16(m0,m5);
						m0=_mm256_add_epi16(m0,m6);

						even=division(division_case,m0,f);//even results

						//multiply with the mask
						m0=_mm256_maddubs_epi16(r0,cy0_sh1);
						m1=_mm256_maddubs_epi16(r1,cy1_sh1);
						m2=_mm256_maddubs_epi16(r2,cy2_sh1);
						m3=_mm256_maddubs_epi16(r3,cy3_sh1);
						m4=_mm256_maddubs_epi16(r4,cy4_sh1);
						m5=_mm256_maddubs_epi16(r5,cy5_sh1);
						m6=_mm256_maddubs_epi16(r6,cy6_sh1);

						//vertical add
						m0=_mm256_add_epi16(m0,m1);
						m0=_mm256_add_epi16(m0,m2);
						m0=_mm256_add_epi16(m0,m3);
						m0=_mm256_add_epi16(m0,m4);
						m0=_mm256_add_epi16(m0,m5);
						m0=_mm256_add_epi16(m0,m6);


						odd=division(division_case,m0,f);//odd results

						//pack to one register
						odd=_mm256_slli_si256(odd,1); //shift one position right
						r0=_mm256_add_epi8(even,odd); //add the odd with the even

					//y filter ends - r0 has now the data to be processed by x filter

				// col iteration computes output pixels of   0,8,16,24
				// col+1 iteration computes output pixels of 1,9,17,25
				// col+2 iteration computes output pixels of 2,10,18
				// col+3 iteration computes output pixels of 3,11,19
				// col+4 iteration computes output pixels of 4,12,20
				// col+5 iteration computes output pixels of 5,13,21
				// col+6 iteration computes output pixels of 6,14,22
				// col+7 iteration computes output pixels of 7,15,23
				//afterwards, col becomes 26 and repeat the above process

					//1st col iteration

					//multiply by the mask
					m0=_mm256_maddubs_epi16(r0,cx0);

					//first horizontal add
					m1=_mm256_srli_si256(m0,2);
					m0=_mm256_add_epi16(m0,m1);//add

					//second horizontal add
					m1=_mm256_srli_si256(m0,4);
					m0=_mm256_add_epi16(m0,m1);//add

					output1=_mm256_and_si256(m0,mask_16_1);//in 16-bit format keep only 0,4,8,12 and discard the others


					//2nd col iteration

					//multiply by the mask
					m0=_mm256_maddubs_epi16(r0,cx1);

					//first horizontal add
					m1=_mm256_srli_si256(m0,2);
					m0=_mm256_add_epi16(m0,m1);//add

					//second horizontal add
					m1=_mm256_srli_si256(m0,4);
					m0=_mm256_add_epi16(m0,m1);//add

					output2=_mm256_and_si256(m0,mask_16_1);//in 16-bit format keep only 0,4,8,12 and discard the others


					//3rd col iteration

					//multiply by the mask
					m0=_mm256_maddubs_epi16(r0,cx2);

					//first horizontal add (complex shift is needed)
					m3=_mm256_srli_si256(m0,2);
					m4=_mm256_and_si256(m0,mask3);
					m4=_mm256_permute2f128_si256(m4,m4,1);
					m4=_mm256_slli_si256(m4,14);
					m3=_mm256_add_epi16(m3,m4);

					m0=_mm256_add_epi16(m0,m3);//add

					//second horizontal add
					m1=_mm256_srli_si256(m0,4);
					m0=_mm256_add_epi16(m0,m1);//add

					output3=_mm256_and_si256(m0,mask_16_2);//in 16-bit format keep only 1,5,9 and discard the others


					//4th col iteration

					//multiply by the mask
					m0=_mm256_maddubs_epi16(r0,cx3);

					//first horizontal add (complex shift is needed)
					m3=_mm256_srli_si256(m0,2);
					m4=_mm256_and_si256(m0,mask3);
					m4=_mm256_permute2f128_si256(m4,m4,1);
					m4=_mm256_slli_si256(m4,14);
					m3=_mm256_add_epi16(m3,m4);

					m0=_mm256_add_epi16(m0,m3);//add

					//second horizontal add
					m1=_mm256_srli_si256(m0,4);
					m0=_mm256_add_epi16(m0,m1);//add

					output4=_mm256_and_si256(m0,mask_16_2);//in 16-bit format keep only 1,5,9 and discard the others


					//5th col iteration

					//multiply by the mask
					m0=_mm256_maddubs_epi16(r0,cx4);

					//first horizontal add
					m1=_mm256_srli_si256(m0,2);
					m0=_mm256_add_epi16(m0,m1);//add

					//second horizontal add (complex shift is needed)
					m3=_mm256_srli_si256(m0,4);
					m4=_mm256_and_si256(m0,mask3);
					m4=_mm256_permute2f128_si256(m4,m4,1);
					m4=_mm256_slli_si256(m4,12);
					m3=_mm256_add_epi16(m3,m4);

					m0=_mm256_add_epi16(m0,m3);//add
					m0=_mm256_and_si256(m0,mask_16_3);//in 16-bit format keep only 2,6,10 and discard the others


					//MERGE output1 with output5 and DIVIDE
					output1=_mm256_add_epi16(m0,output1);
					output1 = division(division_case,output1,f);


					//6th col iteration

					//multiply by the mask
					m0=_mm256_maddubs_epi16(r0,cx5);

					//first horizontal add
					m1=_mm256_srli_si256(m0,2);
					m0=_mm256_add_epi16(m0,m1);//add

					//second horizontal add (complex shift is needed)
					m3=_mm256_srli_si256(m0,4);
					m4=_mm256_and_si256(m0,mask3);
					m4=_mm256_permute2f128_si256(m4,m4,1);
					m4=_mm256_slli_si256(m4,12);
					m3=_mm256_add_epi16(m3,m4);

					m0=_mm256_add_epi16(m0,m3);//add
					m0=_mm256_and_si256(m0,mask_16_3);//in 16-bit format keep only 2,6,10 and discard the others

					//MERGE output1 with output5 and DIVIDE
					output2=_mm256_add_epi16(m0,output2);
					output2 = division(division_case,output2,f);


					//7th col iteration

					//multiply by the mask
					m0=_mm256_maddubs_epi16(r0,cx6);

					//shift so as the 1st useful element is on position zero (complex shift is needed)
					m3=_mm256_srli_si256(m0,6);
					m4=_mm256_and_si256(m0,mask_16_4);
					m4=_mm256_permute2f128_si256(m4,m4,1);
					m4=_mm256_slli_si256(m4,10);
					m0=_mm256_add_epi16(m3,m4);

					//first horizontal add
					m1=_mm256_srli_si256(m0,2);
					m0=_mm256_add_epi16(m0,m1);//add

					//second horizontal add
					m1=_mm256_srli_si256(m0,4);
					m0=_mm256_add_epi16(m0,m1);//add

					m0=_mm256_and_si256(m0,mask_16_1);//in 16-bit format keep only 0,4,8,12 and discard the others
					m0=_mm256_slli_si256(m0,6);//in 16-bit output is on 3,7,11

					//MERGE output1 with output5 and DIVIDE
					output3=_mm256_add_epi16(m0,output3);
					output3 = division(division_case,output3,f);

					//8th col iteration

					//multiply by the mask
					m0=_mm256_maddubs_epi16(r0,cx7);

					//shift so as the 1st useful element is on position zero (complex shift is needed)
					m3=_mm256_srli_si256(m0,6);
					m4=_mm256_and_si256(m0,mask_16_4);
					m4=_mm256_permute2f128_si256(m4,m4,1);
					m4=_mm256_slli_si256(m4,10);
					m0=_mm256_add_epi16(m3,m4);

					//first horizontal add
					m1=_mm256_srli_si256(m0,2);
					m0=_mm256_add_epi16(m0,m1);//add

					//second horizontal add
					m1=_mm256_srli_si256(m0,4);
					m0=_mm256_add_epi16(m0,m1);//add

					m0=_mm256_and_si256(m0,mask_16_1);//in 16-bit format keep only 0,4,8,12 and discard the others
					m0=_mm256_slli_si256(m0,6);//in 16-bit output is on 3,7,11

					//MERGE output1 with output5 and DIVIDE
					output4=_mm256_add_epi16(m0,output4);
					output4 = division(division_case,output4,f);

					//blend output1, output2, output3, output4 into one register
					output2 = _mm256_slli_si256(output2,1);
					output1 = _mm256_add_epi8(output1,output2);

					output4 = _mm256_slli_si256(output4,1);
					output3 = _mm256_add_epi8(output3,output4);

					output1 = _mm256_add_epi8(output1,output3);

					_mm256_storeu_si256( (__m256i *) &filt[row][col],output1 );
				}
			  loop_reminder_7x7_16_blur(frame1,filt,N,row, col, REMINDER_ITERATIONS, division_case, mask_vector_x, mask_vector_y, f, divisor_xy);


				for (col = 0; col <= M-32; col+=26){
					r0=prelude_7x7_16_Ymask_3(N-6, col, frame1, mask_vector_y, division_case, f, mask_prelude);
					prelude_7x7_16_Xmask(filt,N-3,col, r0, mask_vector_x,  division_case,f);
				}
				loop_reminder_7x7_16_blur(frame1,filt,N,N-3, col, REMINDER_ITERATIONS, division_case, mask_vector_x, mask_vector_y, f, divisor_xy);

				for (col = 0; col <= M-32; col+=26){
					r0=prelude_7x7_16_Ymask_2(N-5, col, frame1, mask_vector_y, division_case, f, mask_prelude);
					prelude_7x7_16_Xmask(filt,N-2,col, r0, mask_vector_x,  division_case,f);
				}
				loop_reminder_7x7_16_blur(frame1,filt,N,N-2, col, REMINDER_ITERATIONS, division_case, mask_vector_x, mask_vector_y, f, divisor_xy);

				for (col = 0; col <= M-32; col+=26){
					r0=prelude_7x7_16_Ymask_1(N-4, col, frame1, mask_vector_y, division_case, f, mask_prelude);
					prelude_7x7_16_Xmask(filt,N-1,col, r0, mask_vector_x,  division_case,f);
				}
				loop_reminder_7x7_16_blur(frame1,filt,N,N-1, col, REMINDER_ITERATIONS, division_case, mask_vector_x, mask_vector_y, f, divisor_xy);

		}
	  else { //main loop


	  for (col = 0; col <= M-32; col+=26){

		  if (col==0){

				//load the 7 rows
				r0=_mm256_loadu_si256( (__m256i *) &frame1[row-3][0]);
				r1=_mm256_loadu_si256( (__m256i *) &frame1[row-2][0]);
				r2=_mm256_loadu_si256( (__m256i *) &frame1[row-1][0]);
				r3=_mm256_loadu_si256( (__m256i *) &frame1[row][0]);
				r4=_mm256_loadu_si256( (__m256i *) &frame1[row+1][0]);
				r5=_mm256_loadu_si256( (__m256i *) &frame1[row+2][0]);
				r6=_mm256_loadu_si256( (__m256i *) &frame1[row+3][0]);
				r7=_mm256_loadu_si256( (__m256i *) &frame1[row+4][0]);


		  			//START - extra code needed for prelude
		  					m0=_mm256_slli_si256(r0,3);//shift 3 elements left - equivalent to filling with two zeros inthe beginning
		  					m1=_mm256_slli_si256(r1,3);//shift 3 elements left - equivalent to filling with two zeros inthe beginning
		  					m2=_mm256_slli_si256(r2,3);//shift 3 elements left - equivalent to filling with two zeros inthe beginning
		  					m3=_mm256_slli_si256(r3,3);//shift 3 elements left - equivalent to filling with two zeros inthe beginning
		  					m4=_mm256_slli_si256(r4,3);//shift 3 elements left - equivalent to filling with two zeros inthe beginning
		  					m5=_mm256_slli_si256(r5,3);//shift 3 elements left - equivalent to filling with two zeros inthe beginning
		  					m6=_mm256_slli_si256(r6,3);//shift 3 elements left - equivalent to filling with two zeros inthe beginning
		  					m7=_mm256_slli_si256(r7,3);//shift 3 elements left - equivalent to filling with two zeros inthe beginning


		  					r0=_mm256_and_si256(r0,mask_prelude);
		  					r0=_mm256_permute2f128_si256(r0,r0,1);
		  					r0=_mm256_srli_si256(r0,13);//shift 14 elements
		  					r0=_mm256_add_epi16(m0,r0);

		  					r1=_mm256_and_si256(r1,mask_prelude);
		  					r1=_mm256_permute2f128_si256(r1,r1,1);
		  					r1=_mm256_srli_si256(r1,13);//shift 14 elements
		  					r1=_mm256_add_epi16(m1,r1);

		  					r2=_mm256_and_si256(r2,mask_prelude);
		  					r2=_mm256_permute2f128_si256(r2,r2,1);
		  					r2=_mm256_srli_si256(r2,13);//shift 14 elements
		  					r2=_mm256_add_epi16(m2,r2);

		  					r3=_mm256_and_si256(r3,mask_prelude);
		  					r3=_mm256_permute2f128_si256(r3,r3,1);
		  					r3=_mm256_srli_si256(r3,13);//shift 14 elements
		  					r3=_mm256_add_epi16(m3,r3);

		  					r4=_mm256_and_si256(r4,mask_prelude);
		  					r4=_mm256_permute2f128_si256(r4,r4,1);
		  					r4=_mm256_srli_si256(r4,13);//shift 14 elements
		  					r4=_mm256_add_epi16(m4,r4);

		  					r5=_mm256_and_si256(r5,mask_prelude);
		  					r5=_mm256_permute2f128_si256(r5,r5,1);
		  					r5=_mm256_srli_si256(r5,13);//shift 14 elements
		  					r5=_mm256_add_epi16(m5,r5);

		  					r6=_mm256_and_si256(r6,mask_prelude);
		  					r6=_mm256_permute2f128_si256(r6,r6,1);
		  					r6=_mm256_srli_si256(r6,13);//shift 14 elements
		  					r6=_mm256_add_epi16(m6,r6);

		  					r7=_mm256_and_si256(r7,mask_prelude);
		  					r7=_mm256_permute2f128_si256(r7,r7,1);
		  					r7=_mm256_srli_si256(r7,13);//shift 14 elements
		  					r7=_mm256_add_epi16(m7,r7);

		  			//END - extra code needed for prelude
		  		}
		  else {
				//load the 7 rows
				r0=_mm256_loadu_si256( (__m256i *) &frame1[row-3][col-3]);
				r1=_mm256_loadu_si256( (__m256i *) &frame1[row-2][col-3]);
				r2=_mm256_loadu_si256( (__m256i *) &frame1[row-1][col-3]);
				r3=_mm256_loadu_si256( (__m256i *) &frame1[row][col-3]);
				r4=_mm256_loadu_si256( (__m256i *) &frame1[row+1][col-3]);
				r5=_mm256_loadu_si256( (__m256i *) &frame1[row+2][col-3]);
				r6=_mm256_loadu_si256( (__m256i *) &frame1[row+3][col-3]);
				r7=_mm256_loadu_si256( (__m256i *) &frame1[row+4][col-3]);

		  }

		  //--------------------y filter for row begins--------------------

			//multiply with the mask
				m0=_mm256_maddubs_epi16(r0,cy0);
				m1=_mm256_maddubs_epi16(r1,cy1);
				m2=_mm256_maddubs_epi16(r2,cy2);
				m3=_mm256_maddubs_epi16(r3,cy3);
				m4=_mm256_maddubs_epi16(r4,cy4);
				m5=_mm256_maddubs_epi16(r5,cy5);
				m6=_mm256_maddubs_epi16(r6,cy6);

				//vertical add
				m0=_mm256_add_epi16(m0,m1);
				m0=_mm256_add_epi16(m0,m2);
				m0=_mm256_add_epi16(m0,m3);
				m0=_mm256_add_epi16(m0,m4);
				m0=_mm256_add_epi16(m0,m5);
				m0=_mm256_add_epi16(m0,m6);

				even=division(division_case,m0,f);//even results

				//multiply with the mask
				m0=_mm256_maddubs_epi16(r0,cy0_sh1);
				m1=_mm256_maddubs_epi16(r1,cy1_sh1);
				m2=_mm256_maddubs_epi16(r2,cy2_sh1);
				m3=_mm256_maddubs_epi16(r3,cy3_sh1);
				m4=_mm256_maddubs_epi16(r4,cy4_sh1);
				m5=_mm256_maddubs_epi16(r5,cy5_sh1);
				m6=_mm256_maddubs_epi16(r6,cy6_sh1);

				//vertical add
				m0=_mm256_add_epi16(m0,m1);
				m0=_mm256_add_epi16(m0,m2);
				m0=_mm256_add_epi16(m0,m3);
				m0=_mm256_add_epi16(m0,m4);
				m0=_mm256_add_epi16(m0,m5);
				m0=_mm256_add_epi16(m0,m6);


				odd=division(division_case,m0,f);//odd results

				//pack to one register
				odd=_mm256_slli_si256(odd,1); //shift one position right
				r0=_mm256_add_epi8(even,odd); //add the odd with the even

			//y filter ends - r0 has now the data to be processed by x filter

				  //--------------------y filter for row begins--------------------

					//multiply with the mask
						m0=_mm256_maddubs_epi16(r1,cy0);
						m1=_mm256_maddubs_epi16(r2,cy1);
						m2=_mm256_maddubs_epi16(r3,cy2);
						m3=_mm256_maddubs_epi16(r4,cy3);
						m4=_mm256_maddubs_epi16(r5,cy4);
						m5=_mm256_maddubs_epi16(r6,cy5);
						m6=_mm256_maddubs_epi16(r7,cy6);

						//vertical add
						m0=_mm256_add_epi16(m0,m1);
						m0=_mm256_add_epi16(m0,m2);
						m0=_mm256_add_epi16(m0,m3);
						m0=_mm256_add_epi16(m0,m4);
						m0=_mm256_add_epi16(m0,m5);
						m0=_mm256_add_epi16(m0,m6);

						even=division(division_case,m0,f);//even results

						//multiply with the mask
						m0=_mm256_maddubs_epi16(r1,cy0_sh1);
						m1=_mm256_maddubs_epi16(r2,cy1_sh1);
						m2=_mm256_maddubs_epi16(r3,cy2_sh1);
						m3=_mm256_maddubs_epi16(r4,cy3_sh1);
						m4=_mm256_maddubs_epi16(r5,cy4_sh1);
						m5=_mm256_maddubs_epi16(r6,cy5_sh1);
						m6=_mm256_maddubs_epi16(r7,cy6_sh1);

						//vertical add
						m0=_mm256_add_epi16(m0,m1);
						m0=_mm256_add_epi16(m0,m2);
						m0=_mm256_add_epi16(m0,m3);
						m0=_mm256_add_epi16(m0,m4);
						m0=_mm256_add_epi16(m0,m5);
						m0=_mm256_add_epi16(m0,m6);


						odd=division(division_case,m0,f);//odd results

						//pack to one register
						odd=_mm256_slli_si256(odd,1); //shift one position right
						r1=_mm256_add_epi8(even,odd); //add the odd with the even

					//y filter ends - r0 has now the data to be processed by x filter

		// col iteration computes output pixels of   0,8,16,24
		// col+1 iteration computes output pixels of 1,9,17,25
		// col+2 iteration computes output pixels of 2,10,18
		// col+3 iteration computes output pixels of 3,11,19
		// col+4 iteration computes output pixels of 4,12,20
		// col+5 iteration computes output pixels of 5,13,21
		// col+6 iteration computes output pixels of 6,14,22
		// col+7 iteration computes output pixels of 7,15,23
		//afterwards, col becomes 26 and repeat the above process

			//1st col iteration

			//multiply by the mask
			m0=_mm256_maddubs_epi16(r0,cx0);

			//first horizontal add
			m1=_mm256_srli_si256(m0,2);
			m0=_mm256_add_epi16(m0,m1);//add

			//second horizontal add
			m1=_mm256_srli_si256(m0,4);
			m0=_mm256_add_epi16(m0,m1);//add

			output1=_mm256_and_si256(m0,mask_16_1);//in 16-bit format keep only 0,4,8,12 and discard the others


			//2nd col iteration

			//multiply by the mask
			m0=_mm256_maddubs_epi16(r0,cx1);

			//first horizontal add
			m1=_mm256_srli_si256(m0,2);
			m0=_mm256_add_epi16(m0,m1);//add

			//second horizontal add
			m1=_mm256_srli_si256(m0,4);
			m0=_mm256_add_epi16(m0,m1);//add

			output2=_mm256_and_si256(m0,mask_16_1);//in 16-bit format keep only 0,4,8,12 and discard the others


			//3rd col iteration

			//multiply by the mask
			m0=_mm256_maddubs_epi16(r0,cx2);

			//first horizontal add (complex shift is needed)
			m3=_mm256_srli_si256(m0,2);
			m4=_mm256_and_si256(m0,mask3);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_slli_si256(m4,14);
			m3=_mm256_add_epi16(m3,m4);

			m0=_mm256_add_epi16(m0,m3);//add

			//second horizontal add
			m1=_mm256_srli_si256(m0,4);
			m0=_mm256_add_epi16(m0,m1);//add

			output3=_mm256_and_si256(m0,mask_16_2);//in 16-bit format keep only 1,5,9 and discard the others


			//4th col iteration

			//multiply by the mask
			m0=_mm256_maddubs_epi16(r0,cx3);

			//first horizontal add (complex shift is needed)
			m3=_mm256_srli_si256(m0,2);
			m4=_mm256_and_si256(m0,mask3);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_slli_si256(m4,14);
			m3=_mm256_add_epi16(m3,m4);

			m0=_mm256_add_epi16(m0,m3);//add

			//second horizontal add
			m1=_mm256_srli_si256(m0,4);
			m0=_mm256_add_epi16(m0,m1);//add

			output4=_mm256_and_si256(m0,mask_16_2);//in 16-bit format keep only 1,5,9 and discard the others


			//5th col iteration

			//multiply by the mask
			m0=_mm256_maddubs_epi16(r0,cx4);

			//first horizontal add
			m1=_mm256_srli_si256(m0,2);
			m0=_mm256_add_epi16(m0,m1);//add

			//second horizontal add (complex shift is needed)
			m3=_mm256_srli_si256(m0,4);
			m4=_mm256_and_si256(m0,mask3);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_slli_si256(m4,12);
			m3=_mm256_add_epi16(m3,m4);

			m0=_mm256_add_epi16(m0,m3);//add
			m0=_mm256_and_si256(m0,mask_16_3);//in 16-bit format keep only 2,6,10 and discard the others


			//MERGE output1 with output5 and DIVIDE
			output1=_mm256_add_epi16(m0,output1);
			output1 = division(division_case,output1,f);


			//6th col iteration

			//multiply by the mask
			m0=_mm256_maddubs_epi16(r0,cx5);

			//first horizontal add
			m1=_mm256_srli_si256(m0,2);
			m0=_mm256_add_epi16(m0,m1);//add

			//second horizontal add (complex shift is needed)
			m3=_mm256_srli_si256(m0,4);
			m4=_mm256_and_si256(m0,mask3);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_slli_si256(m4,12);
			m3=_mm256_add_epi16(m3,m4);

			m0=_mm256_add_epi16(m0,m3);//add
			m0=_mm256_and_si256(m0,mask_16_3);//in 16-bit format keep only 2,6,10 and discard the others

			//MERGE output1 with output5 and DIVIDE
			output2=_mm256_add_epi16(m0,output2);
			output2 = division(division_case,output2,f);


			//7th col iteration

			//multiply by the mask
			m0=_mm256_maddubs_epi16(r0,cx6);

			//shift so as the 1st useful element is on position zero (complex shift is needed)
			m3=_mm256_srli_si256(m0,6);
			m4=_mm256_and_si256(m0,mask_16_4);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_slli_si256(m4,10);
			m0=_mm256_add_epi16(m3,m4);

			//first horizontal add
			m1=_mm256_srli_si256(m0,2);
			m0=_mm256_add_epi16(m0,m1);//add

			//second horizontal add
			m1=_mm256_srli_si256(m0,4);
			m0=_mm256_add_epi16(m0,m1);//add

			m0=_mm256_and_si256(m0,mask_16_1);//in 16-bit format keep only 0,4,8,12 and discard the others
			m0=_mm256_slli_si256(m0,6);//in 16-bit output is on 3,7,11

			//MERGE output1 with output5 and DIVIDE
			output3=_mm256_add_epi16(m0,output3);
			output3 = division(division_case,output3,f);

			//8th col iteration

			//multiply by the mask
			m0=_mm256_maddubs_epi16(r0,cx7);

			//shift so as the 1st useful element is on position zero (complex shift is needed)
			m3=_mm256_srli_si256(m0,6);
			m4=_mm256_and_si256(m0,mask_16_4);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_slli_si256(m4,10);
			m0=_mm256_add_epi16(m3,m4);

			//first horizontal add
			m1=_mm256_srli_si256(m0,2);
			m0=_mm256_add_epi16(m0,m1);//add

			//second horizontal add
			m1=_mm256_srli_si256(m0,4);
			m0=_mm256_add_epi16(m0,m1);//add

			m0=_mm256_and_si256(m0,mask_16_1);//in 16-bit format keep only 0,4,8,12 and discard the others
			m0=_mm256_slli_si256(m0,6);//in 16-bit output is on 3,7,11

			//MERGE output1 with output5 and DIVIDE
			output4=_mm256_add_epi16(m0,output4);
			output4 = division(division_case,output4,f);

			//blend output1, output2, output3, output4 into one register
			output2 = _mm256_slli_si256(output2,1);
			output1 = _mm256_add_epi8(output1,output2);

			output4 = _mm256_slli_si256(output4,1);
			output3 = _mm256_add_epi8(output3,output4);

			output1 = _mm256_add_epi8(output1,output3);

			_mm256_storeu_si256( (__m256i *) &filt[row][col],output1 );

			//row+1
			//1st col iteration

				//multiply by the mask
				m0=_mm256_maddubs_epi16(r1,cx0);

				//first horizontal add
				m1=_mm256_srli_si256(m0,2);
				m0=_mm256_add_epi16(m0,m1);//add

				//second horizontal add
				m1=_mm256_srli_si256(m0,4);
				m0=_mm256_add_epi16(m0,m1);//add

				output1=_mm256_and_si256(m0,mask_16_1);//in 16-bit format keep only 0,4,8,12 and discard the others


				//2nd col iteration

				//multiply by the mask
				m0=_mm256_maddubs_epi16(r1,cx1);

				//first horizontal add
				m1=_mm256_srli_si256(m0,2);
				m0=_mm256_add_epi16(m0,m1);//add

				//second horizontal add
				m1=_mm256_srli_si256(m0,4);
				m0=_mm256_add_epi16(m0,m1);//add

				output2=_mm256_and_si256(m0,mask_16_1);//in 16-bit format keep only 0,4,8,12 and discard the others


				//3rd col iteration

				//multiply by the mask
				m0=_mm256_maddubs_epi16(r1,cx2);

				//first horizontal add (complex shift is needed)
				m3=_mm256_srli_si256(m0,2);
				m4=_mm256_and_si256(m0,mask3);
				m4=_mm256_permute2f128_si256(m4,m4,1);
				m4=_mm256_slli_si256(m4,14);
				m3=_mm256_add_epi16(m3,m4);

				m0=_mm256_add_epi16(m0,m3);//add

				//second horizontal add
				m1=_mm256_srli_si256(m0,4);
				m0=_mm256_add_epi16(m0,m1);//add

				output3=_mm256_and_si256(m0,mask_16_2);//in 16-bit format keep only 1,5,9 and discard the others


				//4th col iteration

				//multiply by the mask
				m0=_mm256_maddubs_epi16(r1,cx3);

				//first horizontal add (complex shift is needed)
				m3=_mm256_srli_si256(m0,2);
				m4=_mm256_and_si256(m0,mask3);
				m4=_mm256_permute2f128_si256(m4,m4,1);
				m4=_mm256_slli_si256(m4,14);
				m3=_mm256_add_epi16(m3,m4);

				m0=_mm256_add_epi16(m0,m3);//add

				//second horizontal add
				m1=_mm256_srli_si256(m0,4);
				m0=_mm256_add_epi16(m0,m1);//add

				output4=_mm256_and_si256(m0,mask_16_2);//in 16-bit format keep only 1,5,9 and discard the others


				//5th col iteration

				//multiply by the mask
				m0=_mm256_maddubs_epi16(r1,cx4);

				//first horizontal add
				m1=_mm256_srli_si256(m0,2);
				m0=_mm256_add_epi16(m0,m1);//add

				//second horizontal add (complex shift is needed)
				m3=_mm256_srli_si256(m0,4);
				m4=_mm256_and_si256(m0,mask3);
				m4=_mm256_permute2f128_si256(m4,m4,1);
				m4=_mm256_slli_si256(m4,12);
				m3=_mm256_add_epi16(m3,m4);

				m0=_mm256_add_epi16(m0,m3);//add
				m0=_mm256_and_si256(m0,mask_16_3);//in 16-bit format keep only 2,6,10 and discard the others


				//MERGE output1 with output5 and DIVIDE
				output1=_mm256_add_epi16(m0,output1);
				output1 = division(division_case,output1,f);


				//6th col iteration

				//multiply by the mask
				m0=_mm256_maddubs_epi16(r1,cx5);

				//first horizontal add
				m1=_mm256_srli_si256(m0,2);
				m0=_mm256_add_epi16(m0,m1);//add

				//second horizontal add (complex shift is needed)
				m3=_mm256_srli_si256(m0,4);
				m4=_mm256_and_si256(m0,mask3);
				m4=_mm256_permute2f128_si256(m4,m4,1);
				m4=_mm256_slli_si256(m4,12);
				m3=_mm256_add_epi16(m3,m4);

				m0=_mm256_add_epi16(m0,m3);//add
				m0=_mm256_and_si256(m0,mask_16_3);//in 16-bit format keep only 2,6,10 and discard the others

				//MERGE output1 with output5 and DIVIDE
				output2=_mm256_add_epi16(m0,output2);
				output2 = division(division_case,output2,f);


				//7th col iteration

				//multiply by the mask
				m0=_mm256_maddubs_epi16(r1,cx6);

				//shift so as the 1st useful element is on position zero (complex shift is needed)
				m3=_mm256_srli_si256(m0,6);
				m4=_mm256_and_si256(m0,mask_16_4);
				m4=_mm256_permute2f128_si256(m4,m4,1);
				m4=_mm256_slli_si256(m4,10);
				m0=_mm256_add_epi16(m3,m4);

				//first horizontal add
				m1=_mm256_srli_si256(m0,2);
				m0=_mm256_add_epi16(m0,m1);//add

				//second horizontal add
				m1=_mm256_srli_si256(m0,4);
				m0=_mm256_add_epi16(m0,m1);//add

				m0=_mm256_and_si256(m0,mask_16_1);//in 16-bit format keep only 0,4,8,12 and discard the others
				m0=_mm256_slli_si256(m0,6);//in 16-bit output is on 3,7,11

				//MERGE output1 with output5 and DIVIDE
				output3=_mm256_add_epi16(m0,output3);
				output3 = division(division_case,output3,f);

				//8th col iteration

				//multiply by the mask
				m0=_mm256_maddubs_epi16(r1,cx7);

				//shift so as the 1st useful element is on position zero (complex shift is needed)
				m3=_mm256_srli_si256(m0,6);
				m4=_mm256_and_si256(m0,mask_16_4);
				m4=_mm256_permute2f128_si256(m4,m4,1);
				m4=_mm256_slli_si256(m4,10);
				m0=_mm256_add_epi16(m3,m4);

				//first horizontal add
				m1=_mm256_srli_si256(m0,2);
				m0=_mm256_add_epi16(m0,m1);//add

				//second horizontal add
				m1=_mm256_srli_si256(m0,4);
				m0=_mm256_add_epi16(m0,m1);//add

				m0=_mm256_and_si256(m0,mask_16_1);//in 16-bit format keep only 0,4,8,12 and discard the others
				m0=_mm256_slli_si256(m0,6);//in 16-bit output is on 3,7,11

				//MERGE output1 with output5 and DIVIDE
				output4=_mm256_add_epi16(m0,output4);
				output4 = division(division_case,output4,f);

				//blend output1, output2, output3, output4 into one register
				output2 = _mm256_slli_si256(output2,1);
				output1 = _mm256_add_epi8(output1,output2);

				output4 = _mm256_slli_si256(output4,1);
				output3 = _mm256_add_epi8(output3,output4);

				output1 = _mm256_add_epi8(output1,output3);

				_mm256_storeu_si256( (__m256i *) &filt[row+1][col],output1 );

		}
	  loop_reminder_7x7_16_blur(frame1,filt,N,row, col, REMINDER_ITERATIONS, division_case, mask_vector_x, mask_vector_y, f, divisor_xy);
	  loop_reminder_7x7_16_blur(frame1,filt,N,row+1, col, REMINDER_ITERATIONS, division_case, mask_vector_x, mask_vector_y, f, divisor_xy);


		}
}

}//end of parallel


}


//this routine does y and x filter together, not so efficient
//it is more efficient to compute the y filter for each row and store the result into an array temp[M] and then apply x filter.
void Gaussian_Blur_9x9_16_separable_old(unsigned char **frame1,unsigned char **filt,const unsigned int M, const unsigned int N, signed char *kernel_y, signed char *kernel_x, const unsigned short int divisor_xy){

const signed char fx0=kernel_x[0];
const signed char fx1=kernel_x[1];
const signed char fx2=kernel_x[2];
const signed char fx3=kernel_x[3];
const signed char fx4=kernel_x[4];
const signed char fx5=kernel_x[5];
const signed char fx6=kernel_x[6];
const signed char fx7=kernel_x[7];
const signed char fx8=kernel_x[8];


const signed char fy0=kernel_y[0];
const signed char fy1=kernel_y[1];
const signed char fy2=kernel_y[2];
const signed char fy3=kernel_y[3];
const signed char fy4=kernel_y[4];
const signed char fy5=kernel_y[5];
const signed char fy6=kernel_y[6];
const signed char fy7=kernel_y[7];
const signed char fy8=kernel_y[8];



const signed char mask_vector_x[10][32] __attribute__((aligned(64))) ={
	{fx0,fx1,fx2,fx3,fx4,fx5,fx6,fx7,fx8,0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,fx7,fx8,0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,fx7,fx8,0,0,0},
	{0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,fx7,fx8,0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,fx7,fx8,0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,fx7,fx8,0,0},
	{0,0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,fx7,fx8,0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,fx7,fx8,0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,fx7,fx8,0},
	{0,0,0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,fx7,fx8,0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,fx7,fx8,0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,fx7,fx8},
	{0,0,0,0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,fx7,fx8,0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,fx7,fx8,0,0,0,0,0,0,0,0,0},
	{0,0,0,0,0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,fx7,fx8,0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,fx7,fx8,0,0,0,0,0,0,0,0},
	{0,0,0,0,0,0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,fx7,fx8,0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,fx7,fx8,0,0,0,0,0,0,0},
	{0,0,0,0,0,0,0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,fx7,fx8,0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,fx7,fx8,0,0,0,0,0,0},
	{0,0,0,0,0,0,0,0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,fx7,fx8,0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,fx7,fx8,0,0,0,0,0},
	{0,0,0,0,0,0,0,0,0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,fx7,fx8,0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,fx7,fx8,0,0,0,0},
};

const signed char mask_vector_y[18][32] __attribute__((aligned(64))) ={
	{fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0},
	{fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0},
	{fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0},
	{fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0},
	{fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0},
	{fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0},
	{fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0},
	{fy7,0,fy7,0,fy7,0,fy7,0,fy7,0,fy7,0,fy7,0,fy7,0,fy7,0,fy7,0,fy7,0,fy7,0,fy7,0,fy7,0,fy7,0,fy7,0},
	{fy8,0,fy8,0,fy8,0,fy8,0,fy8,0,fy8,0,fy8,0,fy8,0,fy8,0,fy8,0,fy8,0,fy8,0,fy8,0,fy8,0,fy8,0,fy8,0},

	{0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0},
	{0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1},
	{0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2},
	{0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3},
	{0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4},
	{0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5},
	{0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6},
	{0,fy7,0,fy7,0,fy7,0,fy7,0,fy7,0,fy7,0,fy7,0,fy7,0,fy7,0,fy7,0,fy7,0,fy7,0,fy7,0,fy7,0,fy7,0,fy7},
	{0,fy8,0,fy8,0,fy8,0,fy8,0,fy8,0,fy8,0,fy8,0,fy8,0,fy8,0,fy8,0,fy8,0,fy8,0,fy8,0,fy8,0,fy8,0,fy8},
};

	const __m256i cx0=_mm256_load_si256( (__m256i *) &mask_vector_x[0]);
	const __m256i cx1=_mm256_load_si256( (__m256i *) &mask_vector_x[1]);
	const __m256i cx2=_mm256_load_si256( (__m256i *) &mask_vector_x[2]);
	const __m256i cx3=_mm256_load_si256( (__m256i *) &mask_vector_x[3]);
	const __m256i cx4=_mm256_load_si256( (__m256i *) &mask_vector_x[4]);
	const __m256i cx5=_mm256_load_si256( (__m256i *) &mask_vector_x[5]);
	const __m256i cx6=_mm256_load_si256( (__m256i *) &mask_vector_x[6]);
	const __m256i cx7=_mm256_load_si256( (__m256i *) &mask_vector_x[7]);
	const __m256i cx8=_mm256_load_si256( (__m256i *) &mask_vector_x[8]);
	const __m256i cx9=_mm256_load_si256( (__m256i *) &mask_vector_x[9]);


	const __m256i cy0=_mm256_load_si256( (__m256i *) &mask_vector_y[0]);
	const __m256i cy1=_mm256_load_si256( (__m256i *) &mask_vector_y[1]);
	const __m256i cy2=_mm256_load_si256( (__m256i *) &mask_vector_y[2]);
	const __m256i cy3=_mm256_load_si256( (__m256i *) &mask_vector_y[3]);
	const __m256i cy4=_mm256_load_si256( (__m256i *) &mask_vector_y[4]);
	const __m256i cy5=_mm256_load_si256( (__m256i *) &mask_vector_y[5]);
	const __m256i cy6=_mm256_load_si256( (__m256i *) &mask_vector_y[6]);
	const __m256i cy7=_mm256_load_si256( (__m256i *) &mask_vector_y[7]);
	const __m256i cy8=_mm256_load_si256( (__m256i *) &mask_vector_y[8]);


	const __m256i cy0_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[9]);
	const __m256i cy1_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[10]);
	const __m256i cy2_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[11]);
	const __m256i cy3_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[12]);
	const __m256i cy4_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[13]);
	const __m256i cy5_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[14]);
	const __m256i cy6_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[15]);
	const __m256i cy7_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[16]);
	const __m256i cy8_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[17]);


	const __m256i mask3    = _mm256_set_epi16(0,0,0,0,0,0,0,65535,0,0,0,0,0,0,0,0);
	const __m256i mask3_2  = _mm256_set_epi16(0,0,0,0,0,0,65535,0,0,0,0,0,0,0,0,0);
	const __m256i mask3_3  = _mm256_set_epi16(0,0,0,0,0,65535,0,0,0,0,0,0,0,0,0,0);
	const __m256i mask3_4  = _mm256_set_epi16(0,0,0,0,65535,0,0,0,0,0,0,0,0,0,0,0);

	const __m256i mask_prelude  = _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0);

//	const __m256i mask_even_16  = _mm256_set_epi16(0,0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff);
//	const __m256i mask_odd_16  = _mm256_set_epi16(0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff,0);
	const __m256i mask_16_1   = _mm256_set_epi16(0,0,0,0,0,0xffff,0,0,0,0,0xffff,0,0,0,0,0xffff);
	const __m256i mask_16_2   = _mm256_set_epi16(0,0,0,0,0xffff,0,0,0,0,0xffff,0,0,0,0,0xffff,0);
	const __m256i mask_16_3   = _mm256_set_epi16(0,0,0,0,0,0,0,0,0xffff,0,0,0,0,0xffff,0,0);
	const __m256i mask_16_4   = _mm256_set_epi16(0,0,0,0,0,0,0,0xffff,0,0,0,0,0xffff,0,0,0);
	const __m256i mask_16_4b  = _mm256_set_epi16(0,0,0,0,0,0,0xffff,0,0,0,0,0xffff,0,0,0,0);

	const __m256i mask_16_5  = _mm256_set_epi16(0,0xffff,0,0,0,0,0xffff,0,0,0,0,0xffff,0,0,0,0);
	const __m256i mask_16_6  = _mm256_set_epi16(0xffff,0,0,0,0,0xffff,0,0,0,0,0xffff,0,0,0,0,0);
	const __m256i mask_16_7  = _mm256_set_epi16(0,0,0,0,0xffff,0,0,0,0,0xffff,0,0,0,0,0,0);
	const __m256i mask_16_8  = _mm256_set_epi16(0,0,0xffff,0,0,0,0,0xffff,0,0,0,0,0,0,0,0);


//	const __m256i mask_even_32  = _mm256_set_epi32(0,0xffffffff,0,0xffffffff,0,0xffffffff,0,0xffffffff);
//	const __m256i mask_odd_32  = _mm256_set_epi32(0xffffffff,0,0xffffffff,0,0xffffffff,0,0xffffffff,0);
	//const __m256i mask4  = _mm256_set_epi32(0,0,0,0xffffffff,0,0,0,0);



	//const unsigned int REMINDER_ITERATIONS =  (M-((((M-32)/24)*24)+24)); //M-(last_col_value+24)

	//printf("\nREMINDER_ITERATIONS=%d",REMINDER_ITERATIONS);


	const unsigned int division_case=prepare_for_division(divisor_xy); //determine which is the division case (A, B or C)
	const __m256i f = _mm256_load_si256( (__m256i *) &f_vector[0]);// initialize the division vector
	//const __m256i ones=_mm256_set1_epi16(1);

//	const unsigned int division_case_x=prepare_for_division_32(divisor_x); //determine which is the division case (A, B or C)
//	const __m256i f_x = _mm256_load_si256( (__m256i *) &f_vector_x[0]);// initialize the division vector
//printf("\n%d %d",division_case,b);


#pragma omp parallel
{

unsigned int row,col;
__m256i r0,r1,r2,r3,r4,r5,r6,r7,r8,m0,m1,m2,m3,m4,m5,m6,m7,m8,even,odd;
//__m256i output1,output2,output3,output4;



/*---------------------- Gaussian Blur ---------------------------------*/

    #pragma omp for schedule(static)
	for (row = 4; row < N-4; row++) {

		if (row<4){
		/*	if (row==0){
				for (col = 0; col <= M-32; col+=26){
				r0=prelude_7x7_16_Ymask_1(0, col, frame1, mask_vector_y, division_case, f, mask_prelude);
				prelude_7x7_16_Xmask(filt,row,col, r0, mask_vector_x,  division_case,f);
				}
				loop_reminder_7x7_16_blur(frame1,filt,N,row, col, REMINDER_ITERATIONS, division_case, mask_vector_x, mask_vector_y, f, divisor_xy);

			}
			else if (row==1){
				for (col = 0; col <= M-32; col+=26){
				r0=prelude_7x7_16_Ymask_2(0, col, frame1, mask_vector_y, division_case, f, mask_prelude);
				prelude_7x7_16_Xmask(filt,row,col, r0, mask_vector_x,  division_case,f);
				}
				loop_reminder_7x7_16_blur(frame1,filt,N,row, col, REMINDER_ITERATIONS, division_case, mask_vector_x, mask_vector_y, f, divisor_xy);
			}
			else if (row==2){
				for (col = 0; col <= M-32; col+=26){
				r0=prelude_7x7_16_Ymask_3(0, col, frame1, mask_vector_y, division_case, f, mask_prelude);
				prelude_7x7_16_Xmask(filt,row,col, r0, mask_vector_x,  division_case,f);
				}
				loop_reminder_7x7_16_blur(frame1,filt,N,row, col, REMINDER_ITERATIONS, division_case, mask_vector_x, mask_vector_y, f, divisor_xy);
			}*/
		}

		else if (row>N-5){
		/*	if (row==N-1){
				for (col = 0; col <= M-32; col+=26){
				r0=prelude_7x7_16_Ymask_1(N-4, col, frame1, mask_vector_y, division_case, f, mask_prelude);
				prelude_7x7_16_Xmask(filt,row,col, r0, mask_vector_x,  division_case,f);
			}
			loop_reminder_7x7_16_blur(frame1,filt,N,row, col, REMINDER_ITERATIONS, division_case, mask_vector_x, mask_vector_y, f, divisor_xy);

			}
			else if (row==N-2){
				for (col = 0; col <= M-32; col+=26){
				r0=prelude_7x7_16_Ymask_2(N-5, col, frame1, mask_vector_y, division_case, f, mask_prelude);
				prelude_7x7_16_Xmask(filt,row,col, r0, mask_vector_x,  division_case,f);
			}
			loop_reminder_7x7_16_blur(frame1,filt,N,row, col, REMINDER_ITERATIONS, division_case, mask_vector_x, mask_vector_y, f, divisor_xy);

			}
			else if (row==N-3){
				for (col = 0; col <= M-32; col+=26){
				r0=prelude_7x7_16_Ymask_3(N-6, col, frame1, mask_vector_y, division_case, f, mask_prelude);
				prelude_7x7_16_Xmask(filt,row,col, r0, mask_vector_x,  division_case,f);
			}
			loop_reminder_7x7_16_blur(frame1,filt,N,row, col, REMINDER_ITERATIONS, division_case, mask_vector_x, mask_vector_y, f, divisor_xy);

			}*/
		}

	  else { //main loop


	  for (col = 0; col <= M-32; col+=24){

		  if (col==0){

				//load the 9 rows
				r0=_mm256_loadu_si256( (__m256i *) &frame1[row-4][0]);
				r1=_mm256_loadu_si256( (__m256i *) &frame1[row-3][0]);
				r2=_mm256_loadu_si256( (__m256i *) &frame1[row-2][0]);
				r3=_mm256_loadu_si256( (__m256i *) &frame1[row-1][0]);
				r4=_mm256_loadu_si256( (__m256i *) &frame1[row][0]);
				r5=_mm256_loadu_si256( (__m256i *) &frame1[row+1][0]);
				r6=_mm256_loadu_si256( (__m256i *) &frame1[row+2][0]);
				r7=_mm256_loadu_si256( (__m256i *) &frame1[row+3][0]);
				r8=_mm256_loadu_si256( (__m256i *) &frame1[row+4][0]);

		  		//START - extra code needed for prelude
				  r0=fill_zeros(r0,mask_prelude);
				  r1=fill_zeros(r1,mask_prelude);
				  r2=fill_zeros(r2,mask_prelude);
				  r3=fill_zeros(r3,mask_prelude);
				  r4=fill_zeros(r4,mask_prelude);
				  r5=fill_zeros(r5,mask_prelude);
				  r6=fill_zeros(r6,mask_prelude);
				  r7=fill_zeros(r7,mask_prelude);
				  r8=fill_zeros(r8,mask_prelude);
		  		//END - extra code needed for prelude
		  		}
		  else {
				//load the 9 rows
				r0=_mm256_loadu_si256( (__m256i *) &frame1[row-4][col-4]);
				r1=_mm256_loadu_si256( (__m256i *) &frame1[row-3][col-4]);
				r2=_mm256_loadu_si256( (__m256i *) &frame1[row-2][col-4]);
				r3=_mm256_loadu_si256( (__m256i *) &frame1[row-1][col-4]);
				r4=_mm256_loadu_si256( (__m256i *) &frame1[row][col-4]);
				r5=_mm256_loadu_si256( (__m256i *) &frame1[row+1][col-4]);
				r6=_mm256_loadu_si256( (__m256i *) &frame1[row+2][col-4]);
				r7=_mm256_loadu_si256( (__m256i *) &frame1[row+3][col-4]);
				r8=_mm256_loadu_si256( (__m256i *) &frame1[row+4][col-4]);
		  }

		  //--------------------y filter begins--------------------

			//multiply with the mask
				m0=_mm256_maddubs_epi16(r0,cy0);
				m1=_mm256_maddubs_epi16(r1,cy1);
				m2=_mm256_maddubs_epi16(r2,cy2);
				m3=_mm256_maddubs_epi16(r3,cy3);
				m4=_mm256_maddubs_epi16(r4,cy4);
				m5=_mm256_maddubs_epi16(r5,cy5);
				m6=_mm256_maddubs_epi16(r6,cy6);
				m7=_mm256_maddubs_epi16(r7,cy7);
				m8=_mm256_maddubs_epi16(r8,cy8);

				//vertical add
				m0=_mm256_add_epi16(m0,m1);
				m0=_mm256_add_epi16(m0,m2);
				m0=_mm256_add_epi16(m0,m3);
				m0=_mm256_add_epi16(m0,m4);
				m0=_mm256_add_epi16(m0,m5);
				m0=_mm256_add_epi16(m0,m6);
				m0=_mm256_add_epi16(m0,m7);
				m0=_mm256_add_epi16(m0,m8);

				even=division(division_case,m0,f);//even results

				//multiply with the mask
				m0=_mm256_maddubs_epi16(r0,cy0_sh1);
				m1=_mm256_maddubs_epi16(r1,cy1_sh1);
				m2=_mm256_maddubs_epi16(r2,cy2_sh1);
				m3=_mm256_maddubs_epi16(r3,cy3_sh1);
				m4=_mm256_maddubs_epi16(r4,cy4_sh1);
				m5=_mm256_maddubs_epi16(r5,cy5_sh1);
				m6=_mm256_maddubs_epi16(r6,cy6_sh1);
				m7=_mm256_maddubs_epi16(r7,cy7_sh1);
				m8=_mm256_maddubs_epi16(r8,cy8_sh1);

				//vertical add
				m0=_mm256_add_epi16(m0,m1);
				m0=_mm256_add_epi16(m0,m2);
				m0=_mm256_add_epi16(m0,m3);
				m0=_mm256_add_epi16(m0,m4);
				m0=_mm256_add_epi16(m0,m5);
				m0=_mm256_add_epi16(m0,m6);
				m0=_mm256_add_epi16(m0,m7);
				m0=_mm256_add_epi16(m0,m8);

				odd=division(division_case,m0,f);//odd results

				//pack to one register
				odd=_mm256_slli_si256(odd,1); //shift one position right
				r0=_mm256_add_epi8(even,odd); //add the odd with the even
			//y filter ends - r0 has now the data to be processed by x filter

		// col iteration computes output pixels of   0,10,20
		// col+1 iteration computes output pixels of 1,11,21
		// col+2 iteration computes output pixels of 2,12,22
		// col+3 iteration computes output pixels of 3,13,23
		// col+4 iteration computes output pixels of 4,14
		// col+5 iteration computes output pixels of 5,15
		// col+6 iteration computes output pixels of 6,16
		// col+7 iteration computes output pixels of 7,17
		// col+8 iteration computes output pixels of 8,18
		// col+9 iteration computes output pixels of 9,19
		//afterwards, col becomes 24 and repeat the above process

			//1st col iteration

			//multiply by the mask
			m0=_mm256_maddubs_epi16(r0,cx0);

			//first horizontal add (complex shift is needed)
			m3=_mm256_srli_si256(m0,2);
			m4=_mm256_and_si256(m0,mask3);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_slli_si256(m4,14);
			m3=_mm256_add_epi16(m3,m4);
			m3=_mm256_add_epi16(m0,m3);//add

			//second horizontal add
			m1=_mm256_srli_si256(m3,4);
			m3=_mm256_add_epi16(m3,m1);//add

			//third horizontal add
			m0=_mm256_and_si256(m0,mask_16_5);
			m2=_mm256_srli_si256(m0,8);
			m4=_mm256_and_si256(m0,mask3_2);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_slli_si256(m4,8);
			m2=_mm256_add_epi16(m2,m4);

			m0=_mm256_add_epi16(m2,m3);//add

			even=_mm256_and_si256(m0,mask_16_1);//in 16-bit format keep only 0,5,10 and discard the others


			//2nd col iteration

			//multiply by the mask
			m0=_mm256_maddubs_epi16(r0,cx1);

			//first horizontal add (complex shift is needed)
			m3=_mm256_srli_si256(m0,2);
			m4=_mm256_and_si256(m0,mask3);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_slli_si256(m4,14);
			m3=_mm256_add_epi16(m3,m4);
			m3=_mm256_add_epi16(m0,m3);//add

			//second horizontal add
			m1=_mm256_srli_si256(m3,4);
			m3=_mm256_add_epi16(m3,m1);//add

			//third horizontal add
			m0=_mm256_and_si256(m0,mask_16_5);
			m2=_mm256_srli_si256(m0,8);
			m4=_mm256_and_si256(m0,mask3_2);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_slli_si256(m4,8);
			m2=_mm256_add_epi16(m2,m4);

			m0=_mm256_add_epi16(m2,m3);//add

			odd=_mm256_and_si256(m0,mask_16_1);//in 16-bit format keep only 0,5,10 and discard the others


			//3rd col iteration

			//multiply by the mask
			m0=_mm256_maddubs_epi16(r0,cx2);

			//1st horizontal add
			m1=_mm256_srli_si256(m0,2);
			m2=_mm256_add_epi16(m0,m1);//add

			//2nd horizontal add (complex shift is needed)
			m3=_mm256_srli_si256(m2,4);
			m4=_mm256_and_si256(m2,mask3);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_slli_si256(m4,12);
			m3=_mm256_add_epi16(m3,m4);
			m8=_mm256_add_epi16(m2,m3);//add

			//3rd horizontal add
			m0=_mm256_and_si256(m0,mask_16_6);
			m2=_mm256_srli_si256(m0,8);
			m4=_mm256_and_si256(m0,mask3_3);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_slli_si256(m4,8);
			m2=_mm256_add_epi16(m2,m4);

			m0=_mm256_add_epi16(m2,m8);//add

			m0=_mm256_and_si256(m0,mask_16_2);//in 16-bit format keep only 1,6,11 and discard the others
			even=_mm256_add_epi16(m0,even);

			//4th col iteration

			//multiply by the mask
			m0=_mm256_maddubs_epi16(r0,cx3);

			//1st horizontal add
			m1=_mm256_srli_si256(m0,2);
			m2=_mm256_add_epi16(m0,m1);//add

			//2nd horizontal add (complex shift is needed)
			m3=_mm256_srli_si256(m2,4);
			m4=_mm256_and_si256(m2,mask3);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_slli_si256(m4,12);
			m3=_mm256_add_epi16(m3,m4);
			m8=_mm256_add_epi16(m2,m3);//add

			//3rd horizontal add
			m0=_mm256_and_si256(m0,mask_16_6);
			m2=_mm256_srli_si256(m0,8);
			m4=_mm256_and_si256(m0,mask3_3);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_slli_si256(m4,8);
			m2=_mm256_add_epi16(m2,m4);

			m0=_mm256_add_epi16(m2,m8);//add

			m0=_mm256_and_si256(m0,mask_16_2);//in 16-bit format keep only 1,6,11  and discard the others
			odd=_mm256_add_epi16(m0,odd);

			//5th col iteration

			//multiply by the mask
			m0=_mm256_maddubs_epi16(r0,cx4);

			//first horizontal add (complex shift is needed)
			m3=_mm256_srli_si256(m0,2);
			m4=_mm256_and_si256(m0,mask3);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_slli_si256(m4,14);
			m3=_mm256_add_epi16(m3,m4);
			m2=_mm256_add_epi16(m0,m3);//add

			//2nd horizontal add (complex shift is needed)
			m3=_mm256_srli_si256(m2,4);
			m4=_mm256_and_si256(m2,mask3_2);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_slli_si256(m4,12);
			m3=_mm256_add_epi16(m3,m4);
			m3=_mm256_add_epi16(m2,m3);//add

			//3rd horizontal add
			m0=_mm256_and_si256(m0,mask_16_7);
			m2=_mm256_srli_si256(m0,8);
			m4=_mm256_and_si256(m0,mask3_4);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_slli_si256(m4,8);
			m2=_mm256_add_epi16(m2,m4);

			m0=_mm256_add_epi16(m2,m3);//add
			m0=_mm256_and_si256(m0,mask_16_3);//in 16-bit format keep only 2,7  and discard the others
			even=_mm256_add_epi16(m0,even);

			//6th col iteration

			//multiply by the mask
			m0=_mm256_maddubs_epi16(r0,cx5);

			//first horizontal add (complex shift is needed)
			m3=_mm256_srli_si256(m0,2);
			m4=_mm256_and_si256(m0,mask3);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_slli_si256(m4,14);
			m3=_mm256_add_epi16(m3,m4);
			m2=_mm256_add_epi16(m0,m3);//add

			//2nd horizontal add (complex shift is needed)
			m3=_mm256_srli_si256(m2,4);
			m4=_mm256_and_si256(m2,mask3_2);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_slli_si256(m4,12);
			m3=_mm256_add_epi16(m3,m4);
			m3=_mm256_add_epi16(m2,m3);//add

			//3rd horizontal add
			m0=_mm256_and_si256(m0,mask_16_7);
			m2=_mm256_srli_si256(m0,8);
			m4=_mm256_and_si256(m0,mask3_4);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_slli_si256(m4,8);
			m2=_mm256_add_epi16(m2,m4);

			m0=_mm256_add_epi16(m2,m3);//add
			m0=_mm256_and_si256(m0,mask_16_3);//in 16-bit format keep only 2,7  and discard the others
			odd=_mm256_add_epi16(m0,odd);

			//7th col iteration

			//multiply by the mask
			m0=_mm256_maddubs_epi16(r0,cx6);

			//1st horizontal add
			m1=_mm256_srli_si256(m0,2);
			m0=_mm256_add_epi16(m0,m1);//add

			//2nd horizontal add
			m1=_mm256_srli_si256(m0,4);
			m0=_mm256_add_epi16(m0,m1);//add

			//3rd horizontal add
			m1=_mm256_srli_si256(m0,8);
			m0=_mm256_add_epi16(m0,m1);//add

			m0=_mm256_and_si256(m0,mask_16_4);//in 16-bit format keep only 4,8  and discard the others
			even=_mm256_add_epi16(m0,even);

			//8th col iteration

			//multiply by the mask
			m0=_mm256_maddubs_epi16(r0,cx7);

			//1st horizontal add
			m1=_mm256_srli_si256(m0,2);
			m0=_mm256_add_epi16(m0,m1);//add

			//2nd horizontal add
			m1=_mm256_srli_si256(m0,4);
			m0=_mm256_add_epi16(m0,m1);//add

			//3rd horizontal add
			m1=_mm256_srli_si256(m0,8);
			m0=_mm256_add_epi16(m0,m1);//add

			m0=_mm256_and_si256(m0,mask_16_4);//in 16-bit format keep only 4,8  and discard the others
			odd=_mm256_add_epi16(m0,odd);

			//9th col iteration
			//multiply by the mask
			m0=_mm256_maddubs_epi16(r0,cx8);

			//1st horizontal add
			m1=_mm256_srli_si256(m0,2);
			m2=_mm256_add_epi16(m0,m1);//add

			//2nd horizontal add
			m1=_mm256_srli_si256(m2,4);
			m7=_mm256_add_epi16(m2,m1);//add

			//3rd horizontal add
			m0=_mm256_and_si256(m0,mask_16_8);
			m2=_mm256_srli_si256(m0,8);
			m4=_mm256_and_si256(m0,mask3);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_slli_si256(m4,8);
			m2=_mm256_add_epi16(m2,m4);

			m0=_mm256_add_epi16(m2,m7);//add
			m0=_mm256_and_si256(m0,mask_16_4b);//in 16-bit format keep only 4,9  and discard the others
			even=_mm256_add_epi16(m0,even);

			//10th col iteration
			//multiply by the mask
			m0=_mm256_maddubs_epi16(r0,cx9);

			//1st horizontal add
			m1=_mm256_srli_si256(m0,2);
			m2=_mm256_add_epi16(m0,m1);//add

			//2nd horizontal add
			m1=_mm256_srli_si256(m2,4);
			m7=_mm256_add_epi16(m2,m1);//add

			//3rd horizontal add
			m0=_mm256_and_si256(m0,mask_16_8);
			m2=_mm256_srli_si256(m0,8);
			m4=_mm256_and_si256(m0,mask3);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_slli_si256(m4,8);
			m2=_mm256_add_epi16(m2,m4);

			m0=_mm256_add_epi16(m2,m7);//add
			m0=_mm256_and_si256(m0,mask_16_4b);//in 16-bit format keep only 4,9  and discard the others
			odd=_mm256_add_epi16(m0,odd);

			//division
			even = division(division_case,even,f);
			odd = division(division_case,odd,f);

			//blend even , odd into one register
			odd = _mm256_slli_si256(odd,1);
			even=_mm256_add_epi16(even,odd);

			_mm256_storeu_si256( (__m256i *) &filt[row][col],even );
		}
	  //loop_reminder_7x7_16_blur(frame1,filt,N,row, col, REMINDER_ITERATIONS, division_case, mask_vector_x, mask_vector_y, f, divisor_xy);


		}
}

}//end of parallel


}

__m256i fill_zeros(__m256i r0, const __m256i mask_prelude ){

	__m256i	m0=_mm256_slli_si256(r0,4);//shift 4 elements left - equivalent to filling with four zeros in the beginning

	r0=_mm256_and_si256(r0,mask_prelude);
	r0=_mm256_permute2f128_si256(r0,r0,1);
	r0=_mm256_srli_si256(r0,12); //shift by 12 elements
	return _mm256_add_epi16(m0,r0);
}

__m256i fill_3zeros(__m256i r0, const __m256i mask_prelude ){

	__m256i	m0=_mm256_slli_si256(r0,3);//shift 4 elements left - equivalent to filling with four zeros in the beginning

	r0=_mm256_and_si256(r0,mask_prelude);
	r0=_mm256_permute2f128_si256(r0,r0,1);
	r0=_mm256_srli_si256(r0,13); //shift by 12 elements
	return _mm256_add_epi16(m0,r0);
}

__m256i fill_2zeros(__m256i r0, const __m256i mask_prelude ){

	__m256i	m0=_mm256_slli_si256(r0,2);//shift 4 elements left - equivalent to filling with four zeros in the beginning

	r0=_mm256_and_si256(r0,mask_prelude);
	r0=_mm256_permute2f128_si256(r0,r0,1);
	r0=_mm256_srli_si256(r0,14);
	return _mm256_add_epi16(m0,r0);
}

__m256i fill_1zeros(__m256i r0, const __m256i mask_prelude ){

	__m256i	m0=_mm256_slli_si256(r0,1);//shift 4 elements left - equivalent to filling with four zeros in the beginning

	r0=_mm256_and_si256(r0,mask_prelude);
	r0=_mm256_permute2f128_si256(r0,r0,1);
	r0=_mm256_srli_si256(r0,15);
	return _mm256_add_epi16(m0,r0);
}


void Gaussian_Blur_9x9_16_separable(unsigned char **frame1,unsigned char **filt,const unsigned int M, const unsigned int N, signed char *kernel_y, signed char *kernel_x, const unsigned short int divisor_xy){

const signed char fx0=kernel_x[0];
const signed char fx1=kernel_x[1];
const signed char fx2=kernel_x[2];
const signed char fx3=kernel_x[3];
const signed char fx4=kernel_x[4];
const signed char fx5=kernel_x[5];
const signed char fx6=kernel_x[6];
const signed char fx7=kernel_x[7];
const signed char fx8=kernel_x[8];


const signed char fy0=kernel_y[0];
const signed char fy1=kernel_y[1];
const signed char fy2=kernel_y[2];
const signed char fy3=kernel_y[3];
const signed char fy4=kernel_y[4];
const signed char fy5=kernel_y[5];
const signed char fy6=kernel_y[6];
const signed char fy7=kernel_y[7];
const signed char fy8=kernel_y[8];



const signed char mask_vector_x[10][32] __attribute__((aligned(64))) ={
	{fx0,fx1,fx2,fx3,fx4,fx5,fx6,fx7,fx8,0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,fx7,fx8,0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,fx7,fx8,0,0,0}
};

const signed char mask_vector_y[18][32] __attribute__((aligned(64))) ={
	{fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0},
	{fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0},
	{fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0},
	{fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0},
	{fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0},
	{fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0},
	{fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0},
	{fy7,0,fy7,0,fy7,0,fy7,0,fy7,0,fy7,0,fy7,0,fy7,0,fy7,0,fy7,0,fy7,0,fy7,0,fy7,0,fy7,0,fy7,0,fy7,0},
	{fy8,0,fy8,0,fy8,0,fy8,0,fy8,0,fy8,0,fy8,0,fy8,0,fy8,0,fy8,0,fy8,0,fy8,0,fy8,0,fy8,0,fy8,0,fy8,0},

	{0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0},
	{0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1},
	{0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2},
	{0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3},
	{0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4},
	{0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5},
	{0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6},
	{0,fy7,0,fy7,0,fy7,0,fy7,0,fy7,0,fy7,0,fy7,0,fy7,0,fy7,0,fy7,0,fy7,0,fy7,0,fy7,0,fy7,0,fy7,0,fy7},
	{0,fy8,0,fy8,0,fy8,0,fy8,0,fy8,0,fy8,0,fy8,0,fy8,0,fy8,0,fy8,0,fy8,0,fy8,0,fy8,0,fy8,0,fy8,0,fy8},
};

	const __m256i cx0=_mm256_load_si256( (__m256i *) &mask_vector_x[0]);



	const __m256i cy0=_mm256_load_si256( (__m256i *) &mask_vector_y[0]);
	const __m256i cy1=_mm256_load_si256( (__m256i *) &mask_vector_y[1]);
	const __m256i cy2=_mm256_load_si256( (__m256i *) &mask_vector_y[2]);
	const __m256i cy3=_mm256_load_si256( (__m256i *) &mask_vector_y[3]);
	const __m256i cy4=_mm256_load_si256( (__m256i *) &mask_vector_y[4]);
	const __m256i cy5=_mm256_load_si256( (__m256i *) &mask_vector_y[5]);
	const __m256i cy6=_mm256_load_si256( (__m256i *) &mask_vector_y[6]);
	const __m256i cy7=_mm256_load_si256( (__m256i *) &mask_vector_y[7]);
	const __m256i cy8=_mm256_load_si256( (__m256i *) &mask_vector_y[8]);


	const __m256i cy0_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[9]);
	const __m256i cy1_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[10]);
	const __m256i cy2_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[11]);
	const __m256i cy3_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[12]);
	const __m256i cy4_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[13]);
	const __m256i cy5_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[14]);
	const __m256i cy6_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[15]);
	const __m256i cy7_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[16]);
	const __m256i cy8_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[17]);


	const __m256i mask3    = _mm256_set_epi16(0,0,0,0,0,0,0,65535,0,0,0,0,0,0,0,0);
	const __m256i mask3_2  = _mm256_set_epi16(0,0,0,0,0,0,65535,0,0,0,0,0,0,0,0,0);


	const __m256i mask_prelude  = _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0);
	const __m256i mask_prelude_3  = _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0);
	const __m256i mask_prelude_2  = _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0);
	const __m256i mask_prelude_1  = _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0);

	const __m256i mask_16_1   = _mm256_set_epi16(0,0,0,0,0,0xffff,0,0,0,0,0xffff,0,0,0,0,0xffff);


	const __m256i mask_new_1  = _mm256_set_epi16(0,0,0,0,0,0,0,0,0,0,0xffff,0,0,0,0,0);

	const __m256i mask_16_5  = _mm256_set_epi16(0,0xffff,0,0,0,0,0xffff,0,0,0,0,0xffff,0,0,0,0);


	const unsigned int REMINDER_ITERATIONS_X_mask =  M-((((M-32)/30)*30)+30); //M-(last_col_value+30)
	const unsigned int REMINDER_ITERATIONS_Y_mask =  M-((((M-32)/32)*32)+32);

	//printf("\nREMINDER_ITERATIONS=%u,%u",REMINDER_ITERATIONS_X_mask,REMINDER_ITERATIONS_Y_mask);


	const unsigned int division_case=prepare_for_division(divisor_xy); //determine which is the division case (A, B or C)
	const __m256i f = _mm256_load_si256( (__m256i *) &f_vector[0]);// initialize the division vector
	//const __m256i ones=_mm256_set1_epi16(1);

//	const unsigned int division_case_x=prepare_for_division_32(divisor_x); //determine which is the division case (A, B or C)
//	const __m256i f_x = _mm256_load_si256( (__m256i *) &f_vector_x[0]);// initialize the division vector
//printf("\n%d ",division_case);


#pragma omp parallel
{

unsigned int row,col;
__m256i r0,r1,r2,r3,r4,r5,r6,r7,r8,m0,m1,m2,m3,m4,m5,m6,m7,m8,even,odd;
//__m256i output1,output2,output3,output4;
unsigned char temp[M+32+6] __attribute__((aligned(64)));//temporal storage of the output of y filter.


/*---------------------- Gaussian Blur ---------------------------------*/

    #pragma omp for schedule(static)
	for (row = 3; row < N-3; row++) {

		if (row<4){
				prelude_9x9_16_Ymask_0(0, M, frame1, temp,mask_vector_y, division_case, f, mask_prelude);
				loop_reminder_9x9_16_blur_Y(frame1,filt,temp,N,M,0, REMINDER_ITERATIONS_Y_mask, division_case, mask_vector_y, f, divisor_xy);
				prelude_9x9_16_Xmask(frame1,filt,temp,N,M,0, mask_vector_x,  division_case,f,REMINDER_ITERATIONS_X_mask,REMINDER_ITERATIONS_Y_mask,mask_vector_y,divisor_xy,kernel_x);

				prelude_9x9_16_Ymask_1(0, M, frame1, temp,mask_vector_y, division_case, f, mask_prelude);
				loop_reminder_9x9_16_blur_Y(frame1,filt,temp,N,M,1, REMINDER_ITERATIONS_Y_mask, division_case, mask_vector_y, f, divisor_xy);
				prelude_9x9_16_Xmask(frame1,filt,temp,N,M,1, mask_vector_x,  division_case,f,REMINDER_ITERATIONS_X_mask,REMINDER_ITERATIONS_Y_mask,mask_vector_y,divisor_xy,kernel_x);

				prelude_9x9_16_Ymask_2(0, M, frame1, temp,mask_vector_y, division_case, f, mask_prelude);
				loop_reminder_9x9_16_blur_Y(frame1,filt,temp,N,M,2, REMINDER_ITERATIONS_Y_mask, division_case, mask_vector_y, f, divisor_xy);
				prelude_9x9_16_Xmask(frame1,filt,temp,N,M,2, mask_vector_x,  division_case,f,REMINDER_ITERATIONS_X_mask,REMINDER_ITERATIONS_Y_mask,mask_vector_y,divisor_xy,kernel_x);

				prelude_9x9_16_Ymask_3(0, M, frame1, temp,mask_vector_y, division_case, f, mask_prelude);
				loop_reminder_9x9_16_blur_Y(frame1,filt,temp,N,M,3, REMINDER_ITERATIONS_Y_mask, division_case, mask_vector_y, f, divisor_xy);
				prelude_9x9_16_Xmask(frame1,filt,temp,N,M,3, mask_vector_x,  division_case,f,REMINDER_ITERATIONS_X_mask,REMINDER_ITERATIONS_Y_mask,mask_vector_y,divisor_xy,kernel_x);
		}

		else if (row>N-5){

			prelude_9x9_16_Ymask_0(N-5, M, frame1, temp,mask_vector_y, division_case, f, mask_prelude);
			loop_reminder_9x9_16_blur_Y(frame1,filt,temp,N,M,N-1, REMINDER_ITERATIONS_Y_mask, division_case, mask_vector_y, f, divisor_xy);
			prelude_9x9_16_Xmask(frame1,filt,temp,N,M,N-1, mask_vector_x,  division_case,f,REMINDER_ITERATIONS_X_mask,REMINDER_ITERATIONS_Y_mask,mask_vector_y,divisor_xy,kernel_x);

			prelude_9x9_16_Ymask_1(N-6, M, frame1, temp,mask_vector_y, division_case, f, mask_prelude);
			loop_reminder_9x9_16_blur_Y(frame1,filt,temp,N,M,N-2, REMINDER_ITERATIONS_Y_mask, division_case, mask_vector_y, f, divisor_xy);
			prelude_9x9_16_Xmask(frame1,filt,temp,N,M,N-2, mask_vector_x,  division_case,f,REMINDER_ITERATIONS_X_mask,REMINDER_ITERATIONS_Y_mask,mask_vector_y,divisor_xy,kernel_x);

			prelude_9x9_16_Ymask_2(N-7, M, frame1, temp,mask_vector_y, division_case, f, mask_prelude);
			loop_reminder_9x9_16_blur_Y(frame1,filt,temp,N,M,N-3, REMINDER_ITERATIONS_Y_mask, division_case, mask_vector_y, f, divisor_xy);
			prelude_9x9_16_Xmask(frame1,filt,temp,N,M,N-3, mask_vector_x,  division_case,f,REMINDER_ITERATIONS_X_mask,REMINDER_ITERATIONS_Y_mask,mask_vector_y,divisor_xy,kernel_x);

			prelude_9x9_16_Ymask_3(N-8, M, frame1, temp,mask_vector_y, division_case, f, mask_prelude);
			loop_reminder_9x9_16_blur_Y(frame1,filt,temp,N,M,N-4, REMINDER_ITERATIONS_Y_mask, division_case, mask_vector_y, f, divisor_xy);
			prelude_9x9_16_Xmask(frame1,filt,temp,N,M,N-4, mask_vector_x,  division_case,f,REMINDER_ITERATIONS_X_mask,REMINDER_ITERATIONS_Y_mask,mask_vector_y,divisor_xy,kernel_x);

		}

	  else { //main loop


		  for (col = 0; col <= M-32; col+=32){

					//load the 9 rows
					r0=_mm256_load_si256( (__m256i *) &frame1[row-4][col]);
					r1=_mm256_load_si256( (__m256i *) &frame1[row-3][col]);
					r2=_mm256_load_si256( (__m256i *) &frame1[row-2][col]);
					r3=_mm256_load_si256( (__m256i *) &frame1[row-1][col]);
					r4=_mm256_load_si256( (__m256i *) &frame1[row][col]);
					r5=_mm256_load_si256( (__m256i *) &frame1[row+1][col]);
					r6=_mm256_load_si256( (__m256i *) &frame1[row+2][col]);
					r7=_mm256_load_si256( (__m256i *) &frame1[row+3][col]);
					r8=_mm256_load_si256( (__m256i *) &frame1[row+4][col]);

			  //--------------------y filter begins--------------------

				//multiply with the mask
					m0=_mm256_maddubs_epi16(r0,cy0);
					m1=_mm256_maddubs_epi16(r1,cy1);
					m2=_mm256_maddubs_epi16(r2,cy2);
					m3=_mm256_maddubs_epi16(r3,cy3);
					m4=_mm256_maddubs_epi16(r4,cy4);
					m5=_mm256_maddubs_epi16(r5,cy5);
					m6=_mm256_maddubs_epi16(r6,cy6);
					m7=_mm256_maddubs_epi16(r7,cy7);
					m8=_mm256_maddubs_epi16(r8,cy8);

					//vertical add
					m0=_mm256_add_epi16(m0,m1);
					m0=_mm256_add_epi16(m0,m2);
					m0=_mm256_add_epi16(m0,m3);
					m0=_mm256_add_epi16(m0,m4);
					m0=_mm256_add_epi16(m0,m5);
					m0=_mm256_add_epi16(m0,m6);
					m0=_mm256_add_epi16(m0,m7);
					m0=_mm256_add_epi16(m0,m8);

					even=division(division_case,m0,f);//even results

					//multiply with the mask
					m0=_mm256_maddubs_epi16(r0,cy0_sh1);
					m1=_mm256_maddubs_epi16(r1,cy1_sh1);
					m2=_mm256_maddubs_epi16(r2,cy2_sh1);
					m3=_mm256_maddubs_epi16(r3,cy3_sh1);
					m4=_mm256_maddubs_epi16(r4,cy4_sh1);
					m5=_mm256_maddubs_epi16(r5,cy5_sh1);
					m6=_mm256_maddubs_epi16(r6,cy6_sh1);
					m7=_mm256_maddubs_epi16(r7,cy7_sh1);
					m8=_mm256_maddubs_epi16(r8,cy8_sh1);

					//vertical add
					m0=_mm256_add_epi16(m0,m1);
					m0=_mm256_add_epi16(m0,m2);
					m0=_mm256_add_epi16(m0,m3);
					m0=_mm256_add_epi16(m0,m4);
					m0=_mm256_add_epi16(m0,m5);
					m0=_mm256_add_epi16(m0,m6);
					m0=_mm256_add_epi16(m0,m7);
					m0=_mm256_add_epi16(m0,m8);

					odd=division(division_case,m0,f);//odd results

					//pack to one register
					odd=_mm256_slli_si256(odd,1); //shift one position right
					r0=_mm256_add_epi8(even,odd); //add the odd with the even
				//y filter ends - r0 has now the data to be processed by x filter

				_mm256_store_si256( (__m256i *) &temp[col],r0 );
		  }
	loop_reminder_9x9_16_blur_Y(frame1,filt,temp,N,M,row, REMINDER_ITERATIONS_Y_mask, division_case, mask_vector_y, f, divisor_xy);


	  for (col = 0; col <= M-32; col+=30){

		  if (col==0){

	  			//1st col iteration
	  __m256i	rr0=_mm256_load_si256( (__m256i *) &temp[0]);
		  		r0=fill_zeros(rr0,mask_prelude);
	  			//multiply by the mask
	  			m0=_mm256_maddubs_epi16(r0,cx0);

	  			//first horizontal add (complex shift is needed)
	  			m3=_mm256_srli_si256(m0,2);
	  			m4=_mm256_and_si256(m0,mask3);
	  			m4=_mm256_permute2f128_si256(m4,m4,1);
	  			m4=_mm256_slli_si256(m4,14);
	  			m3=_mm256_add_epi16(m3,m4);
	  			m3=_mm256_add_epi16(m0,m3);//add

	  			//second horizontal add
	  			m1=_mm256_srli_si256(m3,4);
	  			m3=_mm256_add_epi16(m3,m1);//add

	  			//third horizontal add
	  			m0=_mm256_and_si256(m0,mask_16_5);
	  			m2=_mm256_srli_si256(m0,8);
	  			m4=_mm256_and_si256(m0,mask3_2);
	  			m4=_mm256_permute2f128_si256(m4,m4,1);
	  			m4=_mm256_slli_si256(m4,8);
	  			m2=_mm256_add_epi16(m2,m4);

	  			m0=_mm256_add_epi16(m2,m3);//add

	  			even=_mm256_and_si256(m0,mask_16_1);//in 16-bit format keep only 0,5,10 and discard the others


	  			//2nd col iteration
			//	r0=_mm256_load_si256( (__m256i *) &temp[0]);
		  		r0=fill_3zeros(rr0,mask_prelude_3);

	  			//multiply by the mask
	  			m0=_mm256_maddubs_epi16(r0,cx0);

	  			//first horizontal add (complex shift is needed)
	  			m3=_mm256_srli_si256(m0,2);
	  			m4=_mm256_and_si256(m0,mask3);
	  			m4=_mm256_permute2f128_si256(m4,m4,1);
	  			m4=_mm256_slli_si256(m4,14);
	  			m3=_mm256_add_epi16(m3,m4);
	  			m3=_mm256_add_epi16(m0,m3);//add

	  			//second horizontal add
	  			m1=_mm256_srli_si256(m3,4);
	  			m3=_mm256_add_epi16(m3,m1);//add

	  			//third horizontal add
	  			m0=_mm256_and_si256(m0,mask_16_5);
	  			m2=_mm256_srli_si256(m0,8);
	  			m4=_mm256_and_si256(m0,mask3_2);
	  			m4=_mm256_permute2f128_si256(m4,m4,1);
	  			m4=_mm256_slli_si256(m4,8);
	  			m2=_mm256_add_epi16(m2,m4);

	  			m0=_mm256_add_epi16(m2,m3);//add

	  			odd=_mm256_and_si256(m0,mask_16_1);//in 16-bit format keep only 0,5,10 and discard the others


	  			//3rd col iteration
			//	r0=_mm256_load_si256( (__m256i *) &temp[0]);
		  		r0=fill_2zeros(rr0,mask_prelude_2);

	  			//multiply by the mask
	  			m0=_mm256_maddubs_epi16(r0,cx0);

	  			//first horizontal add (complex shift is needed)
	  			m3=_mm256_srli_si256(m0,2);
	  			m4=_mm256_and_si256(m0,mask3);
	  			m4=_mm256_permute2f128_si256(m4,m4,1);
	  			m4=_mm256_slli_si256(m4,14);
	  			m3=_mm256_add_epi16(m3,m4);
	  			m3=_mm256_add_epi16(m0,m3);//add

	  			//second horizontal add
	  			m1=_mm256_srli_si256(m3,4);
	  			m3=_mm256_add_epi16(m3,m1);//add

	  			//third horizontal add
	  			m0=_mm256_and_si256(m0,mask_16_5);
	  			m2=_mm256_srli_si256(m0,8);
	  			m4=_mm256_and_si256(m0,mask3_2);
	  			m4=_mm256_permute2f128_si256(m4,m4,1);
	  			m4=_mm256_slli_si256(m4,8);
	  			m2=_mm256_add_epi16(m2,m4);

	  			m0=_mm256_add_epi16(m2,m3);//add

	  			m0=_mm256_and_si256(m0,mask_16_1);//in 16-bit format keep only 0,5,10 and discard the others
	  			m0=_mm256_slli_si256(m0,2);
	  			even=_mm256_add_epi16(m0,even);

	  			//4th col iteration
				//r0=_mm256_load_si256( (__m256i *) &temp[0]);
		  		r0=fill_1zeros(rr0,mask_prelude_1);

	  			//multiply by the mask
	  			m0=_mm256_maddubs_epi16(r0,cx0);

	  			//first horizontal add (complex shift is needed)
	  			m3=_mm256_srli_si256(m0,2);
	  			m4=_mm256_and_si256(m0,mask3);
	  			m4=_mm256_permute2f128_si256(m4,m4,1);
	  			m4=_mm256_slli_si256(m4,14);
	  			m3=_mm256_add_epi16(m3,m4);
	  			m3=_mm256_add_epi16(m0,m3);//add

	  			//second horizontal add
	  			m1=_mm256_srli_si256(m3,4);
	  			m3=_mm256_add_epi16(m3,m1);//add

	  			//third horizontal add
	  			m0=_mm256_and_si256(m0,mask_16_5);
	  			m2=_mm256_srli_si256(m0,8);
	  			m4=_mm256_and_si256(m0,mask3_2);
	  			m4=_mm256_permute2f128_si256(m4,m4,1);
	  			m4=_mm256_slli_si256(m4,8);
	  			m2=_mm256_add_epi16(m2,m4);

	  			m0=_mm256_add_epi16(m2,m3);//add

	  			m0=_mm256_and_si256(m0,mask_16_1);//in 16-bit format keep only 0,5,10 and discard the others
	  			m0=_mm256_slli_si256(m0,2);
	  			odd=_mm256_add_epi16(m0,odd);

	  			//5th col iteration
				//r0=_mm256_load_si256( (__m256i *) &temp[0]);

				//multiply by the mask
	  			m0=_mm256_maddubs_epi16(rr0,cx0);

	  			//first horizontal add (complex shift is needed)
	  			m3=_mm256_srli_si256(m0,2);
	  			m4=_mm256_and_si256(m0,mask3);
	  			m4=_mm256_permute2f128_si256(m4,m4,1);
	  			m4=_mm256_slli_si256(m4,14);
	  			m3=_mm256_add_epi16(m3,m4);
	  			m3=_mm256_add_epi16(m0,m3);//add

	  			//second horizontal add
	  			m1=_mm256_srli_si256(m3,4);
	  			m3=_mm256_add_epi16(m3,m1);//add

	  			//third horizontal add
	  			m0=_mm256_and_si256(m0,mask_16_5);
	  			m2=_mm256_srli_si256(m0,8);
	  			m4=_mm256_and_si256(m0,mask3_2);
	  			m4=_mm256_permute2f128_si256(m4,m4,1);
	  			m4=_mm256_slli_si256(m4,8);
	  			m2=_mm256_add_epi16(m2,m4);

	  			m0=_mm256_add_epi16(m2,m3);//add

	  			m0=_mm256_and_si256(m0,mask_16_1);//in 16-bit format keep only 0,5,10 and discard the others
	  			m0=_mm256_slli_si256(m0,4);
	  			even=_mm256_add_epi16(m0,even);

	  			//6th col iteration
				r0=_mm256_loadu_si256( (__m256i *) &temp[1]);

	  			//multiply by the mask
	  			m0=_mm256_maddubs_epi16(r0,cx0);

	  			//first horizontal add (complex shift is needed)
	  			m3=_mm256_srli_si256(m0,2);
	  			m4=_mm256_and_si256(m0,mask3);
	  			m4=_mm256_permute2f128_si256(m4,m4,1);
	  			m4=_mm256_slli_si256(m4,14);
	  			m3=_mm256_add_epi16(m3,m4);
	  			m3=_mm256_add_epi16(m0,m3);//add

	  			//second horizontal add
	  			m1=_mm256_srli_si256(m3,4);
	  			m3=_mm256_add_epi16(m3,m1);//add

	  			//third horizontal add
	  			m0=_mm256_and_si256(m0,mask_16_5);
	  			m2=_mm256_srli_si256(m0,8);
	  			m4=_mm256_and_si256(m0,mask3_2);
	  			m4=_mm256_permute2f128_si256(m4,m4,1);
	  			m4=_mm256_slli_si256(m4,8);
	  			m2=_mm256_add_epi16(m2,m4);

	  			m0=_mm256_add_epi16(m2,m3);//add

	  			m0=_mm256_and_si256(m0,mask_16_1);//in 16-bit format keep only 0,5,10 and discard the others
	  			m0=_mm256_slli_si256(m0,4);
	  			odd=_mm256_add_epi16(m0,odd);

	  			//7th col iteration
				r0=_mm256_loadu_si256( (__m256i *) &temp[2]);

	  			//multiply by the mask
	  			m0=_mm256_maddubs_epi16(r0,cx0);

	  			//first horizontal add (complex shift is needed)
	  			m3=_mm256_srli_si256(m0,2);
	  			m4=_mm256_and_si256(m0,mask3);
	  			m4=_mm256_permute2f128_si256(m4,m4,1);
	  			m4=_mm256_slli_si256(m4,14);
	  			m3=_mm256_add_epi16(m3,m4);
	  			m3=_mm256_add_epi16(m0,m3);//add

	  			//second horizontal add
	  			m1=_mm256_srli_si256(m3,4);
	  			m3=_mm256_add_epi16(m3,m1);//add

	  			//third horizontal add
	  			m0=_mm256_and_si256(m0,mask_16_5);
	  			m2=_mm256_srli_si256(m0,8);
	  			m4=_mm256_and_si256(m0,mask3_2);
	  			m4=_mm256_permute2f128_si256(m4,m4,1);
	  			m4=_mm256_slli_si256(m4,8);
	  			m2=_mm256_add_epi16(m2,m4);

	  			m0=_mm256_add_epi16(m2,m3);//add

	  			m0=_mm256_and_si256(m0,mask_16_1);//in 16-bit format keep only 0,5,10 and discard the others

	  			//shift the result 3 positions
	  			m1=_mm256_slli_si256(m0,6);
	  			m2=_mm256_and_si256(m0,mask_new_1);
	  			m2=_mm256_permute2f128_si256(m2,m2,1);
	  			m2=_mm256_srli_si256(m2,10);
	  			m0=_mm256_add_epi16(m2,m1);

	  			even=_mm256_add_epi16(m0,even);

	  			//8th col iteration
				r0=_mm256_loadu_si256( (__m256i *) &temp[3]);

	  			//multiply by the mask
	  			m0=_mm256_maddubs_epi16(r0,cx0);

	  			//first horizontal add (complex shift is needed)
	  			m3=_mm256_srli_si256(m0,2);
	  			m4=_mm256_and_si256(m0,mask3);
	  			m4=_mm256_permute2f128_si256(m4,m4,1);
	  			m4=_mm256_slli_si256(m4,14);
	  			m3=_mm256_add_epi16(m3,m4);
	  			m3=_mm256_add_epi16(m0,m3);//add

	  			//second horizontal add
	  			m1=_mm256_srli_si256(m3,4);
	  			m3=_mm256_add_epi16(m3,m1);//add

	  			//third horizontal add
	  			m0=_mm256_and_si256(m0,mask_16_5);
	  			m2=_mm256_srli_si256(m0,8);
	  			m4=_mm256_and_si256(m0,mask3_2);
	  			m4=_mm256_permute2f128_si256(m4,m4,1);
	  			m4=_mm256_slli_si256(m4,8);
	  			m2=_mm256_add_epi16(m2,m4);

	  			m0=_mm256_add_epi16(m2,m3);//add

	  			m0=_mm256_and_si256(m0,mask_16_1);//in 16-bit format keep only 0,5,10 and discard the others

	  			//shift the result 3 positions
	  			m1=_mm256_slli_si256(m0,6);
	  			m2=_mm256_and_si256(m0,mask_new_1);
	  			m2=_mm256_permute2f128_si256(m2,m2,1);
	  			m2=_mm256_srli_si256(m2,10);
	  			m0=_mm256_add_epi16(m2,m1);

	  			odd=_mm256_add_epi16(m0,odd);

	  			//9th col iteration
				r0=_mm256_loadu_si256( (__m256i *) &temp[4]);

	  			//multiply by the mask
	  			m0=_mm256_maddubs_epi16(r0,cx0);

	  			//first horizontal add (complex shift is needed)
	  			m3=_mm256_srli_si256(m0,2);
	  			m4=_mm256_and_si256(m0,mask3);
	  			m4=_mm256_permute2f128_si256(m4,m4,1);
	  			m4=_mm256_slli_si256(m4,14);
	  			m3=_mm256_add_epi16(m3,m4);
	  			m3=_mm256_add_epi16(m0,m3);//add

	  			//second horizontal add
	  			m1=_mm256_srli_si256(m3,4);
	  			m3=_mm256_add_epi16(m3,m1);//add

	  			//third horizontal add
	  			m0=_mm256_and_si256(m0,mask_16_5);
	  			m2=_mm256_srli_si256(m0,8);
	  			m4=_mm256_and_si256(m0,mask3_2);
	  			m4=_mm256_permute2f128_si256(m4,m4,1);
	  			m4=_mm256_slli_si256(m4,8);
	  			m2=_mm256_add_epi16(m2,m4);

	  			m0=_mm256_add_epi16(m2,m3);//add

	  			m0=_mm256_and_si256(m0,mask_16_1);//in 16-bit format keep only 0,5,10 and discard the others

	  			//shift the result 4 positions
	  			m1=_mm256_slli_si256(m0,8);
	  			m2=_mm256_and_si256(m0,mask_new_1);
	  			m2=_mm256_permute2f128_si256(m2,m2,1);
	  			m2=_mm256_srli_si256(m2,8);
	  			m0=_mm256_add_epi16(m2,m1);

	  			even=_mm256_add_epi16(m0,even);

	  			//10th col iteration
				r0=_mm256_loadu_si256( (__m256i *) &temp[5]);

	  			//multiply by the mask
	  			m0=_mm256_maddubs_epi16(r0,cx0);

	  			//first horizontal add (complex shift is needed)
	  			m3=_mm256_srli_si256(m0,2);
	  			m4=_mm256_and_si256(m0,mask3);
	  			m4=_mm256_permute2f128_si256(m4,m4,1);
	  			m4=_mm256_slli_si256(m4,14);
	  			m3=_mm256_add_epi16(m3,m4);
	  			m3=_mm256_add_epi16(m0,m3);//add

	  			//second horizontal add
	  			m1=_mm256_srli_si256(m3,4);
	  			m3=_mm256_add_epi16(m3,m1);//add

	  			//third horizontal add
	  			m0=_mm256_and_si256(m0,mask_16_5);
	  			m2=_mm256_srli_si256(m0,8);
	  			m4=_mm256_and_si256(m0,mask3_2);
	  			m4=_mm256_permute2f128_si256(m4,m4,1);
	  			m4=_mm256_slli_si256(m4,8);
	  			m2=_mm256_add_epi16(m2,m4);

	  			m0=_mm256_add_epi16(m2,m3);//add

	  			m0=_mm256_and_si256(m0,mask_16_1);//in 16-bit format keep only 0,5,10 and discard the others

	  			//shift the result 4 positions
	  			m1=_mm256_slli_si256(m0,8);
	  			m2=_mm256_and_si256(m0,mask_new_1);
	  			m2=_mm256_permute2f128_si256(m2,m2,1);
	  			m2=_mm256_srli_si256(m2,8);
	  			m0=_mm256_add_epi16(m2,m1);

	  			odd=_mm256_add_epi16(m0,odd);

	  			//division
	  			even = division(division_case,even,f);
	  			odd = division(division_case,odd,f);

	  			//blend even , odd into one register
	  			odd = _mm256_slli_si256(odd,1);
	  			even=_mm256_add_epi16(even,odd);

	  			_mm256_storeu_si256( (__m256i *) &filt[row][0],even );

		  		  	}
		  else {

		  		// col iteration computes output pixels of   0,10,20
		  		// col+1 iteration computes output pixels of 1,11,21
		  		// col+2 iteration computes output pixels of 2,12,22
		  		// col+3 iteration computes output pixels of 3,13,23
		  		// col+4 iteration computes output pixels of 4,14,24
		  		// col+5 iteration computes output pixels of 5,15,25
		  		// col+6 iteration computes output pixels of 6,16,26
		  		// col+7 iteration computes output pixels of 7,17,27
		  		// col+8 iteration computes output pixels of 8,18,28
		  		// col+9 iteration computes output pixels of 9,19,29
		  		//afterwards, col becomes 30 and repeat the above process

		  			//1st col iteration

		  			//multiply by the mask
	  		        r0=_mm256_loadu_si256( (__m256i *) &temp[col-4]);
		  			m0=_mm256_maddubs_epi16(r0,cx0);

		  			//first horizontal add (complex shift is needed)
		  			m3=_mm256_srli_si256(m0,2);
		  			m4=_mm256_and_si256(m0,mask3);
		  			m4=_mm256_permute2f128_si256(m4,m4,1);
		  			m4=_mm256_slli_si256(m4,14);
		  			m3=_mm256_add_epi16(m3,m4);
		  			m3=_mm256_add_epi16(m0,m3);//add

		  			//second horizontal add
		  			m1=_mm256_srli_si256(m3,4);
		  			m3=_mm256_add_epi16(m3,m1);//add

		  			//third horizontal add
		  			m0=_mm256_and_si256(m0,mask_16_5);
		  			m2=_mm256_srli_si256(m0,8);
		  			m4=_mm256_and_si256(m0,mask3_2);
		  			m4=_mm256_permute2f128_si256(m4,m4,1);
		  			m4=_mm256_slli_si256(m4,8);
		  			m2=_mm256_add_epi16(m2,m4);

		  			m0=_mm256_add_epi16(m2,m3);//add

		  			even=_mm256_and_si256(m0,mask_16_1);//in 16-bit format keep only 0,5,10 and discard the others


		  			//2nd col iteration
	  		        r0=_mm256_loadu_si256( (__m256i *) &temp[col-3]);

		  			//multiply by the mask
		  			m0=_mm256_maddubs_epi16(r0,cx0);

		  			//first horizontal add (complex shift is needed)
		  			m3=_mm256_srli_si256(m0,2);
		  			m4=_mm256_and_si256(m0,mask3);
		  			m4=_mm256_permute2f128_si256(m4,m4,1);
		  			m4=_mm256_slli_si256(m4,14);
		  			m3=_mm256_add_epi16(m3,m4);
		  			m3=_mm256_add_epi16(m0,m3);//add

		  			//second horizontal add
		  			m1=_mm256_srli_si256(m3,4);
		  			m3=_mm256_add_epi16(m3,m1);//add

		  			//third horizontal add
		  			m0=_mm256_and_si256(m0,mask_16_5);
		  			m2=_mm256_srli_si256(m0,8);
		  			m4=_mm256_and_si256(m0,mask3_2);
		  			m4=_mm256_permute2f128_si256(m4,m4,1);
		  			m4=_mm256_slli_si256(m4,8);
		  			m2=_mm256_add_epi16(m2,m4);

		  			m0=_mm256_add_epi16(m2,m3);//add

		  			odd=_mm256_and_si256(m0,mask_16_1);//in 16-bit format keep only 0,5,10 and discard the others


		  			//3rd col iteration
	  		        r0=_mm256_loadu_si256( (__m256i *) &temp[col-2]);

		  			//multiply by the mask
		  			m0=_mm256_maddubs_epi16(r0,cx0);

		  			//first horizontal add (complex shift is needed)
		  			m3=_mm256_srli_si256(m0,2);
		  			m4=_mm256_and_si256(m0,mask3);
		  			m4=_mm256_permute2f128_si256(m4,m4,1);
		  			m4=_mm256_slli_si256(m4,14);
		  			m3=_mm256_add_epi16(m3,m4);
		  			m3=_mm256_add_epi16(m0,m3);//add

		  			//second horizontal add
		  			m1=_mm256_srli_si256(m3,4);
		  			m3=_mm256_add_epi16(m3,m1);//add

		  			//third horizontal add
		  			m0=_mm256_and_si256(m0,mask_16_5);
		  			m2=_mm256_srli_si256(m0,8);
		  			m4=_mm256_and_si256(m0,mask3_2);
		  			m4=_mm256_permute2f128_si256(m4,m4,1);
		  			m4=_mm256_slli_si256(m4,8);
		  			m2=_mm256_add_epi16(m2,m4);

		  			m0=_mm256_add_epi16(m2,m3);//add

		  			m0=_mm256_and_si256(m0,mask_16_1);//in 16-bit format keep only 0,5,10 and discard the others
		  			m0=_mm256_slli_si256(m0,2);
		  			even=_mm256_add_epi16(m0,even);

		  			//4th col iteration
	  		        r0=_mm256_loadu_si256( (__m256i *) &temp[col-1]);

		  			//multiply by the mask
		  			m0=_mm256_maddubs_epi16(r0,cx0);

		  			//first horizontal add (complex shift is needed)
		  			m3=_mm256_srli_si256(m0,2);
		  			m4=_mm256_and_si256(m0,mask3);
		  			m4=_mm256_permute2f128_si256(m4,m4,1);
		  			m4=_mm256_slli_si256(m4,14);
		  			m3=_mm256_add_epi16(m3,m4);
		  			m3=_mm256_add_epi16(m0,m3);//add

		  			//second horizontal add
		  			m1=_mm256_srli_si256(m3,4);
		  			m3=_mm256_add_epi16(m3,m1);//add

		  			//third horizontal add
		  			m0=_mm256_and_si256(m0,mask_16_5);
		  			m2=_mm256_srli_si256(m0,8);
		  			m4=_mm256_and_si256(m0,mask3_2);
		  			m4=_mm256_permute2f128_si256(m4,m4,1);
		  			m4=_mm256_slli_si256(m4,8);
		  			m2=_mm256_add_epi16(m2,m4);

		  			m0=_mm256_add_epi16(m2,m3);//add

		  			m0=_mm256_and_si256(m0,mask_16_1);//in 16-bit format keep only 0,5,10 and discard the others
		  			m0=_mm256_slli_si256(m0,2);
		  			odd=_mm256_add_epi16(m0,odd);

		  			//5th col iteration
	  		        r0=_mm256_loadu_si256( (__m256i *) &temp[col]);

		  			//multiply by the mask
		  			m0=_mm256_maddubs_epi16(r0,cx0);

		  			//first horizontal add (complex shift is needed)
		  			m3=_mm256_srli_si256(m0,2);
		  			m4=_mm256_and_si256(m0,mask3);
		  			m4=_mm256_permute2f128_si256(m4,m4,1);
		  			m4=_mm256_slli_si256(m4,14);
		  			m3=_mm256_add_epi16(m3,m4);
		  			m3=_mm256_add_epi16(m0,m3);//add

		  			//second horizontal add
		  			m1=_mm256_srli_si256(m3,4);
		  			m3=_mm256_add_epi16(m3,m1);//add

		  			//third horizontal add
		  			m0=_mm256_and_si256(m0,mask_16_5);
		  			m2=_mm256_srli_si256(m0,8);
		  			m4=_mm256_and_si256(m0,mask3_2);
		  			m4=_mm256_permute2f128_si256(m4,m4,1);
		  			m4=_mm256_slli_si256(m4,8);
		  			m2=_mm256_add_epi16(m2,m4);

		  			m0=_mm256_add_epi16(m2,m3);//add

		  			m0=_mm256_and_si256(m0,mask_16_1);//in 16-bit format keep only 0,5,10 and discard the others
		  			m0=_mm256_slli_si256(m0,4);
		  			even=_mm256_add_epi16(m0,even);

		  			//6th col iteration
	  		        r0=_mm256_loadu_si256( (__m256i *) &temp[col+1]);

		  			//multiply by the mask
		  			m0=_mm256_maddubs_epi16(r0,cx0);

		  			//first horizontal add (complex shift is needed)
		  			m3=_mm256_srli_si256(m0,2);
		  			m4=_mm256_and_si256(m0,mask3);
		  			m4=_mm256_permute2f128_si256(m4,m4,1);
		  			m4=_mm256_slli_si256(m4,14);
		  			m3=_mm256_add_epi16(m3,m4);
		  			m3=_mm256_add_epi16(m0,m3);//add

		  			//second horizontal add
		  			m1=_mm256_srli_si256(m3,4);
		  			m3=_mm256_add_epi16(m3,m1);//add

		  			//third horizontal add
		  			m0=_mm256_and_si256(m0,mask_16_5);
		  			m2=_mm256_srli_si256(m0,8);
		  			m4=_mm256_and_si256(m0,mask3_2);
		  			m4=_mm256_permute2f128_si256(m4,m4,1);
		  			m4=_mm256_slli_si256(m4,8);
		  			m2=_mm256_add_epi16(m2,m4);

		  			m0=_mm256_add_epi16(m2,m3);//add

		  			m0=_mm256_and_si256(m0,mask_16_1);//in 16-bit format keep only 0,5,10 and discard the others
		  			m0=_mm256_slli_si256(m0,4);
		  			odd=_mm256_add_epi16(m0,odd);

		  			//7th col iteration
	  		        r0=_mm256_loadu_si256( (__m256i *) &temp[col+2]);

		  			//multiply by the mask
		  			m0=_mm256_maddubs_epi16(r0,cx0);

		  			//first horizontal add (complex shift is needed)
		  			m3=_mm256_srli_si256(m0,2);
		  			m4=_mm256_and_si256(m0,mask3);
		  			m4=_mm256_permute2f128_si256(m4,m4,1);
		  			m4=_mm256_slli_si256(m4,14);
		  			m3=_mm256_add_epi16(m3,m4);
		  			m3=_mm256_add_epi16(m0,m3);//add

		  			//second horizontal add
		  			m1=_mm256_srli_si256(m3,4);
		  			m3=_mm256_add_epi16(m3,m1);//add

		  			//third horizontal add
		  			m0=_mm256_and_si256(m0,mask_16_5);
		  			m2=_mm256_srli_si256(m0,8);
		  			m4=_mm256_and_si256(m0,mask3_2);
		  			m4=_mm256_permute2f128_si256(m4,m4,1);
		  			m4=_mm256_slli_si256(m4,8);
		  			m2=_mm256_add_epi16(m2,m4);

		  			m0=_mm256_add_epi16(m2,m3);//add

		  			m0=_mm256_and_si256(m0,mask_16_1);//in 16-bit format keep only 0,5,10 and discard the others

		  			//shift the result 3 positions
		  			m1=_mm256_slli_si256(m0,6);
		  			m2=_mm256_and_si256(m0,mask_new_1);
		  			m2=_mm256_permute2f128_si256(m2,m2,1);
		  			m2=_mm256_srli_si256(m2,10);
		  			m0=_mm256_add_epi16(m2,m1);

		  			even=_mm256_add_epi16(m0,even);

		  			//8th col iteration
	  		        r0=_mm256_loadu_si256( (__m256i *) &temp[col+3]);

		  			//multiply by the mask
		  			m0=_mm256_maddubs_epi16(r0,cx0);

		  			//first horizontal add (complex shift is needed)
		  			m3=_mm256_srli_si256(m0,2);
		  			m4=_mm256_and_si256(m0,mask3);
		  			m4=_mm256_permute2f128_si256(m4,m4,1);
		  			m4=_mm256_slli_si256(m4,14);
		  			m3=_mm256_add_epi16(m3,m4);
		  			m3=_mm256_add_epi16(m0,m3);//add

		  			//second horizontal add
		  			m1=_mm256_srli_si256(m3,4);
		  			m3=_mm256_add_epi16(m3,m1);//add

		  			//third horizontal add
		  			m0=_mm256_and_si256(m0,mask_16_5);
		  			m2=_mm256_srli_si256(m0,8);
		  			m4=_mm256_and_si256(m0,mask3_2);
		  			m4=_mm256_permute2f128_si256(m4,m4,1);
		  			m4=_mm256_slli_si256(m4,8);
		  			m2=_mm256_add_epi16(m2,m4);

		  			m0=_mm256_add_epi16(m2,m3);//add

		  			m0=_mm256_and_si256(m0,mask_16_1);//in 16-bit format keep only 0,5,10 and discard the others

		  			//shift the result 3 positions
		  			m1=_mm256_slli_si256(m0,6);
		  			m2=_mm256_and_si256(m0,mask_new_1);
		  			m2=_mm256_permute2f128_si256(m2,m2,1);
		  			m2=_mm256_srli_si256(m2,10);
		  			m0=_mm256_add_epi16(m2,m1);

		  			odd=_mm256_add_epi16(m0,odd);

		  			//9th col iteration
	  		        r0=_mm256_loadu_si256( (__m256i *) &temp[col+4]);

		  			//multiply by the mask
		  			m0=_mm256_maddubs_epi16(r0,cx0);

		  			//first horizontal add (complex shift is needed)
		  			m3=_mm256_srli_si256(m0,2);
		  			m4=_mm256_and_si256(m0,mask3);
		  			m4=_mm256_permute2f128_si256(m4,m4,1);
		  			m4=_mm256_slli_si256(m4,14);
		  			m3=_mm256_add_epi16(m3,m4);
		  			m3=_mm256_add_epi16(m0,m3);//add

		  			//second horizontal add
		  			m1=_mm256_srli_si256(m3,4);
		  			m3=_mm256_add_epi16(m3,m1);//add

		  			//third horizontal add
		  			m0=_mm256_and_si256(m0,mask_16_5);
		  			m2=_mm256_srli_si256(m0,8);
		  			m4=_mm256_and_si256(m0,mask3_2);
		  			m4=_mm256_permute2f128_si256(m4,m4,1);
		  			m4=_mm256_slli_si256(m4,8);
		  			m2=_mm256_add_epi16(m2,m4);

		  			m0=_mm256_add_epi16(m2,m3);//add

		  			m0=_mm256_and_si256(m0,mask_16_1);//in 16-bit format keep only 0,5,10 and discard the others

		  			//shift the result 4 positions
		  			m1=_mm256_slli_si256(m0,8);
		  			m2=_mm256_and_si256(m0,mask_new_1);
		  			m2=_mm256_permute2f128_si256(m2,m2,1);
		  			m2=_mm256_srli_si256(m2,8);
		  			m0=_mm256_add_epi16(m2,m1);

		  			even=_mm256_add_epi16(m0,even);

		  			//10th col iteration
	  		        r0=_mm256_loadu_si256( (__m256i *) &temp[col+5]);

		  			//multiply by the mask
		  			m0=_mm256_maddubs_epi16(r0,cx0);

		  			//first horizontal add (complex shift is needed)
		  			m3=_mm256_srli_si256(m0,2);
		  			m4=_mm256_and_si256(m0,mask3);
		  			m4=_mm256_permute2f128_si256(m4,m4,1);
		  			m4=_mm256_slli_si256(m4,14);
		  			m3=_mm256_add_epi16(m3,m4);
		  			m3=_mm256_add_epi16(m0,m3);//add

		  			//second horizontal add
		  			m1=_mm256_srli_si256(m3,4);
		  			m3=_mm256_add_epi16(m3,m1);//add

		  			//third horizontal add
		  			m0=_mm256_and_si256(m0,mask_16_5);
		  			m2=_mm256_srli_si256(m0,8);
		  			m4=_mm256_and_si256(m0,mask3_2);
		  			m4=_mm256_permute2f128_si256(m4,m4,1);
		  			m4=_mm256_slli_si256(m4,8);
		  			m2=_mm256_add_epi16(m2,m4);

		  			m0=_mm256_add_epi16(m2,m3);//add

		  			m0=_mm256_and_si256(m0,mask_16_1);//in 16-bit format keep only 0,5,10 and discard the others

		  			//shift the result 4 positions
		  			m1=_mm256_slli_si256(m0,8);
		  			m2=_mm256_and_si256(m0,mask_new_1);
		  			m2=_mm256_permute2f128_si256(m2,m2,1);
		  			m2=_mm256_srli_si256(m2,8);
		  			m0=_mm256_add_epi16(m2,m1);

		  			odd=_mm256_add_epi16(m0,odd);

		  			//division
		  			even = division(division_case,even,f);
		  			odd = division(division_case,odd,f);

		  			//blend even , odd into one register
		  			odd = _mm256_slli_si256(odd,1);
		  			even=_mm256_add_epi16(even,odd);

		  			_mm256_storeu_si256( (__m256i *) &filt[row][col],even );

		  		 }

		}

		loop_reminder_9x9_16_blur_X(frame1,filt,temp,N,M,row, col, REMINDER_ITERATIONS_X_mask, division_case, mask_vector_x, f, divisor_xy,kernel_x);

		}
}

}//end of parallel


}





void Gaussian_Blur_9x9_16_separable_extra_array_less_load(unsigned char **frame1,unsigned char **filt,const unsigned int M, const unsigned int N, signed char *kernel_y, signed char *kernel_x, const unsigned short int divisor_xy){

const signed char fx0=kernel_x[0];
const signed char fx1=kernel_x[1];
const signed char fx2=kernel_x[2];
const signed char fx3=kernel_x[3];
const signed char fx4=kernel_x[4];
const signed char fx5=kernel_x[5];
const signed char fx6=kernel_x[6];
const signed char fx7=kernel_x[7];
const signed char fx8=kernel_x[8];


const signed char fy0=kernel_y[0];
const signed char fy1=kernel_y[1];
const signed char fy2=kernel_y[2];
const signed char fy3=kernel_y[3];
const signed char fy4=kernel_y[4];
const signed char fy5=kernel_y[5];
const signed char fy6=kernel_y[6];
const signed char fy7=kernel_y[7];
const signed char fy8=kernel_y[8];



const signed char mask_vector_x[10][32] __attribute__((aligned(64))) ={
	{fx0,fx1,fx2,fx3,fx4,fx5,fx6,fx7,fx8,0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,fx7,fx8,0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,fx7,fx8,0,0,0},
	{0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,fx7,fx8,0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,fx7,fx8,0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,fx7,fx8,0,0},
	{0,0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,fx7,fx8,0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,fx7,fx8,0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,fx7,fx8,0},
	{0,0,0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,fx7,fx8,0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,fx7,fx8,0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,fx7,fx8},
	{0,0,0,0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,fx7,fx8,0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,fx7,fx8,0,0,0,0,0,0,0,0,0},
	{0,0,0,0,0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,fx7,fx8,0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,fx7,fx8,0,0,0,0,0,0,0,0},
	{0,0,0,0,0,0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,fx7,fx8,0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,fx7,fx8,0,0,0,0,0,0,0},
	{0,0,0,0,0,0,0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,fx7,fx8,0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,fx7,fx8,0,0,0,0,0,0},
	{0,0,0,0,0,0,0,0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,fx7,fx8,0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,fx7,fx8,0,0,0,0,0},
	{0,0,0,0,0,0,0,0,0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,fx7,fx8,0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,fx7,fx8,0,0,0,0},
};

const signed char mask_vector_y[18][32] __attribute__((aligned(64))) ={
	{fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0},
	{fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0},
	{fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0},
	{fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0},
	{fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0},
	{fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0},
	{fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0},
	{fy7,0,fy7,0,fy7,0,fy7,0,fy7,0,fy7,0,fy7,0,fy7,0,fy7,0,fy7,0,fy7,0,fy7,0,fy7,0,fy7,0,fy7,0,fy7,0},
	{fy8,0,fy8,0,fy8,0,fy8,0,fy8,0,fy8,0,fy8,0,fy8,0,fy8,0,fy8,0,fy8,0,fy8,0,fy8,0,fy8,0,fy8,0,fy8,0},

	{0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0},
	{0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1},
	{0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2},
	{0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3},
	{0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4},
	{0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5},
	{0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6},
	{0,fy7,0,fy7,0,fy7,0,fy7,0,fy7,0,fy7,0,fy7,0,fy7,0,fy7,0,fy7,0,fy7,0,fy7,0,fy7,0,fy7,0,fy7,0,fy7},
	{0,fy8,0,fy8,0,fy8,0,fy8,0,fy8,0,fy8,0,fy8,0,fy8,0,fy8,0,fy8,0,fy8,0,fy8,0,fy8,0,fy8,0,fy8,0,fy8},
};

	const __m256i cx0=_mm256_load_si256( (__m256i *) &mask_vector_x[0]);
	const __m256i cx1=_mm256_load_si256( (__m256i *) &mask_vector_x[1]);
	const __m256i cx2=_mm256_load_si256( (__m256i *) &mask_vector_x[2]);
	const __m256i cx3=_mm256_load_si256( (__m256i *) &mask_vector_x[3]);
	const __m256i cx4=_mm256_load_si256( (__m256i *) &mask_vector_x[4]);
	const __m256i cx5=_mm256_load_si256( (__m256i *) &mask_vector_x[5]);
	const __m256i cx6=_mm256_load_si256( (__m256i *) &mask_vector_x[6]);
	const __m256i cx7=_mm256_load_si256( (__m256i *) &mask_vector_x[7]);
	const __m256i cx8=_mm256_load_si256( (__m256i *) &mask_vector_x[8]);
	const __m256i cx9=_mm256_load_si256( (__m256i *) &mask_vector_x[9]);


	const __m256i cy0=_mm256_load_si256( (__m256i *) &mask_vector_y[0]);
	const __m256i cy1=_mm256_load_si256( (__m256i *) &mask_vector_y[1]);
	const __m256i cy2=_mm256_load_si256( (__m256i *) &mask_vector_y[2]);
	const __m256i cy3=_mm256_load_si256( (__m256i *) &mask_vector_y[3]);
	const __m256i cy4=_mm256_load_si256( (__m256i *) &mask_vector_y[4]);
	const __m256i cy5=_mm256_load_si256( (__m256i *) &mask_vector_y[5]);
	const __m256i cy6=_mm256_load_si256( (__m256i *) &mask_vector_y[6]);
	const __m256i cy7=_mm256_load_si256( (__m256i *) &mask_vector_y[7]);
	const __m256i cy8=_mm256_load_si256( (__m256i *) &mask_vector_y[8]);


	const __m256i cy0_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[9]);
	const __m256i cy1_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[10]);
	const __m256i cy2_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[11]);
	const __m256i cy3_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[12]);
	const __m256i cy4_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[13]);
	const __m256i cy5_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[14]);
	const __m256i cy6_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[15]);
	const __m256i cy7_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[16]);
	const __m256i cy8_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[17]);


	const __m256i mask3    = _mm256_set_epi16(0,0,0,0,0,0,0,65535,0,0,0,0,0,0,0,0);
	const __m256i mask3_2  = _mm256_set_epi16(0,0,0,0,0,0,65535,0,0,0,0,0,0,0,0,0);
	const __m256i mask3_3  = _mm256_set_epi16(0,0,0,0,0,65535,0,0,0,0,0,0,0,0,0,0);
	const __m256i mask3_4  = _mm256_set_epi16(0,0,0,0,65535,0,0,0,0,0,0,0,0,0,0,0);

	const __m256i mask_prelude  = _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0);
	//const __m256i mask_prelude_3  = _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0);
	//const __m256i mask_prelude_2  = _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0);
	//const __m256i mask_prelude_1  = _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0);

//	const __m256i mask_even_16  = _mm256_set_epi16(0,0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff);
//	const __m256i mask_odd_16  = _mm256_set_epi16(0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff,0);
	const __m256i mask_16_1   = _mm256_set_epi16(0,0,0,0,0,0xffff,0,0,0,0,0xffff,0,0,0,0,0xffff);
	const __m256i mask_16_2   = _mm256_set_epi16(0,0,0,0,0xffff,0,0,0,0,0xffff,0,0,0,0,0xffff,0);
	const __m256i mask_16_3   = _mm256_set_epi16(0,0,0,0,0,0,0,0,0xffff,0,0,0,0,0xffff,0,0);
	const __m256i mask_16_4   = _mm256_set_epi16(0,0,0,0,0,0,0,0xffff,0,0,0,0,0xffff,0,0,0);
	const __m256i mask_16_4b  = _mm256_set_epi16(0,0,0,0,0,0,0xffff,0,0,0,0,0xffff,0,0,0,0);

	//const __m256i mask_new_1  = _mm256_set_epi16(0,0,0,0,0,0,0,0,0,0,0xffff,0,0,0,0,0);

	const __m256i mask_16_5  = _mm256_set_epi16(0,0xffff,0,0,0,0,0xffff,0,0,0,0,0xffff,0,0,0,0);
	const __m256i mask_16_6  = _mm256_set_epi16(0xffff,0,0,0,0,0xffff,0,0,0,0,0xffff,0,0,0,0,0);
	const __m256i mask_16_7  = _mm256_set_epi16(0,0,0,0,0xffff,0,0,0,0,0xffff,0,0,0,0,0,0);
	const __m256i mask_16_8  = _mm256_set_epi16(0,0,0xffff,0,0,0,0,0xffff,0,0,0,0,0,0,0,0);


//	const __m256i mask_even_32  = _mm256_set_epi32(0,0xffffffff,0,0xffffffff,0,0xffffffff,0,0xffffffff);
//	const __m256i mask_odd_32  = _mm256_set_epi32(0xffffffff,0,0xffffffff,0,0xffffffff,0,0xffffffff,0);
	//const __m256i mask4  = _mm256_set_epi32(0,0,0,0xffffffff,0,0,0,0);



//	const unsigned int REMINDER_ITERATIONS =  (M-((((M-32)/24)*24)+24)); //M-(last_col_value+24)

	//printf("\nREMINDER_ITERATIONS=%d",REMINDER_ITERATIONS);


	const unsigned int division_case=prepare_for_division(divisor_xy); //determine which is the division case (A, B or C)
	const __m256i f = _mm256_load_si256( (__m256i *) &f_vector[0]);// initialize the division vector
	//const __m256i ones=_mm256_set1_epi16(1);

//	const unsigned int division_case_x=prepare_for_division_32(divisor_x); //determine which is the division case (A, B or C)
//	const __m256i f_x = _mm256_load_si256( (__m256i *) &f_vector_x[0]);// initialize the division vector
//printf("\n%d %d",division_case,b);


#pragma omp parallel
{

unsigned int row,col;
__m256i r0,r1,r2,r3,r4,r5,r6,r7,r8,r9,m0,m1,m2,m3,m4,m5,m6,m7,m8,even,odd;
//__m256i output1,output2,output3,output4;
unsigned char temp[M] __attribute__((aligned(64)));//temporal storage of the output of y filter.
unsigned char temp2[M] __attribute__((aligned(64)));//temporal storage of the output of y filter.


/*---------------------- Gaussian Blur ---------------------------------*/

    #pragma omp for schedule(static)
	for (row = 4; row < N-5; row+=2) {

		if (row<4){
		/*	if (row==0){
				for (col = 0; col <= M-32; col+=26){
				r0=prelude_7x7_16_Ymask_1(0, col, frame1, mask_vector_y, division_case, f, mask_prelude);
				prelude_7x7_16_Xmask(filt,row,col, r0, mask_vector_x,  division_case,f);
				}
				loop_reminder_7x7_16_blur(frame1,filt,N,row, col, REMINDER_ITERATIONS, division_case, mask_vector_x, mask_vector_y, f, divisor_xy);

			}
			else if (row==1){
				for (col = 0; col <= M-32; col+=26){
				r0=prelude_7x7_16_Ymask_2(0, col, frame1, mask_vector_y, division_case, f, mask_prelude);
				prelude_7x7_16_Xmask(filt,row,col, r0, mask_vector_x,  division_case,f);
				}
				loop_reminder_7x7_16_blur(frame1,filt,N,row, col, REMINDER_ITERATIONS, division_case, mask_vector_x, mask_vector_y, f, divisor_xy);
			}
			else if (row==2){
				for (col = 0; col <= M-32; col+=26){
				r0=prelude_7x7_16_Ymask_3(0, col, frame1, mask_vector_y, division_case, f, mask_prelude);
				prelude_7x7_16_Xmask(filt,row,col, r0, mask_vector_x,  division_case,f);
				}
				loop_reminder_7x7_16_blur(frame1,filt,N,row, col, REMINDER_ITERATIONS, division_case, mask_vector_x, mask_vector_y, f, divisor_xy);
			}*/
		}

		else if (row>N-5){
		/*	if (row==N-1){
				for (col = 0; col <= M-32; col+=26){
				r0=prelude_7x7_16_Ymask_1(N-4, col, frame1, mask_vector_y, division_case, f, mask_prelude);
				prelude_7x7_16_Xmask(filt,row,col, r0, mask_vector_x,  division_case,f);
			}
			loop_reminder_7x7_16_blur(frame1,filt,N,row, col, REMINDER_ITERATIONS, division_case, mask_vector_x, mask_vector_y, f, divisor_xy);

			}
			else if (row==N-2){
				for (col = 0; col <= M-32; col+=26){
				r0=prelude_7x7_16_Ymask_2(N-5, col, frame1, mask_vector_y, division_case, f, mask_prelude);
				prelude_7x7_16_Xmask(filt,row,col, r0, mask_vector_x,  division_case,f);
			}
			loop_reminder_7x7_16_blur(frame1,filt,N,row, col, REMINDER_ITERATIONS, division_case, mask_vector_x, mask_vector_y, f, divisor_xy);

			}
			else if (row==N-3){
				for (col = 0; col <= M-32; col+=26){
				r0=prelude_7x7_16_Ymask_3(N-6, col, frame1, mask_vector_y, division_case, f, mask_prelude);
				prelude_7x7_16_Xmask(filt,row,col, r0, mask_vector_x,  division_case,f);
			}
			loop_reminder_7x7_16_blur(frame1,filt,N,row, col, REMINDER_ITERATIONS, division_case, mask_vector_x, mask_vector_y, f, divisor_xy);

			}*/
		}

	  else { //main loop


	  for (col = 0; col <= M-32; col+=32){

				//load the 9 rows
				r0=_mm256_load_si256( (__m256i *) &frame1[row-4][col]);
				r1=_mm256_load_si256( (__m256i *) &frame1[row-3][col]);
				r2=_mm256_load_si256( (__m256i *) &frame1[row-2][col]);
				r3=_mm256_load_si256( (__m256i *) &frame1[row-1][col]);
				r4=_mm256_load_si256( (__m256i *) &frame1[row][col]);
				r5=_mm256_load_si256( (__m256i *) &frame1[row+1][col]);
				r6=_mm256_load_si256( (__m256i *) &frame1[row+2][col]);
				r7=_mm256_load_si256( (__m256i *) &frame1[row+3][col]);
				r8=_mm256_load_si256( (__m256i *) &frame1[row+4][col]);
				r9=_mm256_load_si256( (__m256i *) &frame1[row+5][col]);

		  //--------------------y filter begins--------------------

			//multiply with the mask
				m0=_mm256_maddubs_epi16(r0,cy0);
				m1=_mm256_maddubs_epi16(r1,cy1);
				m2=_mm256_maddubs_epi16(r2,cy2);
				m3=_mm256_maddubs_epi16(r3,cy3);
				m4=_mm256_maddubs_epi16(r4,cy4);
				m5=_mm256_maddubs_epi16(r5,cy5);
				m6=_mm256_maddubs_epi16(r6,cy6);
				m7=_mm256_maddubs_epi16(r7,cy7);
				m8=_mm256_maddubs_epi16(r8,cy8);

				//vertical add
				m0=_mm256_add_epi16(m0,m1);
				m0=_mm256_add_epi16(m0,m2);
				m0=_mm256_add_epi16(m0,m3);
				m0=_mm256_add_epi16(m0,m4);
				m0=_mm256_add_epi16(m0,m5);
				m0=_mm256_add_epi16(m0,m6);
				m0=_mm256_add_epi16(m0,m7);
				m0=_mm256_add_epi16(m0,m8);

				even=division(division_case,m0,f);//even results

				//multiply with the mask
				m0=_mm256_maddubs_epi16(r0,cy0_sh1);
				m1=_mm256_maddubs_epi16(r1,cy1_sh1);
				m2=_mm256_maddubs_epi16(r2,cy2_sh1);
				m3=_mm256_maddubs_epi16(r3,cy3_sh1);
				m4=_mm256_maddubs_epi16(r4,cy4_sh1);
				m5=_mm256_maddubs_epi16(r5,cy5_sh1);
				m6=_mm256_maddubs_epi16(r6,cy6_sh1);
				m7=_mm256_maddubs_epi16(r7,cy7_sh1);
				m8=_mm256_maddubs_epi16(r8,cy8_sh1);

				//vertical add
				m0=_mm256_add_epi16(m0,m1);
				m0=_mm256_add_epi16(m0,m2);
				m0=_mm256_add_epi16(m0,m3);
				m0=_mm256_add_epi16(m0,m4);
				m0=_mm256_add_epi16(m0,m5);
				m0=_mm256_add_epi16(m0,m6);
				m0=_mm256_add_epi16(m0,m7);
				m0=_mm256_add_epi16(m0,m8);

				odd=division(division_case,m0,f);//odd results

				//pack to one register
				odd=_mm256_slli_si256(odd,1); //shift one position right
				r0=_mm256_add_epi8(even,odd); //add the odd with the even
			//y filter ends - r0 has now the data to be processed by x filter

			_mm256_store_si256( (__m256i *) &temp[col],r0 );

			  //--------------------2nd y filter begins--------------------

				//multiply with the mask
					m0=_mm256_maddubs_epi16(r1,cy0);
					m1=_mm256_maddubs_epi16(r2,cy1);
					m2=_mm256_maddubs_epi16(r3,cy2);
					m3=_mm256_maddubs_epi16(r4,cy3);
					m4=_mm256_maddubs_epi16(r5,cy4);
					m5=_mm256_maddubs_epi16(r6,cy5);
					m6=_mm256_maddubs_epi16(r7,cy6);
					m7=_mm256_maddubs_epi16(r8,cy7);
					m8=_mm256_maddubs_epi16(r9,cy8);

					//vertical add
					m0=_mm256_add_epi16(m0,m1);
					m0=_mm256_add_epi16(m0,m2);
					m0=_mm256_add_epi16(m0,m3);
					m0=_mm256_add_epi16(m0,m4);
					m0=_mm256_add_epi16(m0,m5);
					m0=_mm256_add_epi16(m0,m6);
					m0=_mm256_add_epi16(m0,m7);
					m0=_mm256_add_epi16(m0,m8);

					even=division(division_case,m0,f);//even results

					//multiply with the mask
					m0=_mm256_maddubs_epi16(r1,cy0_sh1);
					m1=_mm256_maddubs_epi16(r2,cy1_sh1);
					m2=_mm256_maddubs_epi16(r3,cy2_sh1);
					m3=_mm256_maddubs_epi16(r4,cy3_sh1);
					m4=_mm256_maddubs_epi16(r5,cy4_sh1);
					m5=_mm256_maddubs_epi16(r6,cy5_sh1);
					m6=_mm256_maddubs_epi16(r7,cy6_sh1);
					m7=_mm256_maddubs_epi16(r8,cy7_sh1);
					m8=_mm256_maddubs_epi16(r9,cy8_sh1);

					//vertical add
					m0=_mm256_add_epi16(m0,m1);
					m0=_mm256_add_epi16(m0,m2);
					m0=_mm256_add_epi16(m0,m3);
					m0=_mm256_add_epi16(m0,m4);
					m0=_mm256_add_epi16(m0,m5);
					m0=_mm256_add_epi16(m0,m6);
					m0=_mm256_add_epi16(m0,m7);
					m0=_mm256_add_epi16(m0,m8);

					odd=division(division_case,m0,f);//odd results

					//pack to one register
					odd=_mm256_slli_si256(odd,1); //shift one position right
					r0=_mm256_add_epi8(even,odd); //add the odd with the even
				//y filter ends - r0 has now the data to be processed by x filter

				_mm256_store_si256( (__m256i *) &temp2[col],r0 );
	  }


	  for (col = 0; col <= M-32; col+=24){

		  if (col==0){
				r0=_mm256_load_si256( (__m256i *) &temp[0]);
		  		r0=fill_zeros(r0,mask_prelude);
				r1=_mm256_load_si256( (__m256i *) &temp2[0]);
		  		r1=fill_zeros(r1,mask_prelude);
		  		  	}
		  else {
		  		  r0=_mm256_loadu_si256( (__m256i *) &temp[col-4]);
		  		  r1=_mm256_loadu_si256( (__m256i *) &temp2[col-4]);

		  		 }

		// col iteration computes output pixels of   0,10,20
		// col+1 iteration computes output pixels of 1,11,21
		// col+2 iteration computes output pixels of 2,12,22
		// col+3 iteration computes output pixels of 3,13,23
		// col+4 iteration computes output pixels of 4,14
		// col+5 iteration computes output pixels of 5,15
		// col+6 iteration computes output pixels of 6,16
		// col+7 iteration computes output pixels of 7,17
		// col+8 iteration computes output pixels of 8,18
		// col+9 iteration computes output pixels of 9,19
		//afterwards, col becomes 24 and repeat the above process


			//1st col iteration

			//multiply by the mask
			m0=_mm256_maddubs_epi16(r0,cx0);

			//first horizontal add (complex shift is needed)
			m3=_mm256_srli_si256(m0,2);
			m4=_mm256_and_si256(m0,mask3);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_slli_si256(m4,14);
			m3=_mm256_add_epi16(m3,m4);
			m3=_mm256_add_epi16(m0,m3);//add

			//second horizontal add
			m1=_mm256_srli_si256(m3,4);
			m3=_mm256_add_epi16(m3,m1);//add

			//third horizontal add
			m0=_mm256_and_si256(m0,mask_16_5);
			m2=_mm256_srli_si256(m0,8);
			m4=_mm256_and_si256(m0,mask3_2);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_slli_si256(m4,8);
			m2=_mm256_add_epi16(m2,m4);

			m0=_mm256_add_epi16(m2,m3);//add

			even=_mm256_and_si256(m0,mask_16_1);//in 16-bit format keep only 0,5,10 and discard the others


			//2nd col iteration

			//multiply by the mask
			m0=_mm256_maddubs_epi16(r0,cx1);

			//first horizontal add (complex shift is needed)
			m3=_mm256_srli_si256(m0,2);
			m4=_mm256_and_si256(m0,mask3);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_slli_si256(m4,14);
			m3=_mm256_add_epi16(m3,m4);
			m3=_mm256_add_epi16(m0,m3);//add

			//second horizontal add
			m1=_mm256_srli_si256(m3,4);
			m3=_mm256_add_epi16(m3,m1);//add

			//third horizontal add
			m0=_mm256_and_si256(m0,mask_16_5);
			m2=_mm256_srli_si256(m0,8);
			m4=_mm256_and_si256(m0,mask3_2);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_slli_si256(m4,8);
			m2=_mm256_add_epi16(m2,m4);

			m0=_mm256_add_epi16(m2,m3);//add

			odd=_mm256_and_si256(m0,mask_16_1);//in 16-bit format keep only 0,5,10 and discard the others


			//3rd col iteration

			//multiply by the mask
			m0=_mm256_maddubs_epi16(r0,cx2);

			//1st horizontal add
			m1=_mm256_srli_si256(m0,2);
			m2=_mm256_add_epi16(m0,m1);//add

			//2nd horizontal add (complex shift is needed)
			m3=_mm256_srli_si256(m2,4);
			m4=_mm256_and_si256(m2,mask3);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_slli_si256(m4,12);
			m3=_mm256_add_epi16(m3,m4);
			m8=_mm256_add_epi16(m2,m3);//add

			//3rd horizontal add
			m0=_mm256_and_si256(m0,mask_16_6);
			m2=_mm256_srli_si256(m0,8);
			m4=_mm256_and_si256(m0,mask3_3);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_slli_si256(m4,8);
			m2=_mm256_add_epi16(m2,m4);

			m0=_mm256_add_epi16(m2,m8);//add

			m0=_mm256_and_si256(m0,mask_16_2);//in 16-bit format keep only 1,6,11 and discard the others
			even=_mm256_add_epi16(m0,even);

			//4th col iteration

			//multiply by the mask
			m0=_mm256_maddubs_epi16(r0,cx3);

			//1st horizontal add
			m1=_mm256_srli_si256(m0,2);
			m2=_mm256_add_epi16(m0,m1);//add

			//2nd horizontal add (complex shift is needed)
			m3=_mm256_srli_si256(m2,4);
			m4=_mm256_and_si256(m2,mask3);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_slli_si256(m4,12);
			m3=_mm256_add_epi16(m3,m4);
			m8=_mm256_add_epi16(m2,m3);//add

			//3rd horizontal add
			m0=_mm256_and_si256(m0,mask_16_6);
			m2=_mm256_srli_si256(m0,8);
			m4=_mm256_and_si256(m0,mask3_3);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_slli_si256(m4,8);
			m2=_mm256_add_epi16(m2,m4);

			m0=_mm256_add_epi16(m2,m8);//add

			m0=_mm256_and_si256(m0,mask_16_2);//in 16-bit format keep only 1,6,11  and discard the others
			odd=_mm256_add_epi16(m0,odd);

			//5th col iteration

			//multiply by the mask
			m0=_mm256_maddubs_epi16(r0,cx4);

			//first horizontal add (complex shift is needed)
			m3=_mm256_srli_si256(m0,2);
			m4=_mm256_and_si256(m0,mask3);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_slli_si256(m4,14);
			m3=_mm256_add_epi16(m3,m4);
			m2=_mm256_add_epi16(m0,m3);//add

			//2nd horizontal add (complex shift is needed)
			m3=_mm256_srli_si256(m2,4);
			m4=_mm256_and_si256(m2,mask3_2);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_slli_si256(m4,12);
			m3=_mm256_add_epi16(m3,m4);
			m3=_mm256_add_epi16(m2,m3);//add

			//3rd horizontal add
			m0=_mm256_and_si256(m0,mask_16_7);
			m2=_mm256_srli_si256(m0,8);
			m4=_mm256_and_si256(m0,mask3_4);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_slli_si256(m4,8);
			m2=_mm256_add_epi16(m2,m4);

			m0=_mm256_add_epi16(m2,m3);//add
			m0=_mm256_and_si256(m0,mask_16_3);//in 16-bit format keep only 2,7  and discard the others
			even=_mm256_add_epi16(m0,even);

			//6th col iteration

			//multiply by the mask
			m0=_mm256_maddubs_epi16(r0,cx5);

			//first horizontal add (complex shift is needed)
			m3=_mm256_srli_si256(m0,2);
			m4=_mm256_and_si256(m0,mask3);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_slli_si256(m4,14);
			m3=_mm256_add_epi16(m3,m4);
			m2=_mm256_add_epi16(m0,m3);//add

			//2nd horizontal add (complex shift is needed)
			m3=_mm256_srli_si256(m2,4);
			m4=_mm256_and_si256(m2,mask3_2);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_slli_si256(m4,12);
			m3=_mm256_add_epi16(m3,m4);
			m3=_mm256_add_epi16(m2,m3);//add

			//3rd horizontal add
			m0=_mm256_and_si256(m0,mask_16_7);
			m2=_mm256_srli_si256(m0,8);
			m4=_mm256_and_si256(m0,mask3_4);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_slli_si256(m4,8);
			m2=_mm256_add_epi16(m2,m4);

			m0=_mm256_add_epi16(m2,m3);//add
			m0=_mm256_and_si256(m0,mask_16_3);//in 16-bit format keep only 2,7  and discard the others
			odd=_mm256_add_epi16(m0,odd);

			//7th col iteration

			//multiply by the mask
			m0=_mm256_maddubs_epi16(r0,cx6);

			//1st horizontal add
			m1=_mm256_srli_si256(m0,2);
			m0=_mm256_add_epi16(m0,m1);//add

			//2nd horizontal add
			m1=_mm256_srli_si256(m0,4);
			m0=_mm256_add_epi16(m0,m1);//add

			//3rd horizontal add
			m1=_mm256_srli_si256(m0,8);
			m0=_mm256_add_epi16(m0,m1);//add

			m0=_mm256_and_si256(m0,mask_16_4);//in 16-bit format keep only 4,8  and discard the others
			even=_mm256_add_epi16(m0,even);

			//8th col iteration

			//multiply by the mask
			m0=_mm256_maddubs_epi16(r0,cx7);

			//1st horizontal add
			m1=_mm256_srli_si256(m0,2);
			m0=_mm256_add_epi16(m0,m1);//add

			//2nd horizontal add
			m1=_mm256_srli_si256(m0,4);
			m0=_mm256_add_epi16(m0,m1);//add

			//3rd horizontal add
			m1=_mm256_srli_si256(m0,8);
			m0=_mm256_add_epi16(m0,m1);//add

			m0=_mm256_and_si256(m0,mask_16_4);//in 16-bit format keep only 4,8  and discard the others
			odd=_mm256_add_epi16(m0,odd);

			//9th col iteration
			//multiply by the mask
			m0=_mm256_maddubs_epi16(r0,cx8);

			//1st horizontal add
			m1=_mm256_srli_si256(m0,2);
			m2=_mm256_add_epi16(m0,m1);//add

			//2nd horizontal add
			m1=_mm256_srli_si256(m2,4);
			m7=_mm256_add_epi16(m2,m1);//add

			//3rd horizontal add
			m0=_mm256_and_si256(m0,mask_16_8);
			m2=_mm256_srli_si256(m0,8);
			m4=_mm256_and_si256(m0,mask3);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_slli_si256(m4,8);
			m2=_mm256_add_epi16(m2,m4);

			m0=_mm256_add_epi16(m2,m7);//add
			m0=_mm256_and_si256(m0,mask_16_4b);//in 16-bit format keep only 4,9  and discard the others
			even=_mm256_add_epi16(m0,even);

			//10th col iteration
			//multiply by the mask
			m0=_mm256_maddubs_epi16(r0,cx9);

			//1st horizontal add
			m1=_mm256_srli_si256(m0,2);
			m2=_mm256_add_epi16(m0,m1);//add

			//2nd horizontal add
			m1=_mm256_srli_si256(m2,4);
			m7=_mm256_add_epi16(m2,m1);//add

			//3rd horizontal add
			m0=_mm256_and_si256(m0,mask_16_8);
			m2=_mm256_srli_si256(m0,8);
			m4=_mm256_and_si256(m0,mask3);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_slli_si256(m4,8);
			m2=_mm256_add_epi16(m2,m4);

			m0=_mm256_add_epi16(m2,m7);//add
			m0=_mm256_and_si256(m0,mask_16_4b);//in 16-bit format keep only 4,9  and discard the others
			odd=_mm256_add_epi16(m0,odd);

			//division
			even = division(division_case,even,f);
			odd = division(division_case,odd,f);

			//blend even , odd into one register
			odd = _mm256_slli_si256(odd,1);
			even=_mm256_add_epi16(even,odd);

			_mm256_storeu_si256( (__m256i *) &filt[row][col],even );

			//row+1
			//1st col iteration

			//multiply by the mask
			m0=_mm256_maddubs_epi16(r1,cx0);

			//first horizontal add (complex shift is needed)
			m3=_mm256_srli_si256(m0,2);
			m4=_mm256_and_si256(m0,mask3);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_slli_si256(m4,14);
			m3=_mm256_add_epi16(m3,m4);
			m3=_mm256_add_epi16(m0,m3);//add

			//second horizontal add
			m1=_mm256_srli_si256(m3,4);
			m3=_mm256_add_epi16(m3,m1);//add

			//third horizontal add
			m0=_mm256_and_si256(m0,mask_16_5);
			m2=_mm256_srli_si256(m0,8);
			m4=_mm256_and_si256(m0,mask3_2);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_slli_si256(m4,8);
			m2=_mm256_add_epi16(m2,m4);

			m0=_mm256_add_epi16(m2,m3);//add

			even=_mm256_and_si256(m0,mask_16_1);//in 16-bit format keep only 0,5,10 and discard the others


			//2nd col iteration

			//multiply by the mask
			m0=_mm256_maddubs_epi16(r1,cx1);

			//first horizontal add (complex shift is needed)
			m3=_mm256_srli_si256(m0,2);
			m4=_mm256_and_si256(m0,mask3);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_slli_si256(m4,14);
			m3=_mm256_add_epi16(m3,m4);
			m3=_mm256_add_epi16(m0,m3);//add

			//second horizontal add
			m1=_mm256_srli_si256(m3,4);
			m3=_mm256_add_epi16(m3,m1);//add

			//third horizontal add
			m0=_mm256_and_si256(m0,mask_16_5);
			m2=_mm256_srli_si256(m0,8);
			m4=_mm256_and_si256(m0,mask3_2);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_slli_si256(m4,8);
			m2=_mm256_add_epi16(m2,m4);

			m0=_mm256_add_epi16(m2,m3);//add

			odd=_mm256_and_si256(m0,mask_16_1);//in 16-bit format keep only 0,5,10 and discard the others


			//3rd col iteration

			//multiply by the mask
			m0=_mm256_maddubs_epi16(r1,cx2);

			//1st horizontal add
			m1=_mm256_srli_si256(m0,2);
			m2=_mm256_add_epi16(m0,m1);//add

			//2nd horizontal add (complex shift is needed)
			m3=_mm256_srli_si256(m2,4);
			m4=_mm256_and_si256(m2,mask3);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_slli_si256(m4,12);
			m3=_mm256_add_epi16(m3,m4);
			m8=_mm256_add_epi16(m2,m3);//add

			//3rd horizontal add
			m0=_mm256_and_si256(m0,mask_16_6);
			m2=_mm256_srli_si256(m0,8);
			m4=_mm256_and_si256(m0,mask3_3);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_slli_si256(m4,8);
			m2=_mm256_add_epi16(m2,m4);

			m0=_mm256_add_epi16(m2,m8);//add

			m0=_mm256_and_si256(m0,mask_16_2);//in 16-bit format keep only 1,6,11 and discard the others
			even=_mm256_add_epi16(m0,even);

			//4th col iteration

			//multiply by the mask
			m0=_mm256_maddubs_epi16(r1,cx3);

			//1st horizontal add
			m1=_mm256_srli_si256(m0,2);
			m2=_mm256_add_epi16(m0,m1);//add

			//2nd horizontal add (complex shift is needed)
			m3=_mm256_srli_si256(m2,4);
			m4=_mm256_and_si256(m2,mask3);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_slli_si256(m4,12);
			m3=_mm256_add_epi16(m3,m4);
			m8=_mm256_add_epi16(m2,m3);//add

			//3rd horizontal add
			m0=_mm256_and_si256(m0,mask_16_6);
			m2=_mm256_srli_si256(m0,8);
			m4=_mm256_and_si256(m0,mask3_3);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_slli_si256(m4,8);
			m2=_mm256_add_epi16(m2,m4);

			m0=_mm256_add_epi16(m2,m8);//add

			m0=_mm256_and_si256(m0,mask_16_2);//in 16-bit format keep only 1,6,11  and discard the others
			odd=_mm256_add_epi16(m0,odd);

			//5th col iteration

			//multiply by the mask
			m0=_mm256_maddubs_epi16(r1,cx4);

			//first horizontal add (complex shift is needed)
			m3=_mm256_srli_si256(m0,2);
			m4=_mm256_and_si256(m0,mask3);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_slli_si256(m4,14);
			m3=_mm256_add_epi16(m3,m4);
			m2=_mm256_add_epi16(m0,m3);//add

			//2nd horizontal add (complex shift is needed)
			m3=_mm256_srli_si256(m2,4);
			m4=_mm256_and_si256(m2,mask3_2);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_slli_si256(m4,12);
			m3=_mm256_add_epi16(m3,m4);
			m3=_mm256_add_epi16(m2,m3);//add

			//3rd horizontal add
			m0=_mm256_and_si256(m0,mask_16_7);
			m2=_mm256_srli_si256(m0,8);
			m4=_mm256_and_si256(m0,mask3_4);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_slli_si256(m4,8);
			m2=_mm256_add_epi16(m2,m4);

			m0=_mm256_add_epi16(m2,m3);//add
			m0=_mm256_and_si256(m0,mask_16_3);//in 16-bit format keep only 2,7  and discard the others
			even=_mm256_add_epi16(m0,even);

			//6th col iteration

			//multiply by the mask
			m0=_mm256_maddubs_epi16(r1,cx5);

			//first horizontal add (complex shift is needed)
			m3=_mm256_srli_si256(m0,2);
			m4=_mm256_and_si256(m0,mask3);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_slli_si256(m4,14);
			m3=_mm256_add_epi16(m3,m4);
			m2=_mm256_add_epi16(m0,m3);//add

			//2nd horizontal add (complex shift is needed)
			m3=_mm256_srli_si256(m2,4);
			m4=_mm256_and_si256(m2,mask3_2);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_slli_si256(m4,12);
			m3=_mm256_add_epi16(m3,m4);
			m3=_mm256_add_epi16(m2,m3);//add

			//3rd horizontal add
			m0=_mm256_and_si256(m0,mask_16_7);
			m2=_mm256_srli_si256(m0,8);
			m4=_mm256_and_si256(m0,mask3_4);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_slli_si256(m4,8);
			m2=_mm256_add_epi16(m2,m4);

			m0=_mm256_add_epi16(m2,m3);//add
			m0=_mm256_and_si256(m0,mask_16_3);//in 16-bit format keep only 2,7  and discard the others
			odd=_mm256_add_epi16(m0,odd);

			//7th col iteration

			//multiply by the mask
			m0=_mm256_maddubs_epi16(r1,cx6);

			//1st horizontal add
			m1=_mm256_srli_si256(m0,2);
			m0=_mm256_add_epi16(m0,m1);//add

			//2nd horizontal add
			m1=_mm256_srli_si256(m0,4);
			m0=_mm256_add_epi16(m0,m1);//add

			//3rd horizontal add
			m1=_mm256_srli_si256(m0,8);
			m0=_mm256_add_epi16(m0,m1);//add

			m0=_mm256_and_si256(m0,mask_16_4);//in 16-bit format keep only 4,8  and discard the others
			even=_mm256_add_epi16(m0,even);

			//8th col iteration

			//multiply by the mask
			m0=_mm256_maddubs_epi16(r1,cx7);

			//1st horizontal add
			m1=_mm256_srli_si256(m0,2);
			m0=_mm256_add_epi16(m0,m1);//add

			//2nd horizontal add
			m1=_mm256_srli_si256(m0,4);
			m0=_mm256_add_epi16(m0,m1);//add

			//3rd horizontal add
			m1=_mm256_srli_si256(m0,8);
			m0=_mm256_add_epi16(m0,m1);//add

			m0=_mm256_and_si256(m0,mask_16_4);//in 16-bit format keep only 4,8  and discard the others
			odd=_mm256_add_epi16(m0,odd);

			//9th col iteration
			//multiply by the mask
			m0=_mm256_maddubs_epi16(r1,cx8);

			//1st horizontal add
			m1=_mm256_srli_si256(m0,2);
			m2=_mm256_add_epi16(m0,m1);//add

			//2nd horizontal add
			m1=_mm256_srli_si256(m2,4);
			m7=_mm256_add_epi16(m2,m1);//add

			//3rd horizontal add
			m0=_mm256_and_si256(m0,mask_16_8);
			m2=_mm256_srli_si256(m0,8);
			m4=_mm256_and_si256(m0,mask3);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_slli_si256(m4,8);
			m2=_mm256_add_epi16(m2,m4);

			m0=_mm256_add_epi16(m2,m7);//add
			m0=_mm256_and_si256(m0,mask_16_4b);//in 16-bit format keep only 4,9  and discard the others
			even=_mm256_add_epi16(m0,even);

			//10th col iteration
			//multiply by the mask
			m0=_mm256_maddubs_epi16(r1,cx9);

			//1st horizontal add
			m1=_mm256_srli_si256(m0,2);
			m2=_mm256_add_epi16(m0,m1);//add

			//2nd horizontal add
			m1=_mm256_srli_si256(m2,4);
			m7=_mm256_add_epi16(m2,m1);//add

			//3rd horizontal add
			m0=_mm256_and_si256(m0,mask_16_8);
			m2=_mm256_srli_si256(m0,8);
			m4=_mm256_and_si256(m0,mask3);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_slli_si256(m4,8);
			m2=_mm256_add_epi16(m2,m4);

			m0=_mm256_add_epi16(m2,m7);//add
			m0=_mm256_and_si256(m0,mask_16_4b);//in 16-bit format keep only 4,9  and discard the others
			odd=_mm256_add_epi16(m0,odd);

			//division
			even = division(division_case,even,f);
			odd = division(division_case,odd,f);

			//blend even , odd into one register
			odd = _mm256_slli_si256(odd,1);
			even=_mm256_add_epi16(even,odd);

			_mm256_storeu_si256( (__m256i *) &filt[row+1][col],even );
		}
	  //loop_reminder_7x7_16_blur(frame1,filt,N,row, col, REMINDER_ITERATIONS, division_case, mask_vector_x, mask_vector_y, f, divisor_xy);


		}
}

}//end of parallel


}



//this function is computing the row=3 and row=N-4 only
//for row=3 input prelude_7x7_Ymask_3(0,...
//for row=N-4 input prelude_7x7_Ymask_3(N-8,...
void prelude_9x9_16_Ymask_3(const int row, const unsigned int M, unsigned char **frame1, unsigned char *temp, const signed char mask_vector_y[][32],  const unsigned int division_case, const __m256i f, const __m256i mask_prelude){

	__m256i r0,r1,r2,r3,r4,r5,r6,r7,m0,m1,m2,m3,m4,m5,m6,m7,even,odd;
	//__m256i ones=_mm256_set1_epi16(1);
	 __m256i cy0,cy1,cy2,cy3,cy4,cy5,cy6,cy7,cy0_sh1,cy1_sh1,cy2_sh1,cy3_sh1,cy4_sh1,cy5_sh1,cy6_sh1,cy7_sh1;

	if (row!=0){ //if row=N-6
	  cy0=_mm256_load_si256( (__m256i *) &mask_vector_y[0]);
	  cy1=_mm256_load_si256( (__m256i *) &mask_vector_y[1]);
	  cy2=_mm256_load_si256( (__m256i *) &mask_vector_y[2]);
	  cy3=_mm256_load_si256( (__m256i *) &mask_vector_y[3]);
	  cy4=_mm256_load_si256( (__m256i *) &mask_vector_y[4]);
	  cy5=_mm256_load_si256( (__m256i *) &mask_vector_y[5]);
	  cy6=_mm256_load_si256( (__m256i *) &mask_vector_y[6]);
	  cy7=_mm256_load_si256( (__m256i *) &mask_vector_y[7]);

	  cy0_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[9]);
	  cy1_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[10]);
	  cy2_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[11]);
	  cy3_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[12]);
	  cy4_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[13]);
	  cy5_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[14]);
	  cy6_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[15]);
	  cy7_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[16]);

	}
	else {
		  cy0=_mm256_load_si256( (__m256i *) &mask_vector_y[1]);
		  cy1=_mm256_load_si256( (__m256i *) &mask_vector_y[2]);
		  cy2=_mm256_load_si256( (__m256i *) &mask_vector_y[3]);
		  cy3=_mm256_load_si256( (__m256i *) &mask_vector_y[4]);
		  cy4=_mm256_load_si256( (__m256i *) &mask_vector_y[5]);
		  cy5=_mm256_load_si256( (__m256i *) &mask_vector_y[6]);
		  cy6=_mm256_load_si256( (__m256i *) &mask_vector_y[7]);
		  cy7=_mm256_load_si256( (__m256i *) &mask_vector_y[8]);

		  cy0_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[10]);
		  cy1_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[11]);
		  cy2_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[12]);
		  cy3_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[13]);
		  cy4_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[14]);
		  cy5_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[15]);
		  cy6_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[16]);
		  cy7_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[17]);
	}

	 for (unsigned int col = 0; col <= M-32; col+=32){

	  //--------------------y filter begins--------------------
	//load the 9 rows
	r0=_mm256_load_si256( (__m256i *) &frame1[row][col]);
	r1=_mm256_load_si256( (__m256i *) &frame1[row+1][col]);
	r2=_mm256_load_si256( (__m256i *) &frame1[row+2][col]);
	r3=_mm256_load_si256( (__m256i *) &frame1[row+3][col]);
	r4=_mm256_load_si256( (__m256i *) &frame1[row+4][col]);
	r5=_mm256_load_si256( (__m256i *) &frame1[row+5][col]);
	r6=_mm256_load_si256( (__m256i *) &frame1[row+6][col]);
	r7=_mm256_load_si256( (__m256i *) &frame1[row+7][col]);

		//multiply with the mask
			m0=_mm256_maddubs_epi16(r0,cy0);
			m1=_mm256_maddubs_epi16(r1,cy1);
			m2=_mm256_maddubs_epi16(r2,cy2);
			m3=_mm256_maddubs_epi16(r3,cy3);
			m4=_mm256_maddubs_epi16(r4,cy4);
			m5=_mm256_maddubs_epi16(r5,cy5);
			m6=_mm256_maddubs_epi16(r6,cy6);
			m7=_mm256_maddubs_epi16(r7,cy7);

			//vertical add
			m0=_mm256_add_epi16(m0,m1);
			m0=_mm256_add_epi16(m0,m2);
			m0=_mm256_add_epi16(m0,m3);
			m0=_mm256_add_epi16(m0,m4);
			m0=_mm256_add_epi16(m0,m5);
			m0=_mm256_add_epi16(m0,m6);
			m0=_mm256_add_epi16(m0,m7);

			even=division(division_case,m0,f);//even results

			//multiply with the mask
			m0=_mm256_maddubs_epi16(r0,cy0_sh1);
			m1=_mm256_maddubs_epi16(r1,cy1_sh1);
			m2=_mm256_maddubs_epi16(r2,cy2_sh1);
			m3=_mm256_maddubs_epi16(r3,cy3_sh1);
			m4=_mm256_maddubs_epi16(r4,cy4_sh1);
			m5=_mm256_maddubs_epi16(r5,cy5_sh1);
			m6=_mm256_maddubs_epi16(r6,cy6_sh1);
			m7=_mm256_maddubs_epi16(r7,cy7_sh1);

			//vertical add
			m0=_mm256_add_epi16(m0,m1);
			m0=_mm256_add_epi16(m0,m2);
			m0=_mm256_add_epi16(m0,m3);
			m0=_mm256_add_epi16(m0,m4);
			m0=_mm256_add_epi16(m0,m5);
			m0=_mm256_add_epi16(m0,m6);
			m0=_mm256_add_epi16(m0,m7);

			odd=division(division_case,m0,f);//odd results

			//pack to one register
			odd=_mm256_slli_si256(odd,1); //shift one position right
			r0 = _mm256_add_epi8(even,odd); //add the odd with the even
			_mm256_store_si256( (__m256i *) &temp[col],r0 );
	  }
		//y filter ends - r0 has now the data to be processed by x filter

}

//this function is computing the row=2 and row=N-3 only
//for row=3 input prelude_7x7_Ymask_3(0,...
//for row=N-3 input prelude_7x7_Ymask_3(N-7,...
void prelude_9x9_16_Ymask_2(const int row, const unsigned int M,unsigned char **frame1, unsigned char *temp, const signed char mask_vector_y[][32],  const unsigned int division_case, const __m256i f, const __m256i mask_prelude){

	__m256i r0,r1,r2,r3,r4,r5,r6,m0,m1,m2,m3,m4,m5,m6,even,odd;
	//__m256i ones=_mm256_set1_epi16(1);
	 __m256i cy0,cy1,cy2,cy3,cy4,cy5,cy6,cy0_sh1,cy1_sh1,cy2_sh1,cy3_sh1,cy4_sh1,cy5_sh1,cy6_sh1;

	if (row!=0){ //if row=N-7
	  cy0=_mm256_load_si256( (__m256i *) &mask_vector_y[0]);
	  cy1=_mm256_load_si256( (__m256i *) &mask_vector_y[1]);
	  cy2=_mm256_load_si256( (__m256i *) &mask_vector_y[2]);
	  cy3=_mm256_load_si256( (__m256i *) &mask_vector_y[3]);
	  cy4=_mm256_load_si256( (__m256i *) &mask_vector_y[4]);
	  cy5=_mm256_load_si256( (__m256i *) &mask_vector_y[5]);
	  cy6=_mm256_load_si256( (__m256i *) &mask_vector_y[6]);

	  cy0_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[9]);
	  cy1_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[10]);
	  cy2_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[11]);
	  cy3_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[12]);
	  cy4_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[13]);
	  cy5_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[14]);
	  cy6_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[15]);

	}
	else {
		  cy0=_mm256_load_si256( (__m256i *) &mask_vector_y[2]);
		  cy1=_mm256_load_si256( (__m256i *) &mask_vector_y[3]);
		  cy2=_mm256_load_si256( (__m256i *) &mask_vector_y[4]);
		  cy3=_mm256_load_si256( (__m256i *) &mask_vector_y[5]);
		  cy4=_mm256_load_si256( (__m256i *) &mask_vector_y[6]);
		  cy5=_mm256_load_si256( (__m256i *) &mask_vector_y[7]);
		  cy6=_mm256_load_si256( (__m256i *) &mask_vector_y[8]);

		  cy0_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[11]);
		  cy1_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[12]);
		  cy2_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[13]);
		  cy3_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[14]);
		  cy4_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[15]);
		  cy5_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[16]);
		  cy6_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[17]);
	}

	  for (unsigned int col = 0; col <= M-32; col+=32){

	  //--------------------y filter begins--------------------
	//load the 9 rows
	r0=_mm256_load_si256( (__m256i *) &frame1[row][col]);
	r1=_mm256_load_si256( (__m256i *) &frame1[row+1][col]);
	r2=_mm256_load_si256( (__m256i *) &frame1[row+2][col]);
	r3=_mm256_load_si256( (__m256i *) &frame1[row+3][col]);
	r4=_mm256_load_si256( (__m256i *) &frame1[row+4][col]);
	r5=_mm256_load_si256( (__m256i *) &frame1[row+5][col]);
	r6=_mm256_load_si256( (__m256i *) &frame1[row+6][col]);

		//multiply with the mask
			m0=_mm256_maddubs_epi16(r0,cy0);
			m1=_mm256_maddubs_epi16(r1,cy1);
			m2=_mm256_maddubs_epi16(r2,cy2);
			m3=_mm256_maddubs_epi16(r3,cy3);
			m4=_mm256_maddubs_epi16(r4,cy4);
			m5=_mm256_maddubs_epi16(r5,cy5);
			m6=_mm256_maddubs_epi16(r6,cy6);

			//vertical add
			m0=_mm256_add_epi16(m0,m1);
			m0=_mm256_add_epi16(m0,m2);
			m0=_mm256_add_epi16(m0,m3);
			m0=_mm256_add_epi16(m0,m4);
			m0=_mm256_add_epi16(m0,m5);
			m0=_mm256_add_epi16(m0,m6);

			even=division(division_case,m0,f);//even results

			//multiply with the mask
			m0=_mm256_maddubs_epi16(r0,cy0_sh1);
			m1=_mm256_maddubs_epi16(r1,cy1_sh1);
			m2=_mm256_maddubs_epi16(r2,cy2_sh1);
			m3=_mm256_maddubs_epi16(r3,cy3_sh1);
			m4=_mm256_maddubs_epi16(r4,cy4_sh1);
			m5=_mm256_maddubs_epi16(r5,cy5_sh1);
			m6=_mm256_maddubs_epi16(r6,cy6_sh1);

			//vertical add
			m0=_mm256_add_epi16(m0,m1);
			m0=_mm256_add_epi16(m0,m2);
			m0=_mm256_add_epi16(m0,m3);
			m0=_mm256_add_epi16(m0,m4);
			m0=_mm256_add_epi16(m0,m5);
			m0=_mm256_add_epi16(m0,m6);

			odd=division(division_case,m0,f);//odd results

			//pack to one register
			odd=_mm256_slli_si256(odd,1); //shift one position right
			r0 = _mm256_add_epi8(even,odd); //add the odd with the even
			_mm256_store_si256( (__m256i *) &temp[col],r0 );
	  }
		//y filter ends - r0 has now the data to be processed by x filter

}


//this function is computing the row=1 and row=N-2 only
//for row=3 input prelude_7x7_Ymask_3(0,...
//for row=N-3 input prelude_7x7_Ymask_3(N-6,...
void prelude_9x9_16_Ymask_1(const int row, const unsigned int M,unsigned char **frame1, unsigned char *temp, const signed char mask_vector_y[][32],  const unsigned int division_case, const __m256i f, const __m256i mask_prelude){

	__m256i r0,r1,r2,r3,r4,r5,m0,m1,m2,m3,m4,m5,even,odd;
	//__m256i ones=_mm256_set1_epi16(1);
	 __m256i cy0,cy1,cy2,cy3,cy4,cy5,cy0_sh1,cy1_sh1,cy2_sh1,cy3_sh1,cy4_sh1,cy5_sh1;

	if (row!=0){ //if row=N-6
	  cy0=_mm256_load_si256( (__m256i *) &mask_vector_y[0]);
	  cy1=_mm256_load_si256( (__m256i *) &mask_vector_y[1]);
	  cy2=_mm256_load_si256( (__m256i *) &mask_vector_y[2]);
	  cy3=_mm256_load_si256( (__m256i *) &mask_vector_y[3]);
	  cy4=_mm256_load_si256( (__m256i *) &mask_vector_y[4]);
	  cy5=_mm256_load_si256( (__m256i *) &mask_vector_y[5]);

	  cy0_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[9]);
	  cy1_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[10]);
	  cy2_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[11]);
	  cy3_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[12]);
	  cy4_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[13]);
	  cy5_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[14]);

	}
	else {
		  cy0=_mm256_load_si256( (__m256i *) &mask_vector_y[3]);
		  cy1=_mm256_load_si256( (__m256i *) &mask_vector_y[4]);
		  cy2=_mm256_load_si256( (__m256i *) &mask_vector_y[5]);
		  cy3=_mm256_load_si256( (__m256i *) &mask_vector_y[6]);
		  cy4=_mm256_load_si256( (__m256i *) &mask_vector_y[7]);
		  cy5=_mm256_load_si256( (__m256i *) &mask_vector_y[8]);

		  cy0_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[12]);
		  cy1_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[13]);
		  cy2_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[14]);
		  cy3_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[15]);
		  cy4_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[16]);
		  cy5_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[17]);
	}

	  for (unsigned int col = 0; col <= M-32; col+=32){

	  //--------------------y filter begins--------------------
	//load the 9 rows
	r0=_mm256_load_si256( (__m256i *) &frame1[row][col]);
	r1=_mm256_load_si256( (__m256i *) &frame1[row+1][col]);
	r2=_mm256_load_si256( (__m256i *) &frame1[row+2][col]);
	r3=_mm256_load_si256( (__m256i *) &frame1[row+3][col]);
	r4=_mm256_load_si256( (__m256i *) &frame1[row+4][col]);
	r5=_mm256_load_si256( (__m256i *) &frame1[row+5][col]);

		//multiply with the mask
			m0=_mm256_maddubs_epi16(r0,cy0);
			m1=_mm256_maddubs_epi16(r1,cy1);
			m2=_mm256_maddubs_epi16(r2,cy2);
			m3=_mm256_maddubs_epi16(r3,cy3);
			m4=_mm256_maddubs_epi16(r4,cy4);
			m5=_mm256_maddubs_epi16(r5,cy5);

			//vertical add
			m0=_mm256_add_epi16(m0,m1);
			m0=_mm256_add_epi16(m0,m2);
			m0=_mm256_add_epi16(m0,m3);
			m0=_mm256_add_epi16(m0,m4);
			m0=_mm256_add_epi16(m0,m5);

			even=division(division_case,m0,f);//even results

			//multiply with the mask
			m0=_mm256_maddubs_epi16(r0,cy0_sh1);
			m1=_mm256_maddubs_epi16(r1,cy1_sh1);
			m2=_mm256_maddubs_epi16(r2,cy2_sh1);
			m3=_mm256_maddubs_epi16(r3,cy3_sh1);
			m4=_mm256_maddubs_epi16(r4,cy4_sh1);
			m5=_mm256_maddubs_epi16(r5,cy5_sh1);

			//vertical add
			m0=_mm256_add_epi16(m0,m1);
			m0=_mm256_add_epi16(m0,m2);
			m0=_mm256_add_epi16(m0,m3);
			m0=_mm256_add_epi16(m0,m4);
			m0=_mm256_add_epi16(m0,m5);

			odd=division(division_case,m0,f);//odd results

			//pack to one register
			odd=_mm256_slli_si256(odd,1); //shift one position right
			r0 = _mm256_add_epi8(even,odd); //add the odd with the even
			_mm256_store_si256( (__m256i *) &temp[col],r0 );
	  }
		//y filter ends - r0 has now the data to be processed by x filter

}


//this function is computing the row=0 and row=N-1 only
//for row=3 input prelude_7x7_Ymask_3(0,...
//for row=N-3 input prelude_7x7_Ymask_3(N-5,...
void prelude_9x9_16_Ymask_0(const int row, const unsigned int M, unsigned char **frame1, unsigned char *temp, const signed char mask_vector_y[][32],  const unsigned int division_case, const __m256i f, const __m256i mask_prelude){

	__m256i r0,r1,r2,r3,r4,m0,m1,m2,m3,m4,even,odd;
	//__m256i ones=_mm256_set1_epi16(1);
	 __m256i cy0,cy1,cy2,cy3,cy4,cy0_sh1,cy1_sh1,cy2_sh1,cy3_sh1,cy4_sh1;

	if (row!=0){ //if row=N-6
	  cy0=_mm256_load_si256( (__m256i *) &mask_vector_y[0]);
	  cy1=_mm256_load_si256( (__m256i *) &mask_vector_y[1]);
	  cy2=_mm256_load_si256( (__m256i *) &mask_vector_y[2]);
	  cy3=_mm256_load_si256( (__m256i *) &mask_vector_y[3]);
	  cy4=_mm256_load_si256( (__m256i *) &mask_vector_y[4]);

	  cy0_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[9]);
	  cy1_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[10]);
	  cy2_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[11]);
	  cy3_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[12]);
	  cy4_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[13]);

	}
	else {
		  cy0=_mm256_load_si256( (__m256i *) &mask_vector_y[4]);
		  cy1=_mm256_load_si256( (__m256i *) &mask_vector_y[5]);
		  cy2=_mm256_load_si256( (__m256i *) &mask_vector_y[6]);
		  cy3=_mm256_load_si256( (__m256i *) &mask_vector_y[7]);
		  cy4=_mm256_load_si256( (__m256i *) &mask_vector_y[8]);

		  cy0_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[13]);
		  cy1_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[14]);
		  cy2_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[15]);
		  cy3_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[16]);
		  cy4_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[17]);
	}

	  for (unsigned int col = 0; col <= M-32; col+=32){

	  //--------------------y filter begins--------------------
	//load the 9 rows
	r0=_mm256_load_si256( (__m256i *) &frame1[row][col]);
	r1=_mm256_load_si256( (__m256i *) &frame1[row+1][col]);
	r2=_mm256_load_si256( (__m256i *) &frame1[row+2][col]);
	r3=_mm256_load_si256( (__m256i *) &frame1[row+3][col]);
	r4=_mm256_load_si256( (__m256i *) &frame1[row+4][col]);

		//multiply with the mask
			m0=_mm256_maddubs_epi16(r0,cy0);
			m1=_mm256_maddubs_epi16(r1,cy1);
			m2=_mm256_maddubs_epi16(r2,cy2);
			m3=_mm256_maddubs_epi16(r3,cy3);
			m4=_mm256_maddubs_epi16(r4,cy4);

			//vertical add
			m0=_mm256_add_epi16(m0,m1);
			m0=_mm256_add_epi16(m0,m2);
			m0=_mm256_add_epi16(m0,m3);
			m0=_mm256_add_epi16(m0,m4);

			even=division(division_case,m0,f);//even results

			//multiply with the mask
			m0=_mm256_maddubs_epi16(r0,cy0_sh1);
			m1=_mm256_maddubs_epi16(r1,cy1_sh1);
			m2=_mm256_maddubs_epi16(r2,cy2_sh1);
			m3=_mm256_maddubs_epi16(r3,cy3_sh1);
			m4=_mm256_maddubs_epi16(r4,cy4_sh1);

			//vertical add
			m0=_mm256_add_epi16(m0,m1);
			m0=_mm256_add_epi16(m0,m2);
			m0=_mm256_add_epi16(m0,m3);
			m0=_mm256_add_epi16(m0,m4);

			odd=division(division_case,m0,f);//odd results

			//pack to one register
			odd=_mm256_slli_si256(odd,1); //shift one position right
			r0 = _mm256_add_epi8(even,odd); //add the odd with the even
			_mm256_store_si256( (__m256i *) &temp[col],r0 );
	  }
		//y filter ends - r0 has now the data to be processed by x filter

}

void prelude_9x9_16_Xmask(unsigned char **frame1,unsigned char **filt,unsigned char *temp,  const unsigned int N,const unsigned int M, const int row, const signed char mask_vector_x[][32],  const unsigned int division_case, const __m256i f,const unsigned int REMINDER_ITERATIONS_X,const unsigned int REMINDER_ITERATIONS_Y,const signed char mask_vector_y[][32],const unsigned short int divisor_xy,signed char *kernel_x){

	__m256i r0,m0,m1,m2,m3,m4,even,odd;

	const __m256i cx0=_mm256_load_si256( (__m256i *) &mask_vector_x[0]);
	const __m256i mask3    = _mm256_set_epi16(0,0,0,0,0,0,0,65535,0,0,0,0,0,0,0,0);
	const __m256i mask3_2  = _mm256_set_epi16(0,0,0,0,0,0,65535,0,0,0,0,0,0,0,0,0);


	const __m256i mask_prelude  = _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0);
	const __m256i mask_prelude_3  = _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0);
	const __m256i mask_prelude_2  = _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0);
	const __m256i mask_prelude_1  = _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0);

	const __m256i mask_16_1   = _mm256_set_epi16(0,0,0,0,0,0xffff,0,0,0,0,0xffff,0,0,0,0,0xffff);
	const __m256i mask_new_1  = _mm256_set_epi16(0,0,0,0,0,0,0,0,0,0,0xffff,0,0,0,0,0);
	const __m256i mask_16_5  = _mm256_set_epi16(0,0xffff,0,0,0,0,0xffff,0,0,0,0,0xffff,0,0,0,0);
	unsigned int col;

	  for ( col = 0; col <= M-32; col+=30){

		  if (col==0){

	  			//1st col iteration
				r0=_mm256_load_si256( (__m256i *) &temp[0]);
		  		r0=fill_zeros(r0,mask_prelude);
	  			//multiply by the mask
	  			m0=_mm256_maddubs_epi16(r0,cx0);

	  			//first horizontal add (complex shift is needed)
	  			m3=_mm256_srli_si256(m0,2);
	  			m4=_mm256_and_si256(m0,mask3);
	  			m4=_mm256_permute2f128_si256(m4,m4,1);
	  			m4=_mm256_slli_si256(m4,14);
	  			m3=_mm256_add_epi16(m3,m4);
	  			m3=_mm256_add_epi16(m0,m3);//add

	  			//second horizontal add
	  			m1=_mm256_srli_si256(m3,4);
	  			m3=_mm256_add_epi16(m3,m1);//add

	  			//third horizontal add
	  			m0=_mm256_and_si256(m0,mask_16_5);
	  			m2=_mm256_srli_si256(m0,8);
	  			m4=_mm256_and_si256(m0,mask3_2);
	  			m4=_mm256_permute2f128_si256(m4,m4,1);
	  			m4=_mm256_slli_si256(m4,8);
	  			m2=_mm256_add_epi16(m2,m4);

	  			m0=_mm256_add_epi16(m2,m3);//add

	  			even=_mm256_and_si256(m0,mask_16_1);//in 16-bit format keep only 0,5,10 and discard the others


	  			//2nd col iteration
				r0=_mm256_load_si256( (__m256i *) &temp[0]);
		  		r0=fill_3zeros(r0,mask_prelude_3);

	  			//multiply by the mask
	  			m0=_mm256_maddubs_epi16(r0,cx0);

	  			//first horizontal add (complex shift is needed)
	  			m3=_mm256_srli_si256(m0,2);
	  			m4=_mm256_and_si256(m0,mask3);
	  			m4=_mm256_permute2f128_si256(m4,m4,1);
	  			m4=_mm256_slli_si256(m4,14);
	  			m3=_mm256_add_epi16(m3,m4);
	  			m3=_mm256_add_epi16(m0,m3);//add

	  			//second horizontal add
	  			m1=_mm256_srli_si256(m3,4);
	  			m3=_mm256_add_epi16(m3,m1);//add

	  			//third horizontal add
	  			m0=_mm256_and_si256(m0,mask_16_5);
	  			m2=_mm256_srli_si256(m0,8);
	  			m4=_mm256_and_si256(m0,mask3_2);
	  			m4=_mm256_permute2f128_si256(m4,m4,1);
	  			m4=_mm256_slli_si256(m4,8);
	  			m2=_mm256_add_epi16(m2,m4);

	  			m0=_mm256_add_epi16(m2,m3);//add

	  			odd=_mm256_and_si256(m0,mask_16_1);//in 16-bit format keep only 0,5,10 and discard the others


	  			//3rd col iteration
				r0=_mm256_load_si256( (__m256i *) &temp[0]);
		  		r0=fill_2zeros(r0,mask_prelude_2);

	  			//multiply by the mask
	  			m0=_mm256_maddubs_epi16(r0,cx0);

	  			//first horizontal add (complex shift is needed)
	  			m3=_mm256_srli_si256(m0,2);
	  			m4=_mm256_and_si256(m0,mask3);
	  			m4=_mm256_permute2f128_si256(m4,m4,1);
	  			m4=_mm256_slli_si256(m4,14);
	  			m3=_mm256_add_epi16(m3,m4);
	  			m3=_mm256_add_epi16(m0,m3);//add

	  			//second horizontal add
	  			m1=_mm256_srli_si256(m3,4);
	  			m3=_mm256_add_epi16(m3,m1);//add

	  			//third horizontal add
	  			m0=_mm256_and_si256(m0,mask_16_5);
	  			m2=_mm256_srli_si256(m0,8);
	  			m4=_mm256_and_si256(m0,mask3_2);
	  			m4=_mm256_permute2f128_si256(m4,m4,1);
	  			m4=_mm256_slli_si256(m4,8);
	  			m2=_mm256_add_epi16(m2,m4);

	  			m0=_mm256_add_epi16(m2,m3);//add

	  			m0=_mm256_and_si256(m0,mask_16_1);//in 16-bit format keep only 0,5,10 and discard the others
	  			m0=_mm256_slli_si256(m0,2);
	  			even=_mm256_add_epi16(m0,even);

	  			//4th col iteration
				r0=_mm256_load_si256( (__m256i *) &temp[0]);
		  		r0=fill_1zeros(r0,mask_prelude_1);

	  			//multiply by the mask
	  			m0=_mm256_maddubs_epi16(r0,cx0);

	  			//first horizontal add (complex shift is needed)
	  			m3=_mm256_srli_si256(m0,2);
	  			m4=_mm256_and_si256(m0,mask3);
	  			m4=_mm256_permute2f128_si256(m4,m4,1);
	  			m4=_mm256_slli_si256(m4,14);
	  			m3=_mm256_add_epi16(m3,m4);
	  			m3=_mm256_add_epi16(m0,m3);//add

	  			//second horizontal add
	  			m1=_mm256_srli_si256(m3,4);
	  			m3=_mm256_add_epi16(m3,m1);//add

	  			//third horizontal add
	  			m0=_mm256_and_si256(m0,mask_16_5);
	  			m2=_mm256_srli_si256(m0,8);
	  			m4=_mm256_and_si256(m0,mask3_2);
	  			m4=_mm256_permute2f128_si256(m4,m4,1);
	  			m4=_mm256_slli_si256(m4,8);
	  			m2=_mm256_add_epi16(m2,m4);

	  			m0=_mm256_add_epi16(m2,m3);//add

	  			m0=_mm256_and_si256(m0,mask_16_1);//in 16-bit format keep only 0,5,10 and discard the others
	  			m0=_mm256_slli_si256(m0,2);
	  			odd=_mm256_add_epi16(m0,odd);

	  			//5th col iteration
				r0=_mm256_load_si256( (__m256i *) &temp[0]);

				//multiply by the mask
	  			m0=_mm256_maddubs_epi16(r0,cx0);

	  			//first horizontal add (complex shift is needed)
	  			m3=_mm256_srli_si256(m0,2);
	  			m4=_mm256_and_si256(m0,mask3);
	  			m4=_mm256_permute2f128_si256(m4,m4,1);
	  			m4=_mm256_slli_si256(m4,14);
	  			m3=_mm256_add_epi16(m3,m4);
	  			m3=_mm256_add_epi16(m0,m3);//add

	  			//second horizontal add
	  			m1=_mm256_srli_si256(m3,4);
	  			m3=_mm256_add_epi16(m3,m1);//add

	  			//third horizontal add
	  			m0=_mm256_and_si256(m0,mask_16_5);
	  			m2=_mm256_srli_si256(m0,8);
	  			m4=_mm256_and_si256(m0,mask3_2);
	  			m4=_mm256_permute2f128_si256(m4,m4,1);
	  			m4=_mm256_slli_si256(m4,8);
	  			m2=_mm256_add_epi16(m2,m4);

	  			m0=_mm256_add_epi16(m2,m3);//add

	  			m0=_mm256_and_si256(m0,mask_16_1);//in 16-bit format keep only 0,5,10 and discard the others
	  			m0=_mm256_slli_si256(m0,4);
	  			even=_mm256_add_epi16(m0,even);

	  			//6th col iteration
				r0=_mm256_loadu_si256( (__m256i *) &temp[1]);

	  			//multiply by the mask
	  			m0=_mm256_maddubs_epi16(r0,cx0);

	  			//first horizontal add (complex shift is needed)
	  			m3=_mm256_srli_si256(m0,2);
	  			m4=_mm256_and_si256(m0,mask3);
	  			m4=_mm256_permute2f128_si256(m4,m4,1);
	  			m4=_mm256_slli_si256(m4,14);
	  			m3=_mm256_add_epi16(m3,m4);
	  			m3=_mm256_add_epi16(m0,m3);//add

	  			//second horizontal add
	  			m1=_mm256_srli_si256(m3,4);
	  			m3=_mm256_add_epi16(m3,m1);//add

	  			//third horizontal add
	  			m0=_mm256_and_si256(m0,mask_16_5);
	  			m2=_mm256_srli_si256(m0,8);
	  			m4=_mm256_and_si256(m0,mask3_2);
	  			m4=_mm256_permute2f128_si256(m4,m4,1);
	  			m4=_mm256_slli_si256(m4,8);
	  			m2=_mm256_add_epi16(m2,m4);

	  			m0=_mm256_add_epi16(m2,m3);//add

	  			m0=_mm256_and_si256(m0,mask_16_1);//in 16-bit format keep only 0,5,10 and discard the others
	  			m0=_mm256_slli_si256(m0,4);
	  			odd=_mm256_add_epi16(m0,odd);

	  			//7th col iteration
				r0=_mm256_loadu_si256( (__m256i *) &temp[2]);

	  			//multiply by the mask
	  			m0=_mm256_maddubs_epi16(r0,cx0);

	  			//first horizontal add (complex shift is needed)
	  			m3=_mm256_srli_si256(m0,2);
	  			m4=_mm256_and_si256(m0,mask3);
	  			m4=_mm256_permute2f128_si256(m4,m4,1);
	  			m4=_mm256_slli_si256(m4,14);
	  			m3=_mm256_add_epi16(m3,m4);
	  			m3=_mm256_add_epi16(m0,m3);//add

	  			//second horizontal add
	  			m1=_mm256_srli_si256(m3,4);
	  			m3=_mm256_add_epi16(m3,m1);//add

	  			//third horizontal add
	  			m0=_mm256_and_si256(m0,mask_16_5);
	  			m2=_mm256_srli_si256(m0,8);
	  			m4=_mm256_and_si256(m0,mask3_2);
	  			m4=_mm256_permute2f128_si256(m4,m4,1);
	  			m4=_mm256_slli_si256(m4,8);
	  			m2=_mm256_add_epi16(m2,m4);

	  			m0=_mm256_add_epi16(m2,m3);//add

	  			m0=_mm256_and_si256(m0,mask_16_1);//in 16-bit format keep only 0,5,10 and discard the others

	  			//shift the result 3 positions
	  			m1=_mm256_slli_si256(m0,6);
	  			m2=_mm256_and_si256(m0,mask_new_1);
	  			m2=_mm256_permute2f128_si256(m2,m2,1);
	  			m2=_mm256_srli_si256(m2,10);
	  			m0=_mm256_add_epi16(m2,m1);

	  			even=_mm256_add_epi16(m0,even);

	  			//8th col iteration
				r0=_mm256_loadu_si256( (__m256i *) &temp[3]);

	  			//multiply by the mask
	  			m0=_mm256_maddubs_epi16(r0,cx0);

	  			//first horizontal add (complex shift is needed)
	  			m3=_mm256_srli_si256(m0,2);
	  			m4=_mm256_and_si256(m0,mask3);
	  			m4=_mm256_permute2f128_si256(m4,m4,1);
	  			m4=_mm256_slli_si256(m4,14);
	  			m3=_mm256_add_epi16(m3,m4);
	  			m3=_mm256_add_epi16(m0,m3);//add

	  			//second horizontal add
	  			m1=_mm256_srli_si256(m3,4);
	  			m3=_mm256_add_epi16(m3,m1);//add

	  			//third horizontal add
	  			m0=_mm256_and_si256(m0,mask_16_5);
	  			m2=_mm256_srli_si256(m0,8);
	  			m4=_mm256_and_si256(m0,mask3_2);
	  			m4=_mm256_permute2f128_si256(m4,m4,1);
	  			m4=_mm256_slli_si256(m4,8);
	  			m2=_mm256_add_epi16(m2,m4);

	  			m0=_mm256_add_epi16(m2,m3);//add

	  			m0=_mm256_and_si256(m0,mask_16_1);//in 16-bit format keep only 0,5,10 and discard the others

	  			//shift the result 3 positions
	  			m1=_mm256_slli_si256(m0,6);
	  			m2=_mm256_and_si256(m0,mask_new_1);
	  			m2=_mm256_permute2f128_si256(m2,m2,1);
	  			m2=_mm256_srli_si256(m2,10);
	  			m0=_mm256_add_epi16(m2,m1);

	  			odd=_mm256_add_epi16(m0,odd);

	  			//9th col iteration
				r0=_mm256_loadu_si256( (__m256i *) &temp[4]);

	  			//multiply by the mask
	  			m0=_mm256_maddubs_epi16(r0,cx0);

	  			//first horizontal add (complex shift is needed)
	  			m3=_mm256_srli_si256(m0,2);
	  			m4=_mm256_and_si256(m0,mask3);
	  			m4=_mm256_permute2f128_si256(m4,m4,1);
	  			m4=_mm256_slli_si256(m4,14);
	  			m3=_mm256_add_epi16(m3,m4);
	  			m3=_mm256_add_epi16(m0,m3);//add

	  			//second horizontal add
	  			m1=_mm256_srli_si256(m3,4);
	  			m3=_mm256_add_epi16(m3,m1);//add

	  			//third horizontal add
	  			m0=_mm256_and_si256(m0,mask_16_5);
	  			m2=_mm256_srli_si256(m0,8);
	  			m4=_mm256_and_si256(m0,mask3_2);
	  			m4=_mm256_permute2f128_si256(m4,m4,1);
	  			m4=_mm256_slli_si256(m4,8);
	  			m2=_mm256_add_epi16(m2,m4);

	  			m0=_mm256_add_epi16(m2,m3);//add

	  			m0=_mm256_and_si256(m0,mask_16_1);//in 16-bit format keep only 0,5,10 and discard the others

	  			//shift the result 4 positions
	  			m1=_mm256_slli_si256(m0,8);
	  			m2=_mm256_and_si256(m0,mask_new_1);
	  			m2=_mm256_permute2f128_si256(m2,m2,1);
	  			m2=_mm256_srli_si256(m2,8);
	  			m0=_mm256_add_epi16(m2,m1);

	  			even=_mm256_add_epi16(m0,even);

	  			//10th col iteration
				r0=_mm256_loadu_si256( (__m256i *) &temp[5]);

	  			//multiply by the mask
	  			m0=_mm256_maddubs_epi16(r0,cx0);

	  			//first horizontal add (complex shift is needed)
	  			m3=_mm256_srli_si256(m0,2);
	  			m4=_mm256_and_si256(m0,mask3);
	  			m4=_mm256_permute2f128_si256(m4,m4,1);
	  			m4=_mm256_slli_si256(m4,14);
	  			m3=_mm256_add_epi16(m3,m4);
	  			m3=_mm256_add_epi16(m0,m3);//add

	  			//second horizontal add
	  			m1=_mm256_srli_si256(m3,4);
	  			m3=_mm256_add_epi16(m3,m1);//add

	  			//third horizontal add
	  			m0=_mm256_and_si256(m0,mask_16_5);
	  			m2=_mm256_srli_si256(m0,8);
	  			m4=_mm256_and_si256(m0,mask3_2);
	  			m4=_mm256_permute2f128_si256(m4,m4,1);
	  			m4=_mm256_slli_si256(m4,8);
	  			m2=_mm256_add_epi16(m2,m4);

	  			m0=_mm256_add_epi16(m2,m3);//add

	  			m0=_mm256_and_si256(m0,mask_16_1);//in 16-bit format keep only 0,5,10 and discard the others

	  			//shift the result 4 positions
	  			m1=_mm256_slli_si256(m0,8);
	  			m2=_mm256_and_si256(m0,mask_new_1);
	  			m2=_mm256_permute2f128_si256(m2,m2,1);
	  			m2=_mm256_srli_si256(m2,8);
	  			m0=_mm256_add_epi16(m2,m1);

	  			odd=_mm256_add_epi16(m0,odd);

	  			//division
	  			even = division(division_case,even,f);
	  			odd = division(division_case,odd,f);

	  			//blend even , odd into one register
	  			odd = _mm256_slli_si256(odd,1);
	  			even=_mm256_add_epi16(even,odd);

	  			_mm256_storeu_si256( (__m256i *) &filt[row][0],even );

		  		  	}
		  else {
		  		// col iteration computes output pixels of   0,10,20
		  		// col+1 iteration computes output pixels of 1,11,21
		  		// col+2 iteration computes output pixels of 2,12,22
		  		// col+3 iteration computes output pixels of 3,13,23
		  		// col+4 iteration computes output pixels of 4,14,24
		  		// col+5 iteration computes output pixels of 5,15,25
		  		// col+6 iteration computes output pixels of 6,16,26
		  		// col+7 iteration computes output pixels of 7,17,27
		  		// col+8 iteration computes output pixels of 8,18,28
		  		// col+9 iteration computes output pixels of 9,19,29
		  		//afterwards, col becomes 30 and repeat the above process

		  			//1st col iteration

		  			//multiply by the mask
	  		        r0=_mm256_loadu_si256( (__m256i *) &temp[col-4]);
		  			m0=_mm256_maddubs_epi16(r0,cx0);

		  			//first horizontal add (complex shift is needed)
		  			m3=_mm256_srli_si256(m0,2);
		  			m4=_mm256_and_si256(m0,mask3);
		  			m4=_mm256_permute2f128_si256(m4,m4,1);
		  			m4=_mm256_slli_si256(m4,14);
		  			m3=_mm256_add_epi16(m3,m4);
		  			m3=_mm256_add_epi16(m0,m3);//add

		  			//second horizontal add
		  			m1=_mm256_srli_si256(m3,4);
		  			m3=_mm256_add_epi16(m3,m1);//add

		  			//third horizontal add
		  			m0=_mm256_and_si256(m0,mask_16_5);
		  			m2=_mm256_srli_si256(m0,8);
		  			m4=_mm256_and_si256(m0,mask3_2);
		  			m4=_mm256_permute2f128_si256(m4,m4,1);
		  			m4=_mm256_slli_si256(m4,8);
		  			m2=_mm256_add_epi16(m2,m4);

		  			m0=_mm256_add_epi16(m2,m3);//add

		  			even=_mm256_and_si256(m0,mask_16_1);//in 16-bit format keep only 0,5,10 and discard the others


		  			//2nd col iteration
	  		        r0=_mm256_loadu_si256( (__m256i *) &temp[col-3]);

		  			//multiply by the mask
		  			m0=_mm256_maddubs_epi16(r0,cx0);

		  			//first horizontal add (complex shift is needed)
		  			m3=_mm256_srli_si256(m0,2);
		  			m4=_mm256_and_si256(m0,mask3);
		  			m4=_mm256_permute2f128_si256(m4,m4,1);
		  			m4=_mm256_slli_si256(m4,14);
		  			m3=_mm256_add_epi16(m3,m4);
		  			m3=_mm256_add_epi16(m0,m3);//add

		  			//second horizontal add
		  			m1=_mm256_srli_si256(m3,4);
		  			m3=_mm256_add_epi16(m3,m1);//add

		  			//third horizontal add
		  			m0=_mm256_and_si256(m0,mask_16_5);
		  			m2=_mm256_srli_si256(m0,8);
		  			m4=_mm256_and_si256(m0,mask3_2);
		  			m4=_mm256_permute2f128_si256(m4,m4,1);
		  			m4=_mm256_slli_si256(m4,8);
		  			m2=_mm256_add_epi16(m2,m4);

		  			m0=_mm256_add_epi16(m2,m3);//add

		  			odd=_mm256_and_si256(m0,mask_16_1);//in 16-bit format keep only 0,5,10 and discard the others


		  			//3rd col iteration
	  		        r0=_mm256_loadu_si256( (__m256i *) &temp[col-2]);

		  			//multiply by the mask
		  			m0=_mm256_maddubs_epi16(r0,cx0);

		  			//first horizontal add (complex shift is needed)
		  			m3=_mm256_srli_si256(m0,2);
		  			m4=_mm256_and_si256(m0,mask3);
		  			m4=_mm256_permute2f128_si256(m4,m4,1);
		  			m4=_mm256_slli_si256(m4,14);
		  			m3=_mm256_add_epi16(m3,m4);
		  			m3=_mm256_add_epi16(m0,m3);//add

		  			//second horizontal add
		  			m1=_mm256_srli_si256(m3,4);
		  			m3=_mm256_add_epi16(m3,m1);//add

		  			//third horizontal add
		  			m0=_mm256_and_si256(m0,mask_16_5);
		  			m2=_mm256_srli_si256(m0,8);
		  			m4=_mm256_and_si256(m0,mask3_2);
		  			m4=_mm256_permute2f128_si256(m4,m4,1);
		  			m4=_mm256_slli_si256(m4,8);
		  			m2=_mm256_add_epi16(m2,m4);

		  			m0=_mm256_add_epi16(m2,m3);//add

		  			m0=_mm256_and_si256(m0,mask_16_1);//in 16-bit format keep only 0,5,10 and discard the others
		  			m0=_mm256_slli_si256(m0,2);
		  			even=_mm256_add_epi16(m0,even);

		  			//4th col iteration
	  		        r0=_mm256_loadu_si256( (__m256i *) &temp[col-1]);

		  			//multiply by the mask
		  			m0=_mm256_maddubs_epi16(r0,cx0);

		  			//first horizontal add (complex shift is needed)
		  			m3=_mm256_srli_si256(m0,2);
		  			m4=_mm256_and_si256(m0,mask3);
		  			m4=_mm256_permute2f128_si256(m4,m4,1);
		  			m4=_mm256_slli_si256(m4,14);
		  			m3=_mm256_add_epi16(m3,m4);
		  			m3=_mm256_add_epi16(m0,m3);//add

		  			//second horizontal add
		  			m1=_mm256_srli_si256(m3,4);
		  			m3=_mm256_add_epi16(m3,m1);//add

		  			//third horizontal add
		  			m0=_mm256_and_si256(m0,mask_16_5);
		  			m2=_mm256_srli_si256(m0,8);
		  			m4=_mm256_and_si256(m0,mask3_2);
		  			m4=_mm256_permute2f128_si256(m4,m4,1);
		  			m4=_mm256_slli_si256(m4,8);
		  			m2=_mm256_add_epi16(m2,m4);

		  			m0=_mm256_add_epi16(m2,m3);//add

		  			m0=_mm256_and_si256(m0,mask_16_1);//in 16-bit format keep only 0,5,10 and discard the others
		  			m0=_mm256_slli_si256(m0,2);
		  			odd=_mm256_add_epi16(m0,odd);

		  			//5th col iteration
	  		        r0=_mm256_loadu_si256( (__m256i *) &temp[col]);

		  			//multiply by the mask
		  			m0=_mm256_maddubs_epi16(r0,cx0);

		  			//first horizontal add (complex shift is needed)
		  			m3=_mm256_srli_si256(m0,2);
		  			m4=_mm256_and_si256(m0,mask3);
		  			m4=_mm256_permute2f128_si256(m4,m4,1);
		  			m4=_mm256_slli_si256(m4,14);
		  			m3=_mm256_add_epi16(m3,m4);
		  			m3=_mm256_add_epi16(m0,m3);//add

		  			//second horizontal add
		  			m1=_mm256_srli_si256(m3,4);
		  			m3=_mm256_add_epi16(m3,m1);//add

		  			//third horizontal add
		  			m0=_mm256_and_si256(m0,mask_16_5);
		  			m2=_mm256_srli_si256(m0,8);
		  			m4=_mm256_and_si256(m0,mask3_2);
		  			m4=_mm256_permute2f128_si256(m4,m4,1);
		  			m4=_mm256_slli_si256(m4,8);
		  			m2=_mm256_add_epi16(m2,m4);

		  			m0=_mm256_add_epi16(m2,m3);//add

		  			m0=_mm256_and_si256(m0,mask_16_1);//in 16-bit format keep only 0,5,10 and discard the others
		  			m0=_mm256_slli_si256(m0,4);
		  			even=_mm256_add_epi16(m0,even);

		  			//6th col iteration
	  		        r0=_mm256_loadu_si256( (__m256i *) &temp[col+1]);

		  			//multiply by the mask
		  			m0=_mm256_maddubs_epi16(r0,cx0);

		  			//first horizontal add (complex shift is needed)
		  			m3=_mm256_srli_si256(m0,2);
		  			m4=_mm256_and_si256(m0,mask3);
		  			m4=_mm256_permute2f128_si256(m4,m4,1);
		  			m4=_mm256_slli_si256(m4,14);
		  			m3=_mm256_add_epi16(m3,m4);
		  			m3=_mm256_add_epi16(m0,m3);//add

		  			//second horizontal add
		  			m1=_mm256_srli_si256(m3,4);
		  			m3=_mm256_add_epi16(m3,m1);//add

		  			//third horizontal add
		  			m0=_mm256_and_si256(m0,mask_16_5);
		  			m2=_mm256_srli_si256(m0,8);
		  			m4=_mm256_and_si256(m0,mask3_2);
		  			m4=_mm256_permute2f128_si256(m4,m4,1);
		  			m4=_mm256_slli_si256(m4,8);
		  			m2=_mm256_add_epi16(m2,m4);

		  			m0=_mm256_add_epi16(m2,m3);//add

		  			m0=_mm256_and_si256(m0,mask_16_1);//in 16-bit format keep only 0,5,10 and discard the others
		  			m0=_mm256_slli_si256(m0,4);
		  			odd=_mm256_add_epi16(m0,odd);

		  			//7th col iteration
	  		        r0=_mm256_loadu_si256( (__m256i *) &temp[col+2]);

		  			//multiply by the mask
		  			m0=_mm256_maddubs_epi16(r0,cx0);

		  			//first horizontal add (complex shift is needed)
		  			m3=_mm256_srli_si256(m0,2);
		  			m4=_mm256_and_si256(m0,mask3);
		  			m4=_mm256_permute2f128_si256(m4,m4,1);
		  			m4=_mm256_slli_si256(m4,14);
		  			m3=_mm256_add_epi16(m3,m4);
		  			m3=_mm256_add_epi16(m0,m3);//add

		  			//second horizontal add
		  			m1=_mm256_srli_si256(m3,4);
		  			m3=_mm256_add_epi16(m3,m1);//add

		  			//third horizontal add
		  			m0=_mm256_and_si256(m0,mask_16_5);
		  			m2=_mm256_srli_si256(m0,8);
		  			m4=_mm256_and_si256(m0,mask3_2);
		  			m4=_mm256_permute2f128_si256(m4,m4,1);
		  			m4=_mm256_slli_si256(m4,8);
		  			m2=_mm256_add_epi16(m2,m4);

		  			m0=_mm256_add_epi16(m2,m3);//add

		  			m0=_mm256_and_si256(m0,mask_16_1);//in 16-bit format keep only 0,5,10 and discard the others

		  			//shift the result 3 positions
		  			m1=_mm256_slli_si256(m0,6);
		  			m2=_mm256_and_si256(m0,mask_new_1);
		  			m2=_mm256_permute2f128_si256(m2,m2,1);
		  			m2=_mm256_srli_si256(m2,10);
		  			m0=_mm256_add_epi16(m2,m1);

		  			even=_mm256_add_epi16(m0,even);

		  			//8th col iteration
	  		        r0=_mm256_loadu_si256( (__m256i *) &temp[col+3]);

		  			//multiply by the mask
		  			m0=_mm256_maddubs_epi16(r0,cx0);

		  			//first horizontal add (complex shift is needed)
		  			m3=_mm256_srli_si256(m0,2);
		  			m4=_mm256_and_si256(m0,mask3);
		  			m4=_mm256_permute2f128_si256(m4,m4,1);
		  			m4=_mm256_slli_si256(m4,14);
		  			m3=_mm256_add_epi16(m3,m4);
		  			m3=_mm256_add_epi16(m0,m3);//add

		  			//second horizontal add
		  			m1=_mm256_srli_si256(m3,4);
		  			m3=_mm256_add_epi16(m3,m1);//add

		  			//third horizontal add
		  			m0=_mm256_and_si256(m0,mask_16_5);
		  			m2=_mm256_srli_si256(m0,8);
		  			m4=_mm256_and_si256(m0,mask3_2);
		  			m4=_mm256_permute2f128_si256(m4,m4,1);
		  			m4=_mm256_slli_si256(m4,8);
		  			m2=_mm256_add_epi16(m2,m4);

		  			m0=_mm256_add_epi16(m2,m3);//add

		  			m0=_mm256_and_si256(m0,mask_16_1);//in 16-bit format keep only 0,5,10 and discard the others

		  			//shift the result 3 positions
		  			m1=_mm256_slli_si256(m0,6);
		  			m2=_mm256_and_si256(m0,mask_new_1);
		  			m2=_mm256_permute2f128_si256(m2,m2,1);
		  			m2=_mm256_srli_si256(m2,10);
		  			m0=_mm256_add_epi16(m2,m1);

		  			odd=_mm256_add_epi16(m0,odd);

		  			//9th col iteration
	  		        r0=_mm256_loadu_si256( (__m256i *) &temp[col+4]);

		  			//multiply by the mask
		  			m0=_mm256_maddubs_epi16(r0,cx0);

		  			//first horizontal add (complex shift is needed)
		  			m3=_mm256_srli_si256(m0,2);
		  			m4=_mm256_and_si256(m0,mask3);
		  			m4=_mm256_permute2f128_si256(m4,m4,1);
		  			m4=_mm256_slli_si256(m4,14);
		  			m3=_mm256_add_epi16(m3,m4);
		  			m3=_mm256_add_epi16(m0,m3);//add

		  			//second horizontal add
		  			m1=_mm256_srli_si256(m3,4);
		  			m3=_mm256_add_epi16(m3,m1);//add

		  			//third horizontal add
		  			m0=_mm256_and_si256(m0,mask_16_5);
		  			m2=_mm256_srli_si256(m0,8);
		  			m4=_mm256_and_si256(m0,mask3_2);
		  			m4=_mm256_permute2f128_si256(m4,m4,1);
		  			m4=_mm256_slli_si256(m4,8);
		  			m2=_mm256_add_epi16(m2,m4);

		  			m0=_mm256_add_epi16(m2,m3);//add

		  			m0=_mm256_and_si256(m0,mask_16_1);//in 16-bit format keep only 0,5,10 and discard the others

		  			//shift the result 4 positions
		  			m1=_mm256_slli_si256(m0,8);
		  			m2=_mm256_and_si256(m0,mask_new_1);
		  			m2=_mm256_permute2f128_si256(m2,m2,1);
		  			m2=_mm256_srli_si256(m2,8);
		  			m0=_mm256_add_epi16(m2,m1);

		  			even=_mm256_add_epi16(m0,even);

		  			//10th col iteration
	  		        r0=_mm256_loadu_si256( (__m256i *) &temp[col+5]);

		  			//multiply by the mask
		  			m0=_mm256_maddubs_epi16(r0,cx0);

		  			//first horizontal add (complex shift is needed)
		  			m3=_mm256_srli_si256(m0,2);
		  			m4=_mm256_and_si256(m0,mask3);
		  			m4=_mm256_permute2f128_si256(m4,m4,1);
		  			m4=_mm256_slli_si256(m4,14);
		  			m3=_mm256_add_epi16(m3,m4);
		  			m3=_mm256_add_epi16(m0,m3);//add

		  			//second horizontal add
		  			m1=_mm256_srli_si256(m3,4);
		  			m3=_mm256_add_epi16(m3,m1);//add

		  			//third horizontal add
		  			m0=_mm256_and_si256(m0,mask_16_5);
		  			m2=_mm256_srli_si256(m0,8);
		  			m4=_mm256_and_si256(m0,mask3_2);
		  			m4=_mm256_permute2f128_si256(m4,m4,1);
		  			m4=_mm256_slli_si256(m4,8);
		  			m2=_mm256_add_epi16(m2,m4);

		  			m0=_mm256_add_epi16(m2,m3);//add

		  			m0=_mm256_and_si256(m0,mask_16_1);//in 16-bit format keep only 0,5,10 and discard the others

		  			//shift the result 4 positions
		  			m1=_mm256_slli_si256(m0,8);
		  			m2=_mm256_and_si256(m0,mask_new_1);
		  			m2=_mm256_permute2f128_si256(m2,m2,1);
		  			m2=_mm256_srli_si256(m2,8);
		  			m0=_mm256_add_epi16(m2,m1);

		  			odd=_mm256_add_epi16(m0,odd);

		  			//division
		  			even = division(division_case,even,f);
		  			odd = division(division_case,odd,f);

		  			//blend even , odd into one register
		  			odd = _mm256_slli_si256(odd,1);
		  			even=_mm256_add_epi16(even,odd);

		  			_mm256_storeu_si256( (__m256i *) &filt[row][col],even );

		  		 }
	  }

	loop_reminder_9x9_16_blur_X(frame1,filt,temp,N,M,row, col, REMINDER_ITERATIONS_X, division_case, mask_vector_x, f, divisor_xy,kernel_x);


}



void loop_reminder_9x9_16_blur(unsigned char **frame1,unsigned char **filt,unsigned char *temp,const unsigned int N,const unsigned int M,const unsigned int row, const unsigned int col,const unsigned int REMINDER_ITERATIONS_X,const unsigned int REMINDER_ITERATIONS_Y,const unsigned int division_case,const signed char mask_vector_x[][32],const signed char mask_vector_y[][32], const __m256i f, const unsigned short int divisor_xy,signed char *kernel_x){

	const __m256i cx0=_mm256_load_si256( (__m256i *) &mask_vector_x[0]);


	const __m256i mask3  = _mm256_set_epi16(0,0,0,0,0,0,0,65535,0,0,0,0,0,0,0,0);

	const __m256i mask_16_1   = _mm256_set_epi16(0,0,0,0,0,0xffff,0,0,0,0,0xffff,0,0,0,0,0xffff);

	__m256i r0,r1,r2,r3,r4,r5,r6,r7,r8,m0,m1,m2,m3,m4,m5,m6,m7,m8,even,odd,output1,reminder_mask;
	const __m256i mask_new_1  = _mm256_set_epi16(0,0,0,0,0,0,0,0,0,0,0xffff,0,0,0,0,0);
	const __m256i mask_16_5  = _mm256_set_epi16(0,0xffff,0,0,0,0,0xffff,0,0,0,0,0xffff,0,0,0,0);

	const __m256i mask3_2  = _mm256_set_epi16(0,0,0,0,0,0,65535,0,0,0,0,0,0,0,0,0);


 if (REMINDER_ITERATIONS_Y==0){//no need for computing the y mask
		_mm256_store_si256( (__m256i *) &temp[M],_mm256_setzero_si256() );
        for (unsigned int g=M+32;g<M+32+6;g++)
        	temp[g]=0;
	}
 else {
		__m256i cy0,cy1,cy2,cy3,cy4,cy5,cy6,cy7,cy8,cy0_sh1,cy1_sh1,cy2_sh1,cy3_sh1,cy4_sh1,cy5_sh1,cy6_sh1,cy7_sh1,cy8_sh1;

	unsigned int col2=M-REMINDER_ITERATIONS_Y;//this is the col index to load the elements for the y mask

	if ((row>3) && (row<N-4)){//main case

		r0=_mm256_loadu_si256( (__m256i *) &frame1[row-4][col2]);
		r1=_mm256_loadu_si256( (__m256i *) &frame1[row-3][col2]);
		r2=_mm256_loadu_si256( (__m256i *) &frame1[row-2][col2]);
		r3=_mm256_loadu_si256( (__m256i *) &frame1[row-1][col2]);
		r4=_mm256_loadu_si256( (__m256i *) &frame1[row][col2]);
		r5=_mm256_loadu_si256( (__m256i *) &frame1[row+1][col2]);
		r6=_mm256_loadu_si256( (__m256i *) &frame1[row+2][col2]);
		r7=_mm256_loadu_si256( (__m256i *) &frame1[row+3][col2]);
		r8=_mm256_loadu_si256( (__m256i *) &frame1[row+4][col2]);

		cy0=_mm256_load_si256( (__m256i *) &mask_vector_y[0]);
		cy1=_mm256_load_si256( (__m256i *) &mask_vector_y[1]);
		cy2=_mm256_load_si256( (__m256i *) &mask_vector_y[2]);
		cy3=_mm256_load_si256( (__m256i *) &mask_vector_y[3]);
		cy4=_mm256_load_si256( (__m256i *) &mask_vector_y[4]);
		cy5=_mm256_load_si256( (__m256i *) &mask_vector_y[5]);
		cy6=_mm256_load_si256( (__m256i *) &mask_vector_y[6]);
		cy7=_mm256_load_si256( (__m256i *) &mask_vector_y[7]);
		cy8=_mm256_load_si256( (__m256i *) &mask_vector_y[8]);

		cy0_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[9]);
		cy1_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[10]);
		cy2_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[11]);
		cy3_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[12]);
		cy4_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[13]);
		cy5_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[14]);
		cy6_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[15]);
		cy7_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[16]);
		cy8_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[17]);

	}
	else {
		if (row==0){
			r0=_mm256_loadu_si256( (__m256i *) &frame1[0][col2]);
			r1=_mm256_loadu_si256( (__m256i *) &frame1[1][col2]);
			r2=_mm256_loadu_si256( (__m256i *) &frame1[2][col2]);
			r3=_mm256_loadu_si256( (__m256i *) &frame1[3][col2]);
			r4=_mm256_loadu_si256( (__m256i *) &frame1[3][col2]);
			r5=_mm256_setzero_si256();
			r6=_mm256_setzero_si256();
			r7=_mm256_setzero_si256();
			r8=_mm256_setzero_si256();

			cy0=_mm256_load_si256( (__m256i *) &mask_vector_y[4]);
			cy1=_mm256_load_si256( (__m256i *) &mask_vector_y[5]);
			cy2=_mm256_load_si256( (__m256i *) &mask_vector_y[6]);
			cy3=_mm256_load_si256( (__m256i *) &mask_vector_y[7]);
			cy4=_mm256_load_si256( (__m256i *) &mask_vector_y[8]);
			cy5=_mm256_setzero_si256();
			cy6=_mm256_setzero_si256();
			cy7=_mm256_setzero_si256();
			cy8=_mm256_setzero_si256();

			cy0_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[13]);
			cy1_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[14]);
			cy2_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[15]);
			cy3_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[16]);
			cy4_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[17]);
			cy5_sh1=_mm256_setzero_si256();
			cy6_sh1=_mm256_setzero_si256();
			cy7_sh1=_mm256_setzero_si256();
			cy8_sh1=_mm256_setzero_si256();

		}
		else if (row==1){
			r0=_mm256_loadu_si256( (__m256i *) &frame1[0][col2]);
			r1=_mm256_loadu_si256( (__m256i *) &frame1[1][col2]);
			r2=_mm256_loadu_si256( (__m256i *) &frame1[2][col2]);
			r3=_mm256_loadu_si256( (__m256i *) &frame1[3][col2]);
			r4=_mm256_loadu_si256( (__m256i *) &frame1[4][col2]);
			r5=_mm256_loadu_si256( (__m256i *) &frame1[5][col2]);
			r6=_mm256_setzero_si256();
			r7=_mm256_setzero_si256();
			r8=_mm256_setzero_si256();

			cy0=_mm256_load_si256( (__m256i *) &mask_vector_y[3]);
			cy1=_mm256_load_si256( (__m256i *) &mask_vector_y[4]);
			cy2=_mm256_load_si256( (__m256i *) &mask_vector_y[5]);
			cy3=_mm256_load_si256( (__m256i *) &mask_vector_y[6]);
			cy4=_mm256_load_si256( (__m256i *) &mask_vector_y[7]);
			cy5=_mm256_load_si256( (__m256i *) &mask_vector_y[8]);
			cy6=_mm256_setzero_si256();
			cy7=_mm256_setzero_si256();
			cy8=_mm256_setzero_si256();

			cy0_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[12]);
			cy1_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[13]);
			cy2_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[14]);
			cy3_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[15]);
			cy4_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[16]);
			cy5_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[17]);
			cy6_sh1=_mm256_setzero_si256();
			cy7_sh1=_mm256_setzero_si256();
			cy8_sh1=_mm256_setzero_si256();
		}
		else if (row==2){
			r0=_mm256_loadu_si256( (__m256i *) &frame1[0][col2]);
			r1=_mm256_loadu_si256( (__m256i *) &frame1[1][col2]);
			r2=_mm256_loadu_si256( (__m256i *) &frame1[2][col2]);
			r3=_mm256_loadu_si256( (__m256i *) &frame1[3][col2]);
			r4=_mm256_loadu_si256( (__m256i *) &frame1[4][col2]);
			r5=_mm256_loadu_si256( (__m256i *) &frame1[5][col2]);
			r6=_mm256_loadu_si256( (__m256i *) &frame1[6][col2]);
			r7=_mm256_setzero_si256();
			r8=_mm256_setzero_si256();

			cy0=_mm256_load_si256( (__m256i *) &mask_vector_y[2]);
			cy1=_mm256_load_si256( (__m256i *) &mask_vector_y[3]);
			cy2=_mm256_load_si256( (__m256i *) &mask_vector_y[4]);
			cy3=_mm256_load_si256( (__m256i *) &mask_vector_y[5]);
			cy4=_mm256_load_si256( (__m256i *) &mask_vector_y[6]);
			cy5=_mm256_load_si256( (__m256i *) &mask_vector_y[7]);
			cy6=_mm256_load_si256( (__m256i *) &mask_vector_y[8]);
			cy7=_mm256_setzero_si256();
			cy8=_mm256_setzero_si256();

			cy0_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[11]);
			cy1_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[12]);
			cy2_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[13]);
			cy3_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[14]);
			cy4_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[15]);
			cy5_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[16]);
			cy6_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[17]);
			cy7_sh1=_mm256_setzero_si256();
			cy8_sh1=_mm256_setzero_si256();
		}
		else if (row==3){
			r0=_mm256_loadu_si256( (__m256i *) &frame1[0][col2]);
			r1=_mm256_loadu_si256( (__m256i *) &frame1[1][col2]);
			r2=_mm256_loadu_si256( (__m256i *) &frame1[2][col2]);
			r3=_mm256_loadu_si256( (__m256i *) &frame1[3][col2]);
			r4=_mm256_loadu_si256( (__m256i *) &frame1[4][col2]);
			r5=_mm256_loadu_si256( (__m256i *) &frame1[5][col2]);
			r6=_mm256_loadu_si256( (__m256i *) &frame1[6][col2]);
			r7=_mm256_loadu_si256( (__m256i *) &frame1[7][col2]);
			r8=_mm256_setzero_si256();

			cy0=_mm256_load_si256( (__m256i *) &mask_vector_y[1]);
			cy1=_mm256_load_si256( (__m256i *) &mask_vector_y[2]);
			cy2=_mm256_load_si256( (__m256i *) &mask_vector_y[3]);
			cy3=_mm256_load_si256( (__m256i *) &mask_vector_y[4]);
			cy4=_mm256_load_si256( (__m256i *) &mask_vector_y[5]);
			cy5=_mm256_load_si256( (__m256i *) &mask_vector_y[6]);
			cy6=_mm256_load_si256( (__m256i *) &mask_vector_y[7]);
			cy7=_mm256_load_si256( (__m256i *) &mask_vector_y[8]);
			cy8=_mm256_setzero_si256();

			cy0_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[10]);
			cy1_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[11]);
			cy2_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[12]);
			cy3_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[13]);
			cy4_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[14]);
			cy5_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[15]);
			cy6_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[16]);
			cy7_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[17]);
			cy8_sh1=_mm256_setzero_si256();
		}
		else if (row==N-4){
			r0=_mm256_setzero_si256();
			r1=_mm256_loadu_si256( (__m256i *) &frame1[N-8][col2]);
			r2=_mm256_loadu_si256( (__m256i *) &frame1[N-7][col2]);
			r3=_mm256_loadu_si256( (__m256i *) &frame1[N-6][col2]);
			r4=_mm256_loadu_si256( (__m256i *) &frame1[N-5][col2]);
			r5=_mm256_loadu_si256( (__m256i *) &frame1[N-4][col2]);
			r6=_mm256_loadu_si256( (__m256i *) &frame1[N-3][col2]);
			r7=_mm256_loadu_si256( (__m256i *) &frame1[N-2][col2]);
			r8=_mm256_loadu_si256( (__m256i *) &frame1[N-1][col2]);

			cy0=_mm256_setzero_si256();
			cy1=_mm256_load_si256( (__m256i *) &mask_vector_y[0]);
			cy2=_mm256_load_si256( (__m256i *) &mask_vector_y[1]);
			cy3=_mm256_load_si256( (__m256i *) &mask_vector_y[2]);
			cy4=_mm256_load_si256( (__m256i *) &mask_vector_y[3]);
			cy5=_mm256_load_si256( (__m256i *) &mask_vector_y[4]);
			cy6=_mm256_load_si256( (__m256i *) &mask_vector_y[5]);
			cy7=_mm256_load_si256( (__m256i *) &mask_vector_y[6]);
			cy8=_mm256_load_si256( (__m256i *) &mask_vector_y[7]);

			cy0_sh1=_mm256_setzero_si256();
			cy1_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[9]);
			cy2_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[10]);
			cy3_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[11]);
			cy4_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[12]);
			cy5_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[13]);
			cy6_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[14]);
			cy7_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[15]);
			cy8_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[16]);

		}
		else if (row==N-3){
			r0=_mm256_setzero_si256();
			r1=_mm256_setzero_si256();
			r2=_mm256_loadu_si256( (__m256i *) &frame1[N-7][col2]);
			r3=_mm256_loadu_si256( (__m256i *) &frame1[N-6][col2]);
			r4=_mm256_loadu_si256( (__m256i *) &frame1[N-5][col2]);
			r5=_mm256_loadu_si256( (__m256i *) &frame1[N-4][col2]);
			r6=_mm256_loadu_si256( (__m256i *) &frame1[N-3][col2]);
			r7=_mm256_loadu_si256( (__m256i *) &frame1[N-2][col2]);
			r8=_mm256_loadu_si256( (__m256i *) &frame1[N-1][col2]);

			cy0=_mm256_setzero_si256();
			cy1=_mm256_setzero_si256();
			cy2=_mm256_load_si256( (__m256i *) &mask_vector_y[0]);
			cy3=_mm256_load_si256( (__m256i *) &mask_vector_y[1]);
			cy4=_mm256_load_si256( (__m256i *) &mask_vector_y[2]);
			cy5=_mm256_load_si256( (__m256i *) &mask_vector_y[3]);
			cy6=_mm256_load_si256( (__m256i *) &mask_vector_y[4]);
			cy7=_mm256_load_si256( (__m256i *) &mask_vector_y[5]);
			cy8=_mm256_load_si256( (__m256i *) &mask_vector_y[6]);

			cy0_sh1=_mm256_setzero_si256();
			cy1_sh1=_mm256_setzero_si256();
			cy2_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[9]);
			cy3_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[10]);
			cy4_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[11]);
			cy5_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[12]);
			cy6_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[13]);
			cy7_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[14]);
			cy8_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[15]);
		}
		else if (row==N-2){
			r0=_mm256_setzero_si256();
			r1=_mm256_setzero_si256();
			r2=_mm256_setzero_si256();
			r3=_mm256_loadu_si256( (__m256i *) &frame1[N-6][col2]);
			r4=_mm256_loadu_si256( (__m256i *) &frame1[N-5][col2]);
			r5=_mm256_loadu_si256( (__m256i *) &frame1[N-4][col2]);
			r6=_mm256_loadu_si256( (__m256i *) &frame1[N-3][col2]);
			r7=_mm256_loadu_si256( (__m256i *) &frame1[N-2][col2]);
			r8=_mm256_loadu_si256( (__m256i *) &frame1[N-1][col2]);

			cy0=_mm256_setzero_si256();
			cy1=_mm256_setzero_si256();
			cy2=_mm256_setzero_si256();
			cy3=_mm256_load_si256( (__m256i *) &mask_vector_y[0]);
			cy4=_mm256_load_si256( (__m256i *) &mask_vector_y[1]);
			cy5=_mm256_load_si256( (__m256i *) &mask_vector_y[2]);
			cy6=_mm256_load_si256( (__m256i *) &mask_vector_y[3]);
			cy7=_mm256_load_si256( (__m256i *) &mask_vector_y[4]);
			cy8=_mm256_load_si256( (__m256i *) &mask_vector_y[5]);

			cy0_sh1=_mm256_setzero_si256();
			cy1_sh1=_mm256_setzero_si256();
			cy2_sh1=_mm256_setzero_si256();
			cy3_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[9]);
			cy4_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[10]);
			cy5_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[11]);
			cy6_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[12]);
			cy7_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[13]);
			cy8_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[14]);
		}
		else if (row==N-1){
			r0=_mm256_setzero_si256();
			r1=_mm256_setzero_si256();
			r2=_mm256_setzero_si256();
			r3=_mm256_setzero_si256();
			r4=_mm256_loadu_si256( (__m256i *) &frame1[N-5][col2]);
			r5=_mm256_loadu_si256( (__m256i *) &frame1[N-4][col2]);
			r6=_mm256_loadu_si256( (__m256i *) &frame1[N-3][col2]);
			r7=_mm256_loadu_si256( (__m256i *) &frame1[N-2][col2]);
			r8=_mm256_loadu_si256( (__m256i *) &frame1[N-1][col2]);

			cy0=_mm256_setzero_si256();
			cy1=_mm256_setzero_si256();
			cy2=_mm256_setzero_si256();
			cy3=_mm256_setzero_si256();
			cy4=_mm256_load_si256( (__m256i *) &mask_vector_y[0]);
			cy5=_mm256_load_si256( (__m256i *) &mask_vector_y[1]);
			cy6=_mm256_load_si256( (__m256i *) &mask_vector_y[2]);
			cy7=_mm256_load_si256( (__m256i *) &mask_vector_y[3]);
			cy8=_mm256_load_si256( (__m256i *) &mask_vector_y[4]);

			cy0_sh1=_mm256_setzero_si256();
			cy1_sh1=_mm256_setzero_si256();
			cy2_sh1=_mm256_setzero_si256();
			cy3_sh1=_mm256_setzero_si256();
			cy4_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[9]);
			cy5_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[10]);
			cy6_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[11]);
			cy7_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[12]);
			cy8_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[13]);
		}
		else {
			printf("\nsomething went wrong");
			exit(EXIT_FAILURE);
		}
	}


	/*
	 * The above load operations load outside of the array bounds; these elements are filled with zeros just after
	 */
	reminder_mask=_mm256_load_si256( (__m256i *) &reminder_mask_9x9[REMINDER_ITERATIONS_Y-1][0]);

	//AND r0-r4 with reminder_mask
	r0=_mm256_and_si256(r0,reminder_mask);
	r1=_mm256_and_si256(r1,reminder_mask);
	r2=_mm256_and_si256(r2,reminder_mask);
	r3=_mm256_and_si256(r3,reminder_mask);
	r4=_mm256_and_si256(r4,reminder_mask);
	r5=_mm256_and_si256(r5,reminder_mask);
	r6=_mm256_and_si256(r6,reminder_mask);
	r7=_mm256_and_si256(r7,reminder_mask);
	r8=_mm256_and_si256(r8,reminder_mask);


//--------------------y filter begins--------------------

//multiply with the mask
	m0=_mm256_maddubs_epi16(r0,cy0);
	m1=_mm256_maddubs_epi16(r1,cy1);
	m2=_mm256_maddubs_epi16(r2,cy2);
	m3=_mm256_maddubs_epi16(r3,cy3);
	m4=_mm256_maddubs_epi16(r4,cy4);
	m5=_mm256_maddubs_epi16(r5,cy5);
	m6=_mm256_maddubs_epi16(r6,cy6);
	m7=_mm256_maddubs_epi16(r7,cy7);
	m8=_mm256_maddubs_epi16(r8,cy8);

	//vertical add
	m0=_mm256_add_epi16(m0,m1);
	m0=_mm256_add_epi16(m0,m2);
	m0=_mm256_add_epi16(m0,m3);
	m0=_mm256_add_epi16(m0,m4);
	m0=_mm256_add_epi16(m0,m5);
	m0=_mm256_add_epi16(m0,m6);
	m0=_mm256_add_epi16(m0,m7);
	m0=_mm256_add_epi16(m0,m8);

	even=division(division_case,m0,f);//even results

	//multiply with the mask
	m0=_mm256_maddubs_epi16(r0,cy0_sh1);
	m1=_mm256_maddubs_epi16(r1,cy1_sh1);
	m2=_mm256_maddubs_epi16(r2,cy2_sh1);
	m3=_mm256_maddubs_epi16(r3,cy3_sh1);
	m4=_mm256_maddubs_epi16(r4,cy4_sh1);
	m5=_mm256_maddubs_epi16(r5,cy5_sh1);
	m6=_mm256_maddubs_epi16(r6,cy6_sh1);
	m7=_mm256_maddubs_epi16(r7,cy7_sh1);
	m8=_mm256_maddubs_epi16(r8,cy8_sh1);

	//vertical add
	m0=_mm256_add_epi16(m0,m1);
	m0=_mm256_add_epi16(m0,m2);
	m0=_mm256_add_epi16(m0,m3);
	m0=_mm256_add_epi16(m0,m4);
	m0=_mm256_add_epi16(m0,m5);
	m0=_mm256_add_epi16(m0,m6);
	m0=_mm256_add_epi16(m0,m7);
	m0=_mm256_add_epi16(m0,m8);

	odd=division(division_case,m0,f);//odd results

	//pack to one register
	odd=_mm256_slli_si256(odd,1); //shift one position right
	r0=_mm256_add_epi8(even,odd); //add the odd with the even


	_mm256_storeu_si256( (__m256i *) &temp[M-REMINDER_ITERATIONS_Y],r0 );

	for (int gg=M;gg<M+32+6;gg++)
	    temp[gg]=0;
//y filter ends - r0 has now the data to be processed by x filter
}



	//multiply by the mask
       r0=_mm256_loadu_si256( (__m256i *) &temp[col-4]);
		m0=_mm256_maddubs_epi16(r0,cx0);


		//first horizontal add (complex shift is needed)
		m3=_mm256_srli_si256(m0,2);
		m4=_mm256_and_si256(m0,mask3);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,14);
		m3=_mm256_add_epi16(m3,m4);
		m3=_mm256_add_epi16(m0,m3);//add


		//second horizontal add
		m1=_mm256_srli_si256(m3,4);
		m3=_mm256_add_epi16(m3,m1);//add


		//third horizontal add
		m0=_mm256_and_si256(m0,mask_16_5);
		m2=_mm256_srli_si256(m0,8);
		m4=_mm256_and_si256(m0,mask3_2);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,8);
		m2=_mm256_add_epi16(m2,m4);

		m0=_mm256_add_epi16(m2,m3);//add

		even=_mm256_and_si256(m0,mask_16_1);//in 16-bit format keep only 0,5,10 and discard the others


		//2nd col iteration
       r0=_mm256_loadu_si256( (__m256i *) &temp[col-3]);

		//multiply by the mask
		m0=_mm256_maddubs_epi16(r0,cx0);

		//first horizontal add (complex shift is needed)
		m3=_mm256_srli_si256(m0,2);
		m4=_mm256_and_si256(m0,mask3);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,14);
		m3=_mm256_add_epi16(m3,m4);
		m3=_mm256_add_epi16(m0,m3);//add

		//second horizontal add
		m1=_mm256_srli_si256(m3,4);
		m3=_mm256_add_epi16(m3,m1);//add

		//third horizontal add
		m0=_mm256_and_si256(m0,mask_16_5);
		m2=_mm256_srli_si256(m0,8);
		m4=_mm256_and_si256(m0,mask3_2);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,8);
		m2=_mm256_add_epi16(m2,m4);

		m0=_mm256_add_epi16(m2,m3);//add

		odd=_mm256_and_si256(m0,mask_16_1);//in 16-bit format keep only 0,5,10 and discard the others


		//3rd col iteration
       r0=_mm256_loadu_si256( (__m256i *) &temp[col-2]);

		//multiply by the mask
		m0=_mm256_maddubs_epi16(r0,cx0);

		//first horizontal add (complex shift is needed)
		m3=_mm256_srli_si256(m0,2);
		m4=_mm256_and_si256(m0,mask3);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,14);
		m3=_mm256_add_epi16(m3,m4);
		m3=_mm256_add_epi16(m0,m3);//add

		//second horizontal add
		m1=_mm256_srli_si256(m3,4);
		m3=_mm256_add_epi16(m3,m1);//add

		//third horizontal add
		m0=_mm256_and_si256(m0,mask_16_5);
		m2=_mm256_srli_si256(m0,8);
		m4=_mm256_and_si256(m0,mask3_2);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,8);
		m2=_mm256_add_epi16(m2,m4);

		m0=_mm256_add_epi16(m2,m3);//add

		m0=_mm256_and_si256(m0,mask_16_1);//in 16-bit format keep only 0,5,10 and discard the others
		m0=_mm256_slli_si256(m0,2);
		even=_mm256_add_epi16(m0,even);

		//4th col iteration
       r0=_mm256_loadu_si256( (__m256i *) &temp[col-1]);

		//multiply by the mask
		m0=_mm256_maddubs_epi16(r0,cx0);

		//first horizontal add (complex shift is needed)
		m3=_mm256_srli_si256(m0,2);
		m4=_mm256_and_si256(m0,mask3);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,14);
		m3=_mm256_add_epi16(m3,m4);
		m3=_mm256_add_epi16(m0,m3);//add

		//second horizontal add
		m1=_mm256_srli_si256(m3,4);
		m3=_mm256_add_epi16(m3,m1);//add

		//third horizontal add
		m0=_mm256_and_si256(m0,mask_16_5);
		m2=_mm256_srli_si256(m0,8);
		m4=_mm256_and_si256(m0,mask3_2);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,8);
		m2=_mm256_add_epi16(m2,m4);

		m0=_mm256_add_epi16(m2,m3);//add

		m0=_mm256_and_si256(m0,mask_16_1);//in 16-bit format keep only 0,5,10 and discard the others
		m0=_mm256_slli_si256(m0,2);
		odd=_mm256_add_epi16(m0,odd);

		//5th col iteration
       r0=_mm256_loadu_si256( (__m256i *) &temp[col]);

		//multiply by the mask
		m0=_mm256_maddubs_epi16(r0,cx0);

		//first horizontal add (complex shift is needed)
		m3=_mm256_srli_si256(m0,2);
		m4=_mm256_and_si256(m0,mask3);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,14);
		m3=_mm256_add_epi16(m3,m4);
		m3=_mm256_add_epi16(m0,m3);//add

		//second horizontal add
		m1=_mm256_srli_si256(m3,4);
		m3=_mm256_add_epi16(m3,m1);//add

		//third horizontal add
		m0=_mm256_and_si256(m0,mask_16_5);
		m2=_mm256_srli_si256(m0,8);
		m4=_mm256_and_si256(m0,mask3_2);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,8);
		m2=_mm256_add_epi16(m2,m4);

		m0=_mm256_add_epi16(m2,m3);//add

		m0=_mm256_and_si256(m0,mask_16_1);//in 16-bit format keep only 0,5,10 and discard the others
		m0=_mm256_slli_si256(m0,4);
		even=_mm256_add_epi16(m0,even);

		//6th col iteration
       r0=_mm256_loadu_si256( (__m256i *) &temp[col+1]);

		//multiply by the mask
		m0=_mm256_maddubs_epi16(r0,cx0);

		//first horizontal add (complex shift is needed)
		m3=_mm256_srli_si256(m0,2);
		m4=_mm256_and_si256(m0,mask3);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,14);
		m3=_mm256_add_epi16(m3,m4);
		m3=_mm256_add_epi16(m0,m3);//add

		//second horizontal add
		m1=_mm256_srli_si256(m3,4);
		m3=_mm256_add_epi16(m3,m1);//add

		//third horizontal add
		m0=_mm256_and_si256(m0,mask_16_5);
		m2=_mm256_srli_si256(m0,8);
		m4=_mm256_and_si256(m0,mask3_2);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,8);
		m2=_mm256_add_epi16(m2,m4);

		m0=_mm256_add_epi16(m2,m3);//add

		m0=_mm256_and_si256(m0,mask_16_1);//in 16-bit format keep only 0,5,10 and discard the others
		m0=_mm256_slli_si256(m0,4);
		odd=_mm256_add_epi16(m0,odd);

		//7th col iteration
       r0=_mm256_loadu_si256( (__m256i *) &temp[col+2]);

		//multiply by the mask
		m0=_mm256_maddubs_epi16(r0,cx0);

		//first horizontal add (complex shift is needed)
		m3=_mm256_srli_si256(m0,2);
		m4=_mm256_and_si256(m0,mask3);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,14);
		m3=_mm256_add_epi16(m3,m4);
		m3=_mm256_add_epi16(m0,m3);//add

		//second horizontal add
		m1=_mm256_srli_si256(m3,4);
		m3=_mm256_add_epi16(m3,m1);//add

		//third horizontal add
		m0=_mm256_and_si256(m0,mask_16_5);
		m2=_mm256_srli_si256(m0,8);
		m4=_mm256_and_si256(m0,mask3_2);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,8);
		m2=_mm256_add_epi16(m2,m4);

		m0=_mm256_add_epi16(m2,m3);//add

		m0=_mm256_and_si256(m0,mask_16_1);//in 16-bit format keep only 0,5,10 and discard the others

		//shift the result 3 positions
		m1=_mm256_slli_si256(m0,6);
		m2=_mm256_and_si256(m0,mask_new_1);
		m2=_mm256_permute2f128_si256(m2,m2,1);
		m2=_mm256_srli_si256(m2,10);
		m0=_mm256_add_epi16(m2,m1);

		even=_mm256_add_epi16(m0,even);

		//8th col iteration
       r0=_mm256_loadu_si256( (__m256i *) &temp[col+3]);

		//multiply by the mask
		m0=_mm256_maddubs_epi16(r0,cx0);

		//first horizontal add (complex shift is needed)
		m3=_mm256_srli_si256(m0,2);
		m4=_mm256_and_si256(m0,mask3);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,14);
		m3=_mm256_add_epi16(m3,m4);
		m3=_mm256_add_epi16(m0,m3);//add

		//second horizontal add
		m1=_mm256_srli_si256(m3,4);
		m3=_mm256_add_epi16(m3,m1);//add

		//third horizontal add
		m0=_mm256_and_si256(m0,mask_16_5);
		m2=_mm256_srli_si256(m0,8);
		m4=_mm256_and_si256(m0,mask3_2);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,8);
		m2=_mm256_add_epi16(m2,m4);

		m0=_mm256_add_epi16(m2,m3);//add

		m0=_mm256_and_si256(m0,mask_16_1);//in 16-bit format keep only 0,5,10 and discard the others

		//shift the result 3 positions
		m1=_mm256_slli_si256(m0,6);
		m2=_mm256_and_si256(m0,mask_new_1);
		m2=_mm256_permute2f128_si256(m2,m2,1);
		m2=_mm256_srli_si256(m2,10);
		m0=_mm256_add_epi16(m2,m1);

		odd=_mm256_add_epi16(m0,odd);

		//9th col iteration
       r0=_mm256_loadu_si256( (__m256i *) &temp[col+4]);

		//multiply by the mask
		m0=_mm256_maddubs_epi16(r0,cx0);

		//first horizontal add (complex shift is needed)
		m3=_mm256_srli_si256(m0,2);
		m4=_mm256_and_si256(m0,mask3);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,14);
		m3=_mm256_add_epi16(m3,m4);
		m3=_mm256_add_epi16(m0,m3);//add

		//second horizontal add
		m1=_mm256_srli_si256(m3,4);
		m3=_mm256_add_epi16(m3,m1);//add

		//third horizontal add
		m0=_mm256_and_si256(m0,mask_16_5);
		m2=_mm256_srli_si256(m0,8);
		m4=_mm256_and_si256(m0,mask3_2);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,8);
		m2=_mm256_add_epi16(m2,m4);

		m0=_mm256_add_epi16(m2,m3);//add

		m0=_mm256_and_si256(m0,mask_16_1);//in 16-bit format keep only 0,5,10 and discard the others

		//shift the result 4 positions
		m1=_mm256_slli_si256(m0,8);
		m2=_mm256_and_si256(m0,mask_new_1);
		m2=_mm256_permute2f128_si256(m2,m2,1);
		m2=_mm256_srli_si256(m2,8);
		m0=_mm256_add_epi16(m2,m1);

		even=_mm256_add_epi16(m0,even);

		//10th col iteration
       r0=_mm256_loadu_si256( (__m256i *) &temp[col+5]);

		//multiply by the mask
		m0=_mm256_maddubs_epi16(r0,cx0);

		//first horizontal add (complex shift is needed)
		m3=_mm256_srli_si256(m0,2);
		m4=_mm256_and_si256(m0,mask3);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,14);
		m3=_mm256_add_epi16(m3,m4);
		m3=_mm256_add_epi16(m0,m3);//add

		//second horizontal add
		m1=_mm256_srli_si256(m3,4);
		m3=_mm256_add_epi16(m3,m1);//add

		//third horizontal add
		m0=_mm256_and_si256(m0,mask_16_5);
		m2=_mm256_srli_si256(m0,8);
		m4=_mm256_and_si256(m0,mask3_2);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,8);
		m2=_mm256_add_epi16(m2,m4);

		m0=_mm256_add_epi16(m2,m3);//add

		m0=_mm256_and_si256(m0,mask_16_1);//in 16-bit format keep only 0,5,10 and discard the others

		//shift the result 4 positions
		m1=_mm256_slli_si256(m0,8);
		m2=_mm256_and_si256(m0,mask_new_1);
		m2=_mm256_permute2f128_si256(m2,m2,1);
		m2=_mm256_srli_si256(m2,8);
		m0=_mm256_add_epi16(m2,m1);

		odd=_mm256_add_epi16(m0,odd);


		//division
		even = division(division_case,even,f);
		odd = division(division_case,odd,f);

		//blend even , odd into one register
		odd = _mm256_slli_si256(odd,1);
		output1=_mm256_add_epi16(even,odd);

		//_mm256_storeu_si256( (__m256i *) &filt[row][col],even );

	switch (REMINDER_ITERATIONS_X){
	case 1:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		  break;
	case 2:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		  break;
	case 3:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		  break;
	case 4:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output1,3);
		  break;
	case 5:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output1,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output1,4);
		  break;
	case 6:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output1,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output1,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output1,5);
		  break;
	case 7:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output1,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output1,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output1,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output1,6);
		  break;
	case 8:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output1,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output1,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output1,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output1,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output1,7);
		  break;
	case 9:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output1,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output1,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output1,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output1,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output1,7);
		filt[row][col+8] = (unsigned char) _mm256_extract_epi8(output1,8);
		  break;
	case 10:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output1,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output1,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output1,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output1,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output1,7);
		filt[row][col+8] = (unsigned char) _mm256_extract_epi8(output1,8);
		filt[row][col+9] = (unsigned char) _mm256_extract_epi8(output1,9);
		  break;
	case 11:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output1,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output1,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output1,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output1,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output1,7);
		filt[row][col+8] = (unsigned char) _mm256_extract_epi8(output1,8);
		filt[row][col+9] = (unsigned char) _mm256_extract_epi8(output1,9);
		filt[row][col+10] = (unsigned char) _mm256_extract_epi8(output1,10);
		  break;
	case 12:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output1,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output1,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output1,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output1,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output1,7);
		filt[row][col+8] = (unsigned char) _mm256_extract_epi8(output1,8);
		filt[row][col+9] = (unsigned char) _mm256_extract_epi8(output1,9);
		filt[row][col+10] = (unsigned char) _mm256_extract_epi8(output1,10);
		filt[row][col+11] = (unsigned char) _mm256_extract_epi8(output1,11);
		  break;
	case 13:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output1,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output1,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output1,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output1,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output1,7);
		filt[row][col+8] = (unsigned char) _mm256_extract_epi8(output1,8);
		filt[row][col+9] = (unsigned char) _mm256_extract_epi8(output1,9);
		filt[row][col+10] = (unsigned char) _mm256_extract_epi8(output1,10);
		filt[row][col+11] = (unsigned char) _mm256_extract_epi8(output1,11);
		filt[row][col+12] = (unsigned char) _mm256_extract_epi8(output1,12);
		  break;
	case 14:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output1,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output1,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output1,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output1,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output1,7);
		filt[row][col+8] = (unsigned char) _mm256_extract_epi8(output1,8);
		filt[row][col+9] = (unsigned char) _mm256_extract_epi8(output1,9);
		filt[row][col+10] = (unsigned char) _mm256_extract_epi8(output1,10);
		filt[row][col+11] = (unsigned char) _mm256_extract_epi8(output1,11);
		filt[row][col+12] = (unsigned char) _mm256_extract_epi8(output1,12);
		filt[row][col+13] = (unsigned char) _mm256_extract_epi8(output1,13);
		  break;
	case 15:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output1,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output1,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output1,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output1,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output1,7);
		filt[row][col+8] = (unsigned char) _mm256_extract_epi8(output1,8);
		filt[row][col+9] = (unsigned char) _mm256_extract_epi8(output1,9);
		filt[row][col+10] = (unsigned char) _mm256_extract_epi8(output1,10);
		filt[row][col+11] = (unsigned char) _mm256_extract_epi8(output1,11);
		filt[row][col+12] = (unsigned char) _mm256_extract_epi8(output1,12);
		filt[row][col+13] = (unsigned char) _mm256_extract_epi8(output1,13);
		filt[row][col+14] = (unsigned char) _mm256_extract_epi8(output1,14);
		  break;
	case 16:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
		  break;
	case 17:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
		  break;
	case 18:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
		  break;
	case 19:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
		  break;
	case 20:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output1,19);
		  break;
	case 21:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output1,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output1,20);
		  break;
	case 22:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output1,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output1,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output1,21);
		break;
	case 23:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output1,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output1,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output1,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output1,22);
		  break;
	case 24:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output1,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output1,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output1,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output1,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output1,23);
		  break;
	case 25:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output1,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output1,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output1,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output1,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output1,23);
	filt[row][col+24] = (unsigned char) _mm256_extract_epi8(output1,24);
		  break;
	case 26:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output1,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output1,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output1,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output1,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output1,23);
	filt[row][col+24] = (unsigned char) _mm256_extract_epi8(output1,24);
	filt[row][col+25] = (unsigned char) _mm256_extract_epi8(output1,25);
		  break;
	case 27:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output1,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output1,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output1,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output1,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output1,23);
	filt[row][col+24] = (unsigned char) _mm256_extract_epi8(output1,24);
	filt[row][col+25] = (unsigned char) _mm256_extract_epi8(output1,25);
	filt[row][col+26] = (unsigned char) _mm256_extract_epi8(output1,26);
		  break;
	case 28:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output1,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output1,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output1,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output1,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output1,23);
	filt[row][col+24] = (unsigned char) _mm256_extract_epi8(output1,24);
	filt[row][col+25] = (unsigned char) _mm256_extract_epi8(output1,25);
	filt[row][col+26] = (unsigned char) _mm256_extract_epi8(output1,26);
	filt[row][col+27] = (unsigned char) _mm256_extract_epi8(output1,27);
		  break;
	case 29:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output1,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output1,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output1,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output1,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output1,23);
	filt[row][col+24] = (unsigned char) _mm256_extract_epi8(output1,24);
	filt[row][col+25] = (unsigned char) _mm256_extract_epi8(output1,25);
	filt[row][col+26] = (unsigned char) _mm256_extract_epi8(output1,26);
	filt[row][col+27] = (unsigned char) _mm256_extract_epi8(output1,27);
	filt[row][col+28] = (unsigned char) _mm256_extract_epi8(output1,28);
		  break;
	case 30:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output1,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output1,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output1,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output1,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output1,23);
	filt[row][col+24] = (unsigned char) _mm256_extract_epi8(output1,24);
	filt[row][col+25] = (unsigned char) _mm256_extract_epi8(output1,25);
	filt[row][col+26] = (unsigned char) _mm256_extract_epi8(output1,26);
	filt[row][col+27] = (unsigned char) _mm256_extract_epi8(output1,27);
	filt[row][col+28] = (unsigned char) _mm256_extract_epi8(output1,28);
	filt[row][col+29] = (unsigned char) _mm256_extract_epi8(output1,29);
		  break;
	case 31:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output1,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output1,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output1,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output1,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output1,23);
	filt[row][col+24] = (unsigned char) _mm256_extract_epi8(output1,24);
	filt[row][col+25] = (unsigned char) _mm256_extract_epi8(output1,25);
	filt[row][col+26] = (unsigned char) _mm256_extract_epi8(output1,26);
	filt[row][col+27] = (unsigned char) _mm256_extract_epi8(output1,27);
	filt[row][col+28] = (unsigned char) _mm256_extract_epi8(output1,28);
	filt[row][col+29] = (unsigned char) _mm256_extract_epi8(output1,29);

	int newPixel = 0;

	newPixel += temp[col+30-4] * kernel_x[0];
	newPixel += temp[col+30-3] * kernel_x[1];
	newPixel += temp[col+30-2] * kernel_x[2];
	newPixel += temp[col+30-1] * kernel_x[3];
	newPixel += temp[col+30-0] * kernel_x[4];

	filt[row][col+30] = (unsigned char) (newPixel / divisor_xy);

		  break;
	default: //REMINDER IS EITHER 27,28,29,30 OR 31
		printf("\nsomething went wrong");
		exit(EXIT_FAILURE);
 	}


}





void loop_reminder_9x9_16_blur_Y(unsigned char **frame1,unsigned char **filt,unsigned char *temp,const unsigned int N,const unsigned int M,const unsigned int row, const unsigned int REMINDER_ITERATIONS_Y,const unsigned int division_case,const signed char mask_vector_y[][32], const __m256i f, const unsigned short int divisor_xy){

	__m256i r0,r1,r2,r3,r4,r5,r6,r7,r8,m0,m1,m2,m3,m4,m5,m6,m7,m8,even,odd,reminder_mask;


 if (REMINDER_ITERATIONS_Y==0){//no need for computing the y mask
		_mm256_store_si256( (__m256i *) &temp[M],_mm256_setzero_si256() );
        for (unsigned int g=M+32;g<M+32+6;g++)
        	temp[g]=0;
	}
 else {
		__m256i cy0,cy1,cy2,cy3,cy4,cy5,cy6,cy7,cy8,cy0_sh1,cy1_sh1,cy2_sh1,cy3_sh1,cy4_sh1,cy5_sh1,cy6_sh1,cy7_sh1,cy8_sh1;

	unsigned int col2=M-REMINDER_ITERATIONS_Y;//this is the col index to load the elements for the y mask

	if ((row>3) && (row<N-4)){//main case

		r0=_mm256_loadu_si256( (__m256i *) &frame1[row-4][col2]);
		r1=_mm256_loadu_si256( (__m256i *) &frame1[row-3][col2]);
		r2=_mm256_loadu_si256( (__m256i *) &frame1[row-2][col2]);
		r3=_mm256_loadu_si256( (__m256i *) &frame1[row-1][col2]);
		r4=_mm256_loadu_si256( (__m256i *) &frame1[row][col2]);
		r5=_mm256_loadu_si256( (__m256i *) &frame1[row+1][col2]);
		r6=_mm256_loadu_si256( (__m256i *) &frame1[row+2][col2]);
		r7=_mm256_loadu_si256( (__m256i *) &frame1[row+3][col2]);
		r8=_mm256_loadu_si256( (__m256i *) &frame1[row+4][col2]);

		cy0=_mm256_load_si256( (__m256i *) &mask_vector_y[0]);
		cy1=_mm256_load_si256( (__m256i *) &mask_vector_y[1]);
		cy2=_mm256_load_si256( (__m256i *) &mask_vector_y[2]);
		cy3=_mm256_load_si256( (__m256i *) &mask_vector_y[3]);
		cy4=_mm256_load_si256( (__m256i *) &mask_vector_y[4]);
		cy5=_mm256_load_si256( (__m256i *) &mask_vector_y[5]);
		cy6=_mm256_load_si256( (__m256i *) &mask_vector_y[6]);
		cy7=_mm256_load_si256( (__m256i *) &mask_vector_y[7]);
		cy8=_mm256_load_si256( (__m256i *) &mask_vector_y[8]);

		cy0_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[9]);
		cy1_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[10]);
		cy2_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[11]);
		cy3_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[12]);
		cy4_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[13]);
		cy5_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[14]);
		cy6_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[15]);
		cy7_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[16]);
		cy8_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[17]);

	}
	else {
		if (row==0){
			r0=_mm256_loadu_si256( (__m256i *) &frame1[0][col2]);
			r1=_mm256_loadu_si256( (__m256i *) &frame1[1][col2]);
			r2=_mm256_loadu_si256( (__m256i *) &frame1[2][col2]);
			r3=_mm256_loadu_si256( (__m256i *) &frame1[3][col2]);
			r4=_mm256_loadu_si256( (__m256i *) &frame1[4][col2]);
			r5=_mm256_setzero_si256();
			r6=_mm256_setzero_si256();
			r7=_mm256_setzero_si256();
			r8=_mm256_setzero_si256();

			cy0=_mm256_load_si256( (__m256i *) &mask_vector_y[4]);
			cy1=_mm256_load_si256( (__m256i *) &mask_vector_y[5]);
			cy2=_mm256_load_si256( (__m256i *) &mask_vector_y[6]);
			cy3=_mm256_load_si256( (__m256i *) &mask_vector_y[7]);
			cy4=_mm256_load_si256( (__m256i *) &mask_vector_y[8]);
			cy5=_mm256_setzero_si256();
			cy6=_mm256_setzero_si256();
			cy7=_mm256_setzero_si256();
			cy8=_mm256_setzero_si256();

			cy0_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[13]);
			cy1_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[14]);
			cy2_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[15]);
			cy3_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[16]);
			cy4_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[17]);
			cy5_sh1=_mm256_setzero_si256();
			cy6_sh1=_mm256_setzero_si256();
			cy7_sh1=_mm256_setzero_si256();
			cy8_sh1=_mm256_setzero_si256();

		}
		else if (row==1){
			r0=_mm256_loadu_si256( (__m256i *) &frame1[0][col2]);
			r1=_mm256_loadu_si256( (__m256i *) &frame1[1][col2]);
			r2=_mm256_loadu_si256( (__m256i *) &frame1[2][col2]);
			r3=_mm256_loadu_si256( (__m256i *) &frame1[3][col2]);
			r4=_mm256_loadu_si256( (__m256i *) &frame1[4][col2]);
			r5=_mm256_loadu_si256( (__m256i *) &frame1[5][col2]);
			r6=_mm256_setzero_si256();
			r7=_mm256_setzero_si256();
			r8=_mm256_setzero_si256();

			cy0=_mm256_load_si256( (__m256i *) &mask_vector_y[3]);
			cy1=_mm256_load_si256( (__m256i *) &mask_vector_y[4]);
			cy2=_mm256_load_si256( (__m256i *) &mask_vector_y[5]);
			cy3=_mm256_load_si256( (__m256i *) &mask_vector_y[6]);
			cy4=_mm256_load_si256( (__m256i *) &mask_vector_y[7]);
			cy5=_mm256_load_si256( (__m256i *) &mask_vector_y[8]);
			cy6=_mm256_setzero_si256();
			cy7=_mm256_setzero_si256();
			cy8=_mm256_setzero_si256();

			cy0_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[12]);
			cy1_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[13]);
			cy2_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[14]);
			cy3_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[15]);
			cy4_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[16]);
			cy5_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[17]);
			cy6_sh1=_mm256_setzero_si256();
			cy7_sh1=_mm256_setzero_si256();
			cy8_sh1=_mm256_setzero_si256();
		}
		else if (row==2){
			r0=_mm256_loadu_si256( (__m256i *) &frame1[0][col2]);
			r1=_mm256_loadu_si256( (__m256i *) &frame1[1][col2]);
			r2=_mm256_loadu_si256( (__m256i *) &frame1[2][col2]);
			r3=_mm256_loadu_si256( (__m256i *) &frame1[3][col2]);
			r4=_mm256_loadu_si256( (__m256i *) &frame1[4][col2]);
			r5=_mm256_loadu_si256( (__m256i *) &frame1[5][col2]);
			r6=_mm256_loadu_si256( (__m256i *) &frame1[6][col2]);
			r7=_mm256_setzero_si256();
			r8=_mm256_setzero_si256();

			cy0=_mm256_load_si256( (__m256i *) &mask_vector_y[2]);
			cy1=_mm256_load_si256( (__m256i *) &mask_vector_y[3]);
			cy2=_mm256_load_si256( (__m256i *) &mask_vector_y[4]);
			cy3=_mm256_load_si256( (__m256i *) &mask_vector_y[5]);
			cy4=_mm256_load_si256( (__m256i *) &mask_vector_y[6]);
			cy5=_mm256_load_si256( (__m256i *) &mask_vector_y[7]);
			cy6=_mm256_load_si256( (__m256i *) &mask_vector_y[8]);
			cy7=_mm256_setzero_si256();
			cy8=_mm256_setzero_si256();

			cy0_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[11]);
			cy1_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[12]);
			cy2_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[13]);
			cy3_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[14]);
			cy4_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[15]);
			cy5_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[16]);
			cy6_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[17]);
			cy7_sh1=_mm256_setzero_si256();
			cy8_sh1=_mm256_setzero_si256();
		}
		else if (row==3){
			r0=_mm256_loadu_si256( (__m256i *) &frame1[0][col2]);
			r1=_mm256_loadu_si256( (__m256i *) &frame1[1][col2]);
			r2=_mm256_loadu_si256( (__m256i *) &frame1[2][col2]);
			r3=_mm256_loadu_si256( (__m256i *) &frame1[3][col2]);
			r4=_mm256_loadu_si256( (__m256i *) &frame1[4][col2]);
			r5=_mm256_loadu_si256( (__m256i *) &frame1[5][col2]);
			r6=_mm256_loadu_si256( (__m256i *) &frame1[6][col2]);
			r7=_mm256_loadu_si256( (__m256i *) &frame1[7][col2]);
			r8=_mm256_setzero_si256();

			cy0=_mm256_load_si256( (__m256i *) &mask_vector_y[1]);
			cy1=_mm256_load_si256( (__m256i *) &mask_vector_y[2]);
			cy2=_mm256_load_si256( (__m256i *) &mask_vector_y[3]);
			cy3=_mm256_load_si256( (__m256i *) &mask_vector_y[4]);
			cy4=_mm256_load_si256( (__m256i *) &mask_vector_y[5]);
			cy5=_mm256_load_si256( (__m256i *) &mask_vector_y[6]);
			cy6=_mm256_load_si256( (__m256i *) &mask_vector_y[7]);
			cy7=_mm256_load_si256( (__m256i *) &mask_vector_y[8]);
			cy8=_mm256_setzero_si256();

			cy0_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[10]);
			cy1_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[11]);
			cy2_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[12]);
			cy3_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[13]);
			cy4_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[14]);
			cy5_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[15]);
			cy6_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[16]);
			cy7_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[17]);
			cy8_sh1=_mm256_setzero_si256();
		}
		else if (row==N-4){
			r0=_mm256_setzero_si256();
			r1=_mm256_loadu_si256( (__m256i *) &frame1[N-8][col2]);
			r2=_mm256_loadu_si256( (__m256i *) &frame1[N-7][col2]);
			r3=_mm256_loadu_si256( (__m256i *) &frame1[N-6][col2]);
			r4=_mm256_loadu_si256( (__m256i *) &frame1[N-5][col2]);
			r5=_mm256_loadu_si256( (__m256i *) &frame1[N-4][col2]);
			r6=_mm256_loadu_si256( (__m256i *) &frame1[N-3][col2]);
			r7=_mm256_loadu_si256( (__m256i *) &frame1[N-2][col2]);
			r8=_mm256_loadu_si256( (__m256i *) &frame1[N-1][col2]);

			cy0=_mm256_setzero_si256();
			cy1=_mm256_load_si256( (__m256i *) &mask_vector_y[0]);
			cy2=_mm256_load_si256( (__m256i *) &mask_vector_y[1]);
			cy3=_mm256_load_si256( (__m256i *) &mask_vector_y[2]);
			cy4=_mm256_load_si256( (__m256i *) &mask_vector_y[3]);
			cy5=_mm256_load_si256( (__m256i *) &mask_vector_y[4]);
			cy6=_mm256_load_si256( (__m256i *) &mask_vector_y[5]);
			cy7=_mm256_load_si256( (__m256i *) &mask_vector_y[6]);
			cy8=_mm256_load_si256( (__m256i *) &mask_vector_y[7]);

			cy0_sh1=_mm256_setzero_si256();
			cy1_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[9]);
			cy2_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[10]);
			cy3_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[11]);
			cy4_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[12]);
			cy5_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[13]);
			cy6_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[14]);
			cy7_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[15]);
			cy8_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[16]);

		}
		else if (row==N-3){
			r0=_mm256_setzero_si256();
			r1=_mm256_setzero_si256();
			r2=_mm256_loadu_si256( (__m256i *) &frame1[N-7][col2]);
			r3=_mm256_loadu_si256( (__m256i *) &frame1[N-6][col2]);
			r4=_mm256_loadu_si256( (__m256i *) &frame1[N-5][col2]);
			r5=_mm256_loadu_si256( (__m256i *) &frame1[N-4][col2]);
			r6=_mm256_loadu_si256( (__m256i *) &frame1[N-3][col2]);
			r7=_mm256_loadu_si256( (__m256i *) &frame1[N-2][col2]);
			r8=_mm256_loadu_si256( (__m256i *) &frame1[N-1][col2]);

			cy0=_mm256_setzero_si256();
			cy1=_mm256_setzero_si256();
			cy2=_mm256_load_si256( (__m256i *) &mask_vector_y[0]);
			cy3=_mm256_load_si256( (__m256i *) &mask_vector_y[1]);
			cy4=_mm256_load_si256( (__m256i *) &mask_vector_y[2]);
			cy5=_mm256_load_si256( (__m256i *) &mask_vector_y[3]);
			cy6=_mm256_load_si256( (__m256i *) &mask_vector_y[4]);
			cy7=_mm256_load_si256( (__m256i *) &mask_vector_y[5]);
			cy8=_mm256_load_si256( (__m256i *) &mask_vector_y[6]);

			cy0_sh1=_mm256_setzero_si256();
			cy1_sh1=_mm256_setzero_si256();
			cy2_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[9]);
			cy3_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[10]);
			cy4_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[11]);
			cy5_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[12]);
			cy6_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[13]);
			cy7_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[14]);
			cy8_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[15]);
		}
		else if (row==N-2){
			r0=_mm256_setzero_si256();
			r1=_mm256_setzero_si256();
			r2=_mm256_setzero_si256();
			r3=_mm256_loadu_si256( (__m256i *) &frame1[N-6][col2]);
			r4=_mm256_loadu_si256( (__m256i *) &frame1[N-5][col2]);
			r5=_mm256_loadu_si256( (__m256i *) &frame1[N-4][col2]);
			r6=_mm256_loadu_si256( (__m256i *) &frame1[N-3][col2]);
			r7=_mm256_loadu_si256( (__m256i *) &frame1[N-2][col2]);
			r8=_mm256_loadu_si256( (__m256i *) &frame1[N-1][col2]);

			cy0=_mm256_setzero_si256();
			cy1=_mm256_setzero_si256();
			cy2=_mm256_setzero_si256();
			cy3=_mm256_load_si256( (__m256i *) &mask_vector_y[0]);
			cy4=_mm256_load_si256( (__m256i *) &mask_vector_y[1]);
			cy5=_mm256_load_si256( (__m256i *) &mask_vector_y[2]);
			cy6=_mm256_load_si256( (__m256i *) &mask_vector_y[3]);
			cy7=_mm256_load_si256( (__m256i *) &mask_vector_y[4]);
			cy8=_mm256_load_si256( (__m256i *) &mask_vector_y[5]);

			cy0_sh1=_mm256_setzero_si256();
			cy1_sh1=_mm256_setzero_si256();
			cy2_sh1=_mm256_setzero_si256();
			cy3_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[9]);
			cy4_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[10]);
			cy5_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[11]);
			cy6_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[12]);
			cy7_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[13]);
			cy8_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[14]);
		}
		else if (row==N-1){
			r0=_mm256_setzero_si256();
			r1=_mm256_setzero_si256();
			r2=_mm256_setzero_si256();
			r3=_mm256_setzero_si256();
			r4=_mm256_loadu_si256( (__m256i *) &frame1[N-5][col2]);
			r5=_mm256_loadu_si256( (__m256i *) &frame1[N-4][col2]);
			r6=_mm256_loadu_si256( (__m256i *) &frame1[N-3][col2]);
			r7=_mm256_loadu_si256( (__m256i *) &frame1[N-2][col2]);
			r8=_mm256_loadu_si256( (__m256i *) &frame1[N-1][col2]);

			cy0=_mm256_setzero_si256();
			cy1=_mm256_setzero_si256();
			cy2=_mm256_setzero_si256();
			cy3=_mm256_setzero_si256();
			cy4=_mm256_load_si256( (__m256i *) &mask_vector_y[0]);
			cy5=_mm256_load_si256( (__m256i *) &mask_vector_y[1]);
			cy6=_mm256_load_si256( (__m256i *) &mask_vector_y[2]);
			cy7=_mm256_load_si256( (__m256i *) &mask_vector_y[3]);
			cy8=_mm256_load_si256( (__m256i *) &mask_vector_y[4]);

			cy0_sh1=_mm256_setzero_si256();
			cy1_sh1=_mm256_setzero_si256();
			cy2_sh1=_mm256_setzero_si256();
			cy3_sh1=_mm256_setzero_si256();
			cy4_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[9]);
			cy5_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[10]);
			cy6_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[11]);
			cy7_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[12]);
			cy8_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[13]);
		}
		else {
			printf("\nsomething went wrong");
			exit(EXIT_FAILURE);
		}
	}


	/*
	 * The above load operations load outside of the array bounds; these elements are filled with zeros just after
	 */
	reminder_mask=_mm256_load_si256( (__m256i *) &reminder_mask_9x9[REMINDER_ITERATIONS_Y-1][0]);

	//AND r0-r4 with reminder_mask
	r0=_mm256_and_si256(r0,reminder_mask);
	r1=_mm256_and_si256(r1,reminder_mask);
	r2=_mm256_and_si256(r2,reminder_mask);
	r3=_mm256_and_si256(r3,reminder_mask);
	r4=_mm256_and_si256(r4,reminder_mask);
	r5=_mm256_and_si256(r5,reminder_mask);
	r6=_mm256_and_si256(r6,reminder_mask);
	r7=_mm256_and_si256(r7,reminder_mask);
	r8=_mm256_and_si256(r8,reminder_mask);


//--------------------y filter begins--------------------

//multiply with the mask
	m0=_mm256_maddubs_epi16(r0,cy0);
	m1=_mm256_maddubs_epi16(r1,cy1);
	m2=_mm256_maddubs_epi16(r2,cy2);
	m3=_mm256_maddubs_epi16(r3,cy3);
	m4=_mm256_maddubs_epi16(r4,cy4);
	m5=_mm256_maddubs_epi16(r5,cy5);
	m6=_mm256_maddubs_epi16(r6,cy6);
	m7=_mm256_maddubs_epi16(r7,cy7);
	m8=_mm256_maddubs_epi16(r8,cy8);

	//vertical add
	m0=_mm256_add_epi16(m0,m1);
	m0=_mm256_add_epi16(m0,m2);
	m0=_mm256_add_epi16(m0,m3);
	m0=_mm256_add_epi16(m0,m4);
	m0=_mm256_add_epi16(m0,m5);
	m0=_mm256_add_epi16(m0,m6);
	m0=_mm256_add_epi16(m0,m7);
	m0=_mm256_add_epi16(m0,m8);

	even=division(division_case,m0,f);//even results

	//multiply with the mask
	m0=_mm256_maddubs_epi16(r0,cy0_sh1);
	m1=_mm256_maddubs_epi16(r1,cy1_sh1);
	m2=_mm256_maddubs_epi16(r2,cy2_sh1);
	m3=_mm256_maddubs_epi16(r3,cy3_sh1);
	m4=_mm256_maddubs_epi16(r4,cy4_sh1);
	m5=_mm256_maddubs_epi16(r5,cy5_sh1);
	m6=_mm256_maddubs_epi16(r6,cy6_sh1);
	m7=_mm256_maddubs_epi16(r7,cy7_sh1);
	m8=_mm256_maddubs_epi16(r8,cy8_sh1);

	//vertical add
	m0=_mm256_add_epi16(m0,m1);
	m0=_mm256_add_epi16(m0,m2);
	m0=_mm256_add_epi16(m0,m3);
	m0=_mm256_add_epi16(m0,m4);
	m0=_mm256_add_epi16(m0,m5);
	m0=_mm256_add_epi16(m0,m6);
	m0=_mm256_add_epi16(m0,m7);
	m0=_mm256_add_epi16(m0,m8);

	odd=division(division_case,m0,f);//odd results

	//pack to one register
	odd=_mm256_slli_si256(odd,1); //shift one position right
	r0=_mm256_add_epi8(even,odd); //add the odd with the even


	_mm256_storeu_si256( (__m256i *) &temp[M-REMINDER_ITERATIONS_Y],r0 );

	for (int gg=M;gg<M+32+6;gg++)
	    temp[gg]=0;
//y filter ends - r0 has now the data to be processed by x filter
}


}





void loop_reminder_9x9_16_blur_X(unsigned char **frame1,unsigned char **filt,unsigned char *temp,const unsigned int N,const unsigned int M,const unsigned int row, const unsigned int col,const unsigned int REMINDER_ITERATIONS_X,const unsigned int division_case,const signed char mask_vector_x[][32], const __m256i f, const unsigned short int divisor_xy,signed char *kernel_x){

	const __m256i cx0=_mm256_load_si256( (__m256i *) &mask_vector_x[0]);


	const __m256i mask3  = _mm256_set_epi16(0,0,0,0,0,0,0,65535,0,0,0,0,0,0,0,0);

	const __m256i mask_16_1   = _mm256_set_epi16(0,0,0,0,0,0xffff,0,0,0,0,0xffff,0,0,0,0,0xffff);

	__m256i r0,m0,m1,m2,m3,m4,even,odd,output1;
	const __m256i mask_new_1  = _mm256_set_epi16(0,0,0,0,0,0,0,0,0,0,0xffff,0,0,0,0,0);
	const __m256i mask_16_5  = _mm256_set_epi16(0,0xffff,0,0,0,0,0xffff,0,0,0,0,0xffff,0,0,0,0);

	const __m256i mask3_2  = _mm256_set_epi16(0,0,0,0,0,0,65535,0,0,0,0,0,0,0,0,0);



	//multiply by the mask
       r0=_mm256_loadu_si256( (__m256i *) &temp[col-4]);
		m0=_mm256_maddubs_epi16(r0,cx0);


		//first horizontal add (complex shift is needed)
		m3=_mm256_srli_si256(m0,2);
		m4=_mm256_and_si256(m0,mask3);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,14);
		m3=_mm256_add_epi16(m3,m4);
		m3=_mm256_add_epi16(m0,m3);//add


		//second horizontal add
		m1=_mm256_srli_si256(m3,4);
		m3=_mm256_add_epi16(m3,m1);//add


		//third horizontal add
		m0=_mm256_and_si256(m0,mask_16_5);
		m2=_mm256_srli_si256(m0,8);
		m4=_mm256_and_si256(m0,mask3_2);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,8);
		m2=_mm256_add_epi16(m2,m4);

		m0=_mm256_add_epi16(m2,m3);//add

		even=_mm256_and_si256(m0,mask_16_1);//in 16-bit format keep only 0,5,10 and discard the others


		//2nd col iteration
       r0=_mm256_loadu_si256( (__m256i *) &temp[col-3]);

		//multiply by the mask
		m0=_mm256_maddubs_epi16(r0,cx0);

		//first horizontal add (complex shift is needed)
		m3=_mm256_srli_si256(m0,2);
		m4=_mm256_and_si256(m0,mask3);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,14);
		m3=_mm256_add_epi16(m3,m4);
		m3=_mm256_add_epi16(m0,m3);//add

		//second horizontal add
		m1=_mm256_srli_si256(m3,4);
		m3=_mm256_add_epi16(m3,m1);//add

		//third horizontal add
		m0=_mm256_and_si256(m0,mask_16_5);
		m2=_mm256_srli_si256(m0,8);
		m4=_mm256_and_si256(m0,mask3_2);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,8);
		m2=_mm256_add_epi16(m2,m4);

		m0=_mm256_add_epi16(m2,m3);//add

		odd=_mm256_and_si256(m0,mask_16_1);//in 16-bit format keep only 0,5,10 and discard the others


		//3rd col iteration
       r0=_mm256_loadu_si256( (__m256i *) &temp[col-2]);

		//multiply by the mask
		m0=_mm256_maddubs_epi16(r0,cx0);

		//first horizontal add (complex shift is needed)
		m3=_mm256_srli_si256(m0,2);
		m4=_mm256_and_si256(m0,mask3);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,14);
		m3=_mm256_add_epi16(m3,m4);
		m3=_mm256_add_epi16(m0,m3);//add

		//second horizontal add
		m1=_mm256_srli_si256(m3,4);
		m3=_mm256_add_epi16(m3,m1);//add

		//third horizontal add
		m0=_mm256_and_si256(m0,mask_16_5);
		m2=_mm256_srli_si256(m0,8);
		m4=_mm256_and_si256(m0,mask3_2);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,8);
		m2=_mm256_add_epi16(m2,m4);

		m0=_mm256_add_epi16(m2,m3);//add

		m0=_mm256_and_si256(m0,mask_16_1);//in 16-bit format keep only 0,5,10 and discard the others
		m0=_mm256_slli_si256(m0,2);
		even=_mm256_add_epi16(m0,even);

		//4th col iteration
       r0=_mm256_loadu_si256( (__m256i *) &temp[col-1]);

		//multiply by the mask
		m0=_mm256_maddubs_epi16(r0,cx0);

		//first horizontal add (complex shift is needed)
		m3=_mm256_srli_si256(m0,2);
		m4=_mm256_and_si256(m0,mask3);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,14);
		m3=_mm256_add_epi16(m3,m4);
		m3=_mm256_add_epi16(m0,m3);//add

		//second horizontal add
		m1=_mm256_srli_si256(m3,4);
		m3=_mm256_add_epi16(m3,m1);//add

		//third horizontal add
		m0=_mm256_and_si256(m0,mask_16_5);
		m2=_mm256_srli_si256(m0,8);
		m4=_mm256_and_si256(m0,mask3_2);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,8);
		m2=_mm256_add_epi16(m2,m4);

		m0=_mm256_add_epi16(m2,m3);//add

		m0=_mm256_and_si256(m0,mask_16_1);//in 16-bit format keep only 0,5,10 and discard the others
		m0=_mm256_slli_si256(m0,2);
		odd=_mm256_add_epi16(m0,odd);

		//5th col iteration
       r0=_mm256_loadu_si256( (__m256i *) &temp[col]);

		//multiply by the mask
		m0=_mm256_maddubs_epi16(r0,cx0);

		//first horizontal add (complex shift is needed)
		m3=_mm256_srli_si256(m0,2);
		m4=_mm256_and_si256(m0,mask3);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,14);
		m3=_mm256_add_epi16(m3,m4);
		m3=_mm256_add_epi16(m0,m3);//add

		//second horizontal add
		m1=_mm256_srli_si256(m3,4);
		m3=_mm256_add_epi16(m3,m1);//add

		//third horizontal add
		m0=_mm256_and_si256(m0,mask_16_5);
		m2=_mm256_srli_si256(m0,8);
		m4=_mm256_and_si256(m0,mask3_2);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,8);
		m2=_mm256_add_epi16(m2,m4);

		m0=_mm256_add_epi16(m2,m3);//add

		m0=_mm256_and_si256(m0,mask_16_1);//in 16-bit format keep only 0,5,10 and discard the others
		m0=_mm256_slli_si256(m0,4);
		even=_mm256_add_epi16(m0,even);

		//6th col iteration
       r0=_mm256_loadu_si256( (__m256i *) &temp[col+1]);

		//multiply by the mask
		m0=_mm256_maddubs_epi16(r0,cx0);

		//first horizontal add (complex shift is needed)
		m3=_mm256_srli_si256(m0,2);
		m4=_mm256_and_si256(m0,mask3);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,14);
		m3=_mm256_add_epi16(m3,m4);
		m3=_mm256_add_epi16(m0,m3);//add

		//second horizontal add
		m1=_mm256_srli_si256(m3,4);
		m3=_mm256_add_epi16(m3,m1);//add

		//third horizontal add
		m0=_mm256_and_si256(m0,mask_16_5);
		m2=_mm256_srli_si256(m0,8);
		m4=_mm256_and_si256(m0,mask3_2);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,8);
		m2=_mm256_add_epi16(m2,m4);

		m0=_mm256_add_epi16(m2,m3);//add

		m0=_mm256_and_si256(m0,mask_16_1);//in 16-bit format keep only 0,5,10 and discard the others
		m0=_mm256_slli_si256(m0,4);
		odd=_mm256_add_epi16(m0,odd);

		//7th col iteration
       r0=_mm256_loadu_si256( (__m256i *) &temp[col+2]);

		//multiply by the mask
		m0=_mm256_maddubs_epi16(r0,cx0);

		//first horizontal add (complex shift is needed)
		m3=_mm256_srli_si256(m0,2);
		m4=_mm256_and_si256(m0,mask3);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,14);
		m3=_mm256_add_epi16(m3,m4);
		m3=_mm256_add_epi16(m0,m3);//add

		//second horizontal add
		m1=_mm256_srli_si256(m3,4);
		m3=_mm256_add_epi16(m3,m1);//add

		//third horizontal add
		m0=_mm256_and_si256(m0,mask_16_5);
		m2=_mm256_srli_si256(m0,8);
		m4=_mm256_and_si256(m0,mask3_2);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,8);
		m2=_mm256_add_epi16(m2,m4);

		m0=_mm256_add_epi16(m2,m3);//add

		m0=_mm256_and_si256(m0,mask_16_1);//in 16-bit format keep only 0,5,10 and discard the others

		//shift the result 3 positions
		m1=_mm256_slli_si256(m0,6);
		m2=_mm256_and_si256(m0,mask_new_1);
		m2=_mm256_permute2f128_si256(m2,m2,1);
		m2=_mm256_srli_si256(m2,10);
		m0=_mm256_add_epi16(m2,m1);

		even=_mm256_add_epi16(m0,even);

		//8th col iteration
       r0=_mm256_loadu_si256( (__m256i *) &temp[col+3]);

		//multiply by the mask
		m0=_mm256_maddubs_epi16(r0,cx0);

		//first horizontal add (complex shift is needed)
		m3=_mm256_srli_si256(m0,2);
		m4=_mm256_and_si256(m0,mask3);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,14);
		m3=_mm256_add_epi16(m3,m4);
		m3=_mm256_add_epi16(m0,m3);//add

		//second horizontal add
		m1=_mm256_srli_si256(m3,4);
		m3=_mm256_add_epi16(m3,m1);//add

		//third horizontal add
		m0=_mm256_and_si256(m0,mask_16_5);
		m2=_mm256_srli_si256(m0,8);
		m4=_mm256_and_si256(m0,mask3_2);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,8);
		m2=_mm256_add_epi16(m2,m4);

		m0=_mm256_add_epi16(m2,m3);//add

		m0=_mm256_and_si256(m0,mask_16_1);//in 16-bit format keep only 0,5,10 and discard the others

		//shift the result 3 positions
		m1=_mm256_slli_si256(m0,6);
		m2=_mm256_and_si256(m0,mask_new_1);
		m2=_mm256_permute2f128_si256(m2,m2,1);
		m2=_mm256_srli_si256(m2,10);
		m0=_mm256_add_epi16(m2,m1);

		odd=_mm256_add_epi16(m0,odd);

		//9th col iteration
       r0=_mm256_loadu_si256( (__m256i *) &temp[col+4]);

		//multiply by the mask
		m0=_mm256_maddubs_epi16(r0,cx0);

		//first horizontal add (complex shift is needed)
		m3=_mm256_srli_si256(m0,2);
		m4=_mm256_and_si256(m0,mask3);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,14);
		m3=_mm256_add_epi16(m3,m4);
		m3=_mm256_add_epi16(m0,m3);//add

		//second horizontal add
		m1=_mm256_srli_si256(m3,4);
		m3=_mm256_add_epi16(m3,m1);//add

		//third horizontal add
		m0=_mm256_and_si256(m0,mask_16_5);
		m2=_mm256_srli_si256(m0,8);
		m4=_mm256_and_si256(m0,mask3_2);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,8);
		m2=_mm256_add_epi16(m2,m4);

		m0=_mm256_add_epi16(m2,m3);//add

		m0=_mm256_and_si256(m0,mask_16_1);//in 16-bit format keep only 0,5,10 and discard the others

		//shift the result 4 positions
		m1=_mm256_slli_si256(m0,8);
		m2=_mm256_and_si256(m0,mask_new_1);
		m2=_mm256_permute2f128_si256(m2,m2,1);
		m2=_mm256_srli_si256(m2,8);
		m0=_mm256_add_epi16(m2,m1);

		even=_mm256_add_epi16(m0,even);

		//10th col iteration
       r0=_mm256_loadu_si256( (__m256i *) &temp[col+5]);

		//multiply by the mask
		m0=_mm256_maddubs_epi16(r0,cx0);

		//first horizontal add (complex shift is needed)
		m3=_mm256_srli_si256(m0,2);
		m4=_mm256_and_si256(m0,mask3);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,14);
		m3=_mm256_add_epi16(m3,m4);
		m3=_mm256_add_epi16(m0,m3);//add

		//second horizontal add
		m1=_mm256_srli_si256(m3,4);
		m3=_mm256_add_epi16(m3,m1);//add

		//third horizontal add
		m0=_mm256_and_si256(m0,mask_16_5);
		m2=_mm256_srli_si256(m0,8);
		m4=_mm256_and_si256(m0,mask3_2);
		m4=_mm256_permute2f128_si256(m4,m4,1);
		m4=_mm256_slli_si256(m4,8);
		m2=_mm256_add_epi16(m2,m4);

		m0=_mm256_add_epi16(m2,m3);//add

		m0=_mm256_and_si256(m0,mask_16_1);//in 16-bit format keep only 0,5,10 and discard the others

		//shift the result 4 positions
		m1=_mm256_slli_si256(m0,8);
		m2=_mm256_and_si256(m0,mask_new_1);
		m2=_mm256_permute2f128_si256(m2,m2,1);
		m2=_mm256_srli_si256(m2,8);
		m0=_mm256_add_epi16(m2,m1);

		odd=_mm256_add_epi16(m0,odd);


		//division
		even = division(division_case,even,f);
		odd = division(division_case,odd,f);

		//blend even , odd into one register
		odd = _mm256_slli_si256(odd,1);
		output1=_mm256_add_epi16(even,odd);

		//_mm256_storeu_si256( (__m256i *) &filt[row][col],even );

	switch (REMINDER_ITERATIONS_X){
	case 1:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		  break;
	case 2:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		  break;
	case 3:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		  break;
	case 4:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output1,3);
		  break;
	case 5:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output1,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output1,4);
		  break;
	case 6:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output1,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output1,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output1,5);
		  break;
	case 7:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output1,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output1,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output1,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output1,6);
		  break;
	case 8:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output1,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output1,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output1,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output1,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output1,7);
		  break;
	case 9:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output1,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output1,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output1,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output1,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output1,7);
		filt[row][col+8] = (unsigned char) _mm256_extract_epi8(output1,8);
		  break;
	case 10:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output1,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output1,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output1,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output1,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output1,7);
		filt[row][col+8] = (unsigned char) _mm256_extract_epi8(output1,8);
		filt[row][col+9] = (unsigned char) _mm256_extract_epi8(output1,9);
		  break;
	case 11:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output1,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output1,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output1,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output1,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output1,7);
		filt[row][col+8] = (unsigned char) _mm256_extract_epi8(output1,8);
		filt[row][col+9] = (unsigned char) _mm256_extract_epi8(output1,9);
		filt[row][col+10] = (unsigned char) _mm256_extract_epi8(output1,10);
		  break;
	case 12:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output1,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output1,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output1,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output1,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output1,7);
		filt[row][col+8] = (unsigned char) _mm256_extract_epi8(output1,8);
		filt[row][col+9] = (unsigned char) _mm256_extract_epi8(output1,9);
		filt[row][col+10] = (unsigned char) _mm256_extract_epi8(output1,10);
		filt[row][col+11] = (unsigned char) _mm256_extract_epi8(output1,11);
		  break;
	case 13:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output1,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output1,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output1,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output1,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output1,7);
		filt[row][col+8] = (unsigned char) _mm256_extract_epi8(output1,8);
		filt[row][col+9] = (unsigned char) _mm256_extract_epi8(output1,9);
		filt[row][col+10] = (unsigned char) _mm256_extract_epi8(output1,10);
		filt[row][col+11] = (unsigned char) _mm256_extract_epi8(output1,11);
		filt[row][col+12] = (unsigned char) _mm256_extract_epi8(output1,12);
		  break;
	case 14:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output1,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output1,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output1,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output1,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output1,7);
		filt[row][col+8] = (unsigned char) _mm256_extract_epi8(output1,8);
		filt[row][col+9] = (unsigned char) _mm256_extract_epi8(output1,9);
		filt[row][col+10] = (unsigned char) _mm256_extract_epi8(output1,10);
		filt[row][col+11] = (unsigned char) _mm256_extract_epi8(output1,11);
		filt[row][col+12] = (unsigned char) _mm256_extract_epi8(output1,12);
		filt[row][col+13] = (unsigned char) _mm256_extract_epi8(output1,13);
		  break;
	case 15:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output1,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output1,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output1,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output1,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output1,7);
		filt[row][col+8] = (unsigned char) _mm256_extract_epi8(output1,8);
		filt[row][col+9] = (unsigned char) _mm256_extract_epi8(output1,9);
		filt[row][col+10] = (unsigned char) _mm256_extract_epi8(output1,10);
		filt[row][col+11] = (unsigned char) _mm256_extract_epi8(output1,11);
		filt[row][col+12] = (unsigned char) _mm256_extract_epi8(output1,12);
		filt[row][col+13] = (unsigned char) _mm256_extract_epi8(output1,13);
		filt[row][col+14] = (unsigned char) _mm256_extract_epi8(output1,14);
		  break;
	case 16:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
		  break;
	case 17:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
		  break;
	case 18:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
		  break;
	case 19:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
		  break;
	case 20:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output1,19);
		  break;
	case 21:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output1,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output1,20);
		  break;
	case 22:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output1,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output1,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output1,21);
		break;
	case 23:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output1,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output1,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output1,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output1,22);
		  break;
	case 24:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output1,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output1,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output1,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output1,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output1,23);
		  break;
	case 25:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output1,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output1,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output1,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output1,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output1,23);
	filt[row][col+24] = (unsigned char) _mm256_extract_epi8(output1,24);
		  break;
	case 26:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output1,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output1,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output1,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output1,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output1,23);
	filt[row][col+24] = (unsigned char) _mm256_extract_epi8(output1,24);
	filt[row][col+25] = (unsigned char) _mm256_extract_epi8(output1,25);
		  break;
	case 27:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output1,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output1,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output1,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output1,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output1,23);
	filt[row][col+24] = (unsigned char) _mm256_extract_epi8(output1,24);
	filt[row][col+25] = (unsigned char) _mm256_extract_epi8(output1,25);
	filt[row][col+26] = (unsigned char) _mm256_extract_epi8(output1,26);
		  break;
	case 28:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output1,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output1,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output1,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output1,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output1,23);
	filt[row][col+24] = (unsigned char) _mm256_extract_epi8(output1,24);
	filt[row][col+25] = (unsigned char) _mm256_extract_epi8(output1,25);
	filt[row][col+26] = (unsigned char) _mm256_extract_epi8(output1,26);
	filt[row][col+27] = (unsigned char) _mm256_extract_epi8(output1,27);
		  break;
	case 29:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output1,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output1,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output1,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output1,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output1,23);
	filt[row][col+24] = (unsigned char) _mm256_extract_epi8(output1,24);
	filt[row][col+25] = (unsigned char) _mm256_extract_epi8(output1,25);
	filt[row][col+26] = (unsigned char) _mm256_extract_epi8(output1,26);
	filt[row][col+27] = (unsigned char) _mm256_extract_epi8(output1,27);
	filt[row][col+28] = (unsigned char) _mm256_extract_epi8(output1,28);
		  break;
	case 30:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output1,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output1,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output1,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output1,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output1,23);
	filt[row][col+24] = (unsigned char) _mm256_extract_epi8(output1,24);
	filt[row][col+25] = (unsigned char) _mm256_extract_epi8(output1,25);
	filt[row][col+26] = (unsigned char) _mm256_extract_epi8(output1,26);
	filt[row][col+27] = (unsigned char) _mm256_extract_epi8(output1,27);
	filt[row][col+28] = (unsigned char) _mm256_extract_epi8(output1,28);
	filt[row][col+29] = (unsigned char) _mm256_extract_epi8(output1,29);
		  break;
	case 31:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output1,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output1,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output1,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output1,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output1,23);
	filt[row][col+24] = (unsigned char) _mm256_extract_epi8(output1,24);
	filt[row][col+25] = (unsigned char) _mm256_extract_epi8(output1,25);
	filt[row][col+26] = (unsigned char) _mm256_extract_epi8(output1,26);
	filt[row][col+27] = (unsigned char) _mm256_extract_epi8(output1,27);
	filt[row][col+28] = (unsigned char) _mm256_extract_epi8(output1,28);
	filt[row][col+29] = (unsigned char) _mm256_extract_epi8(output1,29);

	int newPixel = 0;

	newPixel += temp[col+30-4] * kernel_x[0];
	newPixel += temp[col+30-3] * kernel_x[1];
	newPixel += temp[col+30-2] * kernel_x[2];
	newPixel += temp[col+30-1] * kernel_x[3];
	newPixel += temp[col+30-0] * kernel_x[4];

	filt[row][col+30] = (unsigned char) (newPixel / divisor_xy);

		  break;
	default: //REMINDER IS EITHER 27,28,29,30 OR 31
		printf("\nsomething went wrong");
		exit(EXIT_FAILURE);
 	}


}




void Gaussian_Blur_9x9_32_separable(unsigned char **frame1,unsigned char **filt,const unsigned int M, const unsigned int N, signed char *kernel_y, signed char *kernel_x, const unsigned short int divisor_xy){

const signed char fx0=kernel_x[0];
const signed char fx1=kernel_x[1];
const signed char fx2=kernel_x[2];
const signed char fx3=kernel_x[3];
const signed char fx4=kernel_x[4];
const signed char fx5=kernel_x[5];
const signed char fx6=kernel_x[6];
const signed char fx7=kernel_x[7];
const signed char fx8=kernel_x[8];


const signed char fy0=kernel_y[0];
const signed char fy1=kernel_y[1];
const signed char fy2=kernel_y[2];
const signed char fy3=kernel_y[3];
const signed char fy4=kernel_y[4];
const signed char fy5=kernel_y[5];
const signed char fy6=kernel_y[6];
const signed char fy7=kernel_y[7];
const signed char fy8=kernel_y[8];



const signed char mask_vector_x[10][32] __attribute__((aligned(64))) ={
	{fx0,fx1,fx2,fx3,fx4,fx5,fx6,fx7,fx8,0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,fx7,fx8,0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,fx7,fx8,0,0,0}
};

const signed char mask_vector_y[18][32] __attribute__((aligned(64))) ={
	{fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0},
	{fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0},
	{fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0},
	{fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0},
	{fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0},
	{fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0},
	{fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0},
	{fy7,0,fy7,0,fy7,0,fy7,0,fy7,0,fy7,0,fy7,0,fy7,0,fy7,0,fy7,0,fy7,0,fy7,0,fy7,0,fy7,0,fy7,0,fy7,0},
	{fy8,0,fy8,0,fy8,0,fy8,0,fy8,0,fy8,0,fy8,0,fy8,0,fy8,0,fy8,0,fy8,0,fy8,0,fy8,0,fy8,0,fy8,0,fy8,0},

	{0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0},
	{0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1},
	{0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2},
	{0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3},
	{0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4},
	{0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5},
	{0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6},
	{0,fy7,0,fy7,0,fy7,0,fy7,0,fy7,0,fy7,0,fy7,0,fy7,0,fy7,0,fy7,0,fy7,0,fy7,0,fy7,0,fy7,0,fy7,0,fy7},
	{0,fy8,0,fy8,0,fy8,0,fy8,0,fy8,0,fy8,0,fy8,0,fy8,0,fy8,0,fy8,0,fy8,0,fy8,0,fy8,0,fy8,0,fy8,0,fy8},
};

	const __m256i cx0=_mm256_load_si256( (__m256i *) &mask_vector_x[0]);

	const __m256i mask_even_16  = _mm256_set_epi16(0,0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff);
	const __m256i mask_odd_16  = _mm256_set_epi16(0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff,0);
	const __m256i ones=_mm256_set1_epi16(1);


	const __m256i cy0=_mm256_load_si256( (__m256i *) &mask_vector_y[0]);
	const __m256i cy1=_mm256_load_si256( (__m256i *) &mask_vector_y[1]);
	const __m256i cy2=_mm256_load_si256( (__m256i *) &mask_vector_y[2]);
	const __m256i cy3=_mm256_load_si256( (__m256i *) &mask_vector_y[3]);
	const __m256i cy4=_mm256_load_si256( (__m256i *) &mask_vector_y[4]);
	const __m256i cy5=_mm256_load_si256( (__m256i *) &mask_vector_y[5]);
	const __m256i cy6=_mm256_load_si256( (__m256i *) &mask_vector_y[6]);
	const __m256i cy7=_mm256_load_si256( (__m256i *) &mask_vector_y[7]);
	const __m256i cy8=_mm256_load_si256( (__m256i *) &mask_vector_y[8]);


	const __m256i cy0_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[9]);
	const __m256i cy1_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[10]);
	const __m256i cy2_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[11]);
	const __m256i cy3_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[12]);
	const __m256i cy4_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[13]);
	const __m256i cy5_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[14]);
	const __m256i cy6_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[15]);
	const __m256i cy7_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[16]);
	const __m256i cy8_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[17]);


	const __m256i mask_z    = _mm256_set_epi16(0,0,0,0,0,0,0,0,0,0,0,65535,0,0,0,0);
	const __m256i mask_unz    = _mm256_set_epi16(65535,65535,65535,65535,65535,65535,65535,65535,65535,65535,65535,0,65535,65535,65535,65535);
	const __m256i mask_32_1    = _mm256_set_epi32(0xffffffff,0,0,0xffffffff,0,0,0,0);
	const __m256i mask_32_2    = _mm256_set_epi32(0,0,0,0xffffffff,0,0,0,0);
	const __m256i mask_32_a    = _mm256_set_epi32(0,0,0xffffffff,0,0,0xffffffff,0,0xffffffff);
	const __m256i out_mask_1    = _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255);//0,20
	const __m256i out_mask_2    = _mm256_set_epi8(0,0,0,0,0,0,0,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255,0,0,0,0);//4,24
	const __m256i out_mask_3    = _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, 0,0,0,0,0,0,0,255,0,0,0,0,0,0,0,0);//8
	const __m256i out_mask_4    = _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, 0,0,0,255,0,0,0,0,0,0,0,0,0,0,0,0);//12
	const __m256i out_mask_5    = _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, 0,0,0,0,255,255,0,0,0,0,0,0,0,0,0,0);//10,11


	//const __m256i mask3    = _mm256_set_epi16(0,0,0,0,0,0,0,65535,0,0,0,0,0,0,0,0);
	//const __m256i mask3_2  = _mm256_set_epi16(0,0,0,0,0,0,65535,0,0,0,0,0,0,0,0,0);
	//const __m256i mask3_3  = _mm256_set_epi16(0,0,0,0,0,65535,0,0,0,0,0,0,0,0,0,0);
	//const __m256i mask3_4  = _mm256_set_epi16(0,0,0,0,65535,0,0,0,0,0,0,0,0,0,0,0);

	const __m256i mask_prelude  = _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0);
	const __m256i mask_prelude_3  = _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0);
	const __m256i mask_prelude_2  = _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0);
	const __m256i mask_prelude_1  = _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0);

//	const __m256i mask_even_16  = _mm256_set_epi16(0,0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff);
//	const __m256i mask_odd_16  = _mm256_set_epi16(0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff,0);
	//const __m256i mask_16_1   = _mm256_set_epi16(0,0,0,0,0,0xffff,0,0,0,0,0xffff,0,0,0,0,0xffff);


	//const __m256i mask_new_1  = _mm256_set_epi16(0,0,0,0,0,0,0,0,0,0,0xffff,0,0,0,0,0);

	//const __m256i mask_16_5  = _mm256_set_epi16(0,0xffff,0,0,0,0,0xffff,0,0,0,0,0xffff,0,0,0,0);



	const unsigned int REMINDER_ITERATIONS_X_mask =  M-((((M-32)/30)*30)+30); //M-(last_col_value+30)
	const unsigned int REMINDER_ITERATIONS_Y_mask =  M-((((M-32)/32)*32)+32);

	//printf("\nREMINDER_ITERATIONS=%u,%u",REMINDER_ITERATIONS_X_mask,REMINDER_ITERATIONS_Y_mask);


	const unsigned int division_case=prepare_for_division_32(divisor_xy); //determine which is the division case (A, B or C)
	const __m256i f = _mm256_load_si256( (__m256i *) &f_vector[0]);// initialize the division vector
	//const __m256i ones=_mm256_set1_epi16(1);

//	const unsigned int division_case_x=prepare_for_division_32(divisor_x); //determine which is the division case (A, B or C)
//	const __m256i f_x = _mm256_load_si256( (__m256i *) &f_vector_x[0]);// initialize the division vector
//printf("\n%d",division_case);


#pragma omp parallel
{

unsigned int row,col;
__m256i r0,r1,r2,r3,r4,r5,r6,r7,r8,m0,m1,m2,m3,m4,m5,m6,m7,m8,even,odd,out1,out2,output1,output2,output3,output4,output5;
unsigned char temp[M+32+6] __attribute__((aligned(64)));//temporal storage of the output of y filter.


/*---------------------- Gaussian Blur ---------------------------------*/

    #pragma omp for schedule(static)
	for (row = 3; row < N-3; row++) {

		if (row<4){
				prelude_9x9_32_Ymask_0(0, M, frame1, temp,mask_vector_y, division_case, f, mask_prelude);
				loop_reminder_9x9_32_blur_Y(frame1,filt,temp,N,M,0, REMINDER_ITERATIONS_Y_mask, division_case, mask_vector_y, f, divisor_xy);
				prelude_9x9_32_Xmask(frame1,filt,temp,N,M,0, mask_vector_x,  division_case,f,REMINDER_ITERATIONS_X_mask,REMINDER_ITERATIONS_Y_mask,mask_vector_y,divisor_xy,kernel_x);

				prelude_9x9_32_Ymask_1(0, M, frame1, temp,mask_vector_y, division_case, f, mask_prelude);
				loop_reminder_9x9_32_blur_Y(frame1,filt,temp,N,M,1, REMINDER_ITERATIONS_Y_mask, division_case, mask_vector_y, f, divisor_xy);
				prelude_9x9_32_Xmask(frame1,filt,temp,N,M,1, mask_vector_x,  division_case,f,REMINDER_ITERATIONS_X_mask,REMINDER_ITERATIONS_Y_mask,mask_vector_y,divisor_xy,kernel_x);

				prelude_9x9_32_Ymask_2(0, M, frame1, temp,mask_vector_y, division_case, f, mask_prelude);
				loop_reminder_9x9_32_blur_Y(frame1,filt,temp,N,M,2, REMINDER_ITERATIONS_Y_mask, division_case, mask_vector_y, f, divisor_xy);
				prelude_9x9_32_Xmask(frame1,filt,temp,N,M,2, mask_vector_x,  division_case,f,REMINDER_ITERATIONS_X_mask,REMINDER_ITERATIONS_Y_mask,mask_vector_y,divisor_xy,kernel_x);

				prelude_9x9_32_Ymask_3(0, M, frame1, temp,mask_vector_y, division_case, f, mask_prelude);
				loop_reminder_9x9_32_blur_Y(frame1,filt,temp,N,M,3, REMINDER_ITERATIONS_Y_mask, division_case, mask_vector_y, f, divisor_xy);
				prelude_9x9_32_Xmask(frame1,filt,temp,N,M,3, mask_vector_x,  division_case,f,REMINDER_ITERATIONS_X_mask,REMINDER_ITERATIONS_Y_mask,mask_vector_y,divisor_xy,kernel_x);

		}

		else if (row>N-5){

			prelude_9x9_32_Ymask_0(N-5, M, frame1, temp,mask_vector_y, division_case, f, mask_prelude);
			loop_reminder_9x9_32_blur_Y(frame1,filt,temp,N,M,N-1, REMINDER_ITERATIONS_Y_mask, division_case, mask_vector_y, f, divisor_xy);
			prelude_9x9_32_Xmask(frame1,filt,temp,N,M,N-1, mask_vector_x,  division_case,f,REMINDER_ITERATIONS_X_mask,REMINDER_ITERATIONS_Y_mask,mask_vector_y,divisor_xy,kernel_x);

			prelude_9x9_32_Ymask_1(N-6, M, frame1, temp,mask_vector_y, division_case, f, mask_prelude);
			loop_reminder_9x9_32_blur_Y(frame1,filt,temp,N,M,N-2, REMINDER_ITERATIONS_Y_mask, division_case, mask_vector_y, f, divisor_xy);
			prelude_9x9_32_Xmask(frame1,filt,temp,N,M,N-2, mask_vector_x,  division_case,f,REMINDER_ITERATIONS_X_mask,REMINDER_ITERATIONS_Y_mask,mask_vector_y,divisor_xy,kernel_x);

			prelude_9x9_32_Ymask_2(N-7, M, frame1, temp,mask_vector_y, division_case, f, mask_prelude);
			loop_reminder_9x9_32_blur_Y(frame1,filt,temp,N,M,N-3, REMINDER_ITERATIONS_Y_mask, division_case, mask_vector_y, f, divisor_xy);
			prelude_9x9_32_Xmask(frame1,filt,temp,N,M,N-3, mask_vector_x,  division_case,f,REMINDER_ITERATIONS_X_mask,REMINDER_ITERATIONS_Y_mask,mask_vector_y,divisor_xy,kernel_x);

			prelude_9x9_32_Ymask_3(N-8, M, frame1, temp,mask_vector_y, division_case, f, mask_prelude);
			loop_reminder_9x9_32_blur_Y(frame1,filt,temp,N,M,N-4, REMINDER_ITERATIONS_Y_mask, division_case, mask_vector_y, f, divisor_xy);
			prelude_9x9_32_Xmask(frame1,filt,temp,N,M,N-4, mask_vector_x,  division_case,f,REMINDER_ITERATIONS_X_mask,REMINDER_ITERATIONS_Y_mask,mask_vector_y,divisor_xy,kernel_x);

		}

	  else { //main loop


		  for (col = 0; col <= M-32; col+=32){

					//load the 9 rows
					r0=_mm256_load_si256( (__m256i *) &frame1[row-4][col]);
					r1=_mm256_load_si256( (__m256i *) &frame1[row-3][col]);
					r2=_mm256_load_si256( (__m256i *) &frame1[row-2][col]);
					r3=_mm256_load_si256( (__m256i *) &frame1[row-1][col]);
					r4=_mm256_load_si256( (__m256i *) &frame1[row][col]);
					r5=_mm256_load_si256( (__m256i *) &frame1[row+1][col]);
					r6=_mm256_load_si256( (__m256i *) &frame1[row+2][col]);
					r7=_mm256_load_si256( (__m256i *) &frame1[row+3][col]);
					r8=_mm256_load_si256( (__m256i *) &frame1[row+4][col]);

					  //--------------------y filter begins--------------------

						//multiply by the mask
						m0=_mm256_maddubs_epi16(r0,cy0);
						m1=_mm256_maddubs_epi16(r1,cy1);
						m2=_mm256_maddubs_epi16(r2,cy2);
						m3=_mm256_maddubs_epi16(r3,cy3);
						m4=_mm256_maddubs_epi16(r4,cy4);
						m5=_mm256_maddubs_epi16(r5,cy5);
						m6=_mm256_maddubs_epi16(r6,cy6);
						m7=_mm256_maddubs_epi16(r7,cy7);
						m8=_mm256_maddubs_epi16(r8,cy8);

						//discard odd
				__m256i m0_even=_mm256_and_si256(m0,mask_even_16);
				__m256i	m1_even=_mm256_and_si256(m1,mask_even_16);
				__m256i	m2_even=_mm256_and_si256(m2,mask_even_16);
				__m256i	m3_even=_mm256_and_si256(m3,mask_even_16);
				__m256i	m4_even=_mm256_and_si256(m4,mask_even_16);
				__m256i	m5_even=_mm256_and_si256(m5,mask_even_16);
				__m256i	m6_even=_mm256_and_si256(m6,mask_even_16);
				__m256i	m7_even=_mm256_and_si256(m7,mask_even_16);
				__m256i	m8_even=_mm256_and_si256(m8,mask_even_16);


				//make from 16-bit to 32bit
				  m0_even=_mm256_madd_epi16(m0_even,ones);
				  m1_even=_mm256_madd_epi16(m1_even,ones);
				  m2_even=_mm256_madd_epi16(m2_even,ones);
				  m3_even=_mm256_madd_epi16(m3_even,ones);
				  m4_even=_mm256_madd_epi16(m4_even,ones);
				  m5_even=_mm256_madd_epi16(m5_even,ones);
				  m6_even=_mm256_madd_epi16(m6_even,ones);
				  m7_even=_mm256_madd_epi16(m7_even,ones);
				  m8_even=_mm256_madd_epi16(m8_even,ones);

						//vertical 32-bit add
						m0_even=_mm256_add_epi32(m0_even,m1_even);
						m0_even=_mm256_add_epi32(m0_even,m2_even);
						m0_even=_mm256_add_epi32(m0_even,m3_even);
						m0_even=_mm256_add_epi32(m0_even,m4_even);
						m0_even=_mm256_add_epi32(m0_even,m5_even);
						m0_even=_mm256_add_epi32(m0_even,m6_even);
						m0_even=_mm256_add_epi32(m0_even,m7_even);
						m0_even=_mm256_add_epi32(m0_even,m8_even);


						//discard even
				__m256i	m0_odd=_mm256_and_si256(m0,mask_odd_16);
				__m256i	m1_odd=_mm256_and_si256(m1,mask_odd_16);
				__m256i	m2_odd=_mm256_and_si256(m2,mask_odd_16);
				__m256i	m3_odd=_mm256_and_si256(m3,mask_odd_16);
				__m256i	m4_odd=_mm256_and_si256(m4,mask_odd_16);
				__m256i	m5_odd=_mm256_and_si256(m5,mask_odd_16);
				__m256i	m6_odd=_mm256_and_si256(m6,mask_odd_16);
				__m256i	m7_odd=_mm256_and_si256(m7,mask_odd_16);
				__m256i	m8_odd=_mm256_and_si256(m8,mask_odd_16);


				//make from 16-bit to 32bit
						m0_odd=_mm256_madd_epi16(m0_odd,ones);
						m1_odd=_mm256_madd_epi16(m1_odd,ones);
						m2_odd=_mm256_madd_epi16(m2_odd,ones);
						m3_odd=_mm256_madd_epi16(m3_odd,ones);
						m4_odd=_mm256_madd_epi16(m4_odd,ones);
						m5_odd=_mm256_madd_epi16(m5_odd,ones);
						m6_odd=_mm256_madd_epi16(m6_odd,ones);
						m7_odd=_mm256_madd_epi16(m7_odd,ones);
						m8_odd=_mm256_madd_epi16(m8_odd,ones);

						//vertical 32-bit add
						m0_odd=_mm256_add_epi32(m0_odd,m1_odd);
						m0_odd=_mm256_add_epi32(m0_odd,m2_odd);
						m0_odd=_mm256_add_epi32(m0_odd,m3_odd);
						m0_odd=_mm256_add_epi32(m0_odd,m4_odd);
						m0_odd=_mm256_add_epi32(m0_odd,m5_odd);
						m0_odd=_mm256_add_epi32(m0_odd,m6_odd);
						m0_odd=_mm256_add_epi32(m0_odd,m7_odd);
						m0_odd=_mm256_add_epi32(m0_odd,m8_odd);

						m0_even=division_32(division_case,m0_even,f);//even results
						m0_odd=division_32(division_case,m0_odd,f);//even results

						m0_odd=_mm256_slli_si256(m0_odd,2); //shift two position right
						even=_mm256_add_epi8(m0_even,m0_odd); //add the odd with the even

						//-----------------multiply by the second mask----------
						m0=_mm256_maddubs_epi16(r0,cy0_sh1);
						m1=_mm256_maddubs_epi16(r1,cy1_sh1);
						m2=_mm256_maddubs_epi16(r2,cy2_sh1);
						m3=_mm256_maddubs_epi16(r3,cy3_sh1);
						m4=_mm256_maddubs_epi16(r4,cy4_sh1);
						m5=_mm256_maddubs_epi16(r5,cy5_sh1);
						m6=_mm256_maddubs_epi16(r6,cy6_sh1);
						m7=_mm256_maddubs_epi16(r7,cy7_sh1);
						m8=_mm256_maddubs_epi16(r8,cy8_sh1);


						m0_even=_mm256_and_si256(m0,mask_even_16);
						m1_even=_mm256_and_si256(m1,mask_even_16);
						m2_even=_mm256_and_si256(m2,mask_even_16);
						m3_even=_mm256_and_si256(m3,mask_even_16);
						m4_even=_mm256_and_si256(m4,mask_even_16);
						m5_even=_mm256_and_si256(m5,mask_even_16);
						m6_even=_mm256_and_si256(m6,mask_even_16);
						m7_even=_mm256_and_si256(m7,mask_even_16);
						m8_even=_mm256_and_si256(m8,mask_even_16);

						//make from 16-bit to 32bit
						  m0_even=_mm256_madd_epi16(m0_even,ones);
						  m1_even=_mm256_madd_epi16(m1_even,ones);
						  m2_even=_mm256_madd_epi16(m2_even,ones);
						  m3_even=_mm256_madd_epi16(m3_even,ones);
						  m4_even=_mm256_madd_epi16(m4_even,ones);
						  m5_even=_mm256_madd_epi16(m5_even,ones);
						  m6_even=_mm256_madd_epi16(m6_even,ones);
						  m7_even=_mm256_madd_epi16(m7_even,ones);
						  m8_even=_mm256_madd_epi16(m8_even,ones);


						//vertical 32-bit add
						m0_even=_mm256_add_epi32(m0_even,m1_even);
						m0_even=_mm256_add_epi32(m0_even,m2_even);
						m0_even=_mm256_add_epi32(m0_even,m3_even);
						m0_even=_mm256_add_epi32(m0_even,m4_even);
						m0_even=_mm256_add_epi32(m0_even,m5_even);
						m0_even=_mm256_add_epi32(m0_even,m6_even);
						m0_even=_mm256_add_epi32(m0_even,m7_even);
						m0_even=_mm256_add_epi32(m0_even,m8_even);


						 m0_odd=_mm256_and_si256(m0,mask_odd_16);
						 m1_odd=_mm256_and_si256(m1,mask_odd_16);
						 m2_odd=_mm256_and_si256(m2,mask_odd_16);
						 m3_odd=_mm256_and_si256(m3,mask_odd_16);
						 m4_odd=_mm256_and_si256(m4,mask_odd_16);
						 m5_odd=_mm256_and_si256(m5,mask_odd_16);
						 m6_odd=_mm256_and_si256(m6,mask_odd_16);
						 m7_odd=_mm256_and_si256(m7,mask_odd_16);
						 m8_odd=_mm256_and_si256(m8,mask_odd_16);

						 //make from 16-bit to 32bit
							m0_odd=_mm256_madd_epi16(m0_odd,ones);
							m1_odd=_mm256_madd_epi16(m1_odd,ones);
							m2_odd=_mm256_madd_epi16(m2_odd,ones);
							m3_odd=_mm256_madd_epi16(m3_odd,ones);
							m4_odd=_mm256_madd_epi16(m4_odd,ones);
							m5_odd=_mm256_madd_epi16(m5_odd,ones);
							m6_odd=_mm256_madd_epi16(m6_odd,ones);
							m7_odd=_mm256_madd_epi16(m7_odd,ones);
							m8_odd=_mm256_madd_epi16(m8_odd,ones);


						//vertical 32-bit add
						m0_odd=_mm256_add_epi32(m0_odd,m1_odd);
						m0_odd=_mm256_add_epi32(m0_odd,m2_odd);
						m0_odd=_mm256_add_epi32(m0_odd,m3_odd);
						m0_odd=_mm256_add_epi32(m0_odd,m4_odd);
						m0_odd=_mm256_add_epi32(m0_odd,m5_odd);
						m0_odd=_mm256_add_epi32(m0_odd,m6_odd);
						m0_odd=_mm256_add_epi32(m0_odd,m7_odd);
						m0_odd=_mm256_add_epi32(m0_odd,m8_odd);

						m0_even=division_32(division_case,m0_even,f);//even results
						m0_odd=division_32(division_case,m0_odd,f);//even results

						m0_odd=_mm256_slli_si256(m0_odd,2); //shift two position right
						odd=_mm256_add_epi8(m0_even,m0_odd); //add the odd with the even


						//MERGE AND pack to one register
						odd=_mm256_slli_si256(odd,1); //shift one position right
						r0=_mm256_add_epi8(even,odd); //add the odd with the even

						//y filter ends - r0 has now the data to be processed by x filter

			  			_mm256_store_si256( (__m256i *) &temp[col],r0 );
		  }
	loop_reminder_9x9_32_blur_Y(frame1,filt,temp,N,M,row, REMINDER_ITERATIONS_Y_mask, division_case, mask_vector_y, f, divisor_xy);


	  for (col = 0; col <= M-32; col+=30){

		  if (col==0){

	  			//1st col iteration
		__m256i	rr0=_mm256_load_si256( (__m256i *) &temp[0]);
		  		r0=fill_zeros(rr0,mask_prelude);

	  			//multiply by the mask
	  			m0=_mm256_maddubs_epi16(r0,cx0);

		  			//zero the 4th out of 16 elements
		  			m1=_mm256_and_si256(m0,mask_unz);
		  			//keep a copy of the 4th element
		  			m8=_mm256_and_si256(m0,mask_z);
					//m8=_mm256_slli_si256(m8,2);

		  			//make 16-bit 32-bit
					m1=_mm256_madd_epi16(m1,ones);

					//horizontal add
					m2=_mm256_srli_si256(m1,4);
		  			m2=_mm256_add_epi32(m1,m2);

		  			//keep only 4th and 7th out of 8 elements
		  			m1=_mm256_and_si256(m1,mask_32_1);
		  			m1=_mm256_add_epi32(m1,m8);

		  			//shift m1 and add with m2 (complex shift is needed)
		  			m3=_mm256_srli_si256(m1,8);
		  			m4=_mm256_and_si256(m1,mask_32_2);
		  			m4=_mm256_permute2f128_si256(m4,m4,1);
		  			m4=_mm256_slli_si256(m4,8);
		  			m3=_mm256_add_epi32(m3,m4);
		  			m2=_mm256_add_epi32(m2,m3);

		  			out1=_mm256_and_si256(m2,mask_32_a);


	  			//2nd col iteration
			//	r0=_mm256_load_si256( (__m256i *) &temp[0]);
		  		r0=fill_3zeros(rr0,mask_prelude_3);

	  			//multiply by the mask
		  			m0=_mm256_maddubs_epi16(r0,cx0);

		  			//zero the 4th out of 16 elements
		  			m1=_mm256_and_si256(m0,mask_unz);
		  			//keep a copy of the 4th element
		  			m8=_mm256_and_si256(m0,mask_z);
					//m8=_mm256_slli_si256(m8,2);

		  			//make 16-bit 32-bit
					m1=_mm256_madd_epi16(m1,ones);

					//horizontal add
					m2=_mm256_srli_si256(m1,4);
		  			m2=_mm256_add_epi32(m1,m2);

		  			//keep only 4th and 7th out of 8 elements
		  			m1=_mm256_and_si256(m1,mask_32_1);
		  			m1=_mm256_add_epi32(m1,m8);

		  			//shift m1 and add with m2 (complex shift is needed)
		  			m3=_mm256_srli_si256(m1,8);
		  			m4=_mm256_and_si256(m1,mask_32_2);
		  			m4=_mm256_permute2f128_si256(m4,m4,1);
		  			m4=_mm256_slli_si256(m4,8);
		  			m3=_mm256_add_epi32(m3,m4);
		  			m2=_mm256_add_epi32(m2,m3);

		  			out2=_mm256_and_si256(m2,mask_32_a);//in 32-bit format keep only 0,2,5 and discard the others

		  			//merge out1 and out2 and then divide
		  			out2=_mm256_slli_si256(out2,4);
		  			out1=_mm256_add_epi32(out1,out2);
		  			out1 = division_32(division_case,out1,f);

		  			//elements must be re-arranged before store
		  			m0=_mm256_and_si256(out1,out_mask_1);
		  			m1=_mm256_and_si256(out1,out_mask_2);
		  			m1=_mm256_srli_si256(m1,3);
		  			m2=_mm256_and_si256(out1,out_mask_3);
		  			m2=_mm256_slli_si256(m2,2);
		  			m3=_mm256_and_si256(out1,out_mask_4);
		  			m3=_mm256_srli_si256(m3,1);
		  			m0=_mm256_add_epi8(m0,m1);
		  			m0=_mm256_add_epi8(m0,m2);
		  			output1=_mm256_add_epi8(m0,m3);

	  			//3rd col iteration
				//r0=_mm256_load_si256( (__m256i *) &temp[0]);
		  		r0=fill_2zeros(rr0,mask_prelude_2);

	  			//multiply by the mask
	  			m0=_mm256_maddubs_epi16(r0,cx0);

		  			//zero the 4th out of 16 elements
		  			m1=_mm256_and_si256(m0,mask_unz);
		  			//keep a copy of the 4th element
		  			m8=_mm256_and_si256(m0,mask_z);
					//m8=_mm256_slli_si256(m8,2);

		  			//make 16-bit 32-bit
					m1=_mm256_madd_epi16(m1,ones);

					//horizontal add
					m2=_mm256_srli_si256(m1,4);
		  			m2=_mm256_add_epi32(m1,m2);

		  			//keep only 4th and 7th out of 8 elements
		  			m1=_mm256_and_si256(m1,mask_32_1);
		  			m1=_mm256_add_epi32(m1,m8);

		  			//shift m1 and add with m2 (complex shift is needed)
		  			m3=_mm256_srli_si256(m1,8);
		  			m4=_mm256_and_si256(m1,mask_32_2);
		  			m4=_mm256_permute2f128_si256(m4,m4,1);
		  			m4=_mm256_slli_si256(m4,8);
		  			m3=_mm256_add_epi32(m3,m4);
		  			m2=_mm256_add_epi32(m2,m3);

		  			out1=_mm256_and_si256(m2,mask_32_a);



	  			//4th col iteration
			//	r0=_mm256_load_si256( (__m256i *) &temp[0]);
		  		r0=fill_1zeros(rr0,mask_prelude_1);

	  			//multiply by the mask
			  			m0=_mm256_maddubs_epi16(r0,cx0);

			  			//zero the 4th out of 16 elements
			  			m1=_mm256_and_si256(m0,mask_unz);
			  			//keep a copy of the 4th element
			  			m8=_mm256_and_si256(m0,mask_z);
						//m8=_mm256_slli_si256(m8,2);

			  			//make 16-bit 32-bit
						m1=_mm256_madd_epi16(m1,ones);

						//horizontal add
						m2=_mm256_srli_si256(m1,4);
			  			m2=_mm256_add_epi32(m1,m2);

			  			//keep only 4th and 7th out of 8 elements
			  			m1=_mm256_and_si256(m1,mask_32_1);
			  			m1=_mm256_add_epi32(m1,m8);

			  			//shift m1 and add with m2 (complex shift is needed)
			  			m3=_mm256_srli_si256(m1,8);
			  			m4=_mm256_and_si256(m1,mask_32_2);
			  			m4=_mm256_permute2f128_si256(m4,m4,1);
			  			m4=_mm256_slli_si256(m4,8);
			  			m3=_mm256_add_epi32(m3,m4);
			  			m2=_mm256_add_epi32(m2,m3);

			  			out2=_mm256_and_si256(m2,mask_32_a);//in 32-bit format keep only 0,2,5 and discard the others

			  			//merge out1 and out2 and then divide
			  			out2=_mm256_slli_si256(out2,4);
			  			out1=_mm256_add_epi32(out1,out2);
			  			out1 = division_32(division_case,out1,f);

			  			//elements must be re-arranged before store
			  			m0=_mm256_and_si256(out1,out_mask_1);
			  			m1=_mm256_and_si256(out1,out_mask_2);
			  			m1=_mm256_srli_si256(m1,3);
			  			m2=_mm256_and_si256(out1,out_mask_3);
			  			m2=_mm256_slli_si256(m2,2);
			  			m3=_mm256_and_si256(out1,out_mask_4);
			  			m3=_mm256_srli_si256(m3,1);
			  			m0=_mm256_add_epi8(m0,m1);
			  			m0=_mm256_add_epi8(m0,m2);
			  			output2=_mm256_add_epi8(m0,m3);


	  			//5th col iteration
				//r0=_mm256_load_si256( (__m256i *) &temp[0]);

	  			//multiply by the mask
		  			m0=_mm256_maddubs_epi16(rr0,cx0);

			  			//zero the 4th out of 16 elements
			  			m1=_mm256_and_si256(m0,mask_unz);
			  			//keep a copy of the 4th element
			  			m8=_mm256_and_si256(m0,mask_z);
						//m8=_mm256_slli_si256(m8,2);

			  			//make 16-bit 32-bit
						m1=_mm256_madd_epi16(m1,ones);

						//horizontal add
						m2=_mm256_srli_si256(m1,4);
			  			m2=_mm256_add_epi32(m1,m2);

			  			//keep only 4th and 7th out of 8 elements
			  			m1=_mm256_and_si256(m1,mask_32_1);
			  			m1=_mm256_add_epi32(m1,m8);

			  			//shift m1 and add with m2 (complex shift is needed)
			  			m3=_mm256_srli_si256(m1,8);
			  			m4=_mm256_and_si256(m1,mask_32_2);
			  			m4=_mm256_permute2f128_si256(m4,m4,1);
			  			m4=_mm256_slli_si256(m4,8);
			  			m3=_mm256_add_epi32(m3,m4);
			  			m2=_mm256_add_epi32(m2,m3);

			  			out1=_mm256_and_si256(m2,mask_32_a);

	  			//6th col iteration
				r0=_mm256_loadu_si256( (__m256i *) &temp[1]);

	  			//multiply by the mask
			  			m0=_mm256_maddubs_epi16(r0,cx0);

			  			//zero the 4th out of 16 elements
			  			m1=_mm256_and_si256(m0,mask_unz);
			  			//keep a copy of the 4th element
			  			m8=_mm256_and_si256(m0,mask_z);
						//m8=_mm256_slli_si256(m8,2);

			  			//make 16-bit 32-bit
						m1=_mm256_madd_epi16(m1,ones);

						//horizontal add
						m2=_mm256_srli_si256(m1,4);
			  			m2=_mm256_add_epi32(m1,m2);

			  			//keep only 4th and 7th out of 8 elements
			  			m1=_mm256_and_si256(m1,mask_32_1);
			  			m1=_mm256_add_epi32(m1,m8);

			  			//shift m1 and add with m2 (complex shift is needed)
			  			m3=_mm256_srli_si256(m1,8);
			  			m4=_mm256_and_si256(m1,mask_32_2);
			  			m4=_mm256_permute2f128_si256(m4,m4,1);
			  			m4=_mm256_slli_si256(m4,8);
			  			m3=_mm256_add_epi32(m3,m4);
			  			m2=_mm256_add_epi32(m2,m3);

			  			out2=_mm256_and_si256(m2,mask_32_a);//in 32-bit format keep only 0,2,5 and discard the others

			  			//merge out1 and out2 and then divide
			  			out2=_mm256_slli_si256(out2,4);
			  			out1=_mm256_add_epi32(out1,out2);
			  			out1 = division_32(division_case,out1,f);

			  			//elements must be re-arranged before store
			  			m0=_mm256_and_si256(out1,out_mask_1);
			  			m1=_mm256_and_si256(out1,out_mask_2);
			  			m1=_mm256_srli_si256(m1,3);
			  			m2=_mm256_and_si256(out1,out_mask_3);
			  			m2=_mm256_slli_si256(m2,2);
			  			m3=_mm256_and_si256(out1,out_mask_4);
			  			m3=_mm256_srli_si256(m3,1);
			  			m0=_mm256_add_epi8(m0,m1);
			  			m0=_mm256_add_epi8(m0,m2);
			  			output3=_mm256_add_epi8(m0,m3);


	  			//7th col iteration
				r0=_mm256_loadu_si256( (__m256i *) &temp[2]);

	  			//multiply by the mask
		  			m0=_mm256_maddubs_epi16(r0,cx0);

			  			//zero the 4th out of 16 elements
			  			m1=_mm256_and_si256(m0,mask_unz);
			  			//keep a copy of the 4th element
			  			m8=_mm256_and_si256(m0,mask_z);
						//m8=_mm256_slli_si256(m8,2);

			  			//make 16-bit 32-bit
						m1=_mm256_madd_epi16(m1,ones);

						//horizontal add
						m2=_mm256_srli_si256(m1,4);
			  			m2=_mm256_add_epi32(m1,m2);

			  			//keep only 4th and 7th out of 8 elements
			  			m1=_mm256_and_si256(m1,mask_32_1);
			  			m1=_mm256_add_epi32(m1,m8);

			  			//shift m1 and add with m2 (complex shift is needed)
			  			m3=_mm256_srli_si256(m1,8);
			  			m4=_mm256_and_si256(m1,mask_32_2);
			  			m4=_mm256_permute2f128_si256(m4,m4,1);
			  			m4=_mm256_slli_si256(m4,8);
			  			m3=_mm256_add_epi32(m3,m4);
			  			m2=_mm256_add_epi32(m2,m3);

			  			out1=_mm256_and_si256(m2,mask_32_a);

	  			//8th col iteration
				r0=_mm256_loadu_si256( (__m256i *) &temp[3]);

	  			//multiply by the mask
			  			m0=_mm256_maddubs_epi16(r0,cx0);

			  			//zero the 4th out of 16 elements
			  			m1=_mm256_and_si256(m0,mask_unz);
			  			//keep a copy of the 4th element
			  			m8=_mm256_and_si256(m0,mask_z);
						//m8=_mm256_slli_si256(m8,2);

			  			//make 16-bit 32-bit
						m1=_mm256_madd_epi16(m1,ones);

						//horizontal add
						m2=_mm256_srli_si256(m1,4);
			  			m2=_mm256_add_epi32(m1,m2);

			  			//keep only 4th and 7th out of 8 elements
			  			m1=_mm256_and_si256(m1,mask_32_1);
			  			m1=_mm256_add_epi32(m1,m8);

			  			//shift m1 and add with m2 (complex shift is needed)
			  			m3=_mm256_srli_si256(m1,8);
			  			m4=_mm256_and_si256(m1,mask_32_2);
			  			m4=_mm256_permute2f128_si256(m4,m4,1);
			  			m4=_mm256_slli_si256(m4,8);
			  			m3=_mm256_add_epi32(m3,m4);
			  			m2=_mm256_add_epi32(m2,m3);

			  			out2=_mm256_and_si256(m2,mask_32_a);//in 32-bit format keep only 0,2,5 and discard the others

			  			//merge out1 and out2 and then divide
			  			out2=_mm256_slli_si256(out2,4);
			  			out1=_mm256_add_epi32(out1,out2);
			  			out1 = division_32(division_case,out1,f);

			  			//elements must be re-arranged before store
			  			m0=_mm256_and_si256(out1,out_mask_1);
			  			m1=_mm256_and_si256(out1,out_mask_2);
			  			m1=_mm256_srli_si256(m1,3);
			  			m2=_mm256_and_si256(out1,out_mask_3);
			  			m2=_mm256_slli_si256(m2,2);
			  			m3=_mm256_and_si256(out1,out_mask_4);
			  			m3=_mm256_srli_si256(m3,1);
			  			m0=_mm256_add_epi8(m0,m1);
			  			m0=_mm256_add_epi8(m0,m2);
			  			output4=_mm256_add_epi8(m0,m3);


	  			//9th col iteration
				r0=_mm256_loadu_si256( (__m256i *) &temp[4]);

	  			//multiply by the mask
		  			m0=_mm256_maddubs_epi16(r0,cx0);

			  			//zero the 4th out of 16 elements
			  			m1=_mm256_and_si256(m0,mask_unz);
			  			//keep a copy of the 4th element
			  			m8=_mm256_and_si256(m0,mask_z);
						//m8=_mm256_slli_si256(m8,2);

			  			//make 16-bit 32-bit
						m1=_mm256_madd_epi16(m1,ones);

						//horizontal add
						m2=_mm256_srli_si256(m1,4);
			  			m2=_mm256_add_epi32(m1,m2);

			  			//keep only 4th and 7th out of 8 elements
			  			m1=_mm256_and_si256(m1,mask_32_1);
			  			m1=_mm256_add_epi32(m1,m8);

			  			//shift m1 and add with m2 (complex shift is needed)
			  			m3=_mm256_srli_si256(m1,8);
			  			m4=_mm256_and_si256(m1,mask_32_2);
			  			m4=_mm256_permute2f128_si256(m4,m4,1);
			  			m4=_mm256_slli_si256(m4,8);
			  			m3=_mm256_add_epi32(m3,m4);
			  			m2=_mm256_add_epi32(m2,m3);

			  			out1=_mm256_and_si256(m2,mask_32_a);

	  			//10th col iteration
				r0=_mm256_loadu_si256( (__m256i *) &temp[5]);

	  			//multiply by the mask
			  			m0=_mm256_maddubs_epi16(r0,cx0);

			  			//zero the 4th out of 16 elements
			  			m1=_mm256_and_si256(m0,mask_unz);
			  			//keep a copy of the 4th element
			  			m8=_mm256_and_si256(m0,mask_z);
						//m8=_mm256_slli_si256(m8,2);

			  			//make 16-bit 32-bit
						m1=_mm256_madd_epi16(m1,ones);

						//horizontal add
						m2=_mm256_srli_si256(m1,4);
			  			m2=_mm256_add_epi32(m1,m2);

			  			//keep only 4th and 7th out of 8 elements
			  			m1=_mm256_and_si256(m1,mask_32_1);
			  			m1=_mm256_add_epi32(m1,m8);

			  			//shift m1 and add with m2 (complex shift is needed)
			  			m3=_mm256_srli_si256(m1,8);
			  			m4=_mm256_and_si256(m1,mask_32_2);
			  			m4=_mm256_permute2f128_si256(m4,m4,1);
			  			m4=_mm256_slli_si256(m4,8);
			  			m3=_mm256_add_epi32(m3,m4);
			  			m2=_mm256_add_epi32(m2,m3);

			  			out2=_mm256_and_si256(m2,mask_32_a);//in 32-bit format keep only 0,2,5 and discard the others

			  			//merge out1 and out2 and then divide
			  			out2=_mm256_slli_si256(out2,4);
			  			out1=_mm256_add_epi32(out1,out2);
			  			out1 = division_32(division_case,out1,f);

			  			//elements must be re-arranged before store
			  			m0=_mm256_and_si256(out1,out_mask_1);
			  			m1=_mm256_and_si256(out1,out_mask_2);
			  			m1=_mm256_srli_si256(m1,3);
			  			m2=_mm256_and_si256(out1,out_mask_3);
			  			m2=_mm256_slli_si256(m2,2);
			  			m3=_mm256_and_si256(out1,out_mask_4);
			  			m3=_mm256_srli_si256(m3,1);
			  			m0=_mm256_add_epi8(m0,m1);
			  			m0=_mm256_add_epi8(m0,m2);
			  			output5=_mm256_add_epi8(m0,m3);

			  			//blend the output1:output5
			  			output2=_mm256_slli_si256(output2,2);
			  			output3=_mm256_slli_si256(output3,4);

			  			m3=_mm256_slli_si256(output4,6);
			  			m4=_mm256_and_si256(output4,out_mask_5);
			  			m4=_mm256_permute2f128_si256(m4,m4,1);
			  			m4=_mm256_srli_si256(m4,10);
			  			output4=_mm256_add_epi32(m3,m4);

			  			m3=_mm256_slli_si256(output5,8);
			  			m4=_mm256_and_si256(output5,out_mask_5);
			  			m4=_mm256_permute2f128_si256(m4,m4,1);
			  			m4=_mm256_srli_si256(m4,8);
			  			output5=_mm256_add_epi32(m3,m4);

			  			output1=_mm256_add_epi8(output1,output2);
			  			output1=_mm256_add_epi8(output1,output3);
			  			output1=_mm256_add_epi8(output1,output4);
			  			output1=_mm256_add_epi8(output1,output5);

	  			_mm256_storeu_si256( (__m256i *) &filt[row][0],output1 );

		  		  	}
		  else {

		  		// col iteration computes output pixels of   0,10,20
		  		// col+1 iteration computes output pixels of 1,11,21
		  		// col+2 iteration computes output pixels of 2,12,22
		  		// col+3 iteration computes output pixels of 3,13,23
		  		// col+4 iteration computes output pixels of 4,14,24
		  		// col+5 iteration computes output pixels of 5,15,25
		  		// col+6 iteration computes output pixels of 6,16,26
		  		// col+7 iteration computes output pixels of 7,17,27
		  		// col+8 iteration computes output pixels of 8,18,28
		  		// col+9 iteration computes output pixels of 9,19,29
		  		//afterwards, col becomes 30 and repeat the above process

			  //1st col iteration
		  			//multiply by the mask
	  		        r0=_mm256_loadu_si256( (__m256i *) &temp[col-4]);
		  			m0=_mm256_maddubs_epi16(r0,cx0);

		  			//zero the 4th out of 16 elements
		  			m1=_mm256_and_si256(m0,mask_unz);
		  			//keep a copy of the 4th element
		  			m8=_mm256_and_si256(m0,mask_z);
					//m8=_mm256_slli_si256(m8,2);

		  			//make 16-bit 32-bit
					m1=_mm256_madd_epi16(m1,ones);

					//horizontal add
					m2=_mm256_srli_si256(m1,4);
		  			m2=_mm256_add_epi32(m1,m2);

		  			//keep only 4th and 7th out of 8 elements
		  			m1=_mm256_and_si256(m1,mask_32_1);
		  			m1=_mm256_add_epi32(m1,m8);

		  			//shift m1 and add with m2 (complex shift is needed)
		  			m3=_mm256_srli_si256(m1,8);
		  			m4=_mm256_and_si256(m1,mask_32_2);
		  			m4=_mm256_permute2f128_si256(m4,m4,1);
		  			m4=_mm256_slli_si256(m4,8);
		  			m3=_mm256_add_epi32(m3,m4);
		  			m2=_mm256_add_epi32(m2,m3);
		  			out1=_mm256_and_si256(m2,mask_32_a);//in 32-bit format keep only 0,2,5 and discard the others


		   			//2nd col iteration
	  		        r0=_mm256_loadu_si256( (__m256i *) &temp[col-3]);

		  			//multiply by the mask
		  			m0=_mm256_maddubs_epi16(r0,cx0);

		  			//zero the 4th out of 16 elements
		  			m1=_mm256_and_si256(m0,mask_unz);
		  			//keep a copy of the 4th element
		  			m8=_mm256_and_si256(m0,mask_z);
					//m8=_mm256_slli_si256(m8,2);

		  			//make 16-bit 32-bit
					m1=_mm256_madd_epi16(m1,ones);

					//horizontal add
					m2=_mm256_srli_si256(m1,4);
		  			m2=_mm256_add_epi32(m1,m2);

		  			//keep only 4th and 7th out of 8 elements
		  			m1=_mm256_and_si256(m1,mask_32_1);
		  			m1=_mm256_add_epi32(m1,m8);

		  			//shift m1 and add with m2 (complex shift is needed)
		  			m3=_mm256_srli_si256(m1,8);
		  			m4=_mm256_and_si256(m1,mask_32_2);
		  			m4=_mm256_permute2f128_si256(m4,m4,1);
		  			m4=_mm256_slli_si256(m4,8);
		  			m3=_mm256_add_epi32(m3,m4);
		  			m2=_mm256_add_epi32(m2,m3);

		  			out2=_mm256_and_si256(m2,mask_32_a);//in 32-bit format keep only 0,2,5 and discard the others

		  			//merge out1 and out2 and then divide
		  			out2=_mm256_slli_si256(out2,4);
		  			out1=_mm256_add_epi32(out1,out2);
		  			out1 = division_32(division_case,out1,f);

		  			//elements must be re-arranged before store
		  			m0=_mm256_and_si256(out1,out_mask_1);
		  			m1=_mm256_and_si256(out1,out_mask_2);
		  			m1=_mm256_srli_si256(m1,3);
		  			m2=_mm256_and_si256(out1,out_mask_3);
		  			m2=_mm256_slli_si256(m2,2);
		  			m3=_mm256_and_si256(out1,out_mask_4);
		  			m3=_mm256_srli_si256(m3,1);
		  			m0=_mm256_add_epi8(m0,m1);
		  			m0=_mm256_add_epi8(m0,m2);
		  			output1=_mm256_add_epi8(m0,m3);

		  			//3rd col iteration
	  		        r0=_mm256_loadu_si256( (__m256i *) &temp[col-2]);

		  			//multiply by the mask
		  			m0=_mm256_maddubs_epi16(r0,cx0);

		  			//zero the 4th out of 16 elements
		  			m1=_mm256_and_si256(m0,mask_unz);
		  			//keep a copy of the 4th element
		  			m8=_mm256_and_si256(m0,mask_z);
					//m8=_mm256_slli_si256(m8,2);

		  			//make 16-bit 32-bit
					m1=_mm256_madd_epi16(m1,ones);

					//horizontal add
					m2=_mm256_srli_si256(m1,4);
		  			m2=_mm256_add_epi32(m1,m2);

		  			//keep only 4th and 7th out of 8 elements
		  			m1=_mm256_and_si256(m1,mask_32_1);
		  			m1=_mm256_add_epi32(m1,m8);

		  			//shift m1 and add with m2 (complex shift is needed)
		  			m3=_mm256_srli_si256(m1,8);
		  			m4=_mm256_and_si256(m1,mask_32_2);
		  			m4=_mm256_permute2f128_si256(m4,m4,1);
		  			m4=_mm256_slli_si256(m4,8);
		  			m3=_mm256_add_epi32(m3,m4);
		  			m2=_mm256_add_epi32(m2,m3);

		  			out1=_mm256_and_si256(m2,mask_32_a);//in 32-bit format keep only 0,2,5 and discard the others

		  			//4th col iteration
	  		        r0=_mm256_loadu_si256( (__m256i *) &temp[col-1]);

		  			//multiply by the mask
		  			m0=_mm256_maddubs_epi16(r0,cx0);

		  			//zero the 4th out of 16 elements
		  			m1=_mm256_and_si256(m0,mask_unz);
		  			//keep a copy of the 4th element
		  			m8=_mm256_and_si256(m0,mask_z);
					//m8=_mm256_slli_si256(m8,2);

		  			//make 16-bit 32-bit
					m1=_mm256_madd_epi16(m1,ones);

					//horizontal add
					m2=_mm256_srli_si256(m1,4);
		  			m2=_mm256_add_epi32(m1,m2);

		  			//keep only 4th and 7th out of 8 elements
		  			m1=_mm256_and_si256(m1,mask_32_1);
		  			m1=_mm256_add_epi32(m1,m8);

		  			//shift m1 and add with m2 (complex shift is needed)
		  			m3=_mm256_srli_si256(m1,8);
		  			m4=_mm256_and_si256(m1,mask_32_2);
		  			m4=_mm256_permute2f128_si256(m4,m4,1);
		  			m4=_mm256_slli_si256(m4,8);
		  			m3=_mm256_add_epi32(m3,m4);
		  			m2=_mm256_add_epi32(m2,m3);

		  			out2=_mm256_and_si256(m2,mask_32_a);//in 32-bit format keep only 0,2,5 and discard the others

		  			//merge out1 and out2 and then divide
		  			out2=_mm256_slli_si256(out2,4);
		  			out1=_mm256_add_epi32(out1,out2);
		  			out1 = division_32(division_case,out1,f);

		  			//elements must be re-arranged before store
		  			m0=_mm256_and_si256(out1,out_mask_1);
		  			m1=_mm256_and_si256(out1,out_mask_2);
		  			m1=_mm256_srli_si256(m1,3);
		  			m2=_mm256_and_si256(out1,out_mask_3);
		  			m2=_mm256_slli_si256(m2,2);
		  			m3=_mm256_and_si256(out1,out_mask_4);
		  			m3=_mm256_srli_si256(m3,1);
		  			m0=_mm256_add_epi8(m0,m1);
		  			m0=_mm256_add_epi8(m0,m2);
		  			output2=_mm256_add_epi8(m0,m3);

		  			//5th col iteration
	  		        r0=_mm256_loadu_si256( (__m256i *) &temp[col]);

		  			//multiply by the mask
		  			m0=_mm256_maddubs_epi16(r0,cx0);

		  			//zero the 4th out of 16 elements
		  			m1=_mm256_and_si256(m0,mask_unz);
		  			//keep a copy of the 4th element
		  			m8=_mm256_and_si256(m0,mask_z);
					//m8=_mm256_slli_si256(m8,2);

		  			//make 16-bit 32-bit
					m1=_mm256_madd_epi16(m1,ones);

					//horizontal add
					m2=_mm256_srli_si256(m1,4);
		  			m2=_mm256_add_epi32(m1,m2);

		  			//keep only 4th and 7th out of 8 elements
		  			m1=_mm256_and_si256(m1,mask_32_1);
		  			m1=_mm256_add_epi32(m1,m8);

		  			//shift m1 and add with m2 (complex shift is needed)
		  			m3=_mm256_srli_si256(m1,8);
		  			m4=_mm256_and_si256(m1,mask_32_2);
		  			m4=_mm256_permute2f128_si256(m4,m4,1);
		  			m4=_mm256_slli_si256(m4,8);
		  			m3=_mm256_add_epi32(m3,m4);
		  			m2=_mm256_add_epi32(m2,m3);

		  			out1=_mm256_and_si256(m2,mask_32_a);//in 32-bit format keep only 0,2,5 and discard the others

		  			//6th col iteration
	  		        r0=_mm256_loadu_si256( (__m256i *) &temp[col+1]);

		  			//multiply by the mask
		  			m0=_mm256_maddubs_epi16(r0,cx0);

		  			//zero the 4th out of 16 elements
		  			m1=_mm256_and_si256(m0,mask_unz);
		  			//keep a copy of the 4th element
		  			m8=_mm256_and_si256(m0,mask_z);
					//m8=_mm256_slli_si256(m8,2);

		  			//make 16-bit 32-bit
					m1=_mm256_madd_epi16(m1,ones);

					//horizontal add
					m2=_mm256_srli_si256(m1,4);
		  			m2=_mm256_add_epi32(m1,m2);

		  			//keep only 4th and 7th out of 8 elements
		  			m1=_mm256_and_si256(m1,mask_32_1);
		  			m1=_mm256_add_epi32(m1,m8);

		  			//shift m1 and add with m2 (complex shift is needed)
		  			m3=_mm256_srli_si256(m1,8);
		  			m4=_mm256_and_si256(m1,mask_32_2);
		  			m4=_mm256_permute2f128_si256(m4,m4,1);
		  			m4=_mm256_slli_si256(m4,8);
		  			m3=_mm256_add_epi32(m3,m4);
		  			m2=_mm256_add_epi32(m2,m3);

		  			out2=_mm256_and_si256(m2,mask_32_a);//in 32-bit format keep only 0,2,5 and discard the others

		  			//merge out1 and out2 and then divide
		  			out2=_mm256_slli_si256(out2,4);
		  			out1=_mm256_add_epi32(out1,out2);
		  			out1 = division_32(division_case,out1,f);

		  			//elements must be re-arranged before store
		  			m0=_mm256_and_si256(out1,out_mask_1);
		  			m1=_mm256_and_si256(out1,out_mask_2);
		  			m1=_mm256_srli_si256(m1,3);
		  			m2=_mm256_and_si256(out1,out_mask_3);
		  			m2=_mm256_slli_si256(m2,2);
		  			m3=_mm256_and_si256(out1,out_mask_4);
		  			m3=_mm256_srli_si256(m3,1);
		  			m0=_mm256_add_epi8(m0,m1);
		  			m0=_mm256_add_epi8(m0,m2);
		  			output3=_mm256_add_epi8(m0,m3);

		  			//7th col iteration
	  		        r0=_mm256_loadu_si256( (__m256i *) &temp[col+2]);

		  			//multiply by the mask
		  			m0=_mm256_maddubs_epi16(r0,cx0);

		  			//zero the 4th out of 16 elements
		  			m1=_mm256_and_si256(m0,mask_unz);
		  			//keep a copy of the 4th element
		  			m8=_mm256_and_si256(m0,mask_z);
					//m8=_mm256_slli_si256(m8,2);

		  			//make 16-bit 32-bit
					m1=_mm256_madd_epi16(m1,ones);

					//horizontal add
					m2=_mm256_srli_si256(m1,4);
		  			m2=_mm256_add_epi32(m1,m2);

		  			//keep only 4th and 7th out of 8 elements
		  			m1=_mm256_and_si256(m1,mask_32_1);
		  			m1=_mm256_add_epi32(m1,m8);

		  			//shift m1 and add with m2 (complex shift is needed)
		  			m3=_mm256_srli_si256(m1,8);
		  			m4=_mm256_and_si256(m1,mask_32_2);
		  			m4=_mm256_permute2f128_si256(m4,m4,1);
		  			m4=_mm256_slli_si256(m4,8);
		  			m3=_mm256_add_epi32(m3,m4);
		  			m2=_mm256_add_epi32(m2,m3);

		  			out1=_mm256_and_si256(m2,mask_32_a);//in 32-bit format keep only 0,2,5 and discard the others

		  			//8th col iteration
	  		        r0=_mm256_loadu_si256( (__m256i *) &temp[col+3]);

		  			//multiply by the mask
		  			m0=_mm256_maddubs_epi16(r0,cx0);

		  			//zero the 4th out of 16 elements
		  			m1=_mm256_and_si256(m0,mask_unz);
		  			//keep a copy of the 4th element
		  			m8=_mm256_and_si256(m0,mask_z);
					//m8=_mm256_slli_si256(m8,2);

		  			//make 16-bit 32-bit
					m1=_mm256_madd_epi16(m1,ones);

					//horizontal add
					m2=_mm256_srli_si256(m1,4);
		  			m2=_mm256_add_epi32(m1,m2);

		  			//keep only 4th and 7th out of 8 elements
		  			m1=_mm256_and_si256(m1,mask_32_1);
		  			m1=_mm256_add_epi32(m1,m8);

		  			//shift m1 and add with m2 (complex shift is needed)
		  			m3=_mm256_srli_si256(m1,8);
		  			m4=_mm256_and_si256(m1,mask_32_2);
		  			m4=_mm256_permute2f128_si256(m4,m4,1);
		  			m4=_mm256_slli_si256(m4,8);
		  			m3=_mm256_add_epi32(m3,m4);
		  			m2=_mm256_add_epi32(m2,m3);

		  			out2=_mm256_and_si256(m2,mask_32_a);//in 32-bit format keep only 0,2,5 and discard the others

		  			//merge out1 and out2 and then divide
		  			out2=_mm256_slli_si256(out2,4);
		  			out1=_mm256_add_epi32(out1,out2);
		  			out1 = division_32(division_case,out1,f);

		  			//elements must be re-arranged before store
		  			m0=_mm256_and_si256(out1,out_mask_1);
		  			m1=_mm256_and_si256(out1,out_mask_2);
		  			m1=_mm256_srli_si256(m1,3);
		  			m2=_mm256_and_si256(out1,out_mask_3);
		  			m2=_mm256_slli_si256(m2,2);
		  			m3=_mm256_and_si256(out1,out_mask_4);
		  			m3=_mm256_srli_si256(m3,1);
		  			m0=_mm256_add_epi8(m0,m1);
		  			m0=_mm256_add_epi8(m0,m2);
		  			output4=_mm256_add_epi8(m0,m3);

		  			//9th col iteration
	  		        r0=_mm256_loadu_si256( (__m256i *) &temp[col+4]);

		  			//multiply by the mask
		  			m0=_mm256_maddubs_epi16(r0,cx0);

		  			//zero the 4th out of 16 elements
		  			m1=_mm256_and_si256(m0,mask_unz);
		  			//keep a copy of the 4th element
		  			m8=_mm256_and_si256(m0,mask_z);
					//m8=_mm256_slli_si256(m8,2);

		  			//make 16-bit 32-bit
					m1=_mm256_madd_epi16(m1,ones);

					//horizontal add
					m2=_mm256_srli_si256(m1,4);
		  			m2=_mm256_add_epi32(m1,m2);

		  			//keep only 4th and 7th out of 8 elements
		  			m1=_mm256_and_si256(m1,mask_32_1);
		  			m1=_mm256_add_epi32(m1,m8);

		  			//shift m1 and add with m2 (complex shift is needed)
		  			m3=_mm256_srli_si256(m1,8);
		  			m4=_mm256_and_si256(m1,mask_32_2);
		  			m4=_mm256_permute2f128_si256(m4,m4,1);
		  			m4=_mm256_slli_si256(m4,8);
		  			m3=_mm256_add_epi32(m3,m4);
		  			m2=_mm256_add_epi32(m2,m3);

		  			out1=_mm256_and_si256(m2,mask_32_a);//in 32-bit format keep only 0,2,5 and discard the others

		  			//10th col iteration
	  		        r0=_mm256_loadu_si256( (__m256i *) &temp[col+5]);

		  			//multiply by the mask
		  			m0=_mm256_maddubs_epi16(r0,cx0);

		  			//zero the 4th out of 16 elements
		  			m1=_mm256_and_si256(m0,mask_unz);
		  			//keep a copy of the 4th element
		  			m8=_mm256_and_si256(m0,mask_z);
					//m8=_mm256_slli_si256(m8,2);

		  			//make 16-bit 32-bit
					m1=_mm256_madd_epi16(m1,ones);

					//horizontal add
					m2=_mm256_srli_si256(m1,4);
		  			m2=_mm256_add_epi32(m1,m2);

		  			//keep only 4th and 7th out of 8 elements
		  			m1=_mm256_and_si256(m1,mask_32_1);
		  			m1=_mm256_add_epi32(m1,m8);

		  			//shift m1 and add with m2 (complex shift is needed)
		  			m3=_mm256_srli_si256(m1,8);
		  			m4=_mm256_and_si256(m1,mask_32_2);
		  			m4=_mm256_permute2f128_si256(m4,m4,1);
		  			m4=_mm256_slli_si256(m4,8);
		  			m3=_mm256_add_epi32(m3,m4);
		  			m2=_mm256_add_epi32(m2,m3);

		  			out2=_mm256_and_si256(m2,mask_32_a);//in 32-bit format keep only 0,2,5 and discard the others

		  			//merge out1 and out2 and then divide
		  			out2=_mm256_slli_si256(out2,4);
		  			out1=_mm256_add_epi32(out1,out2);
		  			out1 = division_32(division_case,out1,f);

		  			//elements must be re-arranged before store
		  			m0=_mm256_and_si256(out1,out_mask_1);
		  			m1=_mm256_and_si256(out1,out_mask_2);
		  			m1=_mm256_srli_si256(m1,3);
		  			m2=_mm256_and_si256(out1,out_mask_3);
		  			m2=_mm256_slli_si256(m2,2);
		  			m3=_mm256_and_si256(out1,out_mask_4);
		  			m3=_mm256_srli_si256(m3,1);
		  			m0=_mm256_add_epi8(m0,m1);
		  			m0=_mm256_add_epi8(m0,m2);
		  			output5=_mm256_add_epi8(m0,m3);

		  			//blend the output1:output5
		  			output2=_mm256_slli_si256(output2,2);
		  			output3=_mm256_slli_si256(output3,4);

		  			m3=_mm256_slli_si256(output4,6);
		  			m4=_mm256_and_si256(output4,out_mask_5);
		  			m4=_mm256_permute2f128_si256(m4,m4,1);
		  			m4=_mm256_srli_si256(m4,10);
		  			output4=_mm256_add_epi32(m3,m4);

		  			m3=_mm256_slli_si256(output5,8);
		  			m4=_mm256_and_si256(output5,out_mask_5);
		  			m4=_mm256_permute2f128_si256(m4,m4,1);
		  			m4=_mm256_srli_si256(m4,8);
		  			output5=_mm256_add_epi32(m3,m4);

		  			output1=_mm256_add_epi8(output1,output2);
		  			output1=_mm256_add_epi8(output1,output3);
		  			output1=_mm256_add_epi8(output1,output4);
		  			output1=_mm256_add_epi8(output1,output5);

		  			_mm256_storeu_si256( (__m256i *) &filt[row][col],output1 );

		  		 }

		}

		loop_reminder_9x9_32_blur_X(frame1,filt,temp,N,M,row, col, REMINDER_ITERATIONS_X_mask, division_case, mask_vector_x, f, divisor_xy,kernel_x);

		}
}

}//end of parallel


}



//this function is computing the row=3 and row=N-4 only
//for row=3 input prelude_7x7_Ymask_3(0,...
//for row=N-4 input prelude_7x7_Ymask_3(N-8,...
void prelude_9x9_32_Ymask_3(const int row, const unsigned int M, unsigned char **frame1, unsigned char *temp, const signed char mask_vector_y[][32],  const unsigned int division_case, const __m256i f, const __m256i mask_prelude){

	__m256i r0,r1,r2,r3,r4,r5,r6,r7,m0,m1,m2,m3,m4,m5,m6,m7,even,odd;
	//__m256i ones=_mm256_set1_epi16(1);
	 __m256i cy0,cy1,cy2,cy3,cy4,cy5,cy6,cy7,cy0_sh1,cy1_sh1,cy2_sh1,cy3_sh1,cy4_sh1,cy5_sh1,cy6_sh1,cy7_sh1;
	const __m256i mask_even_16  = _mm256_set_epi16(0,0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff);
	const __m256i mask_odd_16  = _mm256_set_epi16(0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff,0);
	const __m256i ones=_mm256_set1_epi16(1);

	if (row!=0){ //if row=N-6
	  cy0=_mm256_load_si256( (__m256i *) &mask_vector_y[0]);
	  cy1=_mm256_load_si256( (__m256i *) &mask_vector_y[1]);
	  cy2=_mm256_load_si256( (__m256i *) &mask_vector_y[2]);
	  cy3=_mm256_load_si256( (__m256i *) &mask_vector_y[3]);
	  cy4=_mm256_load_si256( (__m256i *) &mask_vector_y[4]);
	  cy5=_mm256_load_si256( (__m256i *) &mask_vector_y[5]);
	  cy6=_mm256_load_si256( (__m256i *) &mask_vector_y[6]);
	  cy7=_mm256_load_si256( (__m256i *) &mask_vector_y[7]);

	  cy0_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[9]);
	  cy1_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[10]);
	  cy2_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[11]);
	  cy3_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[12]);
	  cy4_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[13]);
	  cy5_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[14]);
	  cy6_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[15]);
	  cy7_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[16]);

	}
	else {
		  cy0=_mm256_load_si256( (__m256i *) &mask_vector_y[1]);
		  cy1=_mm256_load_si256( (__m256i *) &mask_vector_y[2]);
		  cy2=_mm256_load_si256( (__m256i *) &mask_vector_y[3]);
		  cy3=_mm256_load_si256( (__m256i *) &mask_vector_y[4]);
		  cy4=_mm256_load_si256( (__m256i *) &mask_vector_y[5]);
		  cy5=_mm256_load_si256( (__m256i *) &mask_vector_y[6]);
		  cy6=_mm256_load_si256( (__m256i *) &mask_vector_y[7]);
		  cy7=_mm256_load_si256( (__m256i *) &mask_vector_y[8]);

		  cy0_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[10]);
		  cy1_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[11]);
		  cy2_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[12]);
		  cy3_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[13]);
		  cy4_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[14]);
		  cy5_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[15]);
		  cy6_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[16]);
		  cy7_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[17]);
	}

	 for (unsigned int col = 0; col <= M-32; col+=32){

	  //--------------------y filter begins--------------------
	//load the 9 rows
	r0=_mm256_load_si256( (__m256i *) &frame1[row][col]);
	r1=_mm256_load_si256( (__m256i *) &frame1[row+1][col]);
	r2=_mm256_load_si256( (__m256i *) &frame1[row+2][col]);
	r3=_mm256_load_si256( (__m256i *) &frame1[row+3][col]);
	r4=_mm256_load_si256( (__m256i *) &frame1[row+4][col]);
	r5=_mm256_load_si256( (__m256i *) &frame1[row+5][col]);
	r6=_mm256_load_si256( (__m256i *) &frame1[row+6][col]);
	r7=_mm256_load_si256( (__m256i *) &frame1[row+7][col]);


	//multiply by the mask
	m0=_mm256_maddubs_epi16(r0,cy0);
	m1=_mm256_maddubs_epi16(r1,cy1);
	m2=_mm256_maddubs_epi16(r2,cy2);
	m3=_mm256_maddubs_epi16(r3,cy3);
	m4=_mm256_maddubs_epi16(r4,cy4);
	m5=_mm256_maddubs_epi16(r5,cy5);
	m6=_mm256_maddubs_epi16(r6,cy6);
	m7=_mm256_maddubs_epi16(r7,cy7);

	//discard odd
__m256i m0_even=_mm256_and_si256(m0,mask_even_16);
__m256i	m1_even=_mm256_and_si256(m1,mask_even_16);
__m256i	m2_even=_mm256_and_si256(m2,mask_even_16);
__m256i	m3_even=_mm256_and_si256(m3,mask_even_16);
__m256i	m4_even=_mm256_and_si256(m4,mask_even_16);
__m256i	m5_even=_mm256_and_si256(m5,mask_even_16);
__m256i	m6_even=_mm256_and_si256(m6,mask_even_16);
__m256i	m7_even=_mm256_and_si256(m7,mask_even_16);


//make from 16-bit to 32bit
m0_even=_mm256_madd_epi16(m0_even,ones);
m1_even=_mm256_madd_epi16(m1_even,ones);
m2_even=_mm256_madd_epi16(m2_even,ones);
m3_even=_mm256_madd_epi16(m3_even,ones);
m4_even=_mm256_madd_epi16(m4_even,ones);
m5_even=_mm256_madd_epi16(m5_even,ones);
m6_even=_mm256_madd_epi16(m6_even,ones);
m7_even=_mm256_madd_epi16(m7_even,ones);

	//vertical 32-bit add
	m0_even=_mm256_add_epi32(m0_even,m1_even);
	m0_even=_mm256_add_epi32(m0_even,m2_even);
	m0_even=_mm256_add_epi32(m0_even,m3_even);
	m0_even=_mm256_add_epi32(m0_even,m4_even);
	m0_even=_mm256_add_epi32(m0_even,m5_even);
	m0_even=_mm256_add_epi32(m0_even,m6_even);
	m0_even=_mm256_add_epi32(m0_even,m7_even);


	//discard even
__m256i	m0_odd=_mm256_and_si256(m0,mask_odd_16);
__m256i	m1_odd=_mm256_and_si256(m1,mask_odd_16);
__m256i	m2_odd=_mm256_and_si256(m2,mask_odd_16);
__m256i	m3_odd=_mm256_and_si256(m3,mask_odd_16);
__m256i	m4_odd=_mm256_and_si256(m4,mask_odd_16);
__m256i	m5_odd=_mm256_and_si256(m5,mask_odd_16);
__m256i	m6_odd=_mm256_and_si256(m6,mask_odd_16);
__m256i	m7_odd=_mm256_and_si256(m7,mask_odd_16);


//make from 16-bit to 32bit
	m0_odd=_mm256_madd_epi16(m0_odd,ones);
	m1_odd=_mm256_madd_epi16(m1_odd,ones);
	m2_odd=_mm256_madd_epi16(m2_odd,ones);
	m3_odd=_mm256_madd_epi16(m3_odd,ones);
	m4_odd=_mm256_madd_epi16(m4_odd,ones);
	m5_odd=_mm256_madd_epi16(m5_odd,ones);
	m6_odd=_mm256_madd_epi16(m6_odd,ones);
	m7_odd=_mm256_madd_epi16(m7_odd,ones);

	//vertical 32-bit add
	m0_odd=_mm256_add_epi32(m0_odd,m1_odd);
	m0_odd=_mm256_add_epi32(m0_odd,m2_odd);
	m0_odd=_mm256_add_epi32(m0_odd,m3_odd);
	m0_odd=_mm256_add_epi32(m0_odd,m4_odd);
	m0_odd=_mm256_add_epi32(m0_odd,m5_odd);
	m0_odd=_mm256_add_epi32(m0_odd,m6_odd);
	m0_odd=_mm256_add_epi32(m0_odd,m7_odd);

	m0_even=division_32(division_case,m0_even,f);//even results
	m0_odd=division_32(division_case,m0_odd,f);//even results

	m0_odd=_mm256_slli_si256(m0_odd,2); //shift two position right
	even=_mm256_add_epi8(m0_even,m0_odd); //add the odd with the even

	//-----------------multiply by the second mask----------
	m0=_mm256_maddubs_epi16(r0,cy0_sh1);
	m1=_mm256_maddubs_epi16(r1,cy1_sh1);
	m2=_mm256_maddubs_epi16(r2,cy2_sh1);
	m3=_mm256_maddubs_epi16(r3,cy3_sh1);
	m4=_mm256_maddubs_epi16(r4,cy4_sh1);
	m5=_mm256_maddubs_epi16(r5,cy5_sh1);
	m6=_mm256_maddubs_epi16(r6,cy6_sh1);
	m7=_mm256_maddubs_epi16(r7,cy7_sh1);


	m0_even=_mm256_and_si256(m0,mask_even_16);
	m1_even=_mm256_and_si256(m1,mask_even_16);
	m2_even=_mm256_and_si256(m2,mask_even_16);
	m3_even=_mm256_and_si256(m3,mask_even_16);
	m4_even=_mm256_and_si256(m4,mask_even_16);
	m5_even=_mm256_and_si256(m5,mask_even_16);
	m6_even=_mm256_and_si256(m6,mask_even_16);
	m7_even=_mm256_and_si256(m7,mask_even_16);

	//make from 16-bit to 32bit
	  m0_even=_mm256_madd_epi16(m0_even,ones);
	  m1_even=_mm256_madd_epi16(m1_even,ones);
	  m2_even=_mm256_madd_epi16(m2_even,ones);
	  m3_even=_mm256_madd_epi16(m3_even,ones);
	  m4_even=_mm256_madd_epi16(m4_even,ones);
	  m5_even=_mm256_madd_epi16(m5_even,ones);
	  m6_even=_mm256_madd_epi16(m6_even,ones);
	  m7_even=_mm256_madd_epi16(m7_even,ones);


	//vertical 32-bit add
	m0_even=_mm256_add_epi32(m0_even,m1_even);
	m0_even=_mm256_add_epi32(m0_even,m2_even);
	m0_even=_mm256_add_epi32(m0_even,m3_even);
	m0_even=_mm256_add_epi32(m0_even,m4_even);
	m0_even=_mm256_add_epi32(m0_even,m5_even);
	m0_even=_mm256_add_epi32(m0_even,m6_even);
	m0_even=_mm256_add_epi32(m0_even,m7_even);


	 m0_odd=_mm256_and_si256(m0,mask_odd_16);
	 m1_odd=_mm256_and_si256(m1,mask_odd_16);
	 m2_odd=_mm256_and_si256(m2,mask_odd_16);
	 m3_odd=_mm256_and_si256(m3,mask_odd_16);
	 m4_odd=_mm256_and_si256(m4,mask_odd_16);
	 m5_odd=_mm256_and_si256(m5,mask_odd_16);
	 m6_odd=_mm256_and_si256(m6,mask_odd_16);
	 m7_odd=_mm256_and_si256(m7,mask_odd_16);

	 //make from 16-bit to 32bit
		m0_odd=_mm256_madd_epi16(m0_odd,ones);
		m1_odd=_mm256_madd_epi16(m1_odd,ones);
		m2_odd=_mm256_madd_epi16(m2_odd,ones);
		m3_odd=_mm256_madd_epi16(m3_odd,ones);
		m4_odd=_mm256_madd_epi16(m4_odd,ones);
		m5_odd=_mm256_madd_epi16(m5_odd,ones);
		m6_odd=_mm256_madd_epi16(m6_odd,ones);
		m7_odd=_mm256_madd_epi16(m7_odd,ones);


	//vertical 32-bit add
	m0_odd=_mm256_add_epi32(m0_odd,m1_odd);
	m0_odd=_mm256_add_epi32(m0_odd,m2_odd);
	m0_odd=_mm256_add_epi32(m0_odd,m3_odd);
	m0_odd=_mm256_add_epi32(m0_odd,m4_odd);
	m0_odd=_mm256_add_epi32(m0_odd,m5_odd);
	m0_odd=_mm256_add_epi32(m0_odd,m6_odd);
	m0_odd=_mm256_add_epi32(m0_odd,m7_odd);

	m0_even=division_32(division_case,m0_even,f);//even results
	m0_odd=division_32(division_case,m0_odd,f);//even results

	m0_odd=_mm256_slli_si256(m0_odd,2); //shift two position right
	odd=_mm256_add_epi8(m0_even,m0_odd); //add the odd with the even


	//MERGE AND pack to one register
	odd=_mm256_slli_si256(odd,1); //shift one position right
	r0=_mm256_add_epi8(even,odd); //add the odd with the even

	//y filter ends - r0 has now the data to be processed by x filter

		_mm256_store_si256( (__m256i *) &temp[col],r0 );

	  }
		//y filter ends - r0 has now the data to be processed by x filter

}



//this function is computing the row=2 and row=N-3 only
//for row=3 input prelude_7x7_Ymask_3(0,...
//for row=N-3 input prelude_7x7_Ymask_3(N-7,...
void prelude_9x9_32_Ymask_2(const int row, const unsigned int M,unsigned char **frame1, unsigned char *temp, const signed char mask_vector_y[][32],  const unsigned int division_case, const __m256i f, const __m256i mask_prelude){

	const __m256i mask_even_16  = _mm256_set_epi16(0,0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff);
	const __m256i mask_odd_16  = _mm256_set_epi16(0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff,0);
	const __m256i ones=_mm256_set1_epi16(1);

	__m256i r0,r1,r2,r3,r4,r5,r6,m0,m1,m2,m3,m4,m5,m6,even,odd;
	//__m256i ones=_mm256_set1_epi16(1);
	 __m256i cy0,cy1,cy2,cy3,cy4,cy5,cy6,cy0_sh1,cy1_sh1,cy2_sh1,cy3_sh1,cy4_sh1,cy5_sh1,cy6_sh1;

	if (row!=0){ //if row=N-7
	  cy0=_mm256_load_si256( (__m256i *) &mask_vector_y[0]);
	  cy1=_mm256_load_si256( (__m256i *) &mask_vector_y[1]);
	  cy2=_mm256_load_si256( (__m256i *) &mask_vector_y[2]);
	  cy3=_mm256_load_si256( (__m256i *) &mask_vector_y[3]);
	  cy4=_mm256_load_si256( (__m256i *) &mask_vector_y[4]);
	  cy5=_mm256_load_si256( (__m256i *) &mask_vector_y[5]);
	  cy6=_mm256_load_si256( (__m256i *) &mask_vector_y[6]);

	  cy0_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[9]);
	  cy1_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[10]);
	  cy2_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[11]);
	  cy3_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[12]);
	  cy4_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[13]);
	  cy5_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[14]);
	  cy6_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[15]);

	}
	else {
		  cy0=_mm256_load_si256( (__m256i *) &mask_vector_y[2]);
		  cy1=_mm256_load_si256( (__m256i *) &mask_vector_y[3]);
		  cy2=_mm256_load_si256( (__m256i *) &mask_vector_y[4]);
		  cy3=_mm256_load_si256( (__m256i *) &mask_vector_y[5]);
		  cy4=_mm256_load_si256( (__m256i *) &mask_vector_y[6]);
		  cy5=_mm256_load_si256( (__m256i *) &mask_vector_y[7]);
		  cy6=_mm256_load_si256( (__m256i *) &mask_vector_y[8]);

		  cy0_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[11]);
		  cy1_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[12]);
		  cy2_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[13]);
		  cy3_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[14]);
		  cy4_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[15]);
		  cy5_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[16]);
		  cy6_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[17]);
	}

	  for (unsigned int col = 0; col <= M-32; col+=32){

	  //--------------------y filter begins--------------------
	//load the 9 rows
	r0=_mm256_load_si256( (__m256i *) &frame1[row][col]);
	r1=_mm256_load_si256( (__m256i *) &frame1[row+1][col]);
	r2=_mm256_load_si256( (__m256i *) &frame1[row+2][col]);
	r3=_mm256_load_si256( (__m256i *) &frame1[row+3][col]);
	r4=_mm256_load_si256( (__m256i *) &frame1[row+4][col]);
	r5=_mm256_load_si256( (__m256i *) &frame1[row+5][col]);
	r6=_mm256_load_si256( (__m256i *) &frame1[row+6][col]);


	//multiply by the mask
	m0=_mm256_maddubs_epi16(r0,cy0);
	m1=_mm256_maddubs_epi16(r1,cy1);
	m2=_mm256_maddubs_epi16(r2,cy2);
	m3=_mm256_maddubs_epi16(r3,cy3);
	m4=_mm256_maddubs_epi16(r4,cy4);
	m5=_mm256_maddubs_epi16(r5,cy5);
	m6=_mm256_maddubs_epi16(r6,cy6);

	//discard odd
__m256i m0_even=_mm256_and_si256(m0,mask_even_16);
__m256i	m1_even=_mm256_and_si256(m1,mask_even_16);
__m256i	m2_even=_mm256_and_si256(m2,mask_even_16);
__m256i	m3_even=_mm256_and_si256(m3,mask_even_16);
__m256i	m4_even=_mm256_and_si256(m4,mask_even_16);
__m256i	m5_even=_mm256_and_si256(m5,mask_even_16);
__m256i	m6_even=_mm256_and_si256(m6,mask_even_16);


//make from 16-bit to 32bit
m0_even=_mm256_madd_epi16(m0_even,ones);
m1_even=_mm256_madd_epi16(m1_even,ones);
m2_even=_mm256_madd_epi16(m2_even,ones);
m3_even=_mm256_madd_epi16(m3_even,ones);
m4_even=_mm256_madd_epi16(m4_even,ones);
m5_even=_mm256_madd_epi16(m5_even,ones);
m6_even=_mm256_madd_epi16(m6_even,ones);

	//vertical 32-bit add
	m0_even=_mm256_add_epi32(m0_even,m1_even);
	m0_even=_mm256_add_epi32(m0_even,m2_even);
	m0_even=_mm256_add_epi32(m0_even,m3_even);
	m0_even=_mm256_add_epi32(m0_even,m4_even);
	m0_even=_mm256_add_epi32(m0_even,m5_even);
	m0_even=_mm256_add_epi32(m0_even,m6_even);


	//discard even
__m256i	m0_odd=_mm256_and_si256(m0,mask_odd_16);
__m256i	m1_odd=_mm256_and_si256(m1,mask_odd_16);
__m256i	m2_odd=_mm256_and_si256(m2,mask_odd_16);
__m256i	m3_odd=_mm256_and_si256(m3,mask_odd_16);
__m256i	m4_odd=_mm256_and_si256(m4,mask_odd_16);
__m256i	m5_odd=_mm256_and_si256(m5,mask_odd_16);
__m256i	m6_odd=_mm256_and_si256(m6,mask_odd_16);


//make from 16-bit to 32bit
	m0_odd=_mm256_madd_epi16(m0_odd,ones);
	m1_odd=_mm256_madd_epi16(m1_odd,ones);
	m2_odd=_mm256_madd_epi16(m2_odd,ones);
	m3_odd=_mm256_madd_epi16(m3_odd,ones);
	m4_odd=_mm256_madd_epi16(m4_odd,ones);
	m5_odd=_mm256_madd_epi16(m5_odd,ones);
	m6_odd=_mm256_madd_epi16(m6_odd,ones);

	//vertical 32-bit add
	m0_odd=_mm256_add_epi32(m0_odd,m1_odd);
	m0_odd=_mm256_add_epi32(m0_odd,m2_odd);
	m0_odd=_mm256_add_epi32(m0_odd,m3_odd);
	m0_odd=_mm256_add_epi32(m0_odd,m4_odd);
	m0_odd=_mm256_add_epi32(m0_odd,m5_odd);
	m0_odd=_mm256_add_epi32(m0_odd,m6_odd);

	m0_even=division_32(division_case,m0_even,f);//even results
	m0_odd=division_32(division_case,m0_odd,f);//even results

	m0_odd=_mm256_slli_si256(m0_odd,2); //shift two position right
	even=_mm256_add_epi8(m0_even,m0_odd); //add the odd with the even

	//-----------------multiply by the second mask----------
	m0=_mm256_maddubs_epi16(r0,cy0_sh1);
	m1=_mm256_maddubs_epi16(r1,cy1_sh1);
	m2=_mm256_maddubs_epi16(r2,cy2_sh1);
	m3=_mm256_maddubs_epi16(r3,cy3_sh1);
	m4=_mm256_maddubs_epi16(r4,cy4_sh1);
	m5=_mm256_maddubs_epi16(r5,cy5_sh1);
	m6=_mm256_maddubs_epi16(r6,cy6_sh1);


	m0_even=_mm256_and_si256(m0,mask_even_16);
	m1_even=_mm256_and_si256(m1,mask_even_16);
	m2_even=_mm256_and_si256(m2,mask_even_16);
	m3_even=_mm256_and_si256(m3,mask_even_16);
	m4_even=_mm256_and_si256(m4,mask_even_16);
	m5_even=_mm256_and_si256(m5,mask_even_16);
	m6_even=_mm256_and_si256(m6,mask_even_16);

	//make from 16-bit to 32bit
	  m0_even=_mm256_madd_epi16(m0_even,ones);
	  m1_even=_mm256_madd_epi16(m1_even,ones);
	  m2_even=_mm256_madd_epi16(m2_even,ones);
	  m3_even=_mm256_madd_epi16(m3_even,ones);
	  m4_even=_mm256_madd_epi16(m4_even,ones);
	  m5_even=_mm256_madd_epi16(m5_even,ones);
	  m6_even=_mm256_madd_epi16(m6_even,ones);


	//vertical 32-bit add
	m0_even=_mm256_add_epi32(m0_even,m1_even);
	m0_even=_mm256_add_epi32(m0_even,m2_even);
	m0_even=_mm256_add_epi32(m0_even,m3_even);
	m0_even=_mm256_add_epi32(m0_even,m4_even);
	m0_even=_mm256_add_epi32(m0_even,m5_even);
	m0_even=_mm256_add_epi32(m0_even,m6_even);


	 m0_odd=_mm256_and_si256(m0,mask_odd_16);
	 m1_odd=_mm256_and_si256(m1,mask_odd_16);
	 m2_odd=_mm256_and_si256(m2,mask_odd_16);
	 m3_odd=_mm256_and_si256(m3,mask_odd_16);
	 m4_odd=_mm256_and_si256(m4,mask_odd_16);
	 m5_odd=_mm256_and_si256(m5,mask_odd_16);
	 m6_odd=_mm256_and_si256(m6,mask_odd_16);

	 //make from 16-bit to 32bit
		m0_odd=_mm256_madd_epi16(m0_odd,ones);
		m1_odd=_mm256_madd_epi16(m1_odd,ones);
		m2_odd=_mm256_madd_epi16(m2_odd,ones);
		m3_odd=_mm256_madd_epi16(m3_odd,ones);
		m4_odd=_mm256_madd_epi16(m4_odd,ones);
		m5_odd=_mm256_madd_epi16(m5_odd,ones);
		m6_odd=_mm256_madd_epi16(m6_odd,ones);


	//vertical 32-bit add
	m0_odd=_mm256_add_epi32(m0_odd,m1_odd);
	m0_odd=_mm256_add_epi32(m0_odd,m2_odd);
	m0_odd=_mm256_add_epi32(m0_odd,m3_odd);
	m0_odd=_mm256_add_epi32(m0_odd,m4_odd);
	m0_odd=_mm256_add_epi32(m0_odd,m5_odd);
	m0_odd=_mm256_add_epi32(m0_odd,m6_odd);

	m0_even=division_32(division_case,m0_even,f);//even results
	m0_odd=division_32(division_case,m0_odd,f);//even results

	m0_odd=_mm256_slli_si256(m0_odd,2); //shift two position right
	odd=_mm256_add_epi8(m0_even,m0_odd); //add the odd with the even


	//MERGE AND pack to one register
	odd=_mm256_slli_si256(odd,1); //shift one position right
	r0=_mm256_add_epi8(even,odd); //add the odd with the even

	//y filter ends - r0 has now the data to be processed by x filter

		_mm256_store_si256( (__m256i *) &temp[col],r0 );

	  }
		//y filter ends - r0 has now the data to be processed by x filter

}


//this function is computing the row=1 and row=N-2 only
//for row=3 input prelude_7x7_Ymask_3(0,...
//for row=N-3 input prelude_7x7_Ymask_3(N-6,...
void prelude_9x9_32_Ymask_1(const int row, const unsigned int M,unsigned char **frame1, unsigned char *temp, const signed char mask_vector_y[][32],  const unsigned int division_case, const __m256i f, const __m256i mask_prelude){
	const __m256i mask_even_16  = _mm256_set_epi16(0,0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff);
	const __m256i mask_odd_16  = _mm256_set_epi16(0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff,0);
	const __m256i ones=_mm256_set1_epi16(1);
	__m256i r0,r1,r2,r3,r4,r5,m0,m1,m2,m3,m4,m5,even,odd;
	//__m256i ones=_mm256_set1_epi16(1);
	 __m256i cy0,cy1,cy2,cy3,cy4,cy5,cy0_sh1,cy1_sh1,cy2_sh1,cy3_sh1,cy4_sh1,cy5_sh1;

	if (row!=0){ //if row=N-6
	  cy0=_mm256_load_si256( (__m256i *) &mask_vector_y[0]);
	  cy1=_mm256_load_si256( (__m256i *) &mask_vector_y[1]);
	  cy2=_mm256_load_si256( (__m256i *) &mask_vector_y[2]);
	  cy3=_mm256_load_si256( (__m256i *) &mask_vector_y[3]);
	  cy4=_mm256_load_si256( (__m256i *) &mask_vector_y[4]);
	  cy5=_mm256_load_si256( (__m256i *) &mask_vector_y[5]);

	  cy0_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[9]);
	  cy1_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[10]);
	  cy2_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[11]);
	  cy3_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[12]);
	  cy4_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[13]);
	  cy5_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[14]);

	}
	else {
		  cy0=_mm256_load_si256( (__m256i *) &mask_vector_y[3]);
		  cy1=_mm256_load_si256( (__m256i *) &mask_vector_y[4]);
		  cy2=_mm256_load_si256( (__m256i *) &mask_vector_y[5]);
		  cy3=_mm256_load_si256( (__m256i *) &mask_vector_y[6]);
		  cy4=_mm256_load_si256( (__m256i *) &mask_vector_y[7]);
		  cy5=_mm256_load_si256( (__m256i *) &mask_vector_y[8]);

		  cy0_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[12]);
		  cy1_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[13]);
		  cy2_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[14]);
		  cy3_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[15]);
		  cy4_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[16]);
		  cy5_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[17]);
	}

	  for (unsigned int col = 0; col <= M-32; col+=32){

	  //--------------------y filter begins--------------------
	//load the 9 rows
	r0=_mm256_load_si256( (__m256i *) &frame1[row][col]);
	r1=_mm256_load_si256( (__m256i *) &frame1[row+1][col]);
	r2=_mm256_load_si256( (__m256i *) &frame1[row+2][col]);
	r3=_mm256_load_si256( (__m256i *) &frame1[row+3][col]);
	r4=_mm256_load_si256( (__m256i *) &frame1[row+4][col]);
	r5=_mm256_load_si256( (__m256i *) &frame1[row+5][col]);

	//multiply by the mask
	m0=_mm256_maddubs_epi16(r0,cy0);
	m1=_mm256_maddubs_epi16(r1,cy1);
	m2=_mm256_maddubs_epi16(r2,cy2);
	m3=_mm256_maddubs_epi16(r3,cy3);
	m4=_mm256_maddubs_epi16(r4,cy4);
	m5=_mm256_maddubs_epi16(r5,cy5);

	//discard odd
__m256i m0_even=_mm256_and_si256(m0,mask_even_16);
__m256i	m1_even=_mm256_and_si256(m1,mask_even_16);
__m256i	m2_even=_mm256_and_si256(m2,mask_even_16);
__m256i	m3_even=_mm256_and_si256(m3,mask_even_16);
__m256i	m4_even=_mm256_and_si256(m4,mask_even_16);
__m256i	m5_even=_mm256_and_si256(m5,mask_even_16);


//make from 16-bit to 32bit
m0_even=_mm256_madd_epi16(m0_even,ones);
m1_even=_mm256_madd_epi16(m1_even,ones);
m2_even=_mm256_madd_epi16(m2_even,ones);
m3_even=_mm256_madd_epi16(m3_even,ones);
m4_even=_mm256_madd_epi16(m4_even,ones);
m5_even=_mm256_madd_epi16(m5_even,ones);

	//vertical 32-bit add
	m0_even=_mm256_add_epi32(m0_even,m1_even);
	m0_even=_mm256_add_epi32(m0_even,m2_even);
	m0_even=_mm256_add_epi32(m0_even,m3_even);
	m0_even=_mm256_add_epi32(m0_even,m4_even);
	m0_even=_mm256_add_epi32(m0_even,m5_even);


	//discard even
__m256i	m0_odd=_mm256_and_si256(m0,mask_odd_16);
__m256i	m1_odd=_mm256_and_si256(m1,mask_odd_16);
__m256i	m2_odd=_mm256_and_si256(m2,mask_odd_16);
__m256i	m3_odd=_mm256_and_si256(m3,mask_odd_16);
__m256i	m4_odd=_mm256_and_si256(m4,mask_odd_16);
__m256i	m5_odd=_mm256_and_si256(m5,mask_odd_16);


//make from 16-bit to 32bit
	m0_odd=_mm256_madd_epi16(m0_odd,ones);
	m1_odd=_mm256_madd_epi16(m1_odd,ones);
	m2_odd=_mm256_madd_epi16(m2_odd,ones);
	m3_odd=_mm256_madd_epi16(m3_odd,ones);
	m4_odd=_mm256_madd_epi16(m4_odd,ones);
	m5_odd=_mm256_madd_epi16(m5_odd,ones);

	//vertical 32-bit add
	m0_odd=_mm256_add_epi32(m0_odd,m1_odd);
	m0_odd=_mm256_add_epi32(m0_odd,m2_odd);
	m0_odd=_mm256_add_epi32(m0_odd,m3_odd);
	m0_odd=_mm256_add_epi32(m0_odd,m4_odd);
	m0_odd=_mm256_add_epi32(m0_odd,m5_odd);

	m0_even=division_32(division_case,m0_even,f);//even results
	m0_odd=division_32(division_case,m0_odd,f);//even results

	m0_odd=_mm256_slli_si256(m0_odd,2); //shift two position right
	even=_mm256_add_epi8(m0_even,m0_odd); //add the odd with the even

	//-----------------multiply by the second mask----------
	m0=_mm256_maddubs_epi16(r0,cy0_sh1);
	m1=_mm256_maddubs_epi16(r1,cy1_sh1);
	m2=_mm256_maddubs_epi16(r2,cy2_sh1);
	m3=_mm256_maddubs_epi16(r3,cy3_sh1);
	m4=_mm256_maddubs_epi16(r4,cy4_sh1);
	m5=_mm256_maddubs_epi16(r5,cy5_sh1);


	m0_even=_mm256_and_si256(m0,mask_even_16);
	m1_even=_mm256_and_si256(m1,mask_even_16);
	m2_even=_mm256_and_si256(m2,mask_even_16);
	m3_even=_mm256_and_si256(m3,mask_even_16);
	m4_even=_mm256_and_si256(m4,mask_even_16);
	m5_even=_mm256_and_si256(m5,mask_even_16);

	//make from 16-bit to 32bit
	  m0_even=_mm256_madd_epi16(m0_even,ones);
	  m1_even=_mm256_madd_epi16(m1_even,ones);
	  m2_even=_mm256_madd_epi16(m2_even,ones);
	  m3_even=_mm256_madd_epi16(m3_even,ones);
	  m4_even=_mm256_madd_epi16(m4_even,ones);
	  m5_even=_mm256_madd_epi16(m5_even,ones);


	//vertical 32-bit add
	m0_even=_mm256_add_epi32(m0_even,m1_even);
	m0_even=_mm256_add_epi32(m0_even,m2_even);
	m0_even=_mm256_add_epi32(m0_even,m3_even);
	m0_even=_mm256_add_epi32(m0_even,m4_even);
	m0_even=_mm256_add_epi32(m0_even,m5_even);


	 m0_odd=_mm256_and_si256(m0,mask_odd_16);
	 m1_odd=_mm256_and_si256(m1,mask_odd_16);
	 m2_odd=_mm256_and_si256(m2,mask_odd_16);
	 m3_odd=_mm256_and_si256(m3,mask_odd_16);
	 m4_odd=_mm256_and_si256(m4,mask_odd_16);
	 m5_odd=_mm256_and_si256(m5,mask_odd_16);

	 //make from 16-bit to 32bit
		m0_odd=_mm256_madd_epi16(m0_odd,ones);
		m1_odd=_mm256_madd_epi16(m1_odd,ones);
		m2_odd=_mm256_madd_epi16(m2_odd,ones);
		m3_odd=_mm256_madd_epi16(m3_odd,ones);
		m4_odd=_mm256_madd_epi16(m4_odd,ones);
		m5_odd=_mm256_madd_epi16(m5_odd,ones);


	//vertical 32-bit add
	m0_odd=_mm256_add_epi32(m0_odd,m1_odd);
	m0_odd=_mm256_add_epi32(m0_odd,m2_odd);
	m0_odd=_mm256_add_epi32(m0_odd,m3_odd);
	m0_odd=_mm256_add_epi32(m0_odd,m4_odd);
	m0_odd=_mm256_add_epi32(m0_odd,m5_odd);

	m0_even=division_32(division_case,m0_even,f);//even results
	m0_odd=division_32(division_case,m0_odd,f);//even results

	m0_odd=_mm256_slli_si256(m0_odd,2); //shift two position right
	odd=_mm256_add_epi8(m0_even,m0_odd); //add the odd with the even


	//MERGE AND pack to one register
	odd=_mm256_slli_si256(odd,1); //shift one position right
	r0=_mm256_add_epi8(even,odd); //add the odd with the even

	//y filter ends - r0 has now the data to be processed by x filter

		_mm256_store_si256( (__m256i *) &temp[col],r0 );

	  }
		//y filter ends - r0 has now the data to be processed by x filter

}


//this function is computing the row=0 and row=N-1 only
//for row=3 input prelude_7x7_Ymask_3(0,...
//for row=N-3 input prelude_7x7_Ymask_3(N-5,...
void prelude_9x9_32_Ymask_0(const int row, const unsigned int M, unsigned char **frame1, unsigned char *temp, const signed char mask_vector_y[][32],  const unsigned int division_case, const __m256i f, const __m256i mask_prelude){
	const __m256i mask_even_16  = _mm256_set_epi16(0,0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff);
	const __m256i mask_odd_16  = _mm256_set_epi16(0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff,0);
	const __m256i ones=_mm256_set1_epi16(1);
	__m256i r0,r1,r2,r3,r4,m0,m1,m2,m3,m4,even,odd;
	//__m256i ones=_mm256_set1_epi16(1);
	 __m256i cy0,cy1,cy2,cy3,cy4,cy0_sh1,cy1_sh1,cy2_sh1,cy3_sh1,cy4_sh1;

	if (row!=0){ //if row=N-6
	  cy0=_mm256_load_si256( (__m256i *) &mask_vector_y[0]);
	  cy1=_mm256_load_si256( (__m256i *) &mask_vector_y[1]);
	  cy2=_mm256_load_si256( (__m256i *) &mask_vector_y[2]);
	  cy3=_mm256_load_si256( (__m256i *) &mask_vector_y[3]);
	  cy4=_mm256_load_si256( (__m256i *) &mask_vector_y[4]);

	  cy0_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[9]);
	  cy1_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[10]);
	  cy2_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[11]);
	  cy3_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[12]);
	  cy4_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[13]);

	}
	else {
		  cy0=_mm256_load_si256( (__m256i *) &mask_vector_y[4]);
		  cy1=_mm256_load_si256( (__m256i *) &mask_vector_y[5]);
		  cy2=_mm256_load_si256( (__m256i *) &mask_vector_y[6]);
		  cy3=_mm256_load_si256( (__m256i *) &mask_vector_y[7]);
		  cy4=_mm256_load_si256( (__m256i *) &mask_vector_y[8]);

		  cy0_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[13]);
		  cy1_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[14]);
		  cy2_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[15]);
		  cy3_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[16]);
		  cy4_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[17]);
	}

	  for (unsigned int col = 0; col <= M-32; col+=32){

	  //--------------------y filter begins--------------------
	//load the 9 rows
	r0=_mm256_load_si256( (__m256i *) &frame1[row][col]);
	r1=_mm256_load_si256( (__m256i *) &frame1[row+1][col]);
	r2=_mm256_load_si256( (__m256i *) &frame1[row+2][col]);
	r3=_mm256_load_si256( (__m256i *) &frame1[row+3][col]);
	r4=_mm256_load_si256( (__m256i *) &frame1[row+4][col]);

	//multiply by the mask
	m0=_mm256_maddubs_epi16(r0,cy0);
	m1=_mm256_maddubs_epi16(r1,cy1);
	m2=_mm256_maddubs_epi16(r2,cy2);
	m3=_mm256_maddubs_epi16(r3,cy3);
	m4=_mm256_maddubs_epi16(r4,cy4);

	//discard odd
__m256i m0_even=_mm256_and_si256(m0,mask_even_16);
__m256i	m1_even=_mm256_and_si256(m1,mask_even_16);
__m256i	m2_even=_mm256_and_si256(m2,mask_even_16);
__m256i	m3_even=_mm256_and_si256(m3,mask_even_16);
__m256i	m4_even=_mm256_and_si256(m4,mask_even_16);


//make from 16-bit to 32bit
m0_even=_mm256_madd_epi16(m0_even,ones);
m1_even=_mm256_madd_epi16(m1_even,ones);
m2_even=_mm256_madd_epi16(m2_even,ones);
m3_even=_mm256_madd_epi16(m3_even,ones);
m4_even=_mm256_madd_epi16(m4_even,ones);

	//vertical 32-bit add
	m0_even=_mm256_add_epi32(m0_even,m1_even);
	m0_even=_mm256_add_epi32(m0_even,m2_even);
	m0_even=_mm256_add_epi32(m0_even,m3_even);
	m0_even=_mm256_add_epi32(m0_even,m4_even);


	//discard even
__m256i	m0_odd=_mm256_and_si256(m0,mask_odd_16);
__m256i	m1_odd=_mm256_and_si256(m1,mask_odd_16);
__m256i	m2_odd=_mm256_and_si256(m2,mask_odd_16);
__m256i	m3_odd=_mm256_and_si256(m3,mask_odd_16);
__m256i	m4_odd=_mm256_and_si256(m4,mask_odd_16);


//make from 16-bit to 32bit
	m0_odd=_mm256_madd_epi16(m0_odd,ones);
	m1_odd=_mm256_madd_epi16(m1_odd,ones);
	m2_odd=_mm256_madd_epi16(m2_odd,ones);
	m3_odd=_mm256_madd_epi16(m3_odd,ones);
	m4_odd=_mm256_madd_epi16(m4_odd,ones);

	//vertical 32-bit add
	m0_odd=_mm256_add_epi32(m0_odd,m1_odd);
	m0_odd=_mm256_add_epi32(m0_odd,m2_odd);
	m0_odd=_mm256_add_epi32(m0_odd,m3_odd);
	m0_odd=_mm256_add_epi32(m0_odd,m4_odd);

	m0_even=division_32(division_case,m0_even,f);//even results
	m0_odd=division_32(division_case,m0_odd,f);//even results

	m0_odd=_mm256_slli_si256(m0_odd,2); //shift two position right
	even=_mm256_add_epi8(m0_even,m0_odd); //add the odd with the even

	//-----------------multiply by the second mask----------
	m0=_mm256_maddubs_epi16(r0,cy0_sh1);
	m1=_mm256_maddubs_epi16(r1,cy1_sh1);
	m2=_mm256_maddubs_epi16(r2,cy2_sh1);
	m3=_mm256_maddubs_epi16(r3,cy3_sh1);
	m4=_mm256_maddubs_epi16(r4,cy4_sh1);


	m0_even=_mm256_and_si256(m0,mask_even_16);
	m1_even=_mm256_and_si256(m1,mask_even_16);
	m2_even=_mm256_and_si256(m2,mask_even_16);
	m3_even=_mm256_and_si256(m3,mask_even_16);
	m4_even=_mm256_and_si256(m4,mask_even_16);

	//make from 16-bit to 32bit
	  m0_even=_mm256_madd_epi16(m0_even,ones);
	  m1_even=_mm256_madd_epi16(m1_even,ones);
	  m2_even=_mm256_madd_epi16(m2_even,ones);
	  m3_even=_mm256_madd_epi16(m3_even,ones);
	  m4_even=_mm256_madd_epi16(m4_even,ones);


	//vertical 32-bit add
	m0_even=_mm256_add_epi32(m0_even,m1_even);
	m0_even=_mm256_add_epi32(m0_even,m2_even);
	m0_even=_mm256_add_epi32(m0_even,m3_even);
	m0_even=_mm256_add_epi32(m0_even,m4_even);


	 m0_odd=_mm256_and_si256(m0,mask_odd_16);
	 m1_odd=_mm256_and_si256(m1,mask_odd_16);
	 m2_odd=_mm256_and_si256(m2,mask_odd_16);
	 m3_odd=_mm256_and_si256(m3,mask_odd_16);
	 m4_odd=_mm256_and_si256(m4,mask_odd_16);

	 //make from 16-bit to 32bit
		m0_odd=_mm256_madd_epi16(m0_odd,ones);
		m1_odd=_mm256_madd_epi16(m1_odd,ones);
		m2_odd=_mm256_madd_epi16(m2_odd,ones);
		m3_odd=_mm256_madd_epi16(m3_odd,ones);
		m4_odd=_mm256_madd_epi16(m4_odd,ones);


	//vertical 32-bit add
	m0_odd=_mm256_add_epi32(m0_odd,m1_odd);
	m0_odd=_mm256_add_epi32(m0_odd,m2_odd);
	m0_odd=_mm256_add_epi32(m0_odd,m3_odd);
	m0_odd=_mm256_add_epi32(m0_odd,m4_odd);

	m0_even=division_32(division_case,m0_even,f);//even results
	m0_odd=division_32(division_case,m0_odd,f);//even results

	m0_odd=_mm256_slli_si256(m0_odd,2); //shift two position right
	odd=_mm256_add_epi8(m0_even,m0_odd); //add the odd with the even


	//MERGE AND pack to one register
	odd=_mm256_slli_si256(odd,1); //shift one position right
	r0=_mm256_add_epi8(even,odd); //add the odd with the even

	//y filter ends - r0 has now the data to be processed by x filter

		_mm256_store_si256( (__m256i *) &temp[col],r0 );

	  }
		//y filter ends - r0 has now the data to be processed by x filter

}



void prelude_9x9_32_Xmask(unsigned char **frame1,unsigned char **filt,unsigned char *temp,  const unsigned int N,const unsigned int M, const int row, const signed char mask_vector_x[][32],  const unsigned int division_case, const __m256i f,const unsigned int REMINDER_ITERATIONS_X,const unsigned int REMINDER_ITERATIONS_Y,const signed char mask_vector_y[][32],const unsigned short int divisor_xy,signed char *kernel_x){

	__m256i r0,m0,m1,m2,m3,m4,m8,out1,out2,output1,output2,output3,output4,output5;

	const __m256i cx0=_mm256_load_si256( (__m256i *) &mask_vector_x[0]);
	const __m256i ones=_mm256_set1_epi16(1);


	const __m256i mask_prelude  = _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0);
	const __m256i mask_prelude_3  = _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0);
	const __m256i mask_prelude_2  = _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0);
	const __m256i mask_prelude_1  = _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0);


	const __m256i mask_z    = _mm256_set_epi16(0,0,0,0,0,0,0,0,0,0,0,65535,0,0,0,0);
	const __m256i mask_unz    = _mm256_set_epi16(65535,65535,65535,65535,65535,65535,65535,65535,65535,65535,65535,0,65535,65535,65535,65535);
	const __m256i mask_32_1    = _mm256_set_epi32(0xffffffff,0,0,0xffffffff,0,0,0,0);
	const __m256i mask_32_2    = _mm256_set_epi32(0,0,0,0xffffffff,0,0,0,0);
	const __m256i mask_32_a    = _mm256_set_epi32(0,0,0xffffffff,0,0,0xffffffff,0,0xffffffff);
	const __m256i out_mask_1    = _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255);//0,20
	const __m256i out_mask_2    = _mm256_set_epi8(0,0,0,0,0,0,0,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255,0,0,0,0);//4,24
	const __m256i out_mask_3    = _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, 0,0,0,0,0,0,0,255,0,0,0,0,0,0,0,0);//8
	const __m256i out_mask_4    = _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, 0,0,0,255,0,0,0,0,0,0,0,0,0,0,0,0);//12
	const __m256i out_mask_5    = _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, 0,0,0,0,255,255,0,0,0,0,0,0,0,0,0,0);//10,11


	unsigned int col;

	  for (col = 0; col <= M-32; col+=30){

		  if (col==0){

	  			//1st col iteration
				r0=_mm256_load_si256( (__m256i *) &temp[0]);
		  		r0=fill_zeros(r0,mask_prelude);

	  			//multiply by the mask
	  			m0=_mm256_maddubs_epi16(r0,cx0);

		  			//zero the 4th out of 16 elements
		  			m1=_mm256_and_si256(m0,mask_unz);
		  			//keep a copy of the 4th element
		  			m8=_mm256_and_si256(m0,mask_z);
					//m8=_mm256_slli_si256(m8,2);

		  			//make 16-bit 32-bit
					m1=_mm256_madd_epi16(m1,ones);

					//horizontal add
					m2=_mm256_srli_si256(m1,4);
		  			m2=_mm256_add_epi32(m1,m2);

		  			//keep only 4th and 7th out of 8 elements
		  			m1=_mm256_and_si256(m1,mask_32_1);
		  			m1=_mm256_add_epi32(m1,m8);

		  			//shift m1 and add with m2 (complex shift is needed)
		  			m3=_mm256_srli_si256(m1,8);
		  			m4=_mm256_and_si256(m1,mask_32_2);
		  			m4=_mm256_permute2f128_si256(m4,m4,1);
		  			m4=_mm256_slli_si256(m4,8);
		  			m3=_mm256_add_epi32(m3,m4);
		  			m2=_mm256_add_epi32(m2,m3);

		  			out1=_mm256_and_si256(m2,mask_32_a);


	  			//2nd col iteration
				r0=_mm256_load_si256( (__m256i *) &temp[0]);
		  		r0=fill_3zeros(r0,mask_prelude_3);

	  			//multiply by the mask
		  			m0=_mm256_maddubs_epi16(r0,cx0);

		  			//zero the 4th out of 16 elements
		  			m1=_mm256_and_si256(m0,mask_unz);
		  			//keep a copy of the 4th element
		  			m8=_mm256_and_si256(m0,mask_z);
					//m8=_mm256_slli_si256(m8,2);

		  			//make 16-bit 32-bit
					m1=_mm256_madd_epi16(m1,ones);

					//horizontal add
					m2=_mm256_srli_si256(m1,4);
		  			m2=_mm256_add_epi32(m1,m2);

		  			//keep only 4th and 7th out of 8 elements
		  			m1=_mm256_and_si256(m1,mask_32_1);
		  			m1=_mm256_add_epi32(m1,m8);

		  			//shift m1 and add with m2 (complex shift is needed)
		  			m3=_mm256_srli_si256(m1,8);
		  			m4=_mm256_and_si256(m1,mask_32_2);
		  			m4=_mm256_permute2f128_si256(m4,m4,1);
		  			m4=_mm256_slli_si256(m4,8);
		  			m3=_mm256_add_epi32(m3,m4);
		  			m2=_mm256_add_epi32(m2,m3);

		  			out2=_mm256_and_si256(m2,mask_32_a);//in 32-bit format keep only 0,2,5 and discard the others

		  			//merge out1 and out2 and then divide
		  			out2=_mm256_slli_si256(out2,4);
		  			out1=_mm256_add_epi32(out1,out2);
		  			out1 = division_32(division_case,out1,f);

		  			//elements must be re-arranged before store
		  			m0=_mm256_and_si256(out1,out_mask_1);
		  			m1=_mm256_and_si256(out1,out_mask_2);
		  			m1=_mm256_srli_si256(m1,3);
		  			m2=_mm256_and_si256(out1,out_mask_3);
		  			m2=_mm256_slli_si256(m2,2);
		  			m3=_mm256_and_si256(out1,out_mask_4);
		  			m3=_mm256_srli_si256(m3,1);
		  			m0=_mm256_add_epi8(m0,m1);
		  			m0=_mm256_add_epi8(m0,m2);
		  			output1=_mm256_add_epi8(m0,m3);

	  			//3rd col iteration
				r0=_mm256_load_si256( (__m256i *) &temp[0]);
		  		r0=fill_2zeros(r0,mask_prelude_2);

	  			//multiply by the mask
	  			m0=_mm256_maddubs_epi16(r0,cx0);

		  			//zero the 4th out of 16 elements
		  			m1=_mm256_and_si256(m0,mask_unz);
		  			//keep a copy of the 4th element
		  			m8=_mm256_and_si256(m0,mask_z);
					//m8=_mm256_slli_si256(m8,2);

		  			//make 16-bit 32-bit
					m1=_mm256_madd_epi16(m1,ones);

					//horizontal add
					m2=_mm256_srli_si256(m1,4);
		  			m2=_mm256_add_epi32(m1,m2);

		  			//keep only 4th and 7th out of 8 elements
		  			m1=_mm256_and_si256(m1,mask_32_1);
		  			m1=_mm256_add_epi32(m1,m8);

		  			//shift m1 and add with m2 (complex shift is needed)
		  			m3=_mm256_srli_si256(m1,8);
		  			m4=_mm256_and_si256(m1,mask_32_2);
		  			m4=_mm256_permute2f128_si256(m4,m4,1);
		  			m4=_mm256_slli_si256(m4,8);
		  			m3=_mm256_add_epi32(m3,m4);
		  			m2=_mm256_add_epi32(m2,m3);

		  			out1=_mm256_and_si256(m2,mask_32_a);



	  			//4th col iteration
				r0=_mm256_load_si256( (__m256i *) &temp[0]);
		  		r0=fill_1zeros(r0,mask_prelude_1);

	  			//multiply by the mask
			  			m0=_mm256_maddubs_epi16(r0,cx0);

			  			//zero the 4th out of 16 elements
			  			m1=_mm256_and_si256(m0,mask_unz);
			  			//keep a copy of the 4th element
			  			m8=_mm256_and_si256(m0,mask_z);
						//m8=_mm256_slli_si256(m8,2);

			  			//make 16-bit 32-bit
						m1=_mm256_madd_epi16(m1,ones);

						//horizontal add
						m2=_mm256_srli_si256(m1,4);
			  			m2=_mm256_add_epi32(m1,m2);

			  			//keep only 4th and 7th out of 8 elements
			  			m1=_mm256_and_si256(m1,mask_32_1);
			  			m1=_mm256_add_epi32(m1,m8);

			  			//shift m1 and add with m2 (complex shift is needed)
			  			m3=_mm256_srli_si256(m1,8);
			  			m4=_mm256_and_si256(m1,mask_32_2);
			  			m4=_mm256_permute2f128_si256(m4,m4,1);
			  			m4=_mm256_slli_si256(m4,8);
			  			m3=_mm256_add_epi32(m3,m4);
			  			m2=_mm256_add_epi32(m2,m3);

			  			out2=_mm256_and_si256(m2,mask_32_a);//in 32-bit format keep only 0,2,5 and discard the others

			  			//merge out1 and out2 and then divide
			  			out2=_mm256_slli_si256(out2,4);
			  			out1=_mm256_add_epi32(out1,out2);
			  			out1 = division_32(division_case,out1,f);

			  			//elements must be re-arranged before store
			  			m0=_mm256_and_si256(out1,out_mask_1);
			  			m1=_mm256_and_si256(out1,out_mask_2);
			  			m1=_mm256_srli_si256(m1,3);
			  			m2=_mm256_and_si256(out1,out_mask_3);
			  			m2=_mm256_slli_si256(m2,2);
			  			m3=_mm256_and_si256(out1,out_mask_4);
			  			m3=_mm256_srli_si256(m3,1);
			  			m0=_mm256_add_epi8(m0,m1);
			  			m0=_mm256_add_epi8(m0,m2);
			  			output2=_mm256_add_epi8(m0,m3);


	  			//5th col iteration
				r0=_mm256_load_si256( (__m256i *) &temp[0]);

	  			//multiply by the mask
		  			m0=_mm256_maddubs_epi16(r0,cx0);

			  			//zero the 4th out of 16 elements
			  			m1=_mm256_and_si256(m0,mask_unz);
			  			//keep a copy of the 4th element
			  			m8=_mm256_and_si256(m0,mask_z);
						//m8=_mm256_slli_si256(m8,2);

			  			//make 16-bit 32-bit
						m1=_mm256_madd_epi16(m1,ones);

						//horizontal add
						m2=_mm256_srli_si256(m1,4);
			  			m2=_mm256_add_epi32(m1,m2);

			  			//keep only 4th and 7th out of 8 elements
			  			m1=_mm256_and_si256(m1,mask_32_1);
			  			m1=_mm256_add_epi32(m1,m8);

			  			//shift m1 and add with m2 (complex shift is needed)
			  			m3=_mm256_srli_si256(m1,8);
			  			m4=_mm256_and_si256(m1,mask_32_2);
			  			m4=_mm256_permute2f128_si256(m4,m4,1);
			  			m4=_mm256_slli_si256(m4,8);
			  			m3=_mm256_add_epi32(m3,m4);
			  			m2=_mm256_add_epi32(m2,m3);

			  			out1=_mm256_and_si256(m2,mask_32_a);

	  			//6th col iteration
				r0=_mm256_loadu_si256( (__m256i *) &temp[1]);

	  			//multiply by the mask
			  			m0=_mm256_maddubs_epi16(r0,cx0);

			  			//zero the 4th out of 16 elements
			  			m1=_mm256_and_si256(m0,mask_unz);
			  			//keep a copy of the 4th element
			  			m8=_mm256_and_si256(m0,mask_z);
						//m8=_mm256_slli_si256(m8,2);

			  			//make 16-bit 32-bit
						m1=_mm256_madd_epi16(m1,ones);

						//horizontal add
						m2=_mm256_srli_si256(m1,4);
			  			m2=_mm256_add_epi32(m1,m2);

			  			//keep only 4th and 7th out of 8 elements
			  			m1=_mm256_and_si256(m1,mask_32_1);
			  			m1=_mm256_add_epi32(m1,m8);

			  			//shift m1 and add with m2 (complex shift is needed)
			  			m3=_mm256_srli_si256(m1,8);
			  			m4=_mm256_and_si256(m1,mask_32_2);
			  			m4=_mm256_permute2f128_si256(m4,m4,1);
			  			m4=_mm256_slli_si256(m4,8);
			  			m3=_mm256_add_epi32(m3,m4);
			  			m2=_mm256_add_epi32(m2,m3);

			  			out2=_mm256_and_si256(m2,mask_32_a);//in 32-bit format keep only 0,2,5 and discard the others

			  			//merge out1 and out2 and then divide
			  			out2=_mm256_slli_si256(out2,4);
			  			out1=_mm256_add_epi32(out1,out2);
			  			out1 = division_32(division_case,out1,f);

			  			//elements must be re-arranged before store
			  			m0=_mm256_and_si256(out1,out_mask_1);
			  			m1=_mm256_and_si256(out1,out_mask_2);
			  			m1=_mm256_srli_si256(m1,3);
			  			m2=_mm256_and_si256(out1,out_mask_3);
			  			m2=_mm256_slli_si256(m2,2);
			  			m3=_mm256_and_si256(out1,out_mask_4);
			  			m3=_mm256_srli_si256(m3,1);
			  			m0=_mm256_add_epi8(m0,m1);
			  			m0=_mm256_add_epi8(m0,m2);
			  			output3=_mm256_add_epi8(m0,m3);


	  			//7th col iteration
				r0=_mm256_loadu_si256( (__m256i *) &temp[2]);

	  			//multiply by the mask
		  			m0=_mm256_maddubs_epi16(r0,cx0);

			  			//zero the 4th out of 16 elements
			  			m1=_mm256_and_si256(m0,mask_unz);
			  			//keep a copy of the 4th element
			  			m8=_mm256_and_si256(m0,mask_z);
						//m8=_mm256_slli_si256(m8,2);

			  			//make 16-bit 32-bit
						m1=_mm256_madd_epi16(m1,ones);

						//horizontal add
						m2=_mm256_srli_si256(m1,4);
			  			m2=_mm256_add_epi32(m1,m2);

			  			//keep only 4th and 7th out of 8 elements
			  			m1=_mm256_and_si256(m1,mask_32_1);
			  			m1=_mm256_add_epi32(m1,m8);

			  			//shift m1 and add with m2 (complex shift is needed)
			  			m3=_mm256_srli_si256(m1,8);
			  			m4=_mm256_and_si256(m1,mask_32_2);
			  			m4=_mm256_permute2f128_si256(m4,m4,1);
			  			m4=_mm256_slli_si256(m4,8);
			  			m3=_mm256_add_epi32(m3,m4);
			  			m2=_mm256_add_epi32(m2,m3);

			  			out1=_mm256_and_si256(m2,mask_32_a);

	  			//8th col iteration
				r0=_mm256_loadu_si256( (__m256i *) &temp[3]);

	  			//multiply by the mask
			  			m0=_mm256_maddubs_epi16(r0,cx0);

			  			//zero the 4th out of 16 elements
			  			m1=_mm256_and_si256(m0,mask_unz);
			  			//keep a copy of the 4th element
			  			m8=_mm256_and_si256(m0,mask_z);
						//m8=_mm256_slli_si256(m8,2);

			  			//make 16-bit 32-bit
						m1=_mm256_madd_epi16(m1,ones);

						//horizontal add
						m2=_mm256_srli_si256(m1,4);
			  			m2=_mm256_add_epi32(m1,m2);

			  			//keep only 4th and 7th out of 8 elements
			  			m1=_mm256_and_si256(m1,mask_32_1);
			  			m1=_mm256_add_epi32(m1,m8);

			  			//shift m1 and add with m2 (complex shift is needed)
			  			m3=_mm256_srli_si256(m1,8);
			  			m4=_mm256_and_si256(m1,mask_32_2);
			  			m4=_mm256_permute2f128_si256(m4,m4,1);
			  			m4=_mm256_slli_si256(m4,8);
			  			m3=_mm256_add_epi32(m3,m4);
			  			m2=_mm256_add_epi32(m2,m3);

			  			out2=_mm256_and_si256(m2,mask_32_a);//in 32-bit format keep only 0,2,5 and discard the others

			  			//merge out1 and out2 and then divide
			  			out2=_mm256_slli_si256(out2,4);
			  			out1=_mm256_add_epi32(out1,out2);
			  			out1 = division_32(division_case,out1,f);

			  			//elements must be re-arranged before store
			  			m0=_mm256_and_si256(out1,out_mask_1);
			  			m1=_mm256_and_si256(out1,out_mask_2);
			  			m1=_mm256_srli_si256(m1,3);
			  			m2=_mm256_and_si256(out1,out_mask_3);
			  			m2=_mm256_slli_si256(m2,2);
			  			m3=_mm256_and_si256(out1,out_mask_4);
			  			m3=_mm256_srli_si256(m3,1);
			  			m0=_mm256_add_epi8(m0,m1);
			  			m0=_mm256_add_epi8(m0,m2);
			  			output4=_mm256_add_epi8(m0,m3);


	  			//9th col iteration
				r0=_mm256_loadu_si256( (__m256i *) &temp[4]);

	  			//multiply by the mask
		  			m0=_mm256_maddubs_epi16(r0,cx0);

			  			//zero the 4th out of 16 elements
			  			m1=_mm256_and_si256(m0,mask_unz);
			  			//keep a copy of the 4th element
			  			m8=_mm256_and_si256(m0,mask_z);
						//m8=_mm256_slli_si256(m8,2);

			  			//make 16-bit 32-bit
						m1=_mm256_madd_epi16(m1,ones);

						//horizontal add
						m2=_mm256_srli_si256(m1,4);
			  			m2=_mm256_add_epi32(m1,m2);

			  			//keep only 4th and 7th out of 8 elements
			  			m1=_mm256_and_si256(m1,mask_32_1);
			  			m1=_mm256_add_epi32(m1,m8);

			  			//shift m1 and add with m2 (complex shift is needed)
			  			m3=_mm256_srli_si256(m1,8);
			  			m4=_mm256_and_si256(m1,mask_32_2);
			  			m4=_mm256_permute2f128_si256(m4,m4,1);
			  			m4=_mm256_slli_si256(m4,8);
			  			m3=_mm256_add_epi32(m3,m4);
			  			m2=_mm256_add_epi32(m2,m3);

			  			out1=_mm256_and_si256(m2,mask_32_a);

	  			//10th col iteration
				r0=_mm256_loadu_si256( (__m256i *) &temp[5]);

	  			//multiply by the mask
			  			m0=_mm256_maddubs_epi16(r0,cx0);

			  			//zero the 4th out of 16 elements
			  			m1=_mm256_and_si256(m0,mask_unz);
			  			//keep a copy of the 4th element
			  			m8=_mm256_and_si256(m0,mask_z);
						//m8=_mm256_slli_si256(m8,2);

			  			//make 16-bit 32-bit
						m1=_mm256_madd_epi16(m1,ones);

						//horizontal add
						m2=_mm256_srli_si256(m1,4);
			  			m2=_mm256_add_epi32(m1,m2);

			  			//keep only 4th and 7th out of 8 elements
			  			m1=_mm256_and_si256(m1,mask_32_1);
			  			m1=_mm256_add_epi32(m1,m8);

			  			//shift m1 and add with m2 (complex shift is needed)
			  			m3=_mm256_srli_si256(m1,8);
			  			m4=_mm256_and_si256(m1,mask_32_2);
			  			m4=_mm256_permute2f128_si256(m4,m4,1);
			  			m4=_mm256_slli_si256(m4,8);
			  			m3=_mm256_add_epi32(m3,m4);
			  			m2=_mm256_add_epi32(m2,m3);

			  			out2=_mm256_and_si256(m2,mask_32_a);//in 32-bit format keep only 0,2,5 and discard the others

			  			//merge out1 and out2 and then divide
			  			out2=_mm256_slli_si256(out2,4);
			  			out1=_mm256_add_epi32(out1,out2);
			  			out1 = division_32(division_case,out1,f);

			  			//elements must be re-arranged before store
			  			m0=_mm256_and_si256(out1,out_mask_1);
			  			m1=_mm256_and_si256(out1,out_mask_2);
			  			m1=_mm256_srli_si256(m1,3);
			  			m2=_mm256_and_si256(out1,out_mask_3);
			  			m2=_mm256_slli_si256(m2,2);
			  			m3=_mm256_and_si256(out1,out_mask_4);
			  			m3=_mm256_srli_si256(m3,1);
			  			m0=_mm256_add_epi8(m0,m1);
			  			m0=_mm256_add_epi8(m0,m2);
			  			output5=_mm256_add_epi8(m0,m3);

			  			//blend the output1:output5
			  			output2=_mm256_slli_si256(output2,2);
			  			output3=_mm256_slli_si256(output3,4);

			  			m3=_mm256_slli_si256(output4,6);
			  			m4=_mm256_and_si256(output4,out_mask_5);
			  			m4=_mm256_permute2f128_si256(m4,m4,1);
			  			m4=_mm256_srli_si256(m4,10);
			  			output4=_mm256_add_epi32(m3,m4);

			  			m3=_mm256_slli_si256(output5,8);
			  			m4=_mm256_and_si256(output5,out_mask_5);
			  			m4=_mm256_permute2f128_si256(m4,m4,1);
			  			m4=_mm256_srli_si256(m4,8);
			  			output5=_mm256_add_epi32(m3,m4);

			  			output1=_mm256_add_epi8(output1,output2);
			  			output1=_mm256_add_epi8(output1,output3);
			  			output1=_mm256_add_epi8(output1,output4);
			  			output1=_mm256_add_epi8(output1,output5);

	  			_mm256_storeu_si256( (__m256i *) &filt[row][0],output1 );

		  		  	}
		  else {

		  		// col iteration computes output pixels of   0,10,20
		  		// col+1 iteration computes output pixels of 1,11,21
		  		// col+2 iteration computes output pixels of 2,12,22
		  		// col+3 iteration computes output pixels of 3,13,23
		  		// col+4 iteration computes output pixels of 4,14,24
		  		// col+5 iteration computes output pixels of 5,15,25
		  		// col+6 iteration computes output pixels of 6,16,26
		  		// col+7 iteration computes output pixels of 7,17,27
		  		// col+8 iteration computes output pixels of 8,18,28
		  		// col+9 iteration computes output pixels of 9,19,29
		  		//afterwards, col becomes 30 and repeat the above process

		  			//1st col iteration
		  			//multiply by the mask
	  		        r0=_mm256_loadu_si256( (__m256i *) &temp[col-4]);
		  			m0=_mm256_maddubs_epi16(r0,cx0);

		  			//zero the 4th out of 16 elements
		  			m1=_mm256_and_si256(m0,mask_unz);
		  			//keep a copy of the 4th element
		  			m8=_mm256_and_si256(m0,mask_z);
					//m8=_mm256_slli_si256(m8,2);

		  			//make 16-bit 32-bit
					m1=_mm256_madd_epi16(m1,ones);

					//horizontal add
					m2=_mm256_srli_si256(m1,4);
		  			m2=_mm256_add_epi32(m1,m2);

		  			//keep only 4th and 7th out of 8 elements
		  			m1=_mm256_and_si256(m1,mask_32_1);
		  			m1=_mm256_add_epi32(m1,m8);

		  			//shift m1 and add with m2 (complex shift is needed)
		  			m3=_mm256_srli_si256(m1,8);
		  			m4=_mm256_and_si256(m1,mask_32_2);
		  			m4=_mm256_permute2f128_si256(m4,m4,1);
		  			m4=_mm256_slli_si256(m4,8);
		  			m3=_mm256_add_epi32(m3,m4);
		  			m2=_mm256_add_epi32(m2,m3);

		  			out1=_mm256_and_si256(m2,mask_32_a);//in 32-bit format keep only 0,2,5 and discard the others


		   			//2nd col iteration
	  		        r0=_mm256_loadu_si256( (__m256i *) &temp[col-3]);

		  			//multiply by the mask
		  			m0=_mm256_maddubs_epi16(r0,cx0);

		  			//zero the 4th out of 16 elements
		  			m1=_mm256_and_si256(m0,mask_unz);
		  			//keep a copy of the 4th element
		  			m8=_mm256_and_si256(m0,mask_z);
					//m8=_mm256_slli_si256(m8,2);

		  			//make 16-bit 32-bit
					m1=_mm256_madd_epi16(m1,ones);

					//horizontal add
					m2=_mm256_srli_si256(m1,4);
		  			m2=_mm256_add_epi32(m1,m2);

		  			//keep only 4th and 7th out of 8 elements
		  			m1=_mm256_and_si256(m1,mask_32_1);
		  			m1=_mm256_add_epi32(m1,m8);

		  			//shift m1 and add with m2 (complex shift is needed)
		  			m3=_mm256_srli_si256(m1,8);
		  			m4=_mm256_and_si256(m1,mask_32_2);
		  			m4=_mm256_permute2f128_si256(m4,m4,1);
		  			m4=_mm256_slli_si256(m4,8);
		  			m3=_mm256_add_epi32(m3,m4);
		  			m2=_mm256_add_epi32(m2,m3);

		  			out2=_mm256_and_si256(m2,mask_32_a);//in 32-bit format keep only 0,2,5 and discard the others

		  			//merge out1 and out2 and then divide
		  			out2=_mm256_slli_si256(out2,4);
		  			out1=_mm256_add_epi32(out1,out2);
		  			out1 = division_32(division_case,out1,f);

		  			//elements must be re-arranged before store
		  			m0=_mm256_and_si256(out1,out_mask_1);
		  			m1=_mm256_and_si256(out1,out_mask_2);
		  			m1=_mm256_srli_si256(m1,3);
		  			m2=_mm256_and_si256(out1,out_mask_3);
		  			m2=_mm256_slli_si256(m2,2);
		  			m3=_mm256_and_si256(out1,out_mask_4);
		  			m3=_mm256_srli_si256(m3,1);
		  			m0=_mm256_add_epi8(m0,m1);
		  			m0=_mm256_add_epi8(m0,m2);
		  			output1=_mm256_add_epi8(m0,m3);

		  			//3rd col iteration
	  		        r0=_mm256_loadu_si256( (__m256i *) &temp[col-2]);

		  			//multiply by the mask
		  			m0=_mm256_maddubs_epi16(r0,cx0);

		  			//zero the 4th out of 16 elements
		  			m1=_mm256_and_si256(m0,mask_unz);
		  			//keep a copy of the 4th element
		  			m8=_mm256_and_si256(m0,mask_z);
					//m8=_mm256_slli_si256(m8,2);

		  			//make 16-bit 32-bit
					m1=_mm256_madd_epi16(m1,ones);

					//horizontal add
					m2=_mm256_srli_si256(m1,4);
		  			m2=_mm256_add_epi32(m1,m2);

		  			//keep only 4th and 7th out of 8 elements
		  			m1=_mm256_and_si256(m1,mask_32_1);
		  			m1=_mm256_add_epi32(m1,m8);

		  			//shift m1 and add with m2 (complex shift is needed)
		  			m3=_mm256_srli_si256(m1,8);
		  			m4=_mm256_and_si256(m1,mask_32_2);
		  			m4=_mm256_permute2f128_si256(m4,m4,1);
		  			m4=_mm256_slli_si256(m4,8);
		  			m3=_mm256_add_epi32(m3,m4);
		  			m2=_mm256_add_epi32(m2,m3);

		  			out1=_mm256_and_si256(m2,mask_32_a);//in 32-bit format keep only 0,2,5 and discard the others

		  			//4th col iteration
	  		        r0=_mm256_loadu_si256( (__m256i *) &temp[col-1]);

		  			//multiply by the mask
		  			m0=_mm256_maddubs_epi16(r0,cx0);

		  			//zero the 4th out of 16 elements
		  			m1=_mm256_and_si256(m0,mask_unz);
		  			//keep a copy of the 4th element
		  			m8=_mm256_and_si256(m0,mask_z);
					//m8=_mm256_slli_si256(m8,2);

		  			//make 16-bit 32-bit
					m1=_mm256_madd_epi16(m1,ones);

					//horizontal add
					m2=_mm256_srli_si256(m1,4);
		  			m2=_mm256_add_epi32(m1,m2);

		  			//keep only 4th and 7th out of 8 elements
		  			m1=_mm256_and_si256(m1,mask_32_1);
		  			m1=_mm256_add_epi32(m1,m8);

		  			//shift m1 and add with m2 (complex shift is needed)
		  			m3=_mm256_srli_si256(m1,8);
		  			m4=_mm256_and_si256(m1,mask_32_2);
		  			m4=_mm256_permute2f128_si256(m4,m4,1);
		  			m4=_mm256_slli_si256(m4,8);
		  			m3=_mm256_add_epi32(m3,m4);
		  			m2=_mm256_add_epi32(m2,m3);

		  			out2=_mm256_and_si256(m2,mask_32_a);//in 32-bit format keep only 0,2,5 and discard the others

		  			//merge out1 and out2 and then divide
		  			out2=_mm256_slli_si256(out2,4);
		  			out1=_mm256_add_epi32(out1,out2);
		  			out1 = division_32(division_case,out1,f);

		  			//elements must be re-arranged before store
		  			m0=_mm256_and_si256(out1,out_mask_1);
		  			m1=_mm256_and_si256(out1,out_mask_2);
		  			m1=_mm256_srli_si256(m1,3);
		  			m2=_mm256_and_si256(out1,out_mask_3);
		  			m2=_mm256_slli_si256(m2,2);
		  			m3=_mm256_and_si256(out1,out_mask_4);
		  			m3=_mm256_srli_si256(m3,1);
		  			m0=_mm256_add_epi8(m0,m1);
		  			m0=_mm256_add_epi8(m0,m2);
		  			output2=_mm256_add_epi8(m0,m3);

		  			//5th col iteration
	  		        r0=_mm256_loadu_si256( (__m256i *) &temp[col]);

		  			//multiply by the mask
		  			m0=_mm256_maddubs_epi16(r0,cx0);

		  			//zero the 4th out of 16 elements
		  			m1=_mm256_and_si256(m0,mask_unz);
		  			//keep a copy of the 4th element
		  			m8=_mm256_and_si256(m0,mask_z);
					//m8=_mm256_slli_si256(m8,2);

		  			//make 16-bit 32-bit
					m1=_mm256_madd_epi16(m1,ones);

					//horizontal add
					m2=_mm256_srli_si256(m1,4);
		  			m2=_mm256_add_epi32(m1,m2);

		  			//keep only 4th and 7th out of 8 elements
		  			m1=_mm256_and_si256(m1,mask_32_1);
		  			m1=_mm256_add_epi32(m1,m8);

		  			//shift m1 and add with m2 (complex shift is needed)
		  			m3=_mm256_srli_si256(m1,8);
		  			m4=_mm256_and_si256(m1,mask_32_2);
		  			m4=_mm256_permute2f128_si256(m4,m4,1);
		  			m4=_mm256_slli_si256(m4,8);
		  			m3=_mm256_add_epi32(m3,m4);
		  			m2=_mm256_add_epi32(m2,m3);

		  			out1=_mm256_and_si256(m2,mask_32_a);//in 32-bit format keep only 0,2,5 and discard the others

		  			//6th col iteration
	  		        r0=_mm256_loadu_si256( (__m256i *) &temp[col+1]);

		  			//multiply by the mask
		  			m0=_mm256_maddubs_epi16(r0,cx0);

		  			//zero the 4th out of 16 elements
		  			m1=_mm256_and_si256(m0,mask_unz);
		  			//keep a copy of the 4th element
		  			m8=_mm256_and_si256(m0,mask_z);
					//m8=_mm256_slli_si256(m8,2);

		  			//make 16-bit 32-bit
					m1=_mm256_madd_epi16(m1,ones);

					//horizontal add
					m2=_mm256_srli_si256(m1,4);
		  			m2=_mm256_add_epi32(m1,m2);

		  			//keep only 4th and 7th out of 8 elements
		  			m1=_mm256_and_si256(m1,mask_32_1);
		  			m1=_mm256_add_epi32(m1,m8);

		  			//shift m1 and add with m2 (complex shift is needed)
		  			m3=_mm256_srli_si256(m1,8);
		  			m4=_mm256_and_si256(m1,mask_32_2);
		  			m4=_mm256_permute2f128_si256(m4,m4,1);
		  			m4=_mm256_slli_si256(m4,8);
		  			m3=_mm256_add_epi32(m3,m4);
		  			m2=_mm256_add_epi32(m2,m3);

		  			out2=_mm256_and_si256(m2,mask_32_a);//in 32-bit format keep only 0,2,5 and discard the others

		  			//merge out1 and out2 and then divide
		  			out2=_mm256_slli_si256(out2,4);
		  			out1=_mm256_add_epi32(out1,out2);
		  			out1 = division_32(division_case,out1,f);

		  			//elements must be re-arranged before store
		  			m0=_mm256_and_si256(out1,out_mask_1);
		  			m1=_mm256_and_si256(out1,out_mask_2);
		  			m1=_mm256_srli_si256(m1,3);
		  			m2=_mm256_and_si256(out1,out_mask_3);
		  			m2=_mm256_slli_si256(m2,2);
		  			m3=_mm256_and_si256(out1,out_mask_4);
		  			m3=_mm256_srli_si256(m3,1);
		  			m0=_mm256_add_epi8(m0,m1);
		  			m0=_mm256_add_epi8(m0,m2);
		  			output3=_mm256_add_epi8(m0,m3);

		  			//7th col iteration
	  		        r0=_mm256_loadu_si256( (__m256i *) &temp[col+2]);

		  			//multiply by the mask
		  			m0=_mm256_maddubs_epi16(r0,cx0);

		  			//zero the 4th out of 16 elements
		  			m1=_mm256_and_si256(m0,mask_unz);
		  			//keep a copy of the 4th element
		  			m8=_mm256_and_si256(m0,mask_z);
					//m8=_mm256_slli_si256(m8,2);

		  			//make 16-bit 32-bit
					m1=_mm256_madd_epi16(m1,ones);

					//horizontal add
					m2=_mm256_srli_si256(m1,4);
		  			m2=_mm256_add_epi32(m1,m2);

		  			//keep only 4th and 7th out of 8 elements
		  			m1=_mm256_and_si256(m1,mask_32_1);
		  			m1=_mm256_add_epi32(m1,m8);

		  			//shift m1 and add with m2 (complex shift is needed)
		  			m3=_mm256_srli_si256(m1,8);
		  			m4=_mm256_and_si256(m1,mask_32_2);
		  			m4=_mm256_permute2f128_si256(m4,m4,1);
		  			m4=_mm256_slli_si256(m4,8);
		  			m3=_mm256_add_epi32(m3,m4);
		  			m2=_mm256_add_epi32(m2,m3);

		  			out1=_mm256_and_si256(m2,mask_32_a);//in 32-bit format keep only 0,2,5 and discard the others

		  			//8th col iteration
	  		        r0=_mm256_loadu_si256( (__m256i *) &temp[col+3]);

		  			//multiply by the mask
		  			m0=_mm256_maddubs_epi16(r0,cx0);

		  			//zero the 4th out of 16 elements
		  			m1=_mm256_and_si256(m0,mask_unz);
		  			//keep a copy of the 4th element
		  			m8=_mm256_and_si256(m0,mask_z);
					//m8=_mm256_slli_si256(m8,2);

		  			//make 16-bit 32-bit
					m1=_mm256_madd_epi16(m1,ones);

					//horizontal add
					m2=_mm256_srli_si256(m1,4);
		  			m2=_mm256_add_epi32(m1,m2);

		  			//keep only 4th and 7th out of 8 elements
		  			m1=_mm256_and_si256(m1,mask_32_1);
		  			m1=_mm256_add_epi32(m1,m8);

		  			//shift m1 and add with m2 (complex shift is needed)
		  			m3=_mm256_srli_si256(m1,8);
		  			m4=_mm256_and_si256(m1,mask_32_2);
		  			m4=_mm256_permute2f128_si256(m4,m4,1);
		  			m4=_mm256_slli_si256(m4,8);
		  			m3=_mm256_add_epi32(m3,m4);
		  			m2=_mm256_add_epi32(m2,m3);

		  			out2=_mm256_and_si256(m2,mask_32_a);//in 32-bit format keep only 0,2,5 and discard the others

		  			//merge out1 and out2 and then divide
		  			out2=_mm256_slli_si256(out2,4);
		  			out1=_mm256_add_epi32(out1,out2);
		  			out1 = division_32(division_case,out1,f);

		  			//elements must be re-arranged before store
		  			m0=_mm256_and_si256(out1,out_mask_1);
		  			m1=_mm256_and_si256(out1,out_mask_2);
		  			m1=_mm256_srli_si256(m1,3);
		  			m2=_mm256_and_si256(out1,out_mask_3);
		  			m2=_mm256_slli_si256(m2,2);
		  			m3=_mm256_and_si256(out1,out_mask_4);
		  			m3=_mm256_srli_si256(m3,1);
		  			m0=_mm256_add_epi8(m0,m1);
		  			m0=_mm256_add_epi8(m0,m2);
		  			output4=_mm256_add_epi8(m0,m3);

		  			//9th col iteration
	  		        r0=_mm256_loadu_si256( (__m256i *) &temp[col+4]);

		  			//multiply by the mask
		  			m0=_mm256_maddubs_epi16(r0,cx0);

		  			//zero the 4th out of 16 elements
		  			m1=_mm256_and_si256(m0,mask_unz);
		  			//keep a copy of the 4th element
		  			m8=_mm256_and_si256(m0,mask_z);
					//m8=_mm256_slli_si256(m8,2);

		  			//make 16-bit 32-bit
					m1=_mm256_madd_epi16(m1,ones);

					//horizontal add
					m2=_mm256_srli_si256(m1,4);
		  			m2=_mm256_add_epi32(m1,m2);

		  			//keep only 4th and 7th out of 8 elements
		  			m1=_mm256_and_si256(m1,mask_32_1);
		  			m1=_mm256_add_epi32(m1,m8);

		  			//shift m1 and add with m2 (complex shift is needed)
		  			m3=_mm256_srli_si256(m1,8);
		  			m4=_mm256_and_si256(m1,mask_32_2);
		  			m4=_mm256_permute2f128_si256(m4,m4,1);
		  			m4=_mm256_slli_si256(m4,8);
		  			m3=_mm256_add_epi32(m3,m4);
		  			m2=_mm256_add_epi32(m2,m3);

		  			out1=_mm256_and_si256(m2,mask_32_a);//in 32-bit format keep only 0,2,5 and discard the others

		  			//10th col iteration
	  		        r0=_mm256_loadu_si256( (__m256i *) &temp[col+5]);

		  			//multiply by the mask
		  			m0=_mm256_maddubs_epi16(r0,cx0);

		  			//zero the 4th out of 16 elements
		  			m1=_mm256_and_si256(m0,mask_unz);
		  			//keep a copy of the 4th element
		  			m8=_mm256_and_si256(m0,mask_z);
					//m8=_mm256_slli_si256(m8,2);

		  			//make 16-bit 32-bit
					m1=_mm256_madd_epi16(m1,ones);

					//horizontal add
					m2=_mm256_srli_si256(m1,4);
		  			m2=_mm256_add_epi32(m1,m2);

		  			//keep only 4th and 7th out of 8 elements
		  			m1=_mm256_and_si256(m1,mask_32_1);
		  			m1=_mm256_add_epi32(m1,m8);

		  			//shift m1 and add with m2 (complex shift is needed)
		  			m3=_mm256_srli_si256(m1,8);
		  			m4=_mm256_and_si256(m1,mask_32_2);
		  			m4=_mm256_permute2f128_si256(m4,m4,1);
		  			m4=_mm256_slli_si256(m4,8);
		  			m3=_mm256_add_epi32(m3,m4);
		  			m2=_mm256_add_epi32(m2,m3);

		  			out2=_mm256_and_si256(m2,mask_32_a);//in 32-bit format keep only 0,2,5 and discard the others

		  			//merge out1 and out2 and then divide
		  			out2=_mm256_slli_si256(out2,4);
		  			out1=_mm256_add_epi32(out1,out2);
		  			out1 = division_32(division_case,out1,f);

		  			//elements must be re-arranged before store
		  			m0=_mm256_and_si256(out1,out_mask_1);
		  			m1=_mm256_and_si256(out1,out_mask_2);
		  			m1=_mm256_srli_si256(m1,3);
		  			m2=_mm256_and_si256(out1,out_mask_3);
		  			m2=_mm256_slli_si256(m2,2);
		  			m3=_mm256_and_si256(out1,out_mask_4);
		  			m3=_mm256_srli_si256(m3,1);
		  			m0=_mm256_add_epi8(m0,m1);
		  			m0=_mm256_add_epi8(m0,m2);
		  			output5=_mm256_add_epi8(m0,m3);

		  			//blend the output1:output5
		  			output2=_mm256_slli_si256(output2,2);
		  			output3=_mm256_slli_si256(output3,4);

		  			m3=_mm256_slli_si256(output4,6);
		  			m4=_mm256_and_si256(output4,out_mask_5);
		  			m4=_mm256_permute2f128_si256(m4,m4,1);
		  			m4=_mm256_srli_si256(m4,10);
		  			output4=_mm256_add_epi32(m3,m4);

		  			m3=_mm256_slli_si256(output5,8);
		  			m4=_mm256_and_si256(output5,out_mask_5);
		  			m4=_mm256_permute2f128_si256(m4,m4,1);
		  			m4=_mm256_srli_si256(m4,8);
		  			output5=_mm256_add_epi32(m3,m4);

		  			output1=_mm256_add_epi8(output1,output2);
		  			output1=_mm256_add_epi8(output1,output3);
		  			output1=_mm256_add_epi8(output1,output4);
		  			output1=_mm256_add_epi8(output1,output5);

		  			_mm256_storeu_si256( (__m256i *) &filt[row][col],output1 );

		  		 }

		}

	loop_reminder_9x9_32_blur_X(frame1,filt,temp,N,M,row, col, REMINDER_ITERATIONS_X, division_case, mask_vector_x, f, divisor_xy,kernel_x);


}



void loop_reminder_9x9_32_blur_Y(unsigned char **frame1,unsigned char **filt,unsigned char *temp,const unsigned int N,const unsigned int M,const unsigned int row, const unsigned int REMINDER_ITERATIONS_Y,const unsigned int division_case,const signed char mask_vector_y[][32], const __m256i f, const unsigned short int divisor_xy){

	__m256i r0,r1,r2,r3,r4,r5,r6,r7,r8,m0,m1,m2,m3,m4,m5,m6,m7,m8,even,odd,reminder_mask;

	const __m256i mask_even_16  = _mm256_set_epi16(0,0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff);
	const __m256i mask_odd_16  = _mm256_set_epi16(0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff,0);
	const __m256i ones=_mm256_set1_epi16(1);

 if (REMINDER_ITERATIONS_Y==0){//no need for computing the y mask
		_mm256_store_si256( (__m256i *) &temp[M],_mm256_setzero_si256() );
        for (unsigned int g=M+32;g<M+32+6;g++)
        	temp[g]=0;
	}
 else {
		__m256i cy0,cy1,cy2,cy3,cy4,cy5,cy6,cy7,cy8,cy0_sh1,cy1_sh1,cy2_sh1,cy3_sh1,cy4_sh1,cy5_sh1,cy6_sh1,cy7_sh1,cy8_sh1;

	unsigned int col2=M-REMINDER_ITERATIONS_Y;//this is the col index to load the elements for the y mask

	if ((row>3) && (row<N-4)){//main case

		r0=_mm256_loadu_si256( (__m256i *) &frame1[row-4][col2]);
		r1=_mm256_loadu_si256( (__m256i *) &frame1[row-3][col2]);
		r2=_mm256_loadu_si256( (__m256i *) &frame1[row-2][col2]);
		r3=_mm256_loadu_si256( (__m256i *) &frame1[row-1][col2]);
		r4=_mm256_loadu_si256( (__m256i *) &frame1[row][col2]);
		r5=_mm256_loadu_si256( (__m256i *) &frame1[row+1][col2]);
		r6=_mm256_loadu_si256( (__m256i *) &frame1[row+2][col2]);
		r7=_mm256_loadu_si256( (__m256i *) &frame1[row+3][col2]);
		r8=_mm256_loadu_si256( (__m256i *) &frame1[row+4][col2]);

		cy0=_mm256_load_si256( (__m256i *) &mask_vector_y[0]);
		cy1=_mm256_load_si256( (__m256i *) &mask_vector_y[1]);
		cy2=_mm256_load_si256( (__m256i *) &mask_vector_y[2]);
		cy3=_mm256_load_si256( (__m256i *) &mask_vector_y[3]);
		cy4=_mm256_load_si256( (__m256i *) &mask_vector_y[4]);
		cy5=_mm256_load_si256( (__m256i *) &mask_vector_y[5]);
		cy6=_mm256_load_si256( (__m256i *) &mask_vector_y[6]);
		cy7=_mm256_load_si256( (__m256i *) &mask_vector_y[7]);
		cy8=_mm256_load_si256( (__m256i *) &mask_vector_y[8]);

		cy0_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[9]);
		cy1_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[10]);
		cy2_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[11]);
		cy3_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[12]);
		cy4_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[13]);
		cy5_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[14]);
		cy6_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[15]);
		cy7_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[16]);
		cy8_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[17]);

	}
	else {
		if (row==0){
			r0=_mm256_loadu_si256( (__m256i *) &frame1[0][col2]);
			r1=_mm256_loadu_si256( (__m256i *) &frame1[1][col2]);
			r2=_mm256_loadu_si256( (__m256i *) &frame1[2][col2]);
			r3=_mm256_loadu_si256( (__m256i *) &frame1[3][col2]);
			r4=_mm256_loadu_si256( (__m256i *) &frame1[4][col2]);
			r5=_mm256_setzero_si256();
			r6=_mm256_setzero_si256();
			r7=_mm256_setzero_si256();
			r8=_mm256_setzero_si256();

			cy0=_mm256_load_si256( (__m256i *) &mask_vector_y[4]);
			cy1=_mm256_load_si256( (__m256i *) &mask_vector_y[5]);
			cy2=_mm256_load_si256( (__m256i *) &mask_vector_y[6]);
			cy3=_mm256_load_si256( (__m256i *) &mask_vector_y[7]);
			cy4=_mm256_load_si256( (__m256i *) &mask_vector_y[8]);
			cy5=_mm256_setzero_si256();
			cy6=_mm256_setzero_si256();
			cy7=_mm256_setzero_si256();
			cy8=_mm256_setzero_si256();

			cy0_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[13]);
			cy1_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[14]);
			cy2_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[15]);
			cy3_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[16]);
			cy4_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[17]);
			cy5_sh1=_mm256_setzero_si256();
			cy6_sh1=_mm256_setzero_si256();
			cy7_sh1=_mm256_setzero_si256();
			cy8_sh1=_mm256_setzero_si256();

		}
		else if (row==1){
			r0=_mm256_loadu_si256( (__m256i *) &frame1[0][col2]);
			r1=_mm256_loadu_si256( (__m256i *) &frame1[1][col2]);
			r2=_mm256_loadu_si256( (__m256i *) &frame1[2][col2]);
			r3=_mm256_loadu_si256( (__m256i *) &frame1[3][col2]);
			r4=_mm256_loadu_si256( (__m256i *) &frame1[4][col2]);
			r5=_mm256_loadu_si256( (__m256i *) &frame1[5][col2]);
			r6=_mm256_setzero_si256();
			r7=_mm256_setzero_si256();
			r8=_mm256_setzero_si256();

			cy0=_mm256_load_si256( (__m256i *) &mask_vector_y[3]);
			cy1=_mm256_load_si256( (__m256i *) &mask_vector_y[4]);
			cy2=_mm256_load_si256( (__m256i *) &mask_vector_y[5]);
			cy3=_mm256_load_si256( (__m256i *) &mask_vector_y[6]);
			cy4=_mm256_load_si256( (__m256i *) &mask_vector_y[7]);
			cy5=_mm256_load_si256( (__m256i *) &mask_vector_y[8]);
			cy6=_mm256_setzero_si256();
			cy7=_mm256_setzero_si256();
			cy8=_mm256_setzero_si256();

			cy0_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[12]);
			cy1_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[13]);
			cy2_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[14]);
			cy3_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[15]);
			cy4_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[16]);
			cy5_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[17]);
			cy6_sh1=_mm256_setzero_si256();
			cy7_sh1=_mm256_setzero_si256();
			cy8_sh1=_mm256_setzero_si256();
		}
		else if (row==2){
			r0=_mm256_loadu_si256( (__m256i *) &frame1[0][col2]);
			r1=_mm256_loadu_si256( (__m256i *) &frame1[1][col2]);
			r2=_mm256_loadu_si256( (__m256i *) &frame1[2][col2]);
			r3=_mm256_loadu_si256( (__m256i *) &frame1[3][col2]);
			r4=_mm256_loadu_si256( (__m256i *) &frame1[4][col2]);
			r5=_mm256_loadu_si256( (__m256i *) &frame1[5][col2]);
			r6=_mm256_loadu_si256( (__m256i *) &frame1[6][col2]);
			r7=_mm256_setzero_si256();
			r8=_mm256_setzero_si256();

			cy0=_mm256_load_si256( (__m256i *) &mask_vector_y[2]);
			cy1=_mm256_load_si256( (__m256i *) &mask_vector_y[3]);
			cy2=_mm256_load_si256( (__m256i *) &mask_vector_y[4]);
			cy3=_mm256_load_si256( (__m256i *) &mask_vector_y[5]);
			cy4=_mm256_load_si256( (__m256i *) &mask_vector_y[6]);
			cy5=_mm256_load_si256( (__m256i *) &mask_vector_y[7]);
			cy6=_mm256_load_si256( (__m256i *) &mask_vector_y[8]);
			cy7=_mm256_setzero_si256();
			cy8=_mm256_setzero_si256();

			cy0_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[11]);
			cy1_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[12]);
			cy2_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[13]);
			cy3_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[14]);
			cy4_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[15]);
			cy5_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[16]);
			cy6_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[17]);
			cy7_sh1=_mm256_setzero_si256();
			cy8_sh1=_mm256_setzero_si256();
		}
		else if (row==3){
			r0=_mm256_loadu_si256( (__m256i *) &frame1[0][col2]);
			r1=_mm256_loadu_si256( (__m256i *) &frame1[1][col2]);
			r2=_mm256_loadu_si256( (__m256i *) &frame1[2][col2]);
			r3=_mm256_loadu_si256( (__m256i *) &frame1[3][col2]);
			r4=_mm256_loadu_si256( (__m256i *) &frame1[4][col2]);
			r5=_mm256_loadu_si256( (__m256i *) &frame1[5][col2]);
			r6=_mm256_loadu_si256( (__m256i *) &frame1[6][col2]);
			r7=_mm256_loadu_si256( (__m256i *) &frame1[7][col2]);
			r8=_mm256_setzero_si256();

			cy0=_mm256_load_si256( (__m256i *) &mask_vector_y[1]);
			cy1=_mm256_load_si256( (__m256i *) &mask_vector_y[2]);
			cy2=_mm256_load_si256( (__m256i *) &mask_vector_y[3]);
			cy3=_mm256_load_si256( (__m256i *) &mask_vector_y[4]);
			cy4=_mm256_load_si256( (__m256i *) &mask_vector_y[5]);
			cy5=_mm256_load_si256( (__m256i *) &mask_vector_y[6]);
			cy6=_mm256_load_si256( (__m256i *) &mask_vector_y[7]);
			cy7=_mm256_load_si256( (__m256i *) &mask_vector_y[8]);
			cy8=_mm256_setzero_si256();

			cy0_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[10]);
			cy1_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[11]);
			cy2_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[12]);
			cy3_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[13]);
			cy4_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[14]);
			cy5_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[15]);
			cy6_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[16]);
			cy7_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[17]);
			cy8_sh1=_mm256_setzero_si256();
		}
		else if (row==N-4){
			r0=_mm256_setzero_si256();
			r1=_mm256_loadu_si256( (__m256i *) &frame1[N-8][col2]);
			r2=_mm256_loadu_si256( (__m256i *) &frame1[N-7][col2]);
			r3=_mm256_loadu_si256( (__m256i *) &frame1[N-6][col2]);
			r4=_mm256_loadu_si256( (__m256i *) &frame1[N-5][col2]);
			r5=_mm256_loadu_si256( (__m256i *) &frame1[N-4][col2]);
			r6=_mm256_loadu_si256( (__m256i *) &frame1[N-3][col2]);
			r7=_mm256_loadu_si256( (__m256i *) &frame1[N-2][col2]);
			r8=_mm256_loadu_si256( (__m256i *) &frame1[N-1][col2]);

			cy0=_mm256_setzero_si256();
			cy1=_mm256_load_si256( (__m256i *) &mask_vector_y[0]);
			cy2=_mm256_load_si256( (__m256i *) &mask_vector_y[1]);
			cy3=_mm256_load_si256( (__m256i *) &mask_vector_y[2]);
			cy4=_mm256_load_si256( (__m256i *) &mask_vector_y[3]);
			cy5=_mm256_load_si256( (__m256i *) &mask_vector_y[4]);
			cy6=_mm256_load_si256( (__m256i *) &mask_vector_y[5]);
			cy7=_mm256_load_si256( (__m256i *) &mask_vector_y[6]);
			cy8=_mm256_load_si256( (__m256i *) &mask_vector_y[7]);

			cy0_sh1=_mm256_setzero_si256();
			cy1_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[9]);
			cy2_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[10]);
			cy3_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[11]);
			cy4_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[12]);
			cy5_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[13]);
			cy6_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[14]);
			cy7_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[15]);
			cy8_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[16]);

		}
		else if (row==N-3){
			r0=_mm256_setzero_si256();
			r1=_mm256_setzero_si256();
			r2=_mm256_loadu_si256( (__m256i *) &frame1[N-7][col2]);
			r3=_mm256_loadu_si256( (__m256i *) &frame1[N-6][col2]);
			r4=_mm256_loadu_si256( (__m256i *) &frame1[N-5][col2]);
			r5=_mm256_loadu_si256( (__m256i *) &frame1[N-4][col2]);
			r6=_mm256_loadu_si256( (__m256i *) &frame1[N-3][col2]);
			r7=_mm256_loadu_si256( (__m256i *) &frame1[N-2][col2]);
			r8=_mm256_loadu_si256( (__m256i *) &frame1[N-1][col2]);

			cy0=_mm256_setzero_si256();
			cy1=_mm256_setzero_si256();
			cy2=_mm256_load_si256( (__m256i *) &mask_vector_y[0]);
			cy3=_mm256_load_si256( (__m256i *) &mask_vector_y[1]);
			cy4=_mm256_load_si256( (__m256i *) &mask_vector_y[2]);
			cy5=_mm256_load_si256( (__m256i *) &mask_vector_y[3]);
			cy6=_mm256_load_si256( (__m256i *) &mask_vector_y[4]);
			cy7=_mm256_load_si256( (__m256i *) &mask_vector_y[5]);
			cy8=_mm256_load_si256( (__m256i *) &mask_vector_y[6]);

			cy0_sh1=_mm256_setzero_si256();
			cy1_sh1=_mm256_setzero_si256();
			cy2_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[9]);
			cy3_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[10]);
			cy4_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[11]);
			cy5_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[12]);
			cy6_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[13]);
			cy7_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[14]);
			cy8_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[15]);
		}
		else if (row==N-2){
			r0=_mm256_setzero_si256();
			r1=_mm256_setzero_si256();
			r2=_mm256_setzero_si256();
			r3=_mm256_loadu_si256( (__m256i *) &frame1[N-6][col2]);
			r4=_mm256_loadu_si256( (__m256i *) &frame1[N-5][col2]);
			r5=_mm256_loadu_si256( (__m256i *) &frame1[N-4][col2]);
			r6=_mm256_loadu_si256( (__m256i *) &frame1[N-3][col2]);
			r7=_mm256_loadu_si256( (__m256i *) &frame1[N-2][col2]);
			r8=_mm256_loadu_si256( (__m256i *) &frame1[N-1][col2]);

			cy0=_mm256_setzero_si256();
			cy1=_mm256_setzero_si256();
			cy2=_mm256_setzero_si256();
			cy3=_mm256_load_si256( (__m256i *) &mask_vector_y[0]);
			cy4=_mm256_load_si256( (__m256i *) &mask_vector_y[1]);
			cy5=_mm256_load_si256( (__m256i *) &mask_vector_y[2]);
			cy6=_mm256_load_si256( (__m256i *) &mask_vector_y[3]);
			cy7=_mm256_load_si256( (__m256i *) &mask_vector_y[4]);
			cy8=_mm256_load_si256( (__m256i *) &mask_vector_y[5]);

			cy0_sh1=_mm256_setzero_si256();
			cy1_sh1=_mm256_setzero_si256();
			cy2_sh1=_mm256_setzero_si256();
			cy3_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[9]);
			cy4_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[10]);
			cy5_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[11]);
			cy6_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[12]);
			cy7_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[13]);
			cy8_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[14]);
		}
		else if (row==N-1){
			r0=_mm256_setzero_si256();
			r1=_mm256_setzero_si256();
			r2=_mm256_setzero_si256();
			r3=_mm256_setzero_si256();
			r4=_mm256_loadu_si256( (__m256i *) &frame1[N-5][col2]);
			r5=_mm256_loadu_si256( (__m256i *) &frame1[N-4][col2]);
			r6=_mm256_loadu_si256( (__m256i *) &frame1[N-3][col2]);
			r7=_mm256_loadu_si256( (__m256i *) &frame1[N-2][col2]);
			r8=_mm256_loadu_si256( (__m256i *) &frame1[N-1][col2]);

			cy0=_mm256_setzero_si256();
			cy1=_mm256_setzero_si256();
			cy2=_mm256_setzero_si256();
			cy3=_mm256_setzero_si256();
			cy4=_mm256_load_si256( (__m256i *) &mask_vector_y[0]);
			cy5=_mm256_load_si256( (__m256i *) &mask_vector_y[1]);
			cy6=_mm256_load_si256( (__m256i *) &mask_vector_y[2]);
			cy7=_mm256_load_si256( (__m256i *) &mask_vector_y[3]);
			cy8=_mm256_load_si256( (__m256i *) &mask_vector_y[4]);

			cy0_sh1=_mm256_setzero_si256();
			cy1_sh1=_mm256_setzero_si256();
			cy2_sh1=_mm256_setzero_si256();
			cy3_sh1=_mm256_setzero_si256();
			cy4_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[9]);
			cy5_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[10]);
			cy6_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[11]);
			cy7_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[12]);
			cy8_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[13]);
		}
		else {
			printf("\nsomething went wrong");
			exit(EXIT_FAILURE);
		}
	}


	/*
	 * The above load operations load outside of the array bounds; these elements are filled with zeros just after
	 */
	reminder_mask=_mm256_load_si256( (__m256i *) &reminder_mask_9x9[REMINDER_ITERATIONS_Y-1][0]);

	//AND r0-r4 with reminder_mask
	r0=_mm256_and_si256(r0,reminder_mask);
	r1=_mm256_and_si256(r1,reminder_mask);
	r2=_mm256_and_si256(r2,reminder_mask);
	r3=_mm256_and_si256(r3,reminder_mask);
	r4=_mm256_and_si256(r4,reminder_mask);
	r5=_mm256_and_si256(r5,reminder_mask);
	r6=_mm256_and_si256(r6,reminder_mask);
	r7=_mm256_and_si256(r7,reminder_mask);
	r8=_mm256_and_si256(r8,reminder_mask);


//--------------------y filter begins--------------------

	//multiply by the mask
	m0=_mm256_maddubs_epi16(r0,cy0);
	m1=_mm256_maddubs_epi16(r1,cy1);
	m2=_mm256_maddubs_epi16(r2,cy2);
	m3=_mm256_maddubs_epi16(r3,cy3);
	m4=_mm256_maddubs_epi16(r4,cy4);
	m5=_mm256_maddubs_epi16(r5,cy5);
	m6=_mm256_maddubs_epi16(r6,cy6);
	m7=_mm256_maddubs_epi16(r7,cy7);
	m8=_mm256_maddubs_epi16(r8,cy8);

	//discard odd
__m256i m0_even=_mm256_and_si256(m0,mask_even_16);
__m256i	m1_even=_mm256_and_si256(m1,mask_even_16);
__m256i	m2_even=_mm256_and_si256(m2,mask_even_16);
__m256i	m3_even=_mm256_and_si256(m3,mask_even_16);
__m256i	m4_even=_mm256_and_si256(m4,mask_even_16);
__m256i	m5_even=_mm256_and_si256(m5,mask_even_16);
__m256i	m6_even=_mm256_and_si256(m6,mask_even_16);
__m256i	m7_even=_mm256_and_si256(m7,mask_even_16);
__m256i	m8_even=_mm256_and_si256(m8,mask_even_16);


//make from 16-bit to 32bit
m0_even=_mm256_madd_epi16(m0_even,ones);
m1_even=_mm256_madd_epi16(m1_even,ones);
m2_even=_mm256_madd_epi16(m2_even,ones);
m3_even=_mm256_madd_epi16(m3_even,ones);
m4_even=_mm256_madd_epi16(m4_even,ones);
m5_even=_mm256_madd_epi16(m5_even,ones);
m6_even=_mm256_madd_epi16(m6_even,ones);
m7_even=_mm256_madd_epi16(m7_even,ones);
m8_even=_mm256_madd_epi16(m8_even,ones);

	//vertical 32-bit add
	m0_even=_mm256_add_epi32(m0_even,m1_even);
	m0_even=_mm256_add_epi32(m0_even,m2_even);
	m0_even=_mm256_add_epi32(m0_even,m3_even);
	m0_even=_mm256_add_epi32(m0_even,m4_even);
	m0_even=_mm256_add_epi32(m0_even,m5_even);
	m0_even=_mm256_add_epi32(m0_even,m6_even);
	m0_even=_mm256_add_epi32(m0_even,m7_even);
	m0_even=_mm256_add_epi32(m0_even,m8_even);


	//discard even
__m256i	m0_odd=_mm256_and_si256(m0,mask_odd_16);
__m256i	m1_odd=_mm256_and_si256(m1,mask_odd_16);
__m256i	m2_odd=_mm256_and_si256(m2,mask_odd_16);
__m256i	m3_odd=_mm256_and_si256(m3,mask_odd_16);
__m256i	m4_odd=_mm256_and_si256(m4,mask_odd_16);
__m256i	m5_odd=_mm256_and_si256(m5,mask_odd_16);
__m256i	m6_odd=_mm256_and_si256(m6,mask_odd_16);
__m256i	m7_odd=_mm256_and_si256(m7,mask_odd_16);
__m256i	m8_odd=_mm256_and_si256(m8,mask_odd_16);


//make from 16-bit to 32bit
	m0_odd=_mm256_madd_epi16(m0_odd,ones);
	m1_odd=_mm256_madd_epi16(m1_odd,ones);
	m2_odd=_mm256_madd_epi16(m2_odd,ones);
	m3_odd=_mm256_madd_epi16(m3_odd,ones);
	m4_odd=_mm256_madd_epi16(m4_odd,ones);
	m5_odd=_mm256_madd_epi16(m5_odd,ones);
	m6_odd=_mm256_madd_epi16(m6_odd,ones);
	m7_odd=_mm256_madd_epi16(m7_odd,ones);
	m8_odd=_mm256_madd_epi16(m8_odd,ones);

	//vertical 32-bit add
	m0_odd=_mm256_add_epi32(m0_odd,m1_odd);
	m0_odd=_mm256_add_epi32(m0_odd,m2_odd);
	m0_odd=_mm256_add_epi32(m0_odd,m3_odd);
	m0_odd=_mm256_add_epi32(m0_odd,m4_odd);
	m0_odd=_mm256_add_epi32(m0_odd,m5_odd);
	m0_odd=_mm256_add_epi32(m0_odd,m6_odd);
	m0_odd=_mm256_add_epi32(m0_odd,m7_odd);
	m0_odd=_mm256_add_epi32(m0_odd,m8_odd);

	m0_even=division_32(division_case,m0_even,f);//even results
	m0_odd=division_32(division_case,m0_odd,f);//even results

	m0_odd=_mm256_slli_si256(m0_odd,2); //shift two position right
	even=_mm256_add_epi8(m0_even,m0_odd); //add the odd with the even

	//-----------------multiply by the second mask----------
	m0=_mm256_maddubs_epi16(r0,cy0_sh1);
	m1=_mm256_maddubs_epi16(r1,cy1_sh1);
	m2=_mm256_maddubs_epi16(r2,cy2_sh1);
	m3=_mm256_maddubs_epi16(r3,cy3_sh1);
	m4=_mm256_maddubs_epi16(r4,cy4_sh1);
	m5=_mm256_maddubs_epi16(r5,cy5_sh1);
	m6=_mm256_maddubs_epi16(r6,cy6_sh1);
	m7=_mm256_maddubs_epi16(r7,cy7_sh1);
	m8=_mm256_maddubs_epi16(r8,cy8_sh1);


	m0_even=_mm256_and_si256(m0,mask_even_16);
	m1_even=_mm256_and_si256(m1,mask_even_16);
	m2_even=_mm256_and_si256(m2,mask_even_16);
	m3_even=_mm256_and_si256(m3,mask_even_16);
	m4_even=_mm256_and_si256(m4,mask_even_16);
	m5_even=_mm256_and_si256(m5,mask_even_16);
	m6_even=_mm256_and_si256(m6,mask_even_16);
	m7_even=_mm256_and_si256(m7,mask_even_16);
	m8_even=_mm256_and_si256(m8,mask_even_16);

	//make from 16-bit to 32bit
	  m0_even=_mm256_madd_epi16(m0_even,ones);
	  m1_even=_mm256_madd_epi16(m1_even,ones);
	  m2_even=_mm256_madd_epi16(m2_even,ones);
	  m3_even=_mm256_madd_epi16(m3_even,ones);
	  m4_even=_mm256_madd_epi16(m4_even,ones);
	  m5_even=_mm256_madd_epi16(m5_even,ones);
	  m6_even=_mm256_madd_epi16(m6_even,ones);
	  m7_even=_mm256_madd_epi16(m7_even,ones);
	  m8_even=_mm256_madd_epi16(m8_even,ones);


	//vertical 32-bit add
	m0_even=_mm256_add_epi32(m0_even,m1_even);
	m0_even=_mm256_add_epi32(m0_even,m2_even);
	m0_even=_mm256_add_epi32(m0_even,m3_even);
	m0_even=_mm256_add_epi32(m0_even,m4_even);
	m0_even=_mm256_add_epi32(m0_even,m5_even);
	m0_even=_mm256_add_epi32(m0_even,m6_even);
	m0_even=_mm256_add_epi32(m0_even,m7_even);
	m0_even=_mm256_add_epi32(m0_even,m8_even);


	 m0_odd=_mm256_and_si256(m0,mask_odd_16);
	 m1_odd=_mm256_and_si256(m1,mask_odd_16);
	 m2_odd=_mm256_and_si256(m2,mask_odd_16);
	 m3_odd=_mm256_and_si256(m3,mask_odd_16);
	 m4_odd=_mm256_and_si256(m4,mask_odd_16);
	 m5_odd=_mm256_and_si256(m5,mask_odd_16);
	 m6_odd=_mm256_and_si256(m6,mask_odd_16);
	 m7_odd=_mm256_and_si256(m7,mask_odd_16);
	 m8_odd=_mm256_and_si256(m8,mask_odd_16);

	 //make from 16-bit to 32bit
		m0_odd=_mm256_madd_epi16(m0_odd,ones);
		m1_odd=_mm256_madd_epi16(m1_odd,ones);
		m2_odd=_mm256_madd_epi16(m2_odd,ones);
		m3_odd=_mm256_madd_epi16(m3_odd,ones);
		m4_odd=_mm256_madd_epi16(m4_odd,ones);
		m5_odd=_mm256_madd_epi16(m5_odd,ones);
		m6_odd=_mm256_madd_epi16(m6_odd,ones);
		m7_odd=_mm256_madd_epi16(m7_odd,ones);
		m8_odd=_mm256_madd_epi16(m8_odd,ones);


	//vertical 32-bit add
	m0_odd=_mm256_add_epi32(m0_odd,m1_odd);
	m0_odd=_mm256_add_epi32(m0_odd,m2_odd);
	m0_odd=_mm256_add_epi32(m0_odd,m3_odd);
	m0_odd=_mm256_add_epi32(m0_odd,m4_odd);
	m0_odd=_mm256_add_epi32(m0_odd,m5_odd);
	m0_odd=_mm256_add_epi32(m0_odd,m6_odd);
	m0_odd=_mm256_add_epi32(m0_odd,m7_odd);
	m0_odd=_mm256_add_epi32(m0_odd,m8_odd);

	m0_even=division_32(division_case,m0_even,f);//even results
	m0_odd=division_32(division_case,m0_odd,f);//even results

	m0_odd=_mm256_slli_si256(m0_odd,2); //shift two position right
	odd=_mm256_add_epi8(m0_even,m0_odd); //add the odd with the even


	//MERGE AND pack to one register
	odd=_mm256_slli_si256(odd,1); //shift one position right
	r0=_mm256_add_epi8(even,odd); //add the odd with the even

	_mm256_storeu_si256( (__m256i *) &temp[M-REMINDER_ITERATIONS_Y],r0 );

	for (int gg=M;gg<M+32+6;gg++)
	    temp[gg]=0;
//y filter ends - r0 has now the data to be processed by x filter
}


}





void loop_reminder_9x9_32_blur_X(unsigned char **frame1,unsigned char **filt,unsigned char *temp,const unsigned int N,const unsigned int M,const unsigned int row, const unsigned int col,const unsigned int REMINDER_ITERATIONS_X,const unsigned int division_case,const signed char mask_vector_x[][32], const __m256i f, const unsigned short int divisor_xy,signed char *kernel_x){

	const __m256i cx0=_mm256_load_si256( (__m256i *) &mask_vector_x[0]);

	const __m256i ones=_mm256_set1_epi16(1);

	const __m256i mask_z    = _mm256_set_epi16(0,0,0,0,0,0,0,0,0,0,0,65535,0,0,0,0);
	const __m256i mask_unz    = _mm256_set_epi16(65535,65535,65535,65535,65535,65535,65535,65535,65535,65535,65535,0,65535,65535,65535,65535);
	const __m256i mask_32_1    = _mm256_set_epi32(0xffffffff,0,0,0xffffffff,0,0,0,0);
	const __m256i mask_32_2    = _mm256_set_epi32(0,0,0,0xffffffff,0,0,0,0);
	const __m256i mask_32_a    = _mm256_set_epi32(0,0,0xffffffff,0,0,0xffffffff,0,0xffffffff);
	const __m256i out_mask_1    = _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255);//0,20
	const __m256i out_mask_2    = _mm256_set_epi8(0,0,0,0,0,0,0,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255,0,0,0,0);//4,24
	const __m256i out_mask_3    = _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, 0,0,0,0,0,0,0,255,0,0,0,0,0,0,0,0);//8
	const __m256i out_mask_4    = _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, 0,0,0,255,0,0,0,0,0,0,0,0,0,0,0,0);//12
	const __m256i out_mask_5    = _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, 0,0,0,0,255,255,0,0,0,0,0,0,0,0,0,0);//10,11


	__m256i r0,m0,m1,m2,m3,m4,m8,output1,output2,output3,output4,output5,out1,out2;


		//1st col iteration
			//multiply by the mask
	        r0=_mm256_loadu_si256( (__m256i *) &temp[col-4]);
			m0=_mm256_maddubs_epi16(r0,cx0);

			//zero the 4th out of 16 elements
			m1=_mm256_and_si256(m0,mask_unz);
			//keep a copy of the 4th element
			m8=_mm256_and_si256(m0,mask_z);
		//m8=_mm256_slli_si256(m8,2);

			//make 16-bit 32-bit
		m1=_mm256_madd_epi16(m1,ones);

		//horizontal add
		m2=_mm256_srli_si256(m1,4);
			m2=_mm256_add_epi32(m1,m2);

			//keep only 4th and 7th out of 8 elements
			m1=_mm256_and_si256(m1,mask_32_1);
			m1=_mm256_add_epi32(m1,m8);

			//shift m1 and add with m2 (complex shift is needed)
			m3=_mm256_srli_si256(m1,8);
			m4=_mm256_and_si256(m1,mask_32_2);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_slli_si256(m4,8);
			m3=_mm256_add_epi32(m3,m4);
			m2=_mm256_add_epi32(m2,m3);

			out1=_mm256_and_si256(m2,mask_32_a);//in 32-bit format keep only 0,2,5 and discard the others


			//2nd col iteration
	        r0=_mm256_loadu_si256( (__m256i *) &temp[col-3]);

			//multiply by the mask
			m0=_mm256_maddubs_epi16(r0,cx0);

			//zero the 4th out of 16 elements
			m1=_mm256_and_si256(m0,mask_unz);
			//keep a copy of the 4th element
			m8=_mm256_and_si256(m0,mask_z);
		//m8=_mm256_slli_si256(m8,2);

			//make 16-bit 32-bit
		m1=_mm256_madd_epi16(m1,ones);

		//horizontal add
		m2=_mm256_srli_si256(m1,4);
			m2=_mm256_add_epi32(m1,m2);

			//keep only 4th and 7th out of 8 elements
			m1=_mm256_and_si256(m1,mask_32_1);
			m1=_mm256_add_epi32(m1,m8);

			//shift m1 and add with m2 (complex shift is needed)
			m3=_mm256_srli_si256(m1,8);
			m4=_mm256_and_si256(m1,mask_32_2);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_slli_si256(m4,8);
			m3=_mm256_add_epi32(m3,m4);
			m2=_mm256_add_epi32(m2,m3);

			out2=_mm256_and_si256(m2,mask_32_a);//in 32-bit format keep only 0,2,5 and discard the others

			//merge out1 and out2 and then divide
			out2=_mm256_slli_si256(out2,4);
			out1=_mm256_add_epi32(out1,out2);
			out1 = division_32(division_case,out1,f);

			//elements must be re-arranged before store
			m0=_mm256_and_si256(out1,out_mask_1);
			m1=_mm256_and_si256(out1,out_mask_2);
			m1=_mm256_srli_si256(m1,3);
			m2=_mm256_and_si256(out1,out_mask_3);
			m2=_mm256_slli_si256(m2,2);
			m3=_mm256_and_si256(out1,out_mask_4);
			m3=_mm256_srli_si256(m3,1);
			m0=_mm256_add_epi8(m0,m1);
			m0=_mm256_add_epi8(m0,m2);
			output1=_mm256_add_epi8(m0,m3);

			//3rd col iteration
	        r0=_mm256_loadu_si256( (__m256i *) &temp[col-2]);

			//multiply by the mask
			m0=_mm256_maddubs_epi16(r0,cx0);

			//zero the 4th out of 16 elements
			m1=_mm256_and_si256(m0,mask_unz);
			//keep a copy of the 4th element
			m8=_mm256_and_si256(m0,mask_z);
		//m8=_mm256_slli_si256(m8,2);

			//make 16-bit 32-bit
		m1=_mm256_madd_epi16(m1,ones);

		//horizontal add
		m2=_mm256_srli_si256(m1,4);
			m2=_mm256_add_epi32(m1,m2);

			//keep only 4th and 7th out of 8 elements
			m1=_mm256_and_si256(m1,mask_32_1);
			m1=_mm256_add_epi32(m1,m8);

			//shift m1 and add with m2 (complex shift is needed)
			m3=_mm256_srli_si256(m1,8);
			m4=_mm256_and_si256(m1,mask_32_2);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_slli_si256(m4,8);
			m3=_mm256_add_epi32(m3,m4);
			m2=_mm256_add_epi32(m2,m3);

			out1=_mm256_and_si256(m2,mask_32_a);//in 32-bit format keep only 0,2,5 and discard the others

			//4th col iteration
	        r0=_mm256_loadu_si256( (__m256i *) &temp[col-1]);

			//multiply by the mask
			m0=_mm256_maddubs_epi16(r0,cx0);

			//zero the 4th out of 16 elements
			m1=_mm256_and_si256(m0,mask_unz);
			//keep a copy of the 4th element
			m8=_mm256_and_si256(m0,mask_z);
		//m8=_mm256_slli_si256(m8,2);

			//make 16-bit 32-bit
		m1=_mm256_madd_epi16(m1,ones);

		//horizontal add
		m2=_mm256_srli_si256(m1,4);
			m2=_mm256_add_epi32(m1,m2);

			//keep only 4th and 7th out of 8 elements
			m1=_mm256_and_si256(m1,mask_32_1);
			m1=_mm256_add_epi32(m1,m8);

			//shift m1 and add with m2 (complex shift is needed)
			m3=_mm256_srli_si256(m1,8);
			m4=_mm256_and_si256(m1,mask_32_2);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_slli_si256(m4,8);
			m3=_mm256_add_epi32(m3,m4);
			m2=_mm256_add_epi32(m2,m3);

			out2=_mm256_and_si256(m2,mask_32_a);//in 32-bit format keep only 0,2,5 and discard the others

			//merge out1 and out2 and then divide
			out2=_mm256_slli_si256(out2,4);
			out1=_mm256_add_epi32(out1,out2);
			out1 = division_32(division_case,out1,f);

			//elements must be re-arranged before store
			m0=_mm256_and_si256(out1,out_mask_1);
			m1=_mm256_and_si256(out1,out_mask_2);
			m1=_mm256_srli_si256(m1,3);
			m2=_mm256_and_si256(out1,out_mask_3);
			m2=_mm256_slli_si256(m2,2);
			m3=_mm256_and_si256(out1,out_mask_4);
			m3=_mm256_srli_si256(m3,1);
			m0=_mm256_add_epi8(m0,m1);
			m0=_mm256_add_epi8(m0,m2);
			output2=_mm256_add_epi8(m0,m3);

			//5th col iteration
	        r0=_mm256_loadu_si256( (__m256i *) &temp[col]);

			//multiply by the mask
			m0=_mm256_maddubs_epi16(r0,cx0);

			//zero the 4th out of 16 elements
			m1=_mm256_and_si256(m0,mask_unz);
			//keep a copy of the 4th element
			m8=_mm256_and_si256(m0,mask_z);
		//m8=_mm256_slli_si256(m8,2);

			//make 16-bit 32-bit
		m1=_mm256_madd_epi16(m1,ones);

		//horizontal add
		m2=_mm256_srli_si256(m1,4);
			m2=_mm256_add_epi32(m1,m2);

			//keep only 4th and 7th out of 8 elements
			m1=_mm256_and_si256(m1,mask_32_1);
			m1=_mm256_add_epi32(m1,m8);

			//shift m1 and add with m2 (complex shift is needed)
			m3=_mm256_srli_si256(m1,8);
			m4=_mm256_and_si256(m1,mask_32_2);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_slli_si256(m4,8);
			m3=_mm256_add_epi32(m3,m4);
			m2=_mm256_add_epi32(m2,m3);

			out1=_mm256_and_si256(m2,mask_32_a);//in 32-bit format keep only 0,2,5 and discard the others

			//6th col iteration
	        r0=_mm256_loadu_si256( (__m256i *) &temp[col+1]);

			//multiply by the mask
			m0=_mm256_maddubs_epi16(r0,cx0);

			//zero the 4th out of 16 elements
			m1=_mm256_and_si256(m0,mask_unz);
			//keep a copy of the 4th element
			m8=_mm256_and_si256(m0,mask_z);
		//m8=_mm256_slli_si256(m8,2);

			//make 16-bit 32-bit
		m1=_mm256_madd_epi16(m1,ones);

		//horizontal add
		m2=_mm256_srli_si256(m1,4);
			m2=_mm256_add_epi32(m1,m2);

			//keep only 4th and 7th out of 8 elements
			m1=_mm256_and_si256(m1,mask_32_1);
			m1=_mm256_add_epi32(m1,m8);

			//shift m1 and add with m2 (complex shift is needed)
			m3=_mm256_srli_si256(m1,8);
			m4=_mm256_and_si256(m1,mask_32_2);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_slli_si256(m4,8);
			m3=_mm256_add_epi32(m3,m4);
			m2=_mm256_add_epi32(m2,m3);

			out2=_mm256_and_si256(m2,mask_32_a);//in 32-bit format keep only 0,2,5 and discard the others

			//merge out1 and out2 and then divide
			out2=_mm256_slli_si256(out2,4);
			out1=_mm256_add_epi32(out1,out2);
			out1 = division_32(division_case,out1,f);

			//elements must be re-arranged before store
			m0=_mm256_and_si256(out1,out_mask_1);
			m1=_mm256_and_si256(out1,out_mask_2);
			m1=_mm256_srli_si256(m1,3);
			m2=_mm256_and_si256(out1,out_mask_3);
			m2=_mm256_slli_si256(m2,2);
			m3=_mm256_and_si256(out1,out_mask_4);
			m3=_mm256_srli_si256(m3,1);
			m0=_mm256_add_epi8(m0,m1);
			m0=_mm256_add_epi8(m0,m2);
			output3=_mm256_add_epi8(m0,m3);

			//7th col iteration
	        r0=_mm256_loadu_si256( (__m256i *) &temp[col+2]);

			//multiply by the mask
			m0=_mm256_maddubs_epi16(r0,cx0);

			//zero the 4th out of 16 elements
			m1=_mm256_and_si256(m0,mask_unz);
			//keep a copy of the 4th element
			m8=_mm256_and_si256(m0,mask_z);
		//m8=_mm256_slli_si256(m8,2);

			//make 16-bit 32-bit
		m1=_mm256_madd_epi16(m1,ones);

		//horizontal add
		m2=_mm256_srli_si256(m1,4);
			m2=_mm256_add_epi32(m1,m2);

			//keep only 4th and 7th out of 8 elements
			m1=_mm256_and_si256(m1,mask_32_1);
			m1=_mm256_add_epi32(m1,m8);

			//shift m1 and add with m2 (complex shift is needed)
			m3=_mm256_srli_si256(m1,8);
			m4=_mm256_and_si256(m1,mask_32_2);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_slli_si256(m4,8);
			m3=_mm256_add_epi32(m3,m4);
			m2=_mm256_add_epi32(m2,m3);

			out1=_mm256_and_si256(m2,mask_32_a);//in 32-bit format keep only 0,2,5 and discard the others

			//8th col iteration
	        r0=_mm256_loadu_si256( (__m256i *) &temp[col+3]);

			//multiply by the mask
			m0=_mm256_maddubs_epi16(r0,cx0);

			//zero the 4th out of 16 elements
			m1=_mm256_and_si256(m0,mask_unz);
			//keep a copy of the 4th element
			m8=_mm256_and_si256(m0,mask_z);
		//m8=_mm256_slli_si256(m8,2);

			//make 16-bit 32-bit
		m1=_mm256_madd_epi16(m1,ones);

		//horizontal add
		m2=_mm256_srli_si256(m1,4);
			m2=_mm256_add_epi32(m1,m2);

			//keep only 4th and 7th out of 8 elements
			m1=_mm256_and_si256(m1,mask_32_1);
			m1=_mm256_add_epi32(m1,m8);

			//shift m1 and add with m2 (complex shift is needed)
			m3=_mm256_srli_si256(m1,8);
			m4=_mm256_and_si256(m1,mask_32_2);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_slli_si256(m4,8);
			m3=_mm256_add_epi32(m3,m4);
			m2=_mm256_add_epi32(m2,m3);

			out2=_mm256_and_si256(m2,mask_32_a);//in 32-bit format keep only 0,2,5 and discard the others

			//merge out1 and out2 and then divide
			out2=_mm256_slli_si256(out2,4);
			out1=_mm256_add_epi32(out1,out2);
			out1 = division_32(division_case,out1,f);

			//elements must be re-arranged before store
			m0=_mm256_and_si256(out1,out_mask_1);
			m1=_mm256_and_si256(out1,out_mask_2);
			m1=_mm256_srli_si256(m1,3);
			m2=_mm256_and_si256(out1,out_mask_3);
			m2=_mm256_slli_si256(m2,2);
			m3=_mm256_and_si256(out1,out_mask_4);
			m3=_mm256_srli_si256(m3,1);
			m0=_mm256_add_epi8(m0,m1);
			m0=_mm256_add_epi8(m0,m2);
			output4=_mm256_add_epi8(m0,m3);

			//9th col iteration
	        r0=_mm256_loadu_si256( (__m256i *) &temp[col+4]);

			//multiply by the mask
			m0=_mm256_maddubs_epi16(r0,cx0);

			//zero the 4th out of 16 elements
			m1=_mm256_and_si256(m0,mask_unz);
			//keep a copy of the 4th element
			m8=_mm256_and_si256(m0,mask_z);
		//m8=_mm256_slli_si256(m8,2);

			//make 16-bit 32-bit
		m1=_mm256_madd_epi16(m1,ones);

		//horizontal add
		m2=_mm256_srli_si256(m1,4);
			m2=_mm256_add_epi32(m1,m2);

			//keep only 4th and 7th out of 8 elements
			m1=_mm256_and_si256(m1,mask_32_1);
			m1=_mm256_add_epi32(m1,m8);

			//shift m1 and add with m2 (complex shift is needed)
			m3=_mm256_srli_si256(m1,8);
			m4=_mm256_and_si256(m1,mask_32_2);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_slli_si256(m4,8);
			m3=_mm256_add_epi32(m3,m4);
			m2=_mm256_add_epi32(m2,m3);

			out1=_mm256_and_si256(m2,mask_32_a);//in 32-bit format keep only 0,2,5 and discard the others

			//10th col iteration
	        r0=_mm256_loadu_si256( (__m256i *) &temp[col+5]);

			//multiply by the mask
			m0=_mm256_maddubs_epi16(r0,cx0);

			//zero the 4th out of 16 elements
			m1=_mm256_and_si256(m0,mask_unz);
			//keep a copy of the 4th element
			m8=_mm256_and_si256(m0,mask_z);
		//m8=_mm256_slli_si256(m8,2);

			//make 16-bit 32-bit
		m1=_mm256_madd_epi16(m1,ones);

		//horizontal add
		m2=_mm256_srli_si256(m1,4);
			m2=_mm256_add_epi32(m1,m2);

			//keep only 4th and 7th out of 8 elements
			m1=_mm256_and_si256(m1,mask_32_1);
			m1=_mm256_add_epi32(m1,m8);

			//shift m1 and add with m2 (complex shift is needed)
			m3=_mm256_srli_si256(m1,8);
			m4=_mm256_and_si256(m1,mask_32_2);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_slli_si256(m4,8);
			m3=_mm256_add_epi32(m3,m4);
			m2=_mm256_add_epi32(m2,m3);

			out2=_mm256_and_si256(m2,mask_32_a);//in 32-bit format keep only 0,2,5 and discard the others

			//merge out1 and out2 and then divide
			out2=_mm256_slli_si256(out2,4);
			out1=_mm256_add_epi32(out1,out2);
			out1 = division_32(division_case,out1,f);

			//elements must be re-arranged before store
			m0=_mm256_and_si256(out1,out_mask_1);
			m1=_mm256_and_si256(out1,out_mask_2);
			m1=_mm256_srli_si256(m1,3);
			m2=_mm256_and_si256(out1,out_mask_3);
			m2=_mm256_slli_si256(m2,2);
			m3=_mm256_and_si256(out1,out_mask_4);
			m3=_mm256_srli_si256(m3,1);
			m0=_mm256_add_epi8(m0,m1);
			m0=_mm256_add_epi8(m0,m2);
			output5=_mm256_add_epi8(m0,m3);

			//blend the output1:output5
			output2=_mm256_slli_si256(output2,2);
			output3=_mm256_slli_si256(output3,4);

			m3=_mm256_slli_si256(output4,6);
			m4=_mm256_and_si256(output4,out_mask_5);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_srli_si256(m4,10);
			output4=_mm256_add_epi32(m3,m4);

			m3=_mm256_slli_si256(output5,8);
			m4=_mm256_and_si256(output5,out_mask_5);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_srli_si256(m4,8);
			output5=_mm256_add_epi32(m3,m4);

			output1=_mm256_add_epi8(output1,output2);
			output1=_mm256_add_epi8(output1,output3);
			output1=_mm256_add_epi8(output1,output4);
			output1=_mm256_add_epi8(output1,output5);


	switch (REMINDER_ITERATIONS_X){
	case 1:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		  break;
	case 2:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		  break;
	case 3:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		  break;
	case 4:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output1,3);
		  break;
	case 5:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output1,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output1,4);
		  break;
	case 6:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output1,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output1,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output1,5);
		  break;
	case 7:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output1,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output1,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output1,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output1,6);
		  break;
	case 8:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output1,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output1,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output1,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output1,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output1,7);
		  break;
	case 9:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output1,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output1,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output1,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output1,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output1,7);
		filt[row][col+8] = (unsigned char) _mm256_extract_epi8(output1,8);
		  break;
	case 10:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output1,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output1,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output1,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output1,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output1,7);
		filt[row][col+8] = (unsigned char) _mm256_extract_epi8(output1,8);
		filt[row][col+9] = (unsigned char) _mm256_extract_epi8(output1,9);
		  break;
	case 11:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output1,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output1,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output1,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output1,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output1,7);
		filt[row][col+8] = (unsigned char) _mm256_extract_epi8(output1,8);
		filt[row][col+9] = (unsigned char) _mm256_extract_epi8(output1,9);
		filt[row][col+10] = (unsigned char) _mm256_extract_epi8(output1,10);
		  break;
	case 12:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output1,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output1,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output1,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output1,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output1,7);
		filt[row][col+8] = (unsigned char) _mm256_extract_epi8(output1,8);
		filt[row][col+9] = (unsigned char) _mm256_extract_epi8(output1,9);
		filt[row][col+10] = (unsigned char) _mm256_extract_epi8(output1,10);
		filt[row][col+11] = (unsigned char) _mm256_extract_epi8(output1,11);
		  break;
	case 13:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output1,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output1,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output1,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output1,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output1,7);
		filt[row][col+8] = (unsigned char) _mm256_extract_epi8(output1,8);
		filt[row][col+9] = (unsigned char) _mm256_extract_epi8(output1,9);
		filt[row][col+10] = (unsigned char) _mm256_extract_epi8(output1,10);
		filt[row][col+11] = (unsigned char) _mm256_extract_epi8(output1,11);
		filt[row][col+12] = (unsigned char) _mm256_extract_epi8(output1,12);
		  break;
	case 14:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output1,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output1,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output1,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output1,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output1,7);
		filt[row][col+8] = (unsigned char) _mm256_extract_epi8(output1,8);
		filt[row][col+9] = (unsigned char) _mm256_extract_epi8(output1,9);
		filt[row][col+10] = (unsigned char) _mm256_extract_epi8(output1,10);
		filt[row][col+11] = (unsigned char) _mm256_extract_epi8(output1,11);
		filt[row][col+12] = (unsigned char) _mm256_extract_epi8(output1,12);
		filt[row][col+13] = (unsigned char) _mm256_extract_epi8(output1,13);
		  break;
	case 15:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output1,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output1,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output1,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output1,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output1,7);
		filt[row][col+8] = (unsigned char) _mm256_extract_epi8(output1,8);
		filt[row][col+9] = (unsigned char) _mm256_extract_epi8(output1,9);
		filt[row][col+10] = (unsigned char) _mm256_extract_epi8(output1,10);
		filt[row][col+11] = (unsigned char) _mm256_extract_epi8(output1,11);
		filt[row][col+12] = (unsigned char) _mm256_extract_epi8(output1,12);
		filt[row][col+13] = (unsigned char) _mm256_extract_epi8(output1,13);
		filt[row][col+14] = (unsigned char) _mm256_extract_epi8(output1,14);
		  break;
	case 16:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
		  break;
	case 17:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
		  break;
	case 18:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
		  break;
	case 19:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
		  break;
	case 20:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output1,19);
		  break;
	case 21:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output1,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output1,20);
		  break;
	case 22:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output1,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output1,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output1,21);
		break;
	case 23:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output1,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output1,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output1,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output1,22);
		  break;
	case 24:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output1,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output1,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output1,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output1,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output1,23);
		  break;
	case 25:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output1,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output1,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output1,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output1,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output1,23);
	filt[row][col+24] = (unsigned char) _mm256_extract_epi8(output1,24);
		  break;
	case 26:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output1,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output1,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output1,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output1,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output1,23);
	filt[row][col+24] = (unsigned char) _mm256_extract_epi8(output1,24);
	filt[row][col+25] = (unsigned char) _mm256_extract_epi8(output1,25);
		  break;
	case 27:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output1,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output1,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output1,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output1,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output1,23);
	filt[row][col+24] = (unsigned char) _mm256_extract_epi8(output1,24);
	filt[row][col+25] = (unsigned char) _mm256_extract_epi8(output1,25);
	filt[row][col+26] = (unsigned char) _mm256_extract_epi8(output1,26);
		  break;
	case 28:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output1,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output1,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output1,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output1,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output1,23);
	filt[row][col+24] = (unsigned char) _mm256_extract_epi8(output1,24);
	filt[row][col+25] = (unsigned char) _mm256_extract_epi8(output1,25);
	filt[row][col+26] = (unsigned char) _mm256_extract_epi8(output1,26);
	filt[row][col+27] = (unsigned char) _mm256_extract_epi8(output1,27);
		  break;
	case 29:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output1,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output1,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output1,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output1,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output1,23);
	filt[row][col+24] = (unsigned char) _mm256_extract_epi8(output1,24);
	filt[row][col+25] = (unsigned char) _mm256_extract_epi8(output1,25);
	filt[row][col+26] = (unsigned char) _mm256_extract_epi8(output1,26);
	filt[row][col+27] = (unsigned char) _mm256_extract_epi8(output1,27);
	filt[row][col+28] = (unsigned char) _mm256_extract_epi8(output1,28);
		  break;
	case 30:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output1,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output1,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output1,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output1,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output1,23);
	filt[row][col+24] = (unsigned char) _mm256_extract_epi8(output1,24);
	filt[row][col+25] = (unsigned char) _mm256_extract_epi8(output1,25);
	filt[row][col+26] = (unsigned char) _mm256_extract_epi8(output1,26);
	filt[row][col+27] = (unsigned char) _mm256_extract_epi8(output1,27);
	filt[row][col+28] = (unsigned char) _mm256_extract_epi8(output1,28);
	filt[row][col+29] = (unsigned char) _mm256_extract_epi8(output1,29);
		  break;
	case 31:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output1,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output1,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output1,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output1,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output1,23);
	filt[row][col+24] = (unsigned char) _mm256_extract_epi8(output1,24);
	filt[row][col+25] = (unsigned char) _mm256_extract_epi8(output1,25);
	filt[row][col+26] = (unsigned char) _mm256_extract_epi8(output1,26);
	filt[row][col+27] = (unsigned char) _mm256_extract_epi8(output1,27);
	filt[row][col+28] = (unsigned char) _mm256_extract_epi8(output1,28);
	filt[row][col+29] = (unsigned char) _mm256_extract_epi8(output1,29);

	int newPixel = 0;

	newPixel += temp[col+30-4] * kernel_x[0];
	newPixel += temp[col+30-3] * kernel_x[1];
	newPixel += temp[col+30-2] * kernel_x[2];
	newPixel += temp[col+30-1] * kernel_x[3];
	newPixel += temp[col+30-0] * kernel_x[4];

	filt[row][col+30] = (unsigned char) (newPixel / divisor_xy);

		  break;
	default: //REMINDER IS EITHER 27,28,29,30 OR 31
		printf("\nsomething went wrong");
		exit(EXIT_FAILURE);
 	}


}




void Gaussian_Blur_7x7_16_separable(unsigned char **frame1,unsigned char **filt,const unsigned int M, const unsigned int N, signed char *kernel_y, signed char *kernel_x, const unsigned short int divisor_xy){

const signed char fx0=kernel_x[0];
const signed char fx1=kernel_x[1];
const signed char fx2=kernel_x[2];
const signed char fx3=kernel_x[3];
const signed char fx4=kernel_x[4];
const signed char fx5=kernel_x[5];
const signed char fx6=kernel_x[6];

const signed char fy0=kernel_y[0];
const signed char fy1=kernel_y[1];
const signed char fy2=kernel_y[2];
const signed char fy3=kernel_y[3];
const signed char fy4=kernel_y[4];
const signed char fy5=kernel_y[5];
const signed char fy6=kernel_y[6];



const signed char mask_vector_x[1][32] __attribute__((aligned(64))) ={
		{fx0,fx1,fx2,fx3,fx4,fx5,fx6,0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,0},
};

const signed char mask_vector_y[14][32] __attribute__((aligned(64))) ={
	{fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0},
	{fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0},
	{fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0},
	{fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0},
	{fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0},
	{fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0},
	{fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0},

	{0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0},
	{0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1},
	{0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2},
	{0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3},
	{0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4},
	{0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5},
	{0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6},
};


	const __m256i cx0=_mm256_load_si256( (__m256i *) &mask_vector_x[0]);



	const __m256i cy0=_mm256_load_si256( (__m256i *) &mask_vector_y[0]);
	const __m256i cy1=_mm256_load_si256( (__m256i *) &mask_vector_y[1]);
	const __m256i cy2=_mm256_load_si256( (__m256i *) &mask_vector_y[2]);
	const __m256i cy3=_mm256_load_si256( (__m256i *) &mask_vector_y[3]);
	const __m256i cy4=_mm256_load_si256( (__m256i *) &mask_vector_y[4]);
	const __m256i cy5=_mm256_load_si256( (__m256i *) &mask_vector_y[5]);
	const __m256i cy6=_mm256_load_si256( (__m256i *) &mask_vector_y[6]);

	const __m256i cy0_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[7]);
	const __m256i cy1_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[8]);
	const __m256i cy2_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[9]);
	const __m256i cy3_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[10]);
	const __m256i cy4_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[11]);
	const __m256i cy5_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[12]);
	const __m256i cy6_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[13]);

	const __m256i mask_prelude  = _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0);


	const __m256i mask_16_1  = _mm256_set_epi16(0,0,0,0xffff,0,0,0,0xffff,0,0,0,0xffff,0,0,0,0xffff);

	const __m256i mask_prelude_3  = _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0);
	const __m256i mask_prelude_2  = _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0);
	const __m256i mask_prelude_1  = _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0);




	const unsigned int REMINDER_ITERATIONS_XY_mask =  M-((((M-32)/32)*32)+32);

	//printf("\nREMINDER_ITERATIONS=%u",REMINDER_ITERATIONS_XY_mask);


	const unsigned int division_case=prepare_for_division(divisor_xy); //determine which is the division case (A, B or C)
	const __m256i f = _mm256_load_si256( (__m256i *) &f_vector[0]);// initialize the division vector
	//const __m256i ones=_mm256_set1_epi16(1);

//	const unsigned int division_case_x=prepare_for_division_32(divisor_x); //determine which is the division case (A, B or C)
//	const __m256i f_x = _mm256_load_si256( (__m256i *) &f_vector_x[0]);// initialize the division vector
//printf("\n%d %d",division_case,b);


#pragma omp parallel
{

unsigned int row,col;
__m256i r0,r1,r2,r3,r4,r5,r6,m0,m1,m2,m3,m4,m5,m6,even,odd;
unsigned char temp[M+32+4] __attribute__((aligned(64)));//temporal storage of the output of y filter.


/*---------------------- Gaussian Blur ---------------------------------*/

    #pragma omp for schedule(static)
	for (row = 2; row < N-2; row++) {

		if (row<3){
				prelude_7x7_16_Ymask_0_new(0, M, frame1, temp,mask_vector_y, division_case, f, mask_prelude);
				loop_reminder_7x7_16_blur_Y(frame1,filt,temp,N,M,0, REMINDER_ITERATIONS_XY_mask, division_case, mask_vector_y, f, divisor_xy);
				prelude_7x7_16_Xmask_new(frame1,filt,temp,N,M,0, mask_vector_x,  division_case,f,REMINDER_ITERATIONS_XY_mask,mask_vector_y,divisor_xy,kernel_x);

				prelude_7x7_16_Ymask_1_new(0, M, frame1, temp,mask_vector_y, division_case, f, mask_prelude);
				loop_reminder_7x7_16_blur_Y(frame1,filt,temp,N,M,1, REMINDER_ITERATIONS_XY_mask, division_case, mask_vector_y, f, divisor_xy);
				prelude_7x7_16_Xmask_new(frame1,filt,temp,N,M,1, mask_vector_x,  division_case,f,REMINDER_ITERATIONS_XY_mask,mask_vector_y,divisor_xy,kernel_x);

				prelude_7x7_16_Ymask_2_new(0, M, frame1, temp,mask_vector_y, division_case, f, mask_prelude);
				loop_reminder_7x7_16_blur_Y(frame1,filt,temp,N,M,2, REMINDER_ITERATIONS_XY_mask, division_case, mask_vector_y, f, divisor_xy);
				prelude_7x7_16_Xmask_new(frame1,filt,temp,N,M,2, mask_vector_x,  division_case,f,REMINDER_ITERATIONS_XY_mask,mask_vector_y,divisor_xy,kernel_x);

		}

		else if (row>N-4){

			prelude_7x7_16_Ymask_0_new(N-4, M, frame1, temp,mask_vector_y, division_case, f, mask_prelude);
			loop_reminder_7x7_16_blur_Y(frame1,filt,temp,N,M,N-1, REMINDER_ITERATIONS_XY_mask, division_case, mask_vector_y, f, divisor_xy);
			prelude_7x7_16_Xmask_new(frame1,filt,temp,N,M,N-1, mask_vector_x,  division_case,f,REMINDER_ITERATIONS_XY_mask,mask_vector_y,divisor_xy,kernel_x);

			prelude_7x7_16_Ymask_1_new(N-5, M, frame1, temp,mask_vector_y, division_case, f, mask_prelude);
			loop_reminder_7x7_16_blur_Y(frame1,filt,temp,N,M,N-2, REMINDER_ITERATIONS_XY_mask, division_case, mask_vector_y, f, divisor_xy);
			prelude_7x7_16_Xmask_new(frame1,filt,temp,N,M,N-2, mask_vector_x,  division_case,f,REMINDER_ITERATIONS_XY_mask,mask_vector_y,divisor_xy,kernel_x);

			prelude_7x7_16_Ymask_2_new(N-6, M, frame1, temp,mask_vector_y, division_case, f, mask_prelude);
			loop_reminder_7x7_16_blur_Y(frame1,filt,temp,N,M,N-3, REMINDER_ITERATIONS_XY_mask, division_case, mask_vector_y, f, divisor_xy);
			prelude_7x7_16_Xmask_new(frame1,filt,temp,N,M,N-3, mask_vector_x,  division_case,f,REMINDER_ITERATIONS_XY_mask,mask_vector_y,divisor_xy,kernel_x);

		}

	  else { //main loop


		  for (col = 0; col <= M-32; col+=32){

					//load the 7 rows
					r0=_mm256_load_si256( (__m256i *) &frame1[row-3][col]);
					r1=_mm256_load_si256( (__m256i *) &frame1[row-2][col]);
					r2=_mm256_load_si256( (__m256i *) &frame1[row-1][col]);
					r3=_mm256_load_si256( (__m256i *) &frame1[row][col]);
					r4=_mm256_load_si256( (__m256i *) &frame1[row+1][col]);
					r5=_mm256_load_si256( (__m256i *) &frame1[row+2][col]);
					r6=_mm256_load_si256( (__m256i *) &frame1[row+3][col]);

			  //--------------------y filter begins--------------------

				//multiply with the mask
					m0=_mm256_maddubs_epi16(r0,cy0);
					m1=_mm256_maddubs_epi16(r1,cy1);
					m2=_mm256_maddubs_epi16(r2,cy2);
					m3=_mm256_maddubs_epi16(r3,cy3);
					m4=_mm256_maddubs_epi16(r4,cy4);
					m5=_mm256_maddubs_epi16(r5,cy5);
					m6=_mm256_maddubs_epi16(r6,cy6);

					//vertical add
					m0=_mm256_add_epi16(m0,m1);
					m0=_mm256_add_epi16(m0,m2);
					m0=_mm256_add_epi16(m0,m3);
					m0=_mm256_add_epi16(m0,m4);
					m0=_mm256_add_epi16(m0,m5);
					m0=_mm256_add_epi16(m0,m6);


					even=division(division_case,m0,f);//even results

					//multiply with the mask
					m0=_mm256_maddubs_epi16(r0,cy0_sh1);
					m1=_mm256_maddubs_epi16(r1,cy1_sh1);
					m2=_mm256_maddubs_epi16(r2,cy2_sh1);
					m3=_mm256_maddubs_epi16(r3,cy3_sh1);
					m4=_mm256_maddubs_epi16(r4,cy4_sh1);
					m5=_mm256_maddubs_epi16(r5,cy5_sh1);
					m6=_mm256_maddubs_epi16(r6,cy6_sh1);

					//vertical add
					m0=_mm256_add_epi16(m0,m1);
					m0=_mm256_add_epi16(m0,m2);
					m0=_mm256_add_epi16(m0,m3);
					m0=_mm256_add_epi16(m0,m4);
					m0=_mm256_add_epi16(m0,m5);
					m0=_mm256_add_epi16(m0,m6);

					odd=division(division_case,m0,f);//odd results

					//pack to one register
					odd=_mm256_slli_si256(odd,1); //shift one position right
					r0=_mm256_add_epi8(even,odd); //add the odd with the even
				//y filter ends - r0 has now the data to be processed by x filter

				_mm256_store_si256( (__m256i *) &temp[col],r0 );
		  }
	loop_reminder_7x7_16_blur_Y(frame1,filt,temp,N,M,row, REMINDER_ITERATIONS_XY_mask, division_case, mask_vector_y, f, divisor_xy);


	  for (col = 0; col <= M-32; col+=32){

		  if (col==0){

	  			//1st col iteration
	  			//multiply by the mask
	  __m256i   rr0=_mm256_load_si256( (__m256i *) &temp[0]);
		  		r0=fill_3zeros(rr0,mask_prelude_3);

				//multiply by the mask
				m0=_mm256_maddubs_epi16(r0,cx0);

				//first horizontal add
				m1=_mm256_srli_si256(m0,2);
				m0=_mm256_add_epi16(m0,m1);//add

				//second horizontal add
				m1=_mm256_srli_si256(m0,4);
				m0=_mm256_add_epi16(m0,m1);//add

				even=_mm256_and_si256(m0,mask_16_1);//in 16-bit format keep only 0,4,8,12 and discard the others

	  			//2nd col iteration
		        //r0=_mm256_load_si256( (__m256i *) &temp[0]);
		  		r0=fill_2zeros(rr0,mask_prelude_2);

				//multiply by the mask
				m0=_mm256_maddubs_epi16(r0,cx0);

				//first horizontal add
				m1=_mm256_srli_si256(m0,2);
				m0=_mm256_add_epi16(m0,m1);//add

				//second horizontal add
				m1=_mm256_srli_si256(m0,4);
				m0=_mm256_add_epi16(m0,m1);//add

				odd=_mm256_and_si256(m0,mask_16_1);//in 16-bit format keep only 0,4,8,12 and discard the others


	  			//3rd col iteration
		       // r0=_mm256_load_si256( (__m256i *) &temp[0]);
		  		r0=fill_1zeros(rr0,mask_prelude_1);

				//multiply by the mask
				m0=_mm256_maddubs_epi16(r0,cx0);

				//first horizontal add
				m1=_mm256_srli_si256(m0,2);
				m0=_mm256_add_epi16(m0,m1);//add

				//second horizontal add
				m1=_mm256_srli_si256(m0,4);
				m0=_mm256_add_epi16(m0,m1);//add

				m0=_mm256_and_si256(m0,mask_16_1);//in 16-bit format keep only 0,4,8,12 and discard the others

				//MERGE even
				m0=_mm256_slli_si256(m0,2);
				even=_mm256_add_epi16(m0,even);


	  			//4th col iteration
		       // r0=_mm256_load_si256( (__m256i *) &temp[0]);

				//multiply by the mask
				m0=_mm256_maddubs_epi16(rr0,cx0);

				//first horizontal add
				m1=_mm256_srli_si256(m0,2);
				m0=_mm256_add_epi16(m0,m1);//add

				//second horizontal add
				m1=_mm256_srli_si256(m0,4);
				m0=_mm256_add_epi16(m0,m1);//add

				m0=_mm256_and_si256(m0,mask_16_1);//in 16-bit format keep only 0,4,8,12 and discard the others

				//MERGE odd
				m0=_mm256_slli_si256(m0,2);
				odd=_mm256_add_epi16(m0,odd);

	  			//5th col iteration
		        r0=_mm256_loadu_si256( (__m256i *) &temp[1]);

				//multiply by the mask
				m0=_mm256_maddubs_epi16(r0,cx0);

				//first horizontal add
				m1=_mm256_srli_si256(m0,2);
				m0=_mm256_add_epi16(m0,m1);//add

				//second horizontal add
				m1=_mm256_srli_si256(m0,4);
				m0=_mm256_add_epi16(m0,m1);//add

				m0=_mm256_and_si256(m0,mask_16_1);//in 16-bit format keep only 0,4,8,12 and discard the others

				//MERGE even
				m0=_mm256_slli_si256(m0,4);
				even=_mm256_add_epi16(m0,even);

	  			//6th col iteration
		        r0=_mm256_loadu_si256( (__m256i *) &temp[2]);

				//multiply by the mask
				m0=_mm256_maddubs_epi16(r0,cx0);

				//first horizontal add
				m1=_mm256_srli_si256(m0,2);
				m0=_mm256_add_epi16(m0,m1);//add

				//second horizontal add
				m1=_mm256_srli_si256(m0,4);
				m0=_mm256_add_epi16(m0,m1);//add

				m0=_mm256_and_si256(m0,mask_16_1);//in 16-bit format keep only 0,4,8,12 and discard the others

				//MERGE odd
				m0=_mm256_slli_si256(m0,4);
				odd=_mm256_add_epi16(m0,odd);

	  			//7th col iteration
		        r0=_mm256_loadu_si256( (__m256i *) &temp[3]);

				//multiply by the mask
				m0=_mm256_maddubs_epi16(r0,cx0);

				//first horizontal add
				m1=_mm256_srli_si256(m0,2);
				m0=_mm256_add_epi16(m0,m1);//add

				//second horizontal add
				m1=_mm256_srli_si256(m0,4);
				m0=_mm256_add_epi16(m0,m1);//add

				m0=_mm256_and_si256(m0,mask_16_1);//in 16-bit format keep only 0,4,8,12 and discard the others

				//MERGE even
				m0=_mm256_slli_si256(m0,6);
				even=_mm256_add_epi16(m0,even);

	  			//8th col iteration
		        r0=_mm256_loadu_si256( (__m256i *) &temp[4]);

				//multiply by the mask
				m0=_mm256_maddubs_epi16(r0,cx0);

				//first horizontal add
				m1=_mm256_srli_si256(m0,2);
				m0=_mm256_add_epi16(m0,m1);//add

				//second horizontal add
				m1=_mm256_srli_si256(m0,4);
				m0=_mm256_add_epi16(m0,m1);//add

				m0=_mm256_and_si256(m0,mask_16_1);//in 16-bit format keep only 0,4,8,12 and discard the others

				//MERGE odd
				m0=_mm256_slli_si256(m0,6);
				odd=_mm256_add_epi16(m0,odd);

				even = division(division_case,even,f);
				odd = division(division_case,odd,f);

				odd=_mm256_slli_si256(odd,1);
				even = _mm256_add_epi8(even,odd);

				_mm256_store_si256( (__m256i *) &filt[row][0],even );

		  		  	}
		  else {

				// col iteration computes output pixels of   0,8,16,24
				// col+1 iteration computes output pixels of 1,9,17,25
				// col+2 iteration computes output pixels of 2,10,18,26
				// col+3 iteration computes output pixels of 3,11,19,27
				// col+4 iteration computes output pixels of 4,12,20,28
				// col+5 iteration computes output pixels of 5,13,21,29
				// col+6 iteration computes output pixels of 6,14,22,30
				// col+7 iteration computes output pixels of 7,15,23,31
				//afterwards, col becomes 32 and repeat the above process

		  			//1st col iteration

		  			//multiply by the mask
	  		        r0=_mm256_loadu_si256( (__m256i *) &temp[col-3]);

					//multiply by the mask
					m0=_mm256_maddubs_epi16(r0,cx0);

					//first horizontal add
					m1=_mm256_srli_si256(m0,2);
					m0=_mm256_add_epi16(m0,m1);//add

					//second horizontal add
					m1=_mm256_srli_si256(m0,4);
					m0=_mm256_add_epi16(m0,m1);//add

					even=_mm256_and_si256(m0,mask_16_1);//in 16-bit format keep only 0,4,8,12 and discard the others

		  			//2nd col iteration
	  		        r0=_mm256_loadu_si256( (__m256i *) &temp[col-2]);

					//multiply by the mask
					m0=_mm256_maddubs_epi16(r0,cx0);

					//first horizontal add
					m1=_mm256_srli_si256(m0,2);
					m0=_mm256_add_epi16(m0,m1);//add

					//second horizontal add
					m1=_mm256_srli_si256(m0,4);
					m0=_mm256_add_epi16(m0,m1);//add

					odd=_mm256_and_si256(m0,mask_16_1);//in 16-bit format keep only 0,4,8,12 and discard the others


		  			//3rd col iteration
	  		        r0=_mm256_loadu_si256( (__m256i *) &temp[col-1]);

					//multiply by the mask
					m0=_mm256_maddubs_epi16(r0,cx0);

					//first horizontal add
					m1=_mm256_srli_si256(m0,2);
					m0=_mm256_add_epi16(m0,m1);//add

					//second horizontal add
					m1=_mm256_srli_si256(m0,4);
					m0=_mm256_add_epi16(m0,m1);//add

					m0=_mm256_and_si256(m0,mask_16_1);//in 16-bit format keep only 0,4,8,12 and discard the others

	  				//MERGE even
					m0=_mm256_slli_si256(m0,2);
	  				even=_mm256_add_epi16(m0,even);


		  			//4th col iteration
	  		        r0=_mm256_load_si256( (__m256i *) &temp[col]);

					//multiply by the mask
					m0=_mm256_maddubs_epi16(r0,cx0);

					//first horizontal add
					m1=_mm256_srli_si256(m0,2);
					m0=_mm256_add_epi16(m0,m1);//add

					//second horizontal add
					m1=_mm256_srli_si256(m0,4);
					m0=_mm256_add_epi16(m0,m1);//add

					m0=_mm256_and_si256(m0,mask_16_1);//in 16-bit format keep only 0,4,8,12 and discard the others

					//MERGE odd
					m0=_mm256_slli_si256(m0,2);
	  				odd=_mm256_add_epi16(m0,odd);

		  			//5th col iteration
	  		        r0=_mm256_loadu_si256( (__m256i *) &temp[col+1]);

					//multiply by the mask
					m0=_mm256_maddubs_epi16(r0,cx0);

					//first horizontal add
					m1=_mm256_srli_si256(m0,2);
					m0=_mm256_add_epi16(m0,m1);//add

					//second horizontal add
					m1=_mm256_srli_si256(m0,4);
					m0=_mm256_add_epi16(m0,m1);//add

					m0=_mm256_and_si256(m0,mask_16_1);//in 16-bit format keep only 0,4,8,12 and discard the others

	  				//MERGE even
					m0=_mm256_slli_si256(m0,4);
	  				even=_mm256_add_epi16(m0,even);

		  			//6th col iteration
	  		        r0=_mm256_loadu_si256( (__m256i *) &temp[col+2]);

					//multiply by the mask
					m0=_mm256_maddubs_epi16(r0,cx0);

					//first horizontal add
					m1=_mm256_srli_si256(m0,2);
					m0=_mm256_add_epi16(m0,m1);//add

					//second horizontal add
					m1=_mm256_srli_si256(m0,4);
					m0=_mm256_add_epi16(m0,m1);//add

					m0=_mm256_and_si256(m0,mask_16_1);//in 16-bit format keep only 0,4,8,12 and discard the others

					//MERGE odd
					m0=_mm256_slli_si256(m0,4);
	  				odd=_mm256_add_epi16(m0,odd);

		  			//7th col iteration
	  		        r0=_mm256_loadu_si256( (__m256i *) &temp[col+3]);

					//multiply by the mask
					m0=_mm256_maddubs_epi16(r0,cx0);

					//first horizontal add
					m1=_mm256_srli_si256(m0,2);
					m0=_mm256_add_epi16(m0,m1);//add

					//second horizontal add
					m1=_mm256_srli_si256(m0,4);
					m0=_mm256_add_epi16(m0,m1);//add

					m0=_mm256_and_si256(m0,mask_16_1);//in 16-bit format keep only 0,4,8,12 and discard the others

	  				//MERGE even
					m0=_mm256_slli_si256(m0,6);
	  				even=_mm256_add_epi16(m0,even);

		  			//8th col iteration
	  		        r0=_mm256_loadu_si256( (__m256i *) &temp[col+4]);

					//multiply by the mask
					m0=_mm256_maddubs_epi16(r0,cx0);

					//first horizontal add
					m1=_mm256_srli_si256(m0,2);
					m0=_mm256_add_epi16(m0,m1);//add

					//second horizontal add
					m1=_mm256_srli_si256(m0,4);
					m0=_mm256_add_epi16(m0,m1);//add

					m0=_mm256_and_si256(m0,mask_16_1);//in 16-bit format keep only 0,4,8,12 and discard the others

					//MERGE odd
					m0=_mm256_slli_si256(m0,6);
	  				odd=_mm256_add_epi16(m0,odd);

	  				even = division(division_case,even,f);
	  				odd = division(division_case,odd,f);

					odd=_mm256_slli_si256(odd,1);
	  				even = _mm256_add_epi8(even,odd);

	  				_mm256_store_si256( (__m256i *) &filt[row][col],even );

		  		 }

		}

		loop_reminder_7x7_16_blur_X(frame1,filt,temp,N,M,row, col, REMINDER_ITERATIONS_XY_mask, division_case, mask_vector_x, f, divisor_xy,kernel_x);

		}
}

}//end of parallel


}



//this function is computing the row=2 and row=N-3 only
//for row=2 input prelude_7x7_Ymask_3(0,...
//for row=N-3 input prelude_7x7_Ymask_3(N-6,...
void prelude_7x7_16_Ymask_2_new(const int row, const unsigned int M, unsigned char **frame1, unsigned char *temp, const signed char mask_vector_y[][32],  const unsigned int division_case, const __m256i f, const __m256i mask_prelude){

	__m256i r0,r1,r2,r3,r4,r5,m0,m1,m2,m3,m4,m5,even,odd;
	//__m256i ones=_mm256_set1_epi16(1);
	 __m256i cy0,cy1,cy2,cy3,cy4,cy5,cy0_sh1,cy1_sh1,cy2_sh1,cy3_sh1,cy4_sh1,cy5_sh1;

	if (row!=0){ //if row=N-6
	  cy0=_mm256_load_si256( (__m256i *) &mask_vector_y[0]);
	  cy1=_mm256_load_si256( (__m256i *) &mask_vector_y[1]);
	  cy2=_mm256_load_si256( (__m256i *) &mask_vector_y[2]);
	  cy3=_mm256_load_si256( (__m256i *) &mask_vector_y[3]);
	  cy4=_mm256_load_si256( (__m256i *) &mask_vector_y[4]);
	  cy5=_mm256_load_si256( (__m256i *) &mask_vector_y[5]);

	  cy0_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[7]);
	  cy1_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[8]);
	  cy2_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[9]);
	  cy3_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[10]);
	  cy4_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[11]);
	  cy5_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[12]);

	}
	else {
		  cy0=_mm256_load_si256( (__m256i *) &mask_vector_y[1]);
		  cy1=_mm256_load_si256( (__m256i *) &mask_vector_y[2]);
		  cy2=_mm256_load_si256( (__m256i *) &mask_vector_y[3]);
		  cy3=_mm256_load_si256( (__m256i *) &mask_vector_y[4]);
		  cy4=_mm256_load_si256( (__m256i *) &mask_vector_y[5]);
		  cy5=_mm256_load_si256( (__m256i *) &mask_vector_y[6]);

		  cy0_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[8]);
		  cy1_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[9]);
		  cy2_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[10]);
		  cy3_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[11]);
		  cy4_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[12]);
		  cy5_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[13]);

	}

	 for (unsigned int col = 0; col <= M-32; col+=32){

	  //--------------------y filter begins--------------------
	//load the 9 rows
	r0=_mm256_load_si256( (__m256i *) &frame1[row][col]);
	r1=_mm256_load_si256( (__m256i *) &frame1[row+1][col]);
	r2=_mm256_load_si256( (__m256i *) &frame1[row+2][col]);
	r3=_mm256_load_si256( (__m256i *) &frame1[row+3][col]);
	r4=_mm256_load_si256( (__m256i *) &frame1[row+4][col]);
	r5=_mm256_load_si256( (__m256i *) &frame1[row+5][col]);


		//multiply with the mask
			m0=_mm256_maddubs_epi16(r0,cy0);
			m1=_mm256_maddubs_epi16(r1,cy1);
			m2=_mm256_maddubs_epi16(r2,cy2);
			m3=_mm256_maddubs_epi16(r3,cy3);
			m4=_mm256_maddubs_epi16(r4,cy4);
			m5=_mm256_maddubs_epi16(r5,cy5);


			//vertical add
			m0=_mm256_add_epi16(m0,m1);
			m0=_mm256_add_epi16(m0,m2);
			m0=_mm256_add_epi16(m0,m3);
			m0=_mm256_add_epi16(m0,m4);
			m0=_mm256_add_epi16(m0,m5);

			even=division(division_case,m0,f);//even results

			//multiply with the mask
			m0=_mm256_maddubs_epi16(r0,cy0_sh1);
			m1=_mm256_maddubs_epi16(r1,cy1_sh1);
			m2=_mm256_maddubs_epi16(r2,cy2_sh1);
			m3=_mm256_maddubs_epi16(r3,cy3_sh1);
			m4=_mm256_maddubs_epi16(r4,cy4_sh1);
			m5=_mm256_maddubs_epi16(r5,cy5_sh1);

			//vertical add
			m0=_mm256_add_epi16(m0,m1);
			m0=_mm256_add_epi16(m0,m2);
			m0=_mm256_add_epi16(m0,m3);
			m0=_mm256_add_epi16(m0,m4);
			m0=_mm256_add_epi16(m0,m5);

			odd=division(division_case,m0,f);//odd results

			//pack to one register
			odd=_mm256_slli_si256(odd,1); //shift one position right
			r0 = _mm256_add_epi8(even,odd); //add the odd with the even
			_mm256_store_si256( (__m256i *) &temp[col],r0 );
	  }
		//y filter ends - r0 has now the data to be processed by x filter

}

//this function is computing the row=1 and row=N-2 only
//for row=1 input prelude_7x7_Ymask_3(0,...
//for row=N-2 input prelude_7x7_Ymask_3(N-5,...
void prelude_7x7_16_Ymask_1_new(const int row, const unsigned int M, unsigned char **frame1, unsigned char *temp, const signed char mask_vector_y[][32],  const unsigned int division_case, const __m256i f, const __m256i mask_prelude){

	__m256i r0,r1,r2,r3,r4,m0,m1,m2,m3,m4,even,odd;
	//__m256i ones=_mm256_set1_epi16(1);
	 __m256i cy0,cy1,cy2,cy3,cy4,cy0_sh1,cy1_sh1,cy2_sh1,cy3_sh1,cy4_sh1;

	if (row!=0){ //if row=N-6
	  cy0=_mm256_load_si256( (__m256i *) &mask_vector_y[0]);
	  cy1=_mm256_load_si256( (__m256i *) &mask_vector_y[1]);
	  cy2=_mm256_load_si256( (__m256i *) &mask_vector_y[2]);
	  cy3=_mm256_load_si256( (__m256i *) &mask_vector_y[3]);
	  cy4=_mm256_load_si256( (__m256i *) &mask_vector_y[4]);

	  cy0_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[7]);
	  cy1_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[8]);
	  cy2_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[9]);
	  cy3_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[10]);
	  cy4_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[11]);

	}
	else {
		  cy0=_mm256_load_si256( (__m256i *) &mask_vector_y[2]);
		  cy1=_mm256_load_si256( (__m256i *) &mask_vector_y[3]);
		  cy2=_mm256_load_si256( (__m256i *) &mask_vector_y[4]);
		  cy3=_mm256_load_si256( (__m256i *) &mask_vector_y[5]);
		  cy4=_mm256_load_si256( (__m256i *) &mask_vector_y[6]);

		  cy0_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[9]);
		  cy1_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[10]);
		  cy2_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[11]);
		  cy3_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[12]);
		  cy4_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[13]);

	}

	 for (unsigned int col = 0; col <= M-32; col+=32){

	  //--------------------y filter begins--------------------
	//load the 9 rows
	r0=_mm256_load_si256( (__m256i *) &frame1[row][col]);
	r1=_mm256_load_si256( (__m256i *) &frame1[row+1][col]);
	r2=_mm256_load_si256( (__m256i *) &frame1[row+2][col]);
	r3=_mm256_load_si256( (__m256i *) &frame1[row+3][col]);
	r4=_mm256_load_si256( (__m256i *) &frame1[row+4][col]);


		//multiply with the mask
			m0=_mm256_maddubs_epi16(r0,cy0);
			m1=_mm256_maddubs_epi16(r1,cy1);
			m2=_mm256_maddubs_epi16(r2,cy2);
			m3=_mm256_maddubs_epi16(r3,cy3);
			m4=_mm256_maddubs_epi16(r4,cy4);

			//vertical add
			m0=_mm256_add_epi16(m0,m1);
			m0=_mm256_add_epi16(m0,m2);
			m0=_mm256_add_epi16(m0,m3);
			m0=_mm256_add_epi16(m0,m4);

			even=division(division_case,m0,f);//even results

			//multiply with the mask
			m0=_mm256_maddubs_epi16(r0,cy0_sh1);
			m1=_mm256_maddubs_epi16(r1,cy1_sh1);
			m2=_mm256_maddubs_epi16(r2,cy2_sh1);
			m3=_mm256_maddubs_epi16(r3,cy3_sh1);
			m4=_mm256_maddubs_epi16(r4,cy4_sh1);

			//vertical add
			m0=_mm256_add_epi16(m0,m1);
			m0=_mm256_add_epi16(m0,m2);
			m0=_mm256_add_epi16(m0,m3);
			m0=_mm256_add_epi16(m0,m4);

			odd=division(division_case,m0,f);//odd results

			//pack to one register
			odd=_mm256_slli_si256(odd,1); //shift one position right
			r0 = _mm256_add_epi8(even,odd); //add the odd with the even
			_mm256_store_si256( (__m256i *) &temp[col],r0 );
	  }
		//y filter ends - r0 has now the data to be processed by x filter

}

//this function is computing the row=0 and row=N-1 only
//for row=0 input prelude_7x7_Ymask_3(0,...
//for row=N-1 input prelude_7x7_Ymask_3(N-4,...
void prelude_7x7_16_Ymask_0_new(const int row, const unsigned int M, unsigned char **frame1, unsigned char *temp, const signed char mask_vector_y[][32],  const unsigned int division_case, const __m256i f, const __m256i mask_prelude){

	__m256i r0,r1,r2,r3,m0,m1,m2,m3,even,odd;
	//__m256i ones=_mm256_set1_epi16(1);
	 __m256i cy0,cy1,cy2,cy3,cy0_sh1,cy1_sh1,cy2_sh1,cy3_sh1;

	if (row!=0){ //if row=N-6
	  cy0=_mm256_load_si256( (__m256i *) &mask_vector_y[0]);
	  cy1=_mm256_load_si256( (__m256i *) &mask_vector_y[1]);
	  cy2=_mm256_load_si256( (__m256i *) &mask_vector_y[2]);
	  cy3=_mm256_load_si256( (__m256i *) &mask_vector_y[3]);

	  cy0_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[7]);
	  cy1_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[8]);
	  cy2_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[9]);
	  cy3_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[10]);
	}
	else {
		  cy0=_mm256_load_si256( (__m256i *) &mask_vector_y[3]);
		  cy1=_mm256_load_si256( (__m256i *) &mask_vector_y[4]);
		  cy2=_mm256_load_si256( (__m256i *) &mask_vector_y[5]);
		  cy3=_mm256_load_si256( (__m256i *) &mask_vector_y[6]);

		  cy0_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[10]);
		  cy1_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[11]);
		  cy2_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[12]);
		  cy3_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[13]);

	}

	 for (unsigned int col = 0; col <= M-32; col+=32){

	  //--------------------y filter begins--------------------
	//load the 9 rows
	r0=_mm256_load_si256( (__m256i *) &frame1[row][col]);
	r1=_mm256_load_si256( (__m256i *) &frame1[row+1][col]);
	r2=_mm256_load_si256( (__m256i *) &frame1[row+2][col]);
	r3=_mm256_load_si256( (__m256i *) &frame1[row+3][col]);


		//multiply with the mask
			m0=_mm256_maddubs_epi16(r0,cy0);
			m1=_mm256_maddubs_epi16(r1,cy1);
			m2=_mm256_maddubs_epi16(r2,cy2);
			m3=_mm256_maddubs_epi16(r3,cy3);

			//vertical add
			m0=_mm256_add_epi16(m0,m1);
			m0=_mm256_add_epi16(m0,m2);
			m0=_mm256_add_epi16(m0,m3);

			even=division(division_case,m0,f);//even results

			//multiply with the mask
			m0=_mm256_maddubs_epi16(r0,cy0_sh1);
			m1=_mm256_maddubs_epi16(r1,cy1_sh1);
			m2=_mm256_maddubs_epi16(r2,cy2_sh1);
			m3=_mm256_maddubs_epi16(r3,cy3_sh1);

			//vertical add
			m0=_mm256_add_epi16(m0,m1);
			m0=_mm256_add_epi16(m0,m2);
			m0=_mm256_add_epi16(m0,m3);

			odd=division(division_case,m0,f);//odd results

			//pack to one register
			odd=_mm256_slli_si256(odd,1); //shift one position right
			r0 = _mm256_add_epi8(even,odd); //add the odd with the even
			_mm256_store_si256( (__m256i *) &temp[col],r0 );
	  }
		//y filter ends - r0 has now the data to be processed by x filter

}



void prelude_7x7_16_Xmask_new(unsigned char **frame1,unsigned char **filt,unsigned char *temp,  const unsigned int N,const unsigned int M, const int row, const signed char mask_vector_x[][32],  const unsigned int division_case, const __m256i f,const unsigned int REMINDER_ITERATIONS_XY ,const signed char mask_vector_y[][32],const unsigned short int divisor_xy,signed char *kernel_x){

	__m256i r0,m0,m1,even,odd;

	const __m256i cx0=_mm256_load_si256( (__m256i *) &mask_vector_x[0]);

	const __m256i mask_16_1  = _mm256_set_epi16(0,0,0,0xffff,0,0,0,0xffff,0,0,0,0xffff,0,0,0,0xffff);

	const __m256i mask_prelude_3  = _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0);
	const __m256i mask_prelude_2  = _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0);
	const __m256i mask_prelude_1  = _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0);



	unsigned int col;


	  for (col = 0; col <= M-32; col+=32){

		  if (col==0){

	  			//1st col iteration
	  			//multiply by the mask
		        r0=_mm256_loadu_si256( (__m256i *) &temp[0]);
		  		r0=fill_3zeros(r0,mask_prelude_3);

				//multiply by the mask
				m0=_mm256_maddubs_epi16(r0,cx0);

				//first horizontal add
				m1=_mm256_srli_si256(m0,2);
				m0=_mm256_add_epi16(m0,m1);//add

				//second horizontal add
				m1=_mm256_srli_si256(m0,4);
				m0=_mm256_add_epi16(m0,m1);//add

				even=_mm256_and_si256(m0,mask_16_1);//in 16-bit format keep only 0,4,8,12 and discard the others

	  			//2nd col iteration
		        r0=_mm256_loadu_si256( (__m256i *) &temp[0]);
		  		r0=fill_2zeros(r0,mask_prelude_2);

				//multiply by the mask
				m0=_mm256_maddubs_epi16(r0,cx0);

				//first horizontal add
				m1=_mm256_srli_si256(m0,2);
				m0=_mm256_add_epi16(m0,m1);//add

				//second horizontal add
				m1=_mm256_srli_si256(m0,4);
				m0=_mm256_add_epi16(m0,m1);//add

				odd=_mm256_and_si256(m0,mask_16_1);//in 16-bit format keep only 0,4,8,12 and discard the others


	  			//3rd col iteration
		        r0=_mm256_loadu_si256( (__m256i *) &temp[0]);
		  		r0=fill_1zeros(r0,mask_prelude_1);

				//multiply by the mask
				m0=_mm256_maddubs_epi16(r0,cx0);

				//first horizontal add
				m1=_mm256_srli_si256(m0,2);
				m0=_mm256_add_epi16(m0,m1);//add

				//second horizontal add
				m1=_mm256_srli_si256(m0,4);
				m0=_mm256_add_epi16(m0,m1);//add

				m0=_mm256_and_si256(m0,mask_16_1);//in 16-bit format keep only 0,4,8,12 and discard the others

				//MERGE even
				m0=_mm256_slli_si256(m0,2);
				even=_mm256_add_epi16(m0,even);


	  			//4th col iteration
		        r0=_mm256_loadu_si256( (__m256i *) &temp[0]);

				//multiply by the mask
				m0=_mm256_maddubs_epi16(r0,cx0);

				//first horizontal add
				m1=_mm256_srli_si256(m0,2);
				m0=_mm256_add_epi16(m0,m1);//add

				//second horizontal add
				m1=_mm256_srli_si256(m0,4);
				m0=_mm256_add_epi16(m0,m1);//add

				m0=_mm256_and_si256(m0,mask_16_1);//in 16-bit format keep only 0,4,8,12 and discard the others

				//MERGE odd
				m0=_mm256_slli_si256(m0,2);
				odd=_mm256_add_epi16(m0,odd);

	  			//5th col iteration
		        r0=_mm256_loadu_si256( (__m256i *) &temp[1]);

				//multiply by the mask
				m0=_mm256_maddubs_epi16(r0,cx0);

				//first horizontal add
				m1=_mm256_srli_si256(m0,2);
				m0=_mm256_add_epi16(m0,m1);//add

				//second horizontal add
				m1=_mm256_srli_si256(m0,4);
				m0=_mm256_add_epi16(m0,m1);//add

				m0=_mm256_and_si256(m0,mask_16_1);//in 16-bit format keep only 0,4,8,12 and discard the others

				//MERGE even
				m0=_mm256_slli_si256(m0,4);
				even=_mm256_add_epi16(m0,even);

	  			//6th col iteration
		        r0=_mm256_loadu_si256( (__m256i *) &temp[2]);

				//multiply by the mask
				m0=_mm256_maddubs_epi16(r0,cx0);

				//first horizontal add
				m1=_mm256_srli_si256(m0,2);
				m0=_mm256_add_epi16(m0,m1);//add

				//second horizontal add
				m1=_mm256_srli_si256(m0,4);
				m0=_mm256_add_epi16(m0,m1);//add

				m0=_mm256_and_si256(m0,mask_16_1);//in 16-bit format keep only 0,4,8,12 and discard the others

				//MERGE odd
				m0=_mm256_slli_si256(m0,4);
				odd=_mm256_add_epi16(m0,odd);

	  			//7th col iteration
		        r0=_mm256_loadu_si256( (__m256i *) &temp[3]);

				//multiply by the mask
				m0=_mm256_maddubs_epi16(r0,cx0);

				//first horizontal add
				m1=_mm256_srli_si256(m0,2);
				m0=_mm256_add_epi16(m0,m1);//add

				//second horizontal add
				m1=_mm256_srli_si256(m0,4);
				m0=_mm256_add_epi16(m0,m1);//add

				m0=_mm256_and_si256(m0,mask_16_1);//in 16-bit format keep only 0,4,8,12 and discard the others

				//MERGE even
				m0=_mm256_slli_si256(m0,6);
				even=_mm256_add_epi16(m0,even);

	  			//8th col iteration
		        r0=_mm256_loadu_si256( (__m256i *) &temp[4]);

				//multiply by the mask
				m0=_mm256_maddubs_epi16(r0,cx0);

				//first horizontal add
				m1=_mm256_srli_si256(m0,2);
				m0=_mm256_add_epi16(m0,m1);//add

				//second horizontal add
				m1=_mm256_srli_si256(m0,4);
				m0=_mm256_add_epi16(m0,m1);//add

				m0=_mm256_and_si256(m0,mask_16_1);//in 16-bit format keep only 0,4,8,12 and discard the others

				//MERGE odd
				m0=_mm256_slli_si256(m0,6);
				odd=_mm256_add_epi16(m0,odd);

				even = division(division_case,even,f);
				odd = division(division_case,odd,f);

				odd=_mm256_slli_si256(odd,1);
				even = _mm256_add_epi8(even,odd);

				_mm256_store_si256( (__m256i *) &filt[row][0],even );

		  		  	}
		  else {

				// col iteration computes output pixels of   0,8,16,24
				// col+1 iteration computes output pixels of 1,9,17,25
				// col+2 iteration computes output pixels of 2,10,18,26
				// col+3 iteration computes output pixels of 3,11,19,27
				// col+4 iteration computes output pixels of 4,12,20,28
				// col+5 iteration computes output pixels of 5,13,21,29
				// col+6 iteration computes output pixels of 6,14,22,30
				// col+7 iteration computes output pixels of 7,15,23,31
				//afterwards, col becomes 32 and repeat the above process

		  			//1st col iteration

		  			//multiply by the mask
	  		        r0=_mm256_loadu_si256( (__m256i *) &temp[col-3]);

					//multiply by the mask
					m0=_mm256_maddubs_epi16(r0,cx0);

					//first horizontal add
					m1=_mm256_srli_si256(m0,2);
					m0=_mm256_add_epi16(m0,m1);//add

					//second horizontal add
					m1=_mm256_srli_si256(m0,4);
					m0=_mm256_add_epi16(m0,m1);//add

					even=_mm256_and_si256(m0,mask_16_1);//in 16-bit format keep only 0,4,8,12 and discard the others

		  			//2nd col iteration
	  		        r0=_mm256_loadu_si256( (__m256i *) &temp[col-2]);

					//multiply by the mask
					m0=_mm256_maddubs_epi16(r0,cx0);

					//first horizontal add
					m1=_mm256_srli_si256(m0,2);
					m0=_mm256_add_epi16(m0,m1);//add

					//second horizontal add
					m1=_mm256_srli_si256(m0,4);
					m0=_mm256_add_epi16(m0,m1);//add

					odd=_mm256_and_si256(m0,mask_16_1);//in 16-bit format keep only 0,4,8,12 and discard the others


		  			//3rd col iteration
	  		        r0=_mm256_loadu_si256( (__m256i *) &temp[col-1]);

					//multiply by the mask
					m0=_mm256_maddubs_epi16(r0,cx0);

					//first horizontal add
					m1=_mm256_srli_si256(m0,2);
					m0=_mm256_add_epi16(m0,m1);//add

					//second horizontal add
					m1=_mm256_srli_si256(m0,4);
					m0=_mm256_add_epi16(m0,m1);//add

					m0=_mm256_and_si256(m0,mask_16_1);//in 16-bit format keep only 0,4,8,12 and discard the others

	  				//MERGE even
					m0=_mm256_slli_si256(m0,2);
	  				even=_mm256_add_epi16(m0,even);


		  			//4th col iteration
	  		        r0=_mm256_loadu_si256( (__m256i *) &temp[col]);

					//multiply by the mask
					m0=_mm256_maddubs_epi16(r0,cx0);

					//first horizontal add
					m1=_mm256_srli_si256(m0,2);
					m0=_mm256_add_epi16(m0,m1);//add

					//second horizontal add
					m1=_mm256_srli_si256(m0,4);
					m0=_mm256_add_epi16(m0,m1);//add

					m0=_mm256_and_si256(m0,mask_16_1);//in 16-bit format keep only 0,4,8,12 and discard the others

					//MERGE odd
					m0=_mm256_slli_si256(m0,2);
	  				odd=_mm256_add_epi16(m0,odd);

		  			//5th col iteration
	  		        r0=_mm256_loadu_si256( (__m256i *) &temp[col+1]);

					//multiply by the mask
					m0=_mm256_maddubs_epi16(r0,cx0);

					//first horizontal add
					m1=_mm256_srli_si256(m0,2);
					m0=_mm256_add_epi16(m0,m1);//add

					//second horizontal add
					m1=_mm256_srli_si256(m0,4);
					m0=_mm256_add_epi16(m0,m1);//add

					m0=_mm256_and_si256(m0,mask_16_1);//in 16-bit format keep only 0,4,8,12 and discard the others

	  				//MERGE even
					m0=_mm256_slli_si256(m0,4);
	  				even=_mm256_add_epi16(m0,even);

		  			//6th col iteration
	  		        r0=_mm256_loadu_si256( (__m256i *) &temp[col+2]);

					//multiply by the mask
					m0=_mm256_maddubs_epi16(r0,cx0);

					//first horizontal add
					m1=_mm256_srli_si256(m0,2);
					m0=_mm256_add_epi16(m0,m1);//add

					//second horizontal add
					m1=_mm256_srli_si256(m0,4);
					m0=_mm256_add_epi16(m0,m1);//add

					m0=_mm256_and_si256(m0,mask_16_1);//in 16-bit format keep only 0,4,8,12 and discard the others

					//MERGE odd
					m0=_mm256_slli_si256(m0,4);
	  				odd=_mm256_add_epi16(m0,odd);

		  			//7th col iteration
	  		        r0=_mm256_loadu_si256( (__m256i *) &temp[col+3]);

					//multiply by the mask
					m0=_mm256_maddubs_epi16(r0,cx0);

					//first horizontal add
					m1=_mm256_srli_si256(m0,2);
					m0=_mm256_add_epi16(m0,m1);//add

					//second horizontal add
					m1=_mm256_srli_si256(m0,4);
					m0=_mm256_add_epi16(m0,m1);//add

					m0=_mm256_and_si256(m0,mask_16_1);//in 16-bit format keep only 0,4,8,12 and discard the others

	  				//MERGE even
					m0=_mm256_slli_si256(m0,6);
	  				even=_mm256_add_epi16(m0,even);

		  			//8th col iteration
	  		        r0=_mm256_loadu_si256( (__m256i *) &temp[col+4]);

					//multiply by the mask
					m0=_mm256_maddubs_epi16(r0,cx0);

					//first horizontal add
					m1=_mm256_srli_si256(m0,2);
					m0=_mm256_add_epi16(m0,m1);//add

					//second horizontal add
					m1=_mm256_srli_si256(m0,4);
					m0=_mm256_add_epi16(m0,m1);//add

					m0=_mm256_and_si256(m0,mask_16_1);//in 16-bit format keep only 0,4,8,12 and discard the others

					//MERGE odd
					m0=_mm256_slli_si256(m0,6);
	  				odd=_mm256_add_epi16(m0,odd);

	  				even = division(division_case,even,f);
	  				odd = division(division_case,odd,f);

					odd=_mm256_slli_si256(odd,1);
	  				even = _mm256_add_epi8(even,odd);

	  				_mm256_store_si256( (__m256i *) &filt[row][col],even );

		  		 }

		}

	loop_reminder_7x7_16_blur_X(frame1,filt,temp,N,M,row, col, REMINDER_ITERATIONS_XY, division_case, mask_vector_x, f, divisor_xy,kernel_x);


}




void loop_reminder_7x7_16_blur_Y(unsigned char **frame1,unsigned char **filt,unsigned char *temp,const unsigned int N,const unsigned int M,const unsigned int row, const unsigned int REMINDER_ITERATIONS_Y,const unsigned int division_case,const signed char mask_vector_y[][32], const __m256i f, const unsigned short int divisor_xy){

	__m256i r0,r1,r2,r3,r4,r5,r6,m0,m1,m2,m3,m4,m5,m6,even,odd,reminder_mask;


 if (REMINDER_ITERATIONS_Y==0){//no need for computing the y mask
		_mm256_store_si256( (__m256i *) &temp[M],_mm256_setzero_si256() );
        for (unsigned int g=M+32;g<M+32+6;g++)
        	temp[g]=0;
	}
 else {
		__m256i cy0,cy1,cy2,cy3,cy4,cy5,cy6,cy0_sh1,cy1_sh1,cy2_sh1,cy3_sh1,cy4_sh1,cy5_sh1,cy6_sh1;

	unsigned int col2=M-REMINDER_ITERATIONS_Y;//this is the col index to load the elements for the y mask

	if ((row>2) && (row<N-3)){//main case

		r0=_mm256_loadu_si256( (__m256i *) &frame1[row-3][col2]);
		r1=_mm256_loadu_si256( (__m256i *) &frame1[row-2][col2]);
		r2=_mm256_loadu_si256( (__m256i *) &frame1[row-1][col2]);
		r3=_mm256_loadu_si256( (__m256i *) &frame1[row][col2]);
		r4=_mm256_loadu_si256( (__m256i *) &frame1[row+1][col2]);
		r5=_mm256_loadu_si256( (__m256i *) &frame1[row+2][col2]);
		r6=_mm256_loadu_si256( (__m256i *) &frame1[row+3][col2]);

		cy0=_mm256_load_si256( (__m256i *) &mask_vector_y[0]);
		cy1=_mm256_load_si256( (__m256i *) &mask_vector_y[1]);
		cy2=_mm256_load_si256( (__m256i *) &mask_vector_y[2]);
		cy3=_mm256_load_si256( (__m256i *) &mask_vector_y[3]);
		cy4=_mm256_load_si256( (__m256i *) &mask_vector_y[4]);
		cy5=_mm256_load_si256( (__m256i *) &mask_vector_y[5]);
		cy6=_mm256_load_si256( (__m256i *) &mask_vector_y[6]);

		cy0_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[7]);
		cy1_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[8]);
		cy2_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[9]);
		cy3_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[10]);
		cy4_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[11]);
		cy5_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[12]);
		cy6_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[13]);


	}
	else {
		if (row==0){
			r0=_mm256_loadu_si256( (__m256i *) &frame1[0][col2]);
			r1=_mm256_loadu_si256( (__m256i *) &frame1[1][col2]);
			r2=_mm256_loadu_si256( (__m256i *) &frame1[2][col2]);
			r3=_mm256_loadu_si256( (__m256i *) &frame1[3][col2]);
			r4=_mm256_setzero_si256();
			r5=_mm256_setzero_si256();
			r6=_mm256_setzero_si256();


			cy0=_mm256_load_si256( (__m256i *) &mask_vector_y[3]);
			cy1=_mm256_load_si256( (__m256i *) &mask_vector_y[4]);
			cy2=_mm256_load_si256( (__m256i *) &mask_vector_y[5]);
			cy3=_mm256_load_si256( (__m256i *) &mask_vector_y[6]);
			cy4=_mm256_setzero_si256();
			cy5=_mm256_setzero_si256();
			cy6=_mm256_setzero_si256();


			cy0_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[10]);
			cy1_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[11]);
			cy2_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[12]);
			cy3_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[13]);
			cy4_sh1=_mm256_setzero_si256();
			cy5_sh1=_mm256_setzero_si256();
			cy6_sh1=_mm256_setzero_si256();

		}
		else if (row==1){
			r0=_mm256_loadu_si256( (__m256i *) &frame1[0][col2]);
			r1=_mm256_loadu_si256( (__m256i *) &frame1[1][col2]);
			r2=_mm256_loadu_si256( (__m256i *) &frame1[2][col2]);
			r3=_mm256_loadu_si256( (__m256i *) &frame1[3][col2]);
			r4=_mm256_loadu_si256( (__m256i *) &frame1[4][col2]);
			r5=_mm256_setzero_si256();
			r6=_mm256_setzero_si256();


			cy0=_mm256_load_si256( (__m256i *) &mask_vector_y[2]);
			cy1=_mm256_load_si256( (__m256i *) &mask_vector_y[3]);
			cy2=_mm256_load_si256( (__m256i *) &mask_vector_y[4]);
			cy3=_mm256_load_si256( (__m256i *) &mask_vector_y[5]);
			cy4=_mm256_load_si256( (__m256i *) &mask_vector_y[6]);
			cy5=_mm256_setzero_si256();
			cy6=_mm256_setzero_si256();


			cy0_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[9]);
			cy1_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[10]);
			cy2_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[11]);
			cy3_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[12]);
			cy4_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[13]);
			cy5_sh1=_mm256_setzero_si256();
			cy6_sh1=_mm256_setzero_si256();

		}
		else if (row==2){
			r0=_mm256_loadu_si256( (__m256i *) &frame1[0][col2]);
			r1=_mm256_loadu_si256( (__m256i *) &frame1[1][col2]);
			r2=_mm256_loadu_si256( (__m256i *) &frame1[2][col2]);
			r3=_mm256_loadu_si256( (__m256i *) &frame1[3][col2]);
			r4=_mm256_loadu_si256( (__m256i *) &frame1[4][col2]);
			r5=_mm256_loadu_si256( (__m256i *) &frame1[5][col2]);
			r6=_mm256_setzero_si256();


			cy0=_mm256_load_si256( (__m256i *) &mask_vector_y[1]);
			cy1=_mm256_load_si256( (__m256i *) &mask_vector_y[2]);
			cy2=_mm256_load_si256( (__m256i *) &mask_vector_y[3]);
			cy3=_mm256_load_si256( (__m256i *) &mask_vector_y[4]);
			cy4=_mm256_load_si256( (__m256i *) &mask_vector_y[5]);
			cy5=_mm256_load_si256( (__m256i *) &mask_vector_y[6]);
			cy6=_mm256_setzero_si256();

			cy0_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[8]);
			cy1_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[9]);
			cy2_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[10]);
			cy3_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[11]);
			cy4_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[12]);
			cy5_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[13]);
			cy6_sh1=_mm256_setzero_si256();

		}

		else if (row==N-3){
			r0=_mm256_setzero_si256();
			r1=_mm256_loadu_si256( (__m256i *) &frame1[N-6][col2]);
			r2=_mm256_loadu_si256( (__m256i *) &frame1[N-5][col2]);
			r3=_mm256_loadu_si256( (__m256i *) &frame1[N-4][col2]);
			r4=_mm256_loadu_si256( (__m256i *) &frame1[N-3][col2]);
			r5=_mm256_loadu_si256( (__m256i *) &frame1[N-2][col2]);
			r6=_mm256_loadu_si256( (__m256i *) &frame1[N-1][col2]);

			cy0=_mm256_setzero_si256();
			cy1=_mm256_load_si256( (__m256i *) &mask_vector_y[0]);
			cy2=_mm256_load_si256( (__m256i *) &mask_vector_y[1]);
			cy3=_mm256_load_si256( (__m256i *) &mask_vector_y[2]);
			cy4=_mm256_load_si256( (__m256i *) &mask_vector_y[3]);
			cy5=_mm256_load_si256( (__m256i *) &mask_vector_y[4]);
			cy6=_mm256_load_si256( (__m256i *) &mask_vector_y[5]);

			cy0_sh1=_mm256_setzero_si256();
			cy1_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[7]);
			cy2_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[8]);
			cy3_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[9]);
			cy4_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[10]);
			cy5_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[11]);
			cy6_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[12]);
		}
		else if (row==N-2){
			r0=_mm256_setzero_si256();
			r1=_mm256_setzero_si256();
			r2=_mm256_loadu_si256( (__m256i *) &frame1[N-5][col2]);
			r3=_mm256_loadu_si256( (__m256i *) &frame1[N-4][col2]);
			r4=_mm256_loadu_si256( (__m256i *) &frame1[N-3][col2]);
			r5=_mm256_loadu_si256( (__m256i *) &frame1[N-2][col2]);
			r6=_mm256_loadu_si256( (__m256i *) &frame1[N-1][col2]);

			cy0=_mm256_setzero_si256();
			cy1=_mm256_setzero_si256();
			cy2=_mm256_load_si256( (__m256i *) &mask_vector_y[0]);
			cy3=_mm256_load_si256( (__m256i *) &mask_vector_y[1]);
			cy4=_mm256_load_si256( (__m256i *) &mask_vector_y[2]);
			cy5=_mm256_load_si256( (__m256i *) &mask_vector_y[3]);
			cy6=_mm256_load_si256( (__m256i *) &mask_vector_y[4]);

			cy0_sh1=_mm256_setzero_si256();
			cy1_sh1=_mm256_setzero_si256();
			cy2_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[7]);
			cy3_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[8]);
			cy4_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[9]);
			cy5_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[10]);
			cy6_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[11]);
		}
		else if (row==N-1){
			r0=_mm256_setzero_si256();
			r1=_mm256_setzero_si256();
			r2=_mm256_setzero_si256();
			r3=_mm256_loadu_si256( (__m256i *) &frame1[N-4][col2]);
			r4=_mm256_loadu_si256( (__m256i *) &frame1[N-3][col2]);
			r5=_mm256_loadu_si256( (__m256i *) &frame1[N-2][col2]);
			r6=_mm256_loadu_si256( (__m256i *) &frame1[N-1][col2]);

			cy0=_mm256_setzero_si256();
			cy1=_mm256_setzero_si256();
			cy2=_mm256_setzero_si256();
			cy3=_mm256_load_si256( (__m256i *) &mask_vector_y[0]);
			cy4=_mm256_load_si256( (__m256i *) &mask_vector_y[1]);
			cy5=_mm256_load_si256( (__m256i *) &mask_vector_y[2]);
			cy6=_mm256_load_si256( (__m256i *) &mask_vector_y[3]);

			cy0_sh1=_mm256_setzero_si256();
			cy1_sh1=_mm256_setzero_si256();
			cy2_sh1=_mm256_setzero_si256();
			cy3_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[7]);
			cy4_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[8]);
			cy5_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[9]);
			cy6_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[10]);
		}
		else {
			printf("\nsomething went wrong");
			exit(EXIT_FAILURE);
		}
	}


	/*
	 * The above load operations load outside of the array bounds; these elements are filled with zeros just after
	 */
	reminder_mask=_mm256_load_si256( (__m256i *) &reminder_mask_9x9[REMINDER_ITERATIONS_Y-1][0]);

	//AND r0-r4 with reminder_mask
	r0=_mm256_and_si256(r0,reminder_mask);
	r1=_mm256_and_si256(r1,reminder_mask);
	r2=_mm256_and_si256(r2,reminder_mask);
	r3=_mm256_and_si256(r3,reminder_mask);
	r4=_mm256_and_si256(r4,reminder_mask);
	r5=_mm256_and_si256(r5,reminder_mask);
	r6=_mm256_and_si256(r6,reminder_mask);


//--------------------y filter begins--------------------

//multiply with the mask
	m0=_mm256_maddubs_epi16(r0,cy0);
	m1=_mm256_maddubs_epi16(r1,cy1);
	m2=_mm256_maddubs_epi16(r2,cy2);
	m3=_mm256_maddubs_epi16(r3,cy3);
	m4=_mm256_maddubs_epi16(r4,cy4);
	m5=_mm256_maddubs_epi16(r5,cy5);
	m6=_mm256_maddubs_epi16(r6,cy6);

	//vertical add
	m0=_mm256_add_epi16(m0,m1);
	m0=_mm256_add_epi16(m0,m2);
	m0=_mm256_add_epi16(m0,m3);
	m0=_mm256_add_epi16(m0,m4);
	m0=_mm256_add_epi16(m0,m5);
	m0=_mm256_add_epi16(m0,m6);


	even=division(division_case,m0,f);//even results

	//multiply with the mask
	m0=_mm256_maddubs_epi16(r0,cy0_sh1);
	m1=_mm256_maddubs_epi16(r1,cy1_sh1);
	m2=_mm256_maddubs_epi16(r2,cy2_sh1);
	m3=_mm256_maddubs_epi16(r3,cy3_sh1);
	m4=_mm256_maddubs_epi16(r4,cy4_sh1);
	m5=_mm256_maddubs_epi16(r5,cy5_sh1);
	m6=_mm256_maddubs_epi16(r6,cy6_sh1);


	//vertical add
	m0=_mm256_add_epi16(m0,m1);
	m0=_mm256_add_epi16(m0,m2);
	m0=_mm256_add_epi16(m0,m3);
	m0=_mm256_add_epi16(m0,m4);
	m0=_mm256_add_epi16(m0,m5);
	m0=_mm256_add_epi16(m0,m6);


	odd=division(division_case,m0,f);//odd results

	//pack to one register
	odd=_mm256_slli_si256(odd,1); //shift one position right
	r0=_mm256_add_epi8(even,odd); //add the odd with the even


	_mm256_storeu_si256( (__m256i *) &temp[M-REMINDER_ITERATIONS_Y],r0 );

	for (int gg=M;gg<M+32+6;gg++)
	    temp[gg]=0;
//y filter ends - r0 has now the data to be processed by x filter
}


}



int loop_reminder_7x7_16_blur_X(unsigned char **frame1,unsigned char **filt,unsigned char *temp,const unsigned int N,const unsigned int M,const unsigned int row, const unsigned int col,const unsigned int REMINDER_ITERATIONS_X,const unsigned int division_case,const signed char mask_vector_x[][32], const __m256i f, const unsigned short int divisor_xy,signed char *kernel_x){

	const __m256i cx0=_mm256_load_si256( (__m256i *) &mask_vector_x[0]);
__m256i r0,m0,m1,even,odd,output1;


const __m256i mask_16_1  = _mm256_set_epi16(0,0,0,0xffff,0,0,0,0xffff,0,0,0,0xffff,0,0,0,0xffff);



	if (REMINDER_ITERATIONS_X==0){
		return 0;}

		//1st col iteration

			//multiply by the mask
	        r0=_mm256_loadu_si256( (__m256i *) &temp[col-3]);

		//multiply by the mask
		m0=_mm256_maddubs_epi16(r0,cx0);

		//first horizontal add
		m1=_mm256_srli_si256(m0,2);
		m0=_mm256_add_epi16(m0,m1);//add

		//second horizontal add
		m1=_mm256_srli_si256(m0,4);
		m0=_mm256_add_epi16(m0,m1);//add

		even=_mm256_and_si256(m0,mask_16_1);//in 16-bit format keep only 0,4,8,12 and discard the others

			//2nd col iteration
	        r0=_mm256_loadu_si256( (__m256i *) &temp[col-2]);

		//multiply by the mask
		m0=_mm256_maddubs_epi16(r0,cx0);

		//first horizontal add
		m1=_mm256_srli_si256(m0,2);
		m0=_mm256_add_epi16(m0,m1);//add

		//second horizontal add
		m1=_mm256_srli_si256(m0,4);
		m0=_mm256_add_epi16(m0,m1);//add

		odd=_mm256_and_si256(m0,mask_16_1);//in 16-bit format keep only 0,4,8,12 and discard the others


			//3rd col iteration
	        r0=_mm256_loadu_si256( (__m256i *) &temp[col-1]);

		//multiply by the mask
		m0=_mm256_maddubs_epi16(r0,cx0);

		//first horizontal add
		m1=_mm256_srli_si256(m0,2);
		m0=_mm256_add_epi16(m0,m1);//add

		//second horizontal add
		m1=_mm256_srli_si256(m0,4);
		m0=_mm256_add_epi16(m0,m1);//add

		m0=_mm256_and_si256(m0,mask_16_1);//in 16-bit format keep only 0,4,8,12 and discard the others

			//MERGE even
		m0=_mm256_slli_si256(m0,2);
			even=_mm256_add_epi16(m0,even);


			//4th col iteration
	        r0=_mm256_loadu_si256( (__m256i *) &temp[col]);

		//multiply by the mask
		m0=_mm256_maddubs_epi16(r0,cx0);

		//first horizontal add
		m1=_mm256_srli_si256(m0,2);
		m0=_mm256_add_epi16(m0,m1);//add

		//second horizontal add
		m1=_mm256_srli_si256(m0,4);
		m0=_mm256_add_epi16(m0,m1);//add

		m0=_mm256_and_si256(m0,mask_16_1);//in 16-bit format keep only 0,4,8,12 and discard the others

		//MERGE odd
		m0=_mm256_slli_si256(m0,2);
			odd=_mm256_add_epi16(m0,odd);

			//5th col iteration
	        r0=_mm256_loadu_si256( (__m256i *) &temp[col+1]);

		//multiply by the mask
		m0=_mm256_maddubs_epi16(r0,cx0);

		//first horizontal add
		m1=_mm256_srli_si256(m0,2);
		m0=_mm256_add_epi16(m0,m1);//add

		//second horizontal add
		m1=_mm256_srli_si256(m0,4);
		m0=_mm256_add_epi16(m0,m1);//add

		m0=_mm256_and_si256(m0,mask_16_1);//in 16-bit format keep only 0,4,8,12 and discard the others

			//MERGE even
		m0=_mm256_slli_si256(m0,4);
			even=_mm256_add_epi16(m0,even);

			//6th col iteration
	        r0=_mm256_loadu_si256( (__m256i *) &temp[col+2]);

		//multiply by the mask
		m0=_mm256_maddubs_epi16(r0,cx0);

		//first horizontal add
		m1=_mm256_srli_si256(m0,2);
		m0=_mm256_add_epi16(m0,m1);//add

		//second horizontal add
		m1=_mm256_srli_si256(m0,4);
		m0=_mm256_add_epi16(m0,m1);//add

		m0=_mm256_and_si256(m0,mask_16_1);//in 16-bit format keep only 0,4,8,12 and discard the others

		//MERGE odd
		m0=_mm256_slli_si256(m0,4);
			odd=_mm256_add_epi16(m0,odd);

			//7th col iteration
	        r0=_mm256_loadu_si256( (__m256i *) &temp[col+3]);

		//multiply by the mask
		m0=_mm256_maddubs_epi16(r0,cx0);

		//first horizontal add
		m1=_mm256_srli_si256(m0,2);
		m0=_mm256_add_epi16(m0,m1);//add

		//second horizontal add
		m1=_mm256_srli_si256(m0,4);
		m0=_mm256_add_epi16(m0,m1);//add

		m0=_mm256_and_si256(m0,mask_16_1);//in 16-bit format keep only 0,4,8,12 and discard the others

			//MERGE even
		m0=_mm256_slli_si256(m0,6);
			even=_mm256_add_epi16(m0,even);

			//8th col iteration
	        r0=_mm256_loadu_si256( (__m256i *) &temp[col+4]);

		//multiply by the mask
		m0=_mm256_maddubs_epi16(r0,cx0);

		//first horizontal add
		m1=_mm256_srli_si256(m0,2);
		m0=_mm256_add_epi16(m0,m1);//add

		//second horizontal add
		m1=_mm256_srli_si256(m0,4);
		m0=_mm256_add_epi16(m0,m1);//add

		m0=_mm256_and_si256(m0,mask_16_1);//in 16-bit format keep only 0,4,8,12 and discard the others

		//MERGE odd
		m0=_mm256_slli_si256(m0,6);
			odd=_mm256_add_epi16(m0,odd);

			even = division(division_case,even,f);
			odd = division(division_case,odd,f);

		odd=_mm256_slli_si256(odd,1);
			output1 = _mm256_add_epi8(even,odd);


	switch (REMINDER_ITERATIONS_X){
	case 1:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		  break;
	case 2:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		  break;
	case 3:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		  break;
	case 4:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output1,3);
		  break;
	case 5:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output1,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output1,4);
		  break;
	case 6:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output1,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output1,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output1,5);
		  break;
	case 7:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output1,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output1,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output1,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output1,6);
		  break;
	case 8:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output1,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output1,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output1,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output1,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output1,7);
		  break;
	case 9:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output1,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output1,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output1,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output1,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output1,7);
		filt[row][col+8] = (unsigned char) _mm256_extract_epi8(output1,8);
		  break;
	case 10:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output1,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output1,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output1,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output1,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output1,7);
		filt[row][col+8] = (unsigned char) _mm256_extract_epi8(output1,8);
		filt[row][col+9] = (unsigned char) _mm256_extract_epi8(output1,9);
		  break;
	case 11:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output1,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output1,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output1,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output1,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output1,7);
		filt[row][col+8] = (unsigned char) _mm256_extract_epi8(output1,8);
		filt[row][col+9] = (unsigned char) _mm256_extract_epi8(output1,9);
		filt[row][col+10] = (unsigned char) _mm256_extract_epi8(output1,10);
		  break;
	case 12:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output1,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output1,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output1,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output1,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output1,7);
		filt[row][col+8] = (unsigned char) _mm256_extract_epi8(output1,8);
		filt[row][col+9] = (unsigned char) _mm256_extract_epi8(output1,9);
		filt[row][col+10] = (unsigned char) _mm256_extract_epi8(output1,10);
		filt[row][col+11] = (unsigned char) _mm256_extract_epi8(output1,11);
		  break;
	case 13:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output1,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output1,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output1,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output1,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output1,7);
		filt[row][col+8] = (unsigned char) _mm256_extract_epi8(output1,8);
		filt[row][col+9] = (unsigned char) _mm256_extract_epi8(output1,9);
		filt[row][col+10] = (unsigned char) _mm256_extract_epi8(output1,10);
		filt[row][col+11] = (unsigned char) _mm256_extract_epi8(output1,11);
		filt[row][col+12] = (unsigned char) _mm256_extract_epi8(output1,12);
		  break;
	case 14:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output1,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output1,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output1,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output1,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output1,7);
		filt[row][col+8] = (unsigned char) _mm256_extract_epi8(output1,8);
		filt[row][col+9] = (unsigned char) _mm256_extract_epi8(output1,9);
		filt[row][col+10] = (unsigned char) _mm256_extract_epi8(output1,10);
		filt[row][col+11] = (unsigned char) _mm256_extract_epi8(output1,11);
		filt[row][col+12] = (unsigned char) _mm256_extract_epi8(output1,12);
		filt[row][col+13] = (unsigned char) _mm256_extract_epi8(output1,13);
		  break;
	case 15:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output1,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output1,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output1,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output1,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output1,7);
		filt[row][col+8] = (unsigned char) _mm256_extract_epi8(output1,8);
		filt[row][col+9] = (unsigned char) _mm256_extract_epi8(output1,9);
		filt[row][col+10] = (unsigned char) _mm256_extract_epi8(output1,10);
		filt[row][col+11] = (unsigned char) _mm256_extract_epi8(output1,11);
		filt[row][col+12] = (unsigned char) _mm256_extract_epi8(output1,12);
		filt[row][col+13] = (unsigned char) _mm256_extract_epi8(output1,13);
		filt[row][col+14] = (unsigned char) _mm256_extract_epi8(output1,14);
		  break;
	case 16:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
		  break;
	case 17:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
		  break;
	case 18:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
		  break;
	case 19:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
		  break;
	case 20:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output1,19);
		  break;
	case 21:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output1,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output1,20);
		  break;
	case 22:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output1,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output1,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output1,21);
		break;
	case 23:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output1,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output1,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output1,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output1,22);
		  break;
	case 24:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output1,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output1,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output1,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output1,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output1,23);
		  break;
	case 25:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output1,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output1,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output1,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output1,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output1,23);
	filt[row][col+24] = (unsigned char) _mm256_extract_epi8(output1,24);
		  break;
	case 26:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output1,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output1,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output1,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output1,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output1,23);
	filt[row][col+24] = (unsigned char) _mm256_extract_epi8(output1,24);
	filt[row][col+25] = (unsigned char) _mm256_extract_epi8(output1,25);
		  break;
	case 27:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output1,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output1,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output1,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output1,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output1,23);
	filt[row][col+24] = (unsigned char) _mm256_extract_epi8(output1,24);
	filt[row][col+25] = (unsigned char) _mm256_extract_epi8(output1,25);
	filt[row][col+26] = (unsigned char) _mm256_extract_epi8(output1,26);
		  break;
	case 28:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output1,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output1,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output1,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output1,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output1,23);
	filt[row][col+24] = (unsigned char) _mm256_extract_epi8(output1,24);
	filt[row][col+25] = (unsigned char) _mm256_extract_epi8(output1,25);
	filt[row][col+26] = (unsigned char) _mm256_extract_epi8(output1,26);
	filt[row][col+27] = (unsigned char) _mm256_extract_epi8(output1,27);
		  break;
	case 29:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output1,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output1,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output1,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output1,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output1,23);
	filt[row][col+24] = (unsigned char) _mm256_extract_epi8(output1,24);
	filt[row][col+25] = (unsigned char) _mm256_extract_epi8(output1,25);
	filt[row][col+26] = (unsigned char) _mm256_extract_epi8(output1,26);
	filt[row][col+27] = (unsigned char) _mm256_extract_epi8(output1,27);
	filt[row][col+28] = (unsigned char) _mm256_extract_epi8(output1,28);
		  break;
	case 30:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output1,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output1,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output1,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output1,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output1,23);
	filt[row][col+24] = (unsigned char) _mm256_extract_epi8(output1,24);
	filt[row][col+25] = (unsigned char) _mm256_extract_epi8(output1,25);
	filt[row][col+26] = (unsigned char) _mm256_extract_epi8(output1,26);
	filt[row][col+27] = (unsigned char) _mm256_extract_epi8(output1,27);
	filt[row][col+28] = (unsigned char) _mm256_extract_epi8(output1,28);
	filt[row][col+29] = (unsigned char) _mm256_extract_epi8(output1,29);
		  break;
	case 31:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output1,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output1,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output1,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output1,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output1,23);
	filt[row][col+24] = (unsigned char) _mm256_extract_epi8(output1,24);
	filt[row][col+25] = (unsigned char) _mm256_extract_epi8(output1,25);
	filt[row][col+26] = (unsigned char) _mm256_extract_epi8(output1,26);
	filt[row][col+27] = (unsigned char) _mm256_extract_epi8(output1,27);
	filt[row][col+28] = (unsigned char) _mm256_extract_epi8(output1,28);
	filt[row][col+29] = (unsigned char) _mm256_extract_epi8(output1,29);

	int newPixel = 0;

	newPixel += temp[col+30-4] * kernel_x[0];
	newPixel += temp[col+30-3] * kernel_x[1];
	newPixel += temp[col+30-2] * kernel_x[2];
	newPixel += temp[col+30-1] * kernel_x[3];
	newPixel += temp[col+30-0] * kernel_x[4];

	filt[row][col+30] = (unsigned char) (newPixel / divisor_xy);

		  break;
	default:
		printf("\nsomething went wrong");
		exit(EXIT_FAILURE);
 	}

	return 0;

}





void Gaussian_Blur_7x7_32_separable(unsigned char **frame1,unsigned char **filt,const unsigned int M, const unsigned int N, signed char *kernel_y, signed char *kernel_x, const unsigned short int divisor_xy){

const signed char fx0=kernel_x[0];
const signed char fx1=kernel_x[1];
const signed char fx2=kernel_x[2];
const signed char fx3=kernel_x[3];
const signed char fx4=kernel_x[4];
const signed char fx5=kernel_x[5];
const signed char fx6=kernel_x[6];

const signed char fy0=kernel_y[0];
const signed char fy1=kernel_y[1];
const signed char fy2=kernel_y[2];
const signed char fy3=kernel_y[3];
const signed char fy4=kernel_y[4];
const signed char fy5=kernel_y[5];
const signed char fy6=kernel_y[6];



const signed char mask_vector_x[1][32] __attribute__((aligned(64))) ={
		{fx0,fx1,fx2,fx3,fx4,fx5,fx6,0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,0,fx0,fx1,fx2,fx3,fx4,fx5,fx6,0},
};

const signed char mask_vector_y[14][32] __attribute__((aligned(64))) ={
	{fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0},
	{fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0},
	{fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0},
	{fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0},
	{fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0},
	{fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0},
	{fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0},

	{0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0},
	{0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1},
	{0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2},
	{0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3},
	{0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4},
	{0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5,0,fy5},
	{0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6,0,fy6},
};


	const __m256i cx0=_mm256_load_si256( (__m256i *) &mask_vector_x[0]);
	const __m256i mask_even_16  = _mm256_set_epi16(0,0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff);
	const __m256i mask_odd_16  = _mm256_set_epi16(0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff,0);
	const __m256i ones=_mm256_set1_epi16(1);


	const __m256i cy0=_mm256_load_si256( (__m256i *) &mask_vector_y[0]);
	const __m256i cy1=_mm256_load_si256( (__m256i *) &mask_vector_y[1]);
	const __m256i cy2=_mm256_load_si256( (__m256i *) &mask_vector_y[2]);
	const __m256i cy3=_mm256_load_si256( (__m256i *) &mask_vector_y[3]);
	const __m256i cy4=_mm256_load_si256( (__m256i *) &mask_vector_y[4]);
	const __m256i cy5=_mm256_load_si256( (__m256i *) &mask_vector_y[5]);
	const __m256i cy6=_mm256_load_si256( (__m256i *) &mask_vector_y[6]);

	const __m256i cy0_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[7]);
	const __m256i cy1_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[8]);
	const __m256i cy2_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[9]);
	const __m256i cy3_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[10]);
	const __m256i cy4_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[11]);
	const __m256i cy5_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[12]);
	const __m256i cy6_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[13]);

	const __m256i mask_even_32  = _mm256_set_epi32(0,0xffffffff,0,0xffffffff,0,0xffffffff,0,0xffffffff);

	const __m256i mask_prelude_3  = _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0);
	const __m256i mask_prelude_2  = _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0);
	const __m256i mask_prelude_1  = _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0);
	const __m256i mask_prelude  = _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0);




	const unsigned int REMINDER_ITERATIONS_XY_mask =  M-((((M-32)/32)*32)+32);

	//printf("\nREMINDER_ITERATIONS=%u",REMINDER_ITERATIONS_XY_mask);


	const unsigned int division_case=prepare_for_division_32(divisor_xy); //determine which is the division case (A, B or C)
	const __m256i f = _mm256_load_si256( (__m256i *) &f_vector[0]);// initialize the division vector
	//const __m256i ones=_mm256_set1_epi16(1);

//	const unsigned int division_case_x=prepare_for_division_32(divisor_x); //determine which is the division case (A, B or C)
//	const __m256i f_x = _mm256_load_si256( (__m256i *) &f_vector_x[0]);// initialize the division vector
//printf("\n%d %d",division_case,b);


#pragma omp parallel
{

unsigned int row,col;
__m256i r0,r1,r2,r3,r4,r5,r6,m0,m1,m2,m3,m4,m5,m6,even,odd,output1,output2,output3,output4;
unsigned char temp[M+32+4] __attribute__((aligned(64)));//temporal storage of the output of y filter.


/*---------------------- Gaussian Blur ---------------------------------*/

    #pragma omp for schedule(static)
	for (row = 2; row < N-2; row++) {

		if (row<3){
				prelude_7x7_32_Ymask_0_new(0, M, frame1, temp,mask_vector_y, division_case, f, mask_prelude);
				loop_reminder_7x7_32_blur_Y(frame1,filt,temp,N,M,0, REMINDER_ITERATIONS_XY_mask, division_case, mask_vector_y, f, divisor_xy);
				prelude_7x7_32_Xmask_new(frame1,filt,temp,N,M,0, mask_vector_x,  division_case,f,REMINDER_ITERATIONS_XY_mask,mask_vector_y,divisor_xy,kernel_x);

				prelude_7x7_32_Ymask_1_new(0, M, frame1, temp,mask_vector_y, division_case, f, mask_prelude);
				loop_reminder_7x7_32_blur_Y(frame1,filt,temp,N,M,1, REMINDER_ITERATIONS_XY_mask, division_case, mask_vector_y, f, divisor_xy);
				prelude_7x7_32_Xmask_new(frame1,filt,temp,N,M,1, mask_vector_x,  division_case,f,REMINDER_ITERATIONS_XY_mask,mask_vector_y,divisor_xy,kernel_x);

				prelude_7x7_32_Ymask_2_new(0, M, frame1, temp,mask_vector_y, division_case, f, mask_prelude);
				loop_reminder_7x7_32_blur_Y(frame1,filt,temp,N,M,2, REMINDER_ITERATIONS_XY_mask, division_case, mask_vector_y, f, divisor_xy);
				prelude_7x7_32_Xmask_new(frame1,filt,temp,N,M,2, mask_vector_x,  division_case,f,REMINDER_ITERATIONS_XY_mask,mask_vector_y,divisor_xy,kernel_x);

		}

		else if (row>N-4){

			prelude_7x7_32_Ymask_0_new(N-4, M, frame1, temp,mask_vector_y, division_case, f, mask_prelude);
			loop_reminder_7x7_32_blur_Y(frame1,filt,temp,N,M,N-1, REMINDER_ITERATIONS_XY_mask, division_case, mask_vector_y, f, divisor_xy);
			prelude_7x7_32_Xmask_new(frame1,filt,temp,N,M,N-1, mask_vector_x,  division_case,f,REMINDER_ITERATIONS_XY_mask,mask_vector_y,divisor_xy,kernel_x);

			prelude_7x7_32_Ymask_1_new(N-5, M, frame1, temp,mask_vector_y, division_case, f, mask_prelude);
			loop_reminder_7x7_32_blur_Y(frame1,filt,temp,N,M,N-2, REMINDER_ITERATIONS_XY_mask, division_case, mask_vector_y, f, divisor_xy);
			prelude_7x7_32_Xmask_new(frame1,filt,temp,N,M,N-2, mask_vector_x,  division_case,f,REMINDER_ITERATIONS_XY_mask,mask_vector_y,divisor_xy,kernel_x);

			prelude_7x7_32_Ymask_2_new(N-6, M, frame1, temp,mask_vector_y, division_case, f, mask_prelude);
			loop_reminder_7x7_32_blur_Y(frame1,filt,temp,N,M,N-3, REMINDER_ITERATIONS_XY_mask, division_case, mask_vector_y, f, divisor_xy);
			prelude_7x7_32_Xmask_new(frame1,filt,temp,N,M,N-3, mask_vector_x,  division_case,f,REMINDER_ITERATIONS_XY_mask,mask_vector_y,divisor_xy,kernel_x);

		}

	  else { //main loop


		  for (col = 0; col <= M-32; col+=32){

					//load the 7 rows
					r0=_mm256_load_si256( (__m256i *) &frame1[row-3][col]);
					r1=_mm256_load_si256( (__m256i *) &frame1[row-2][col]);
					r2=_mm256_load_si256( (__m256i *) &frame1[row-1][col]);
					r3=_mm256_load_si256( (__m256i *) &frame1[row][col]);
					r4=_mm256_load_si256( (__m256i *) &frame1[row+1][col]);
					r5=_mm256_load_si256( (__m256i *) &frame1[row+2][col]);
					r6=_mm256_load_si256( (__m256i *) &frame1[row+3][col]);

			  //--------------------y filter begins--------------------
					//multiply by the mask
					m0=_mm256_maddubs_epi16(r0,cy0);
					m1=_mm256_maddubs_epi16(r1,cy1);
					m2=_mm256_maddubs_epi16(r2,cy2);
					m3=_mm256_maddubs_epi16(r3,cy3);
					m4=_mm256_maddubs_epi16(r4,cy4);
					m5=_mm256_maddubs_epi16(r5,cy5);
					m6=_mm256_maddubs_epi16(r6,cy6);

					//discard odd
			__m256i m0_even=_mm256_and_si256(m0,mask_even_16);
			__m256i	m1_even=_mm256_and_si256(m1,mask_even_16);
			__m256i	m2_even=_mm256_and_si256(m2,mask_even_16);
			__m256i	m3_even=_mm256_and_si256(m3,mask_even_16);
			__m256i	m4_even=_mm256_and_si256(m4,mask_even_16);
			__m256i	m5_even=_mm256_and_si256(m5,mask_even_16);
			__m256i	m6_even=_mm256_and_si256(m6,mask_even_16);

			//make from 16-bit to 32bit
			  m0_even=_mm256_madd_epi16(m0_even,ones);
			  m1_even=_mm256_madd_epi16(m1_even,ones);
			  m2_even=_mm256_madd_epi16(m2_even,ones);
			  m3_even=_mm256_madd_epi16(m3_even,ones);
			  m4_even=_mm256_madd_epi16(m4_even,ones);
			  m5_even=_mm256_madd_epi16(m5_even,ones);
			  m6_even=_mm256_madd_epi16(m6_even,ones);

					//vertical 32-bit add
					m0_even=_mm256_add_epi32(m0_even,m1_even);
					m0_even=_mm256_add_epi32(m0_even,m2_even);
					m0_even=_mm256_add_epi32(m0_even,m3_even);
					m0_even=_mm256_add_epi32(m0_even,m4_even);
					m0_even=_mm256_add_epi32(m0_even,m5_even);
					m0_even=_mm256_add_epi32(m0_even,m6_even);

					//discard even
			__m256i	m0_odd=_mm256_and_si256(m0,mask_odd_16);
			__m256i	m1_odd=_mm256_and_si256(m1,mask_odd_16);
			__m256i	m2_odd=_mm256_and_si256(m2,mask_odd_16);
			__m256i	m3_odd=_mm256_and_si256(m3,mask_odd_16);
			__m256i	m4_odd=_mm256_and_si256(m4,mask_odd_16);
			__m256i	m5_odd=_mm256_and_si256(m5,mask_odd_16);
			__m256i	m6_odd=_mm256_and_si256(m6,mask_odd_16);

			//make from 16-bit to 32bit
					m0_odd=_mm256_madd_epi16(m0_odd,ones);
					m1_odd=_mm256_madd_epi16(m1_odd,ones);
					m2_odd=_mm256_madd_epi16(m2_odd,ones);
					m3_odd=_mm256_madd_epi16(m3_odd,ones);
					m4_odd=_mm256_madd_epi16(m4_odd,ones);
					m5_odd=_mm256_madd_epi16(m5_odd,ones);
					m6_odd=_mm256_madd_epi16(m6_odd,ones);

					//vertical 32-bit add
					m0_odd=_mm256_add_epi32(m0_odd,m1_odd);
					m0_odd=_mm256_add_epi32(m0_odd,m2_odd);
					m0_odd=_mm256_add_epi32(m0_odd,m3_odd);
					m0_odd=_mm256_add_epi32(m0_odd,m4_odd);
					m0_odd=_mm256_add_epi32(m0_odd,m5_odd);
					m0_odd=_mm256_add_epi32(m0_odd,m6_odd);

					m0_even=division_32(division_case,m0_even,f);//even results
					m0_odd=division_32(division_case,m0_odd,f);//even results

					m0_odd=_mm256_slli_si256(m0_odd,2); //shift two position right
					even=_mm256_add_epi8(m0_even,m0_odd); //add the odd with the even

					//-----------------multiply by the second mask----------
					m0=_mm256_maddubs_epi16(r0,cy0_sh1);
					m1=_mm256_maddubs_epi16(r1,cy1_sh1);
					m2=_mm256_maddubs_epi16(r2,cy2_sh1);
					m3=_mm256_maddubs_epi16(r3,cy3_sh1);
					m4=_mm256_maddubs_epi16(r4,cy4_sh1);
					m5=_mm256_maddubs_epi16(r5,cy5_sh1);
					m6=_mm256_maddubs_epi16(r6,cy6_sh1);


					m0_even=_mm256_and_si256(m0,mask_even_16);
					m1_even=_mm256_and_si256(m1,mask_even_16);
					m2_even=_mm256_and_si256(m2,mask_even_16);
					m3_even=_mm256_and_si256(m3,mask_even_16);
					m4_even=_mm256_and_si256(m4,mask_even_16);
					m5_even=_mm256_and_si256(m5,mask_even_16);
					m6_even=_mm256_and_si256(m6,mask_even_16);

					//make from 16-bit to 32bit
					  m0_even=_mm256_madd_epi16(m0_even,ones);
					  m1_even=_mm256_madd_epi16(m1_even,ones);
					  m2_even=_mm256_madd_epi16(m2_even,ones);
					  m3_even=_mm256_madd_epi16(m3_even,ones);
					  m4_even=_mm256_madd_epi16(m4_even,ones);
					  m5_even=_mm256_madd_epi16(m5_even,ones);
					  m6_even=_mm256_madd_epi16(m6_even,ones);

					//vertical 32-bit add
					m0_even=_mm256_add_epi32(m0_even,m1_even);
					m0_even=_mm256_add_epi32(m0_even,m2_even);
					m0_even=_mm256_add_epi32(m0_even,m3_even);
					m0_even=_mm256_add_epi32(m0_even,m4_even);
					m0_even=_mm256_add_epi32(m0_even,m5_even);
					m0_even=_mm256_add_epi32(m0_even,m6_even);

					 m0_odd=_mm256_and_si256(m0,mask_odd_16);
					 m1_odd=_mm256_and_si256(m1,mask_odd_16);
					 m2_odd=_mm256_and_si256(m2,mask_odd_16);
					 m3_odd=_mm256_and_si256(m3,mask_odd_16);
					 m4_odd=_mm256_and_si256(m4,mask_odd_16);
					 m5_odd=_mm256_and_si256(m5,mask_odd_16);
					 m6_odd=_mm256_and_si256(m6,mask_odd_16);

					 //make from 16-bit to 32bit
						m0_odd=_mm256_madd_epi16(m0_odd,ones);
						m1_odd=_mm256_madd_epi16(m1_odd,ones);
						m2_odd=_mm256_madd_epi16(m2_odd,ones);
						m3_odd=_mm256_madd_epi16(m3_odd,ones);
						m4_odd=_mm256_madd_epi16(m4_odd,ones);
						m5_odd=_mm256_madd_epi16(m5_odd,ones);
						m6_odd=_mm256_madd_epi16(m6_odd,ones);


					//vertical 32-bit add
					m0_odd=_mm256_add_epi32(m0_odd,m1_odd);
					m0_odd=_mm256_add_epi32(m0_odd,m2_odd);
					m0_odd=_mm256_add_epi32(m0_odd,m3_odd);
					m0_odd=_mm256_add_epi32(m0_odd,m4_odd);
					m0_odd=_mm256_add_epi32(m0_odd,m5_odd);
					m0_odd=_mm256_add_epi32(m0_odd,m6_odd);

					m0_even=division_32(division_case,m0_even,f);//even results
					m0_odd=division_32(division_case,m0_odd,f);//even results

					m0_odd=_mm256_slli_si256(m0_odd,2); //shift two position right
					odd=_mm256_add_epi8(m0_even,m0_odd); //add the odd with the even


					//MERGE AND pack to one register
					odd=_mm256_slli_si256(odd,1); //shift one position right
					r0=_mm256_add_epi8(even,odd); //add the odd with the even
				//y filter ends - r0 has now the data to be processed by x filter

				_mm256_store_si256( (__m256i *) &temp[col],r0 );
		  }
	loop_reminder_7x7_32_blur_Y(frame1,filt,temp,N,M,row, REMINDER_ITERATIONS_XY_mask, division_case, mask_vector_y, f, divisor_xy);


	  for (col = 0; col <= M-32; col+=32){

		  if (col==0){

		  		//1st col iteration

		__m256i rr0=_mm256_load_si256( (__m256i *) &temp[0]);
		  		r0=fill_3zeros(rr0,mask_prelude_3);

				//multiply by the mask
				m0=_mm256_maddubs_epi16(r0,cx0);

				//make 16-bit values 32-bit (horizontal add)
				m0=_mm256_madd_epi16(m0,ones);

				//last horizontal add
				m1=_mm256_srli_si256(m0,4);//shift by one integer
				m0=_mm256_add_epi32(m0,m1);//add
				output1=_mm256_and_si256(m0,mask_even_32);//keep only 0,2,4,6 and discard 1,3,5,7


		  			//2nd col iteration
	  		       // r0=_mm256_loadu_si256( (__m256i *) &temp[0]);
			  		r0=fill_2zeros(rr0,mask_prelude_2);

					//multiply by the mask
					m0=_mm256_maddubs_epi16(r0,cx0);

					//make 16-bit values 32-bit (horizontal add)
					m0=_mm256_madd_epi16(m0,ones);

					//last horizontal add
					m1=_mm256_srli_si256(m0,4);//shift by one integer
					m0=_mm256_add_epi32(m0,m1);//add
					output2=_mm256_and_si256(m0,mask_even_32);//keep only 0,2,4,6 and discard 1,3,5,7


		  			//3rd col iteration
	  		      //  r0=_mm256_loadu_si256( (__m256i *) &temp[0]);
			  		r0=fill_1zeros(rr0,mask_prelude_1);

					//multiply by the mask
					m0=_mm256_maddubs_epi16(r0,cx0);

					//make 16-bit values 32-bit (horizontal add)
					m0=_mm256_madd_epi16(m0,ones);

					//last horizontal add
					m1=_mm256_srli_si256(m0,4);//shift by one integer
					m0=_mm256_add_epi32(m0,m1);//add
					output3=_mm256_and_si256(m0,mask_even_32);//keep only 0,2,4,6 and discard 1,3,5,7


		  			//4th col iteration
	  		   //     r0=_mm256_loadu_si256( (__m256i *) &temp[0]);

					//multiply by the mask
					m0=_mm256_maddubs_epi16(rr0,cx0);

					//make 16-bit values 32-bit (horizontal add)
					m0=_mm256_madd_epi16(m0,ones);

					//last horizontal add
					m1=_mm256_srli_si256(m0,4);//shift by one integer
					m0=_mm256_add_epi32(m0,m1);//add
					output4=_mm256_and_si256(m0,mask_even_32);//keep only 0,2,4,6 and discard 1,3,5,7

		  			//5th col iteration
	  		        r0=_mm256_loadu_si256( (__m256i *) &temp[1]);

					//multiply by the mask
					m0=_mm256_maddubs_epi16(r0,cx0);

					//make 16-bit values 32-bit (horizontal add)
					m0=_mm256_madd_epi16(m0,ones);

					//last horizontal add
					m1=_mm256_srli_si256(m0,4);//shift by one integer
					m0=_mm256_add_epi32(m0,m1);//add
					m0=_mm256_and_si256(m0,mask_even_32);//keep only 0,2,4,6 and discard 1,3,5,7

	  				//MERGE output1
					m0=_mm256_slli_si256(m0,4);
	  				output1=_mm256_add_epi32(output1,m0);

		  			//6th col iteration
	  		        r0=_mm256_loadu_si256( (__m256i *) &temp[2]);

					//multiply by the mask
					m0=_mm256_maddubs_epi16(r0,cx0);

					//make 16-bit values 32-bit (horizontal add)
					m0=_mm256_madd_epi16(m0,ones);

					//last horizontal add
					m1=_mm256_srli_si256(m0,4);//shift by one integer
					m0=_mm256_add_epi32(m0,m1);//add
					m0=_mm256_and_si256(m0,mask_even_32);//keep only 0,2,4,6 and discard 1,3,5,7

	  				//MERGE output1
					m0=_mm256_slli_si256(m0,4);
	  				output2=_mm256_add_epi32(output2,m0);

		  			//7th col iteration
	  		        r0=_mm256_loadu_si256( (__m256i *) &temp[3]);

					//multiply by the mask
					m0=_mm256_maddubs_epi16(r0,cx0);

					//make 16-bit values 32-bit (horizontal add)
					m0=_mm256_madd_epi16(m0,ones);

					//last horizontal add
					m1=_mm256_srli_si256(m0,4);//shift by one integer
					m0=_mm256_add_epi32(m0,m1);//add
					m0=_mm256_and_si256(m0,mask_even_32);//keep only 0,2,4,6 and discard 1,3,5,7

	  				//MERGE output1
					m0=_mm256_slli_si256(m0,4);
	  				output3=_mm256_add_epi32(output3,m0);

		  			//8th col iteration
	  		        r0=_mm256_loadu_si256( (__m256i *) &temp[4]);

					//multiply by the mask
					m0=_mm256_maddubs_epi16(r0,cx0);

					//make 16-bit values 32-bit (horizontal add)
					m0=_mm256_madd_epi16(m0,ones);

					//last horizontal add
					m1=_mm256_srli_si256(m0,4);//shift by one integer
					m0=_mm256_add_epi32(m0,m1);//add
					m0=_mm256_and_si256(m0,mask_even_32);//keep only 0,2,4,6 and discard 1,3,5,7

	  				//MERGE output1
					m0=_mm256_slli_si256(m0,4);
	  				output4=_mm256_add_epi32(output4,m0);


	  				output1 = division_32(division_case,output1,f);
	  				output2 = division_32(division_case,output2,f);
	  				output3 = division_32(division_case,output3,f);
	  				output4 = division_32(division_case,output4,f);

					output2=_mm256_slli_si256(output2,1);
					output3=_mm256_slli_si256(output3,2);
					output4=_mm256_slli_si256(output4,3);

	  				output1 = _mm256_add_epi8(output1,output2);
	  				output1 = _mm256_add_epi8(output1,output3);
	  				output1 = _mm256_add_epi8(output1,output4);

	  				_mm256_store_si256( (__m256i *) &filt[row][0],output1);

		  		  	}
		  else {

				// col iteration computes output pixels of   0,8,16,24
				// col+1 iteration computes output pixels of 1,9,17,25
				// col+2 iteration computes output pixels of 2,10,18,26
				// col+3 iteration computes output pixels of 3,11,19,27
				// col+4 iteration computes output pixels of 4,12,20,28
				// col+5 iteration computes output pixels of 5,13,21,29
				// col+6 iteration computes output pixels of 6,14,22,30
				// col+7 iteration computes output pixels of 7,15,23,31
				//afterwards, col becomes 32 and repeat the above process

		  		//1st col iteration

			  r0=_mm256_loadu_si256( (__m256i *) &temp[col-3]);

				//multiply by the mask
				m0=_mm256_maddubs_epi16(r0,cx0);

				//make 16-bit values 32-bit (horizontal add)
				m0=_mm256_madd_epi16(m0,ones);

				//last horizontal add
				m1=_mm256_srli_si256(m0,4);//shift by one integer
				m0=_mm256_add_epi32(m0,m1);//add
				output1=_mm256_and_si256(m0,mask_even_32);//keep only 0,2,4,6 and discard 1,3,5,7


		  			//2nd col iteration
	  		        r0=_mm256_loadu_si256( (__m256i *) &temp[col-2]);

					//multiply by the mask
					m0=_mm256_maddubs_epi16(r0,cx0);

					//make 16-bit values 32-bit (horizontal add)
					m0=_mm256_madd_epi16(m0,ones);

					//last horizontal add
					m1=_mm256_srli_si256(m0,4);//shift by one integer
					m0=_mm256_add_epi32(m0,m1);//add
					output2=_mm256_and_si256(m0,mask_even_32);//keep only 0,2,4,6 and discard 1,3,5,7


		  			//3rd col iteration
	  		        r0=_mm256_loadu_si256( (__m256i *) &temp[col-1]);

					//multiply by the mask
					m0=_mm256_maddubs_epi16(r0,cx0);

					//make 16-bit values 32-bit (horizontal add)
					m0=_mm256_madd_epi16(m0,ones);

					//last horizontal add
					m1=_mm256_srli_si256(m0,4);//shift by one integer
					m0=_mm256_add_epi32(m0,m1);//add
					output3=_mm256_and_si256(m0,mask_even_32);//keep only 0,2,4,6 and discard 1,3,5,7


		  			//4th col iteration
	  		        r0=_mm256_load_si256( (__m256i *) &temp[col]);

					//multiply by the mask
					m0=_mm256_maddubs_epi16(r0,cx0);

					//make 16-bit values 32-bit (horizontal add)
					m0=_mm256_madd_epi16(m0,ones);

					//last horizontal add
					m1=_mm256_srli_si256(m0,4);//shift by one integer
					m0=_mm256_add_epi32(m0,m1);//add
					output4=_mm256_and_si256(m0,mask_even_32);//keep only 0,2,4,6 and discard 1,3,5,7

		  			//5th col iteration
	  		        r0=_mm256_loadu_si256( (__m256i *) &temp[col+1]);

					//multiply by the mask
					m0=_mm256_maddubs_epi16(r0,cx0);

					//make 16-bit values 32-bit (horizontal add)
					m0=_mm256_madd_epi16(m0,ones);

					//last horizontal add
					m1=_mm256_srli_si256(m0,4);//shift by one integer
					m0=_mm256_add_epi32(m0,m1);//add
					m0=_mm256_and_si256(m0,mask_even_32);//keep only 0,2,4,6 and discard 1,3,5,7

	  				//MERGE output1
					m0=_mm256_slli_si256(m0,4);
	  				output1=_mm256_add_epi32(output1,m0);

		  			//6th col iteration
	  		        r0=_mm256_loadu_si256( (__m256i *) &temp[col+2]);

					//multiply by the mask
					m0=_mm256_maddubs_epi16(r0,cx0);

					//make 16-bit values 32-bit (horizontal add)
					m0=_mm256_madd_epi16(m0,ones);

					//last horizontal add
					m1=_mm256_srli_si256(m0,4);//shift by one integer
					m0=_mm256_add_epi32(m0,m1);//add
					m0=_mm256_and_si256(m0,mask_even_32);//keep only 0,2,4,6 and discard 1,3,5,7

	  				//MERGE output1
					m0=_mm256_slli_si256(m0,4);
	  				output2=_mm256_add_epi32(output2,m0);

		  			//7th col iteration
	  		        r0=_mm256_loadu_si256( (__m256i *) &temp[col+3]);

					//multiply by the mask
					m0=_mm256_maddubs_epi16(r0,cx0);

					//make 16-bit values 32-bit (horizontal add)
					m0=_mm256_madd_epi16(m0,ones);

					//last horizontal add
					m1=_mm256_srli_si256(m0,4);//shift by one integer
					m0=_mm256_add_epi32(m0,m1);//add
					m0=_mm256_and_si256(m0,mask_even_32);//keep only 0,2,4,6 and discard 1,3,5,7

	  				//MERGE output1
					m0=_mm256_slli_si256(m0,4);
	  				output3=_mm256_add_epi32(output3,m0);

		  			//8th col iteration
	  		        r0=_mm256_loadu_si256( (__m256i *) &temp[col+4]);

					//multiply by the mask
					m0=_mm256_maddubs_epi16(r0,cx0);

					//make 16-bit values 32-bit (horizontal add)
					m0=_mm256_madd_epi16(m0,ones);

					//last horizontal add
					m1=_mm256_srli_si256(m0,4);//shift by one integer
					m0=_mm256_add_epi32(m0,m1);//add
					m0=_mm256_and_si256(m0,mask_even_32);//keep only 0,2,4,6 and discard 1,3,5,7

	  				//MERGE output1
					m0=_mm256_slli_si256(m0,4);
	  				output4=_mm256_add_epi32(output4,m0);


	  				output1 = division_32(division_case,output1,f);
	  				output2 = division_32(division_case,output2,f);
	  				output3 = division_32(division_case,output3,f);
	  				output4 = division_32(division_case,output4,f);

					output2=_mm256_slli_si256(output2,1);
					output3=_mm256_slli_si256(output3,2);
					output4=_mm256_slli_si256(output4,3);

	  				output1 = _mm256_add_epi8(output1,output2);
	  				output1 = _mm256_add_epi8(output1,output3);
	  				output1 = _mm256_add_epi8(output1,output4);

	  				_mm256_store_si256( (__m256i *) &filt[row][col],output1);

		  		 }

		}

		loop_reminder_7x7_32_blur_X(frame1,filt,temp,N,M,row, col, REMINDER_ITERATIONS_XY_mask, division_case, mask_vector_x, f, divisor_xy,kernel_x);

		}
}

}//end of parallel


}



//this function is computing the row=2 and row=N-3 only
//for row=2 input prelude_7x7_Ymask_3(0,...
//for row=N-3 input prelude_7x7_Ymask_3(N-6,...
void prelude_7x7_32_Ymask_2_new(const int row, const unsigned int M, unsigned char **frame1, unsigned char *temp, const signed char mask_vector_y[][32],  const unsigned int division_case, const __m256i f, const __m256i mask_prelude){

	__m256i r0,r1,r2,r3,r4,r5,m0,m1,m2,m3,m4,m5,even,odd;
	//__m256i ones=_mm256_set1_epi16(1);
	 __m256i cy0,cy1,cy2,cy3,cy4,cy5,cy0_sh1,cy1_sh1,cy2_sh1,cy3_sh1,cy4_sh1,cy5_sh1;
	 const __m256i mask_even_16  = _mm256_set_epi16(0,0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff);
	 	const __m256i mask_odd_16  = _mm256_set_epi16(0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff,0);
	 	const __m256i ones=_mm256_set1_epi16(1);

	if (row!=0){ //if row=N-6
	  cy0=_mm256_load_si256( (__m256i *) &mask_vector_y[0]);
	  cy1=_mm256_load_si256( (__m256i *) &mask_vector_y[1]);
	  cy2=_mm256_load_si256( (__m256i *) &mask_vector_y[2]);
	  cy3=_mm256_load_si256( (__m256i *) &mask_vector_y[3]);
	  cy4=_mm256_load_si256( (__m256i *) &mask_vector_y[4]);
	  cy5=_mm256_load_si256( (__m256i *) &mask_vector_y[5]);

	  cy0_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[7]);
	  cy1_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[8]);
	  cy2_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[9]);
	  cy3_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[10]);
	  cy4_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[11]);
	  cy5_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[12]);

	}
	else {
		  cy0=_mm256_load_si256( (__m256i *) &mask_vector_y[1]);
		  cy1=_mm256_load_si256( (__m256i *) &mask_vector_y[2]);
		  cy2=_mm256_load_si256( (__m256i *) &mask_vector_y[3]);
		  cy3=_mm256_load_si256( (__m256i *) &mask_vector_y[4]);
		  cy4=_mm256_load_si256( (__m256i *) &mask_vector_y[5]);
		  cy5=_mm256_load_si256( (__m256i *) &mask_vector_y[6]);

		  cy0_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[8]);
		  cy1_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[9]);
		  cy2_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[10]);
		  cy3_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[11]);
		  cy4_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[12]);
		  cy5_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[13]);

	}

	 for (unsigned int col = 0; col <= M-32; col+=32){

	  //--------------------y filter begins--------------------
	//load the 7 rows
	r0=_mm256_load_si256( (__m256i *) &frame1[row][col]);
	r1=_mm256_load_si256( (__m256i *) &frame1[row+1][col]);
	r2=_mm256_load_si256( (__m256i *) &frame1[row+2][col]);
	r3=_mm256_load_si256( (__m256i *) &frame1[row+3][col]);
	r4=_mm256_load_si256( (__m256i *) &frame1[row+4][col]);
	r5=_mm256_load_si256( (__m256i *) &frame1[row+5][col]);


	//multiply by the mask
		m0=_mm256_maddubs_epi16(r0,cy0);
		m1=_mm256_maddubs_epi16(r1,cy1);
		m2=_mm256_maddubs_epi16(r2,cy2);
		m3=_mm256_maddubs_epi16(r3,cy3);
		m4=_mm256_maddubs_epi16(r4,cy4);
		m5=_mm256_maddubs_epi16(r5,cy5);

		//discard odd
__m256i m0_even=_mm256_and_si256(m0,mask_even_16);
__m256i	m1_even=_mm256_and_si256(m1,mask_even_16);
__m256i	m2_even=_mm256_and_si256(m2,mask_even_16);
__m256i	m3_even=_mm256_and_si256(m3,mask_even_16);
__m256i	m4_even=_mm256_and_si256(m4,mask_even_16);
__m256i	m5_even=_mm256_and_si256(m5,mask_even_16);

//make from 16-bit to 32bit
  m0_even=_mm256_madd_epi16(m0_even,ones);
  m1_even=_mm256_madd_epi16(m1_even,ones);
  m2_even=_mm256_madd_epi16(m2_even,ones);
  m3_even=_mm256_madd_epi16(m3_even,ones);
  m4_even=_mm256_madd_epi16(m4_even,ones);
  m5_even=_mm256_madd_epi16(m5_even,ones);

		//vertical 32-bit add
		m0_even=_mm256_add_epi32(m0_even,m1_even);
		m0_even=_mm256_add_epi32(m0_even,m2_even);
		m0_even=_mm256_add_epi32(m0_even,m3_even);
		m0_even=_mm256_add_epi32(m0_even,m4_even);
		m0_even=_mm256_add_epi32(m0_even,m5_even);

		//discard even
__m256i	m0_odd=_mm256_and_si256(m0,mask_odd_16);
__m256i	m1_odd=_mm256_and_si256(m1,mask_odd_16);
__m256i	m2_odd=_mm256_and_si256(m2,mask_odd_16);
__m256i	m3_odd=_mm256_and_si256(m3,mask_odd_16);
__m256i	m4_odd=_mm256_and_si256(m4,mask_odd_16);
__m256i	m5_odd=_mm256_and_si256(m5,mask_odd_16);

//make from 16-bit to 32bit
		m0_odd=_mm256_madd_epi16(m0_odd,ones);
		m1_odd=_mm256_madd_epi16(m1_odd,ones);
		m2_odd=_mm256_madd_epi16(m2_odd,ones);
		m3_odd=_mm256_madd_epi16(m3_odd,ones);
		m4_odd=_mm256_madd_epi16(m4_odd,ones);
		m5_odd=_mm256_madd_epi16(m5_odd,ones);

		//vertical 32-bit add
		m0_odd=_mm256_add_epi32(m0_odd,m1_odd);
		m0_odd=_mm256_add_epi32(m0_odd,m2_odd);
		m0_odd=_mm256_add_epi32(m0_odd,m3_odd);
		m0_odd=_mm256_add_epi32(m0_odd,m4_odd);
		m0_odd=_mm256_add_epi32(m0_odd,m5_odd);

		m0_even=division_32(division_case,m0_even,f);//even results
		m0_odd=division_32(division_case,m0_odd,f);//even results

		m0_odd=_mm256_slli_si256(m0_odd,2); //shift two position right
		even=_mm256_add_epi8(m0_even,m0_odd); //add the odd with the even

		//-----------------multiply by the second mask----------
		m0=_mm256_maddubs_epi16(r0,cy0_sh1);
		m1=_mm256_maddubs_epi16(r1,cy1_sh1);
		m2=_mm256_maddubs_epi16(r2,cy2_sh1);
		m3=_mm256_maddubs_epi16(r3,cy3_sh1);
		m4=_mm256_maddubs_epi16(r4,cy4_sh1);
		m5=_mm256_maddubs_epi16(r5,cy5_sh1);


		m0_even=_mm256_and_si256(m0,mask_even_16);
		m1_even=_mm256_and_si256(m1,mask_even_16);
		m2_even=_mm256_and_si256(m2,mask_even_16);
		m3_even=_mm256_and_si256(m3,mask_even_16);
		m4_even=_mm256_and_si256(m4,mask_even_16);
		m5_even=_mm256_and_si256(m5,mask_even_16);

		//make from 16-bit to 32bit
		  m0_even=_mm256_madd_epi16(m0_even,ones);
		  m1_even=_mm256_madd_epi16(m1_even,ones);
		  m2_even=_mm256_madd_epi16(m2_even,ones);
		  m3_even=_mm256_madd_epi16(m3_even,ones);
		  m4_even=_mm256_madd_epi16(m4_even,ones);
		  m5_even=_mm256_madd_epi16(m5_even,ones);

		//vertical 32-bit add
		m0_even=_mm256_add_epi32(m0_even,m1_even);
		m0_even=_mm256_add_epi32(m0_even,m2_even);
		m0_even=_mm256_add_epi32(m0_even,m3_even);
		m0_even=_mm256_add_epi32(m0_even,m4_even);
		m0_even=_mm256_add_epi32(m0_even,m5_even);

		 m0_odd=_mm256_and_si256(m0,mask_odd_16);
		 m1_odd=_mm256_and_si256(m1,mask_odd_16);
		 m2_odd=_mm256_and_si256(m2,mask_odd_16);
		 m3_odd=_mm256_and_si256(m3,mask_odd_16);
		 m4_odd=_mm256_and_si256(m4,mask_odd_16);
		 m5_odd=_mm256_and_si256(m5,mask_odd_16);

		 //make from 16-bit to 32bit
			m0_odd=_mm256_madd_epi16(m0_odd,ones);
			m1_odd=_mm256_madd_epi16(m1_odd,ones);
			m2_odd=_mm256_madd_epi16(m2_odd,ones);
			m3_odd=_mm256_madd_epi16(m3_odd,ones);
			m4_odd=_mm256_madd_epi16(m4_odd,ones);
			m5_odd=_mm256_madd_epi16(m5_odd,ones);


		//vertical 32-bit add
		m0_odd=_mm256_add_epi32(m0_odd,m1_odd);
		m0_odd=_mm256_add_epi32(m0_odd,m2_odd);
		m0_odd=_mm256_add_epi32(m0_odd,m3_odd);
		m0_odd=_mm256_add_epi32(m0_odd,m4_odd);
		m0_odd=_mm256_add_epi32(m0_odd,m5_odd);

		m0_even=division_32(division_case,m0_even,f);//even results
		m0_odd=division_32(division_case,m0_odd,f);//even results

		m0_odd=_mm256_slli_si256(m0_odd,2); //shift two position right
		odd=_mm256_add_epi8(m0_even,m0_odd); //add the odd with the even


		//MERGE AND pack to one register
		odd=_mm256_slli_si256(odd,1); //shift one position right
		r0=_mm256_add_epi8(even,odd); //add the odd with the even

			_mm256_store_si256( (__m256i *) &temp[col],r0 );
	  }
		//y filter ends - r0 has now the data to be processed by x filter

}

//this function is computing the row=1 and row=N-2 only
//for row=1 input prelude_7x7_Ymask_3(0,...
//for row=N-2 input prelude_7x7_Ymask_3(N-5,...
void prelude_7x7_32_Ymask_1_new(const int row, const unsigned int M, unsigned char **frame1, unsigned char *temp, const signed char mask_vector_y[][32],  const unsigned int division_case, const __m256i f, const __m256i mask_prelude){
	const __m256i mask_even_16  = _mm256_set_epi16(0,0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff);
		const __m256i mask_odd_16  = _mm256_set_epi16(0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff,0);
		const __m256i ones=_mm256_set1_epi16(1);
	__m256i r0,r1,r2,r3,r4,m0,m1,m2,m3,m4,even,odd;
	//__m256i ones=_mm256_set1_epi16(1);
	 __m256i cy0,cy1,cy2,cy3,cy4,cy0_sh1,cy1_sh1,cy2_sh1,cy3_sh1,cy4_sh1;

	if (row!=0){ //if row=N-6
	  cy0=_mm256_load_si256( (__m256i *) &mask_vector_y[0]);
	  cy1=_mm256_load_si256( (__m256i *) &mask_vector_y[1]);
	  cy2=_mm256_load_si256( (__m256i *) &mask_vector_y[2]);
	  cy3=_mm256_load_si256( (__m256i *) &mask_vector_y[3]);
	  cy4=_mm256_load_si256( (__m256i *) &mask_vector_y[4]);

	  cy0_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[7]);
	  cy1_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[8]);
	  cy2_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[9]);
	  cy3_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[10]);
	  cy4_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[11]);

	}
	else {
		  cy0=_mm256_load_si256( (__m256i *) &mask_vector_y[2]);
		  cy1=_mm256_load_si256( (__m256i *) &mask_vector_y[3]);
		  cy2=_mm256_load_si256( (__m256i *) &mask_vector_y[4]);
		  cy3=_mm256_load_si256( (__m256i *) &mask_vector_y[5]);
		  cy4=_mm256_load_si256( (__m256i *) &mask_vector_y[6]);

		  cy0_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[9]);
		  cy1_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[10]);
		  cy2_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[11]);
		  cy3_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[12]);
		  cy4_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[13]);

	}

	 for (unsigned int col = 0; col <= M-32; col+=32){

	  //--------------------y filter begins--------------------
	//load the 9 rows
	r0=_mm256_load_si256( (__m256i *) &frame1[row][col]);
	r1=_mm256_load_si256( (__m256i *) &frame1[row+1][col]);
	r2=_mm256_load_si256( (__m256i *) &frame1[row+2][col]);
	r3=_mm256_load_si256( (__m256i *) &frame1[row+3][col]);
	r4=_mm256_load_si256( (__m256i *) &frame1[row+4][col]);



	//multiply by the mask
		m0=_mm256_maddubs_epi16(r0,cy0);
		m1=_mm256_maddubs_epi16(r1,cy1);
		m2=_mm256_maddubs_epi16(r2,cy2);
		m3=_mm256_maddubs_epi16(r3,cy3);
		m4=_mm256_maddubs_epi16(r4,cy4);

		//discard odd
__m256i m0_even=_mm256_and_si256(m0,mask_even_16);
__m256i	m1_even=_mm256_and_si256(m1,mask_even_16);
__m256i	m2_even=_mm256_and_si256(m2,mask_even_16);
__m256i	m3_even=_mm256_and_si256(m3,mask_even_16);
__m256i	m4_even=_mm256_and_si256(m4,mask_even_16);

//make from 16-bit to 32bit
  m0_even=_mm256_madd_epi16(m0_even,ones);
  m1_even=_mm256_madd_epi16(m1_even,ones);
  m2_even=_mm256_madd_epi16(m2_even,ones);
  m3_even=_mm256_madd_epi16(m3_even,ones);
  m4_even=_mm256_madd_epi16(m4_even,ones);

		//vertical 32-bit add
		m0_even=_mm256_add_epi32(m0_even,m1_even);
		m0_even=_mm256_add_epi32(m0_even,m2_even);
		m0_even=_mm256_add_epi32(m0_even,m3_even);
		m0_even=_mm256_add_epi32(m0_even,m4_even);

		//discard even
__m256i	m0_odd=_mm256_and_si256(m0,mask_odd_16);
__m256i	m1_odd=_mm256_and_si256(m1,mask_odd_16);
__m256i	m2_odd=_mm256_and_si256(m2,mask_odd_16);
__m256i	m3_odd=_mm256_and_si256(m3,mask_odd_16);
__m256i	m4_odd=_mm256_and_si256(m4,mask_odd_16);

//make from 16-bit to 32bit
		m0_odd=_mm256_madd_epi16(m0_odd,ones);
		m1_odd=_mm256_madd_epi16(m1_odd,ones);
		m2_odd=_mm256_madd_epi16(m2_odd,ones);
		m3_odd=_mm256_madd_epi16(m3_odd,ones);
		m4_odd=_mm256_madd_epi16(m4_odd,ones);

		//vertical 32-bit add
		m0_odd=_mm256_add_epi32(m0_odd,m1_odd);
		m0_odd=_mm256_add_epi32(m0_odd,m2_odd);
		m0_odd=_mm256_add_epi32(m0_odd,m3_odd);
		m0_odd=_mm256_add_epi32(m0_odd,m4_odd);

		m0_even=division_32(division_case,m0_even,f);//even results
		m0_odd=division_32(division_case,m0_odd,f);//even results

		m0_odd=_mm256_slli_si256(m0_odd,2); //shift two position right
		even=_mm256_add_epi8(m0_even,m0_odd); //add the odd with the even

		//-----------------multiply by the second mask----------
		m0=_mm256_maddubs_epi16(r0,cy0_sh1);
		m1=_mm256_maddubs_epi16(r1,cy1_sh1);
		m2=_mm256_maddubs_epi16(r2,cy2_sh1);
		m3=_mm256_maddubs_epi16(r3,cy3_sh1);
		m4=_mm256_maddubs_epi16(r4,cy4_sh1);


		m0_even=_mm256_and_si256(m0,mask_even_16);
		m1_even=_mm256_and_si256(m1,mask_even_16);
		m2_even=_mm256_and_si256(m2,mask_even_16);
		m3_even=_mm256_and_si256(m3,mask_even_16);
		m4_even=_mm256_and_si256(m4,mask_even_16);

		//make from 16-bit to 32bit
		  m0_even=_mm256_madd_epi16(m0_even,ones);
		  m1_even=_mm256_madd_epi16(m1_even,ones);
		  m2_even=_mm256_madd_epi16(m2_even,ones);
		  m3_even=_mm256_madd_epi16(m3_even,ones);
		  m4_even=_mm256_madd_epi16(m4_even,ones);

		//vertical 32-bit add
		m0_even=_mm256_add_epi32(m0_even,m1_even);
		m0_even=_mm256_add_epi32(m0_even,m2_even);
		m0_even=_mm256_add_epi32(m0_even,m3_even);
		m0_even=_mm256_add_epi32(m0_even,m4_even);

		 m0_odd=_mm256_and_si256(m0,mask_odd_16);
		 m1_odd=_mm256_and_si256(m1,mask_odd_16);
		 m2_odd=_mm256_and_si256(m2,mask_odd_16);
		 m3_odd=_mm256_and_si256(m3,mask_odd_16);
		 m4_odd=_mm256_and_si256(m4,mask_odd_16);

		 //make from 16-bit to 32bit
			m0_odd=_mm256_madd_epi16(m0_odd,ones);
			m1_odd=_mm256_madd_epi16(m1_odd,ones);
			m2_odd=_mm256_madd_epi16(m2_odd,ones);
			m3_odd=_mm256_madd_epi16(m3_odd,ones);
			m4_odd=_mm256_madd_epi16(m4_odd,ones);


		//vertical 32-bit add
		m0_odd=_mm256_add_epi32(m0_odd,m1_odd);
		m0_odd=_mm256_add_epi32(m0_odd,m2_odd);
		m0_odd=_mm256_add_epi32(m0_odd,m3_odd);
		m0_odd=_mm256_add_epi32(m0_odd,m4_odd);

		m0_even=division_32(division_case,m0_even,f);//even results
		m0_odd=division_32(division_case,m0_odd,f);//even results

		m0_odd=_mm256_slli_si256(m0_odd,2); //shift two position right
		odd=_mm256_add_epi8(m0_even,m0_odd); //add the odd with the even


		//MERGE AND pack to one register
		odd=_mm256_slli_si256(odd,1); //shift one position right
		r0=_mm256_add_epi8(even,odd); //add the odd with the even

	_mm256_store_si256( (__m256i *) &temp[col],r0 );
	  }
		//y filter ends - r0 has now the data to be processed by x filter

}

//this function is computing the row=0 and row=N-1 only
//for row=0 input prelude_7x7_Ymask_3(0,...
//for row=N-1 input prelude_7x7_Ymask_3(N-4,...
void prelude_7x7_32_Ymask_0_new(const int row, const unsigned int M, unsigned char **frame1, unsigned char *temp, const signed char mask_vector_y[][32],  const unsigned int division_case, const __m256i f, const __m256i mask_prelude){
	const __m256i mask_even_16  = _mm256_set_epi16(0,0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff);
		const __m256i mask_odd_16  = _mm256_set_epi16(0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff,0);
		const __m256i ones=_mm256_set1_epi16(1);
	__m256i r0,r1,r2,r3,m0,m1,m2,m3,even,odd;
	//__m256i ones=_mm256_set1_epi16(1);
	 __m256i cy0,cy1,cy2,cy3,cy0_sh1,cy1_sh1,cy2_sh1,cy3_sh1;

	if (row!=0){ //if row=N-6
	  cy0=_mm256_load_si256( (__m256i *) &mask_vector_y[0]);
	  cy1=_mm256_load_si256( (__m256i *) &mask_vector_y[1]);
	  cy2=_mm256_load_si256( (__m256i *) &mask_vector_y[2]);
	  cy3=_mm256_load_si256( (__m256i *) &mask_vector_y[3]);

	  cy0_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[7]);
	  cy1_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[8]);
	  cy2_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[9]);
	  cy3_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[10]);
	}
	else {
		  cy0=_mm256_load_si256( (__m256i *) &mask_vector_y[3]);
		  cy1=_mm256_load_si256( (__m256i *) &mask_vector_y[4]);
		  cy2=_mm256_load_si256( (__m256i *) &mask_vector_y[5]);
		  cy3=_mm256_load_si256( (__m256i *) &mask_vector_y[6]);

		  cy0_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[10]);
		  cy1_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[11]);
		  cy2_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[12]);
		  cy3_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[13]);

	}

	 for (unsigned int col = 0; col <= M-32; col+=32){

	  //--------------------y filter begins--------------------
	//load the 9 rows
	r0=_mm256_load_si256( (__m256i *) &frame1[row][col]);
	r1=_mm256_load_si256( (__m256i *) &frame1[row+1][col]);
	r2=_mm256_load_si256( (__m256i *) &frame1[row+2][col]);
	r3=_mm256_load_si256( (__m256i *) &frame1[row+3][col]);




	//multiply by the mask
		m0=_mm256_maddubs_epi16(r0,cy0);
		m1=_mm256_maddubs_epi16(r1,cy1);
		m2=_mm256_maddubs_epi16(r2,cy2);
		m3=_mm256_maddubs_epi16(r3,cy3);


		//discard odd
__m256i m0_even=_mm256_and_si256(m0,mask_even_16);
__m256i	m1_even=_mm256_and_si256(m1,mask_even_16);
__m256i	m2_even=_mm256_and_si256(m2,mask_even_16);
__m256i	m3_even=_mm256_and_si256(m3,mask_even_16);


//make from 16-bit to 32bit
  m0_even=_mm256_madd_epi16(m0_even,ones);
  m1_even=_mm256_madd_epi16(m1_even,ones);
  m2_even=_mm256_madd_epi16(m2_even,ones);
  m3_even=_mm256_madd_epi16(m3_even,ones);


		//vertical 32-bit add
		m0_even=_mm256_add_epi32(m0_even,m1_even);
		m0_even=_mm256_add_epi32(m0_even,m2_even);
		m0_even=_mm256_add_epi32(m0_even,m3_even);


		//discard even
__m256i	m0_odd=_mm256_and_si256(m0,mask_odd_16);
__m256i	m1_odd=_mm256_and_si256(m1,mask_odd_16);
__m256i	m2_odd=_mm256_and_si256(m2,mask_odd_16);
__m256i	m3_odd=_mm256_and_si256(m3,mask_odd_16);


//make from 16-bit to 32bit
		m0_odd=_mm256_madd_epi16(m0_odd,ones);
		m1_odd=_mm256_madd_epi16(m1_odd,ones);
		m2_odd=_mm256_madd_epi16(m2_odd,ones);
		m3_odd=_mm256_madd_epi16(m3_odd,ones);


		//vertical 32-bit add
		m0_odd=_mm256_add_epi32(m0_odd,m1_odd);
		m0_odd=_mm256_add_epi32(m0_odd,m2_odd);
		m0_odd=_mm256_add_epi32(m0_odd,m3_odd);


		m0_even=division_32(division_case,m0_even,f);//even results
		m0_odd=division_32(division_case,m0_odd,f);//even results

		m0_odd=_mm256_slli_si256(m0_odd,2); //shift two position right
		even=_mm256_add_epi8(m0_even,m0_odd); //add the odd with the even

		//-----------------multiply by the second mask----------
		m0=_mm256_maddubs_epi16(r0,cy0_sh1);
		m1=_mm256_maddubs_epi16(r1,cy1_sh1);
		m2=_mm256_maddubs_epi16(r2,cy2_sh1);
		m3=_mm256_maddubs_epi16(r3,cy3_sh1);

		m0_even=_mm256_and_si256(m0,mask_even_16);
		m1_even=_mm256_and_si256(m1,mask_even_16);
		m2_even=_mm256_and_si256(m2,mask_even_16);
		m3_even=_mm256_and_si256(m3,mask_even_16);


		//make from 16-bit to 32bit
		  m0_even=_mm256_madd_epi16(m0_even,ones);
		  m1_even=_mm256_madd_epi16(m1_even,ones);
		  m2_even=_mm256_madd_epi16(m2_even,ones);
		  m3_even=_mm256_madd_epi16(m3_even,ones);

		//vertical 32-bit add
		m0_even=_mm256_add_epi32(m0_even,m1_even);
		m0_even=_mm256_add_epi32(m0_even,m2_even);
		m0_even=_mm256_add_epi32(m0_even,m3_even);


		 m0_odd=_mm256_and_si256(m0,mask_odd_16);
		 m1_odd=_mm256_and_si256(m1,mask_odd_16);
		 m2_odd=_mm256_and_si256(m2,mask_odd_16);
		 m3_odd=_mm256_and_si256(m3,mask_odd_16);


		 //make from 16-bit to 32bit
			m0_odd=_mm256_madd_epi16(m0_odd,ones);
			m1_odd=_mm256_madd_epi16(m1_odd,ones);
			m2_odd=_mm256_madd_epi16(m2_odd,ones);
			m3_odd=_mm256_madd_epi16(m3_odd,ones);


		//vertical 32-bit add
		m0_odd=_mm256_add_epi32(m0_odd,m1_odd);
		m0_odd=_mm256_add_epi32(m0_odd,m2_odd);
		m0_odd=_mm256_add_epi32(m0_odd,m3_odd);

		m0_even=division_32(division_case,m0_even,f);//even results
		m0_odd=division_32(division_case,m0_odd,f);//even results

		m0_odd=_mm256_slli_si256(m0_odd,2); //shift two position right
		odd=_mm256_add_epi8(m0_even,m0_odd); //add the odd with the even


		//MERGE AND pack to one register
		odd=_mm256_slli_si256(odd,1); //shift one position right
		r0=_mm256_add_epi8(even,odd); //add the odd with the even

			_mm256_store_si256( (__m256i *) &temp[col],r0 );
	  }
		//y filter ends - r0 has now the data to be processed by x filter

}



void prelude_7x7_32_Xmask_new(unsigned char **frame1,unsigned char **filt,unsigned char *temp,  const unsigned int N,const unsigned int M, const int row, const signed char mask_vector_x[][32],  const unsigned int division_case, const __m256i f,const unsigned int REMINDER_ITERATIONS_XY ,const signed char mask_vector_y[][32],const unsigned short int divisor_xy,signed char *kernel_x){

	__m256i r0,m0,m1,output1,output2,output3,output4;

	const __m256i cx0=_mm256_load_si256( (__m256i *) &mask_vector_x[0]);


	const __m256i mask_prelude_3  = _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0);
	const __m256i mask_prelude_2  = _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0);
	const __m256i mask_prelude_1  = _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0);

	const __m256i ones=_mm256_set1_epi16(1);
	const __m256i mask_even_32  = _mm256_set_epi32(0,0xffffffff,0,0xffffffff,0,0xffffffff,0,0xffffffff);


	unsigned int col;



	  for (col = 0; col <= M-32; col+=32){

		  if (col==0){

		  		//1st col iteration

			  r0=_mm256_loadu_si256( (__m256i *) &temp[0]);
		  		r0=fill_3zeros(r0,mask_prelude_3);

				//multiply by the mask
				m0=_mm256_maddubs_epi16(r0,cx0);

				//make 16-bit values 32-bit (horizontal add)
				m0=_mm256_madd_epi16(m0,ones);

				//last horizontal add
				m1=_mm256_srli_si256(m0,4);//shift by one integer
				m0=_mm256_add_epi32(m0,m1);//add
				output1=_mm256_and_si256(m0,mask_even_32);//keep only 0,2,4,6 and discard 1,3,5,7


		  			//2nd col iteration
	  		        r0=_mm256_loadu_si256( (__m256i *) &temp[0]);
			  		r0=fill_2zeros(r0,mask_prelude_2);

					//multiply by the mask
					m0=_mm256_maddubs_epi16(r0,cx0);

					//make 16-bit values 32-bit (horizontal add)
					m0=_mm256_madd_epi16(m0,ones);

					//last horizontal add
					m1=_mm256_srli_si256(m0,4);//shift by one integer
					m0=_mm256_add_epi32(m0,m1);//add
					output2=_mm256_and_si256(m0,mask_even_32);//keep only 0,2,4,6 and discard 1,3,5,7


		  			//3rd col iteration
	  		        r0=_mm256_loadu_si256( (__m256i *) &temp[0]);
			  		r0=fill_1zeros(r0,mask_prelude_1);

					//multiply by the mask
					m0=_mm256_maddubs_epi16(r0,cx0);

					//make 16-bit values 32-bit (horizontal add)
					m0=_mm256_madd_epi16(m0,ones);

					//last horizontal add
					m1=_mm256_srli_si256(m0,4);//shift by one integer
					m0=_mm256_add_epi32(m0,m1);//add
					output3=_mm256_and_si256(m0,mask_even_32);//keep only 0,2,4,6 and discard 1,3,5,7


		  			//4th col iteration
	  		        r0=_mm256_loadu_si256( (__m256i *) &temp[0]);

					//multiply by the mask
					m0=_mm256_maddubs_epi16(r0,cx0);

					//make 16-bit values 32-bit (horizontal add)
					m0=_mm256_madd_epi16(m0,ones);

					//last horizontal add
					m1=_mm256_srli_si256(m0,4);//shift by one integer
					m0=_mm256_add_epi32(m0,m1);//add
					output4=_mm256_and_si256(m0,mask_even_32);//keep only 0,2,4,6 and discard 1,3,5,7

		  			//5th col iteration
	  		        r0=_mm256_loadu_si256( (__m256i *) &temp[1]);

					//multiply by the mask
					m0=_mm256_maddubs_epi16(r0,cx0);

					//make 16-bit values 32-bit (horizontal add)
					m0=_mm256_madd_epi16(m0,ones);

					//last horizontal add
					m1=_mm256_srli_si256(m0,4);//shift by one integer
					m0=_mm256_add_epi32(m0,m1);//add
					m0=_mm256_and_si256(m0,mask_even_32);//keep only 0,2,4,6 and discard 1,3,5,7

	  				//MERGE output1
					m0=_mm256_slli_si256(m0,4);
	  				output1=_mm256_add_epi32(output1,m0);

		  			//6th col iteration
	  		        r0=_mm256_loadu_si256( (__m256i *) &temp[2]);

					//multiply by the mask
					m0=_mm256_maddubs_epi16(r0,cx0);

					//make 16-bit values 32-bit (horizontal add)
					m0=_mm256_madd_epi16(m0,ones);

					//last horizontal add
					m1=_mm256_srli_si256(m0,4);//shift by one integer
					m0=_mm256_add_epi32(m0,m1);//add
					m0=_mm256_and_si256(m0,mask_even_32);//keep only 0,2,4,6 and discard 1,3,5,7

	  				//MERGE output1
					m0=_mm256_slli_si256(m0,4);
	  				output2=_mm256_add_epi32(output2,m0);

		  			//7th col iteration
	  		        r0=_mm256_loadu_si256( (__m256i *) &temp[3]);

					//multiply by the mask
					m0=_mm256_maddubs_epi16(r0,cx0);

					//make 16-bit values 32-bit (horizontal add)
					m0=_mm256_madd_epi16(m0,ones);

					//last horizontal add
					m1=_mm256_srli_si256(m0,4);//shift by one integer
					m0=_mm256_add_epi32(m0,m1);//add
					m0=_mm256_and_si256(m0,mask_even_32);//keep only 0,2,4,6 and discard 1,3,5,7

	  				//MERGE output1
					m0=_mm256_slli_si256(m0,4);
	  				output3=_mm256_add_epi32(output3,m0);

		  			//8th col iteration
	  		        r0=_mm256_loadu_si256( (__m256i *) &temp[4]);

					//multiply by the mask
					m0=_mm256_maddubs_epi16(r0,cx0);

					//make 16-bit values 32-bit (horizontal add)
					m0=_mm256_madd_epi16(m0,ones);

					//last horizontal add
					m1=_mm256_srli_si256(m0,4);//shift by one integer
					m0=_mm256_add_epi32(m0,m1);//add
					m0=_mm256_and_si256(m0,mask_even_32);//keep only 0,2,4,6 and discard 1,3,5,7

	  				//MERGE output1
					m0=_mm256_slli_si256(m0,4);
	  				output4=_mm256_add_epi32(output4,m0);


	  				output1 = division_32(division_case,output1,f);
	  				output2 = division_32(division_case,output2,f);
	  				output3 = division_32(division_case,output3,f);
	  				output4 = division_32(division_case,output4,f);

					output2=_mm256_slli_si256(output2,1);
					output3=_mm256_slli_si256(output3,2);
					output4=_mm256_slli_si256(output4,3);

	  				output1 = _mm256_add_epi8(output1,output2);
	  				output1 = _mm256_add_epi8(output1,output3);
	  				output1 = _mm256_add_epi8(output1,output4);

	  				_mm256_store_si256( (__m256i *) &filt[row][0],output1);

		  		  	}
		  else {

				// col iteration computes output pixels of   0,8,16,24
				// col+1 iteration computes output pixels of 1,9,17,25
				// col+2 iteration computes output pixels of 2,10,18,26
				// col+3 iteration computes output pixels of 3,11,19,27
				// col+4 iteration computes output pixels of 4,12,20,28
				// col+5 iteration computes output pixels of 5,13,21,29
				// col+6 iteration computes output pixels of 6,14,22,30
				// col+7 iteration computes output pixels of 7,15,23,31
				//afterwards, col becomes 32 and repeat the above process

		  		//1st col iteration

			  r0=_mm256_loadu_si256( (__m256i *) &temp[col-3]);

				//multiply by the mask
				m0=_mm256_maddubs_epi16(r0,cx0);

				//make 16-bit values 32-bit (horizontal add)
				m0=_mm256_madd_epi16(m0,ones);

				//last horizontal add
				m1=_mm256_srli_si256(m0,4);//shift by one integer
				m0=_mm256_add_epi32(m0,m1);//add
				output1=_mm256_and_si256(m0,mask_even_32);//keep only 0,2,4,6 and discard 1,3,5,7


		  			//2nd col iteration
	  		        r0=_mm256_loadu_si256( (__m256i *) &temp[col-2]);

					//multiply by the mask
					m0=_mm256_maddubs_epi16(r0,cx0);

					//make 16-bit values 32-bit (horizontal add)
					m0=_mm256_madd_epi16(m0,ones);

					//last horizontal add
					m1=_mm256_srli_si256(m0,4);//shift by one integer
					m0=_mm256_add_epi32(m0,m1);//add
					output2=_mm256_and_si256(m0,mask_even_32);//keep only 0,2,4,6 and discard 1,3,5,7


		  			//3rd col iteration
	  		        r0=_mm256_loadu_si256( (__m256i *) &temp[col-1]);

					//multiply by the mask
					m0=_mm256_maddubs_epi16(r0,cx0);

					//make 16-bit values 32-bit (horizontal add)
					m0=_mm256_madd_epi16(m0,ones);

					//last horizontal add
					m1=_mm256_srli_si256(m0,4);//shift by one integer
					m0=_mm256_add_epi32(m0,m1);//add
					output3=_mm256_and_si256(m0,mask_even_32);//keep only 0,2,4,6 and discard 1,3,5,7


		  			//4th col iteration
	  		        r0=_mm256_loadu_si256( (__m256i *) &temp[col]);

					//multiply by the mask
					m0=_mm256_maddubs_epi16(r0,cx0);

					//make 16-bit values 32-bit (horizontal add)
					m0=_mm256_madd_epi16(m0,ones);

					//last horizontal add
					m1=_mm256_srli_si256(m0,4);//shift by one integer
					m0=_mm256_add_epi32(m0,m1);//add
					output4=_mm256_and_si256(m0,mask_even_32);//keep only 0,2,4,6 and discard 1,3,5,7

		  			//5th col iteration
	  		        r0=_mm256_loadu_si256( (__m256i *) &temp[col+1]);

					//multiply by the mask
					m0=_mm256_maddubs_epi16(r0,cx0);

					//make 16-bit values 32-bit (horizontal add)
					m0=_mm256_madd_epi16(m0,ones);

					//last horizontal add
					m1=_mm256_srli_si256(m0,4);//shift by one integer
					m0=_mm256_add_epi32(m0,m1);//add
					m0=_mm256_and_si256(m0,mask_even_32);//keep only 0,2,4,6 and discard 1,3,5,7

	  				//MERGE output1
					m0=_mm256_slli_si256(m0,4);
	  				output1=_mm256_add_epi32(output1,m0);

		  			//6th col iteration
	  		        r0=_mm256_loadu_si256( (__m256i *) &temp[col+2]);

					//multiply by the mask
					m0=_mm256_maddubs_epi16(r0,cx0);

					//make 16-bit values 32-bit (horizontal add)
					m0=_mm256_madd_epi16(m0,ones);

					//last horizontal add
					m1=_mm256_srli_si256(m0,4);//shift by one integer
					m0=_mm256_add_epi32(m0,m1);//add
					m0=_mm256_and_si256(m0,mask_even_32);//keep only 0,2,4,6 and discard 1,3,5,7

	  				//MERGE output1
					m0=_mm256_slli_si256(m0,4);
	  				output2=_mm256_add_epi32(output2,m0);

		  			//7th col iteration
	  		        r0=_mm256_loadu_si256( (__m256i *) &temp[col+3]);

					//multiply by the mask
					m0=_mm256_maddubs_epi16(r0,cx0);

					//make 16-bit values 32-bit (horizontal add)
					m0=_mm256_madd_epi16(m0,ones);

					//last horizontal add
					m1=_mm256_srli_si256(m0,4);//shift by one integer
					m0=_mm256_add_epi32(m0,m1);//add
					m0=_mm256_and_si256(m0,mask_even_32);//keep only 0,2,4,6 and discard 1,3,5,7

	  				//MERGE output1
					m0=_mm256_slli_si256(m0,4);
	  				output3=_mm256_add_epi32(output3,m0);

		  			//8th col iteration
	  		        r0=_mm256_loadu_si256( (__m256i *) &temp[col+4]);

					//multiply by the mask
					m0=_mm256_maddubs_epi16(r0,cx0);

					//make 16-bit values 32-bit (horizontal add)
					m0=_mm256_madd_epi16(m0,ones);

					//last horizontal add
					m1=_mm256_srli_si256(m0,4);//shift by one integer
					m0=_mm256_add_epi32(m0,m1);//add
					m0=_mm256_and_si256(m0,mask_even_32);//keep only 0,2,4,6 and discard 1,3,5,7

	  				//MERGE output1
					m0=_mm256_slli_si256(m0,4);
	  				output4=_mm256_add_epi32(output4,m0);


	  				output1 = division_32(division_case,output1,f);
	  				output2 = division_32(division_case,output2,f);
	  				output3 = division_32(division_case,output3,f);
	  				output4 = division_32(division_case,output4,f);

					output2=_mm256_slli_si256(output2,1);
					output3=_mm256_slli_si256(output3,2);
					output4=_mm256_slli_si256(output4,3);

	  				output1 = _mm256_add_epi8(output1,output2);
	  				output1 = _mm256_add_epi8(output1,output3);
	  				output1 = _mm256_add_epi8(output1,output4);

	  				_mm256_store_si256( (__m256i *) &filt[row][col],output1);

		  		 }

		}


	loop_reminder_7x7_32_blur_X(frame1,filt,temp,N,M,row, col, REMINDER_ITERATIONS_XY, division_case, mask_vector_x, f, divisor_xy,kernel_x);


}




void loop_reminder_7x7_32_blur_Y(unsigned char **frame1,unsigned char **filt,unsigned char *temp,const unsigned int N,const unsigned int M,const unsigned int row, const unsigned int REMINDER_ITERATIONS_Y,const unsigned int division_case,const signed char mask_vector_y[][32], const __m256i f, const unsigned short int divisor_xy){

	__m256i r0,r1,r2,r3,r4,r5,r6,m0,m1,m2,m3,m4,m5,m6,even,odd,reminder_mask;
	const __m256i ones=_mm256_set1_epi16(1);
	const __m256i mask_even_16  = _mm256_set_epi16(0,0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff);
	const __m256i mask_odd_16  = _mm256_set_epi16(0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff,0,0xffff,0);

 if (REMINDER_ITERATIONS_Y==0){//no need for computing the y mask
		_mm256_store_si256( (__m256i *) &temp[M],_mm256_setzero_si256() );
        for (unsigned int g=M+32;g<M+32+6;g++)
        	temp[g]=0;
	}
 else {
		__m256i cy0,cy1,cy2,cy3,cy4,cy5,cy6,cy0_sh1,cy1_sh1,cy2_sh1,cy3_sh1,cy4_sh1,cy5_sh1,cy6_sh1;

	unsigned int col2=M-REMINDER_ITERATIONS_Y;//this is the col index to load the elements for the y mask

	if ((row>2) && (row<N-3)){//main case

		r0=_mm256_loadu_si256( (__m256i *) &frame1[row-3][col2]);
		r1=_mm256_loadu_si256( (__m256i *) &frame1[row-2][col2]);
		r2=_mm256_loadu_si256( (__m256i *) &frame1[row-1][col2]);
		r3=_mm256_loadu_si256( (__m256i *) &frame1[row][col2]);
		r4=_mm256_loadu_si256( (__m256i *) &frame1[row+1][col2]);
		r5=_mm256_loadu_si256( (__m256i *) &frame1[row+2][col2]);
		r6=_mm256_loadu_si256( (__m256i *) &frame1[row+3][col2]);

		cy0=_mm256_load_si256( (__m256i *) &mask_vector_y[0]);
		cy1=_mm256_load_si256( (__m256i *) &mask_vector_y[1]);
		cy2=_mm256_load_si256( (__m256i *) &mask_vector_y[2]);
		cy3=_mm256_load_si256( (__m256i *) &mask_vector_y[3]);
		cy4=_mm256_load_si256( (__m256i *) &mask_vector_y[4]);
		cy5=_mm256_load_si256( (__m256i *) &mask_vector_y[5]);
		cy6=_mm256_load_si256( (__m256i *) &mask_vector_y[6]);

		cy0_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[7]);
		cy1_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[8]);
		cy2_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[9]);
		cy3_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[10]);
		cy4_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[11]);
		cy5_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[12]);
		cy6_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[13]);


	}
	else {
		if (row==0){
			r0=_mm256_loadu_si256( (__m256i *) &frame1[0][col2]);
			r1=_mm256_loadu_si256( (__m256i *) &frame1[1][col2]);
			r2=_mm256_loadu_si256( (__m256i *) &frame1[2][col2]);
			r3=_mm256_loadu_si256( (__m256i *) &frame1[3][col2]);
			r4=_mm256_setzero_si256();
			r5=_mm256_setzero_si256();
			r6=_mm256_setzero_si256();


			cy0=_mm256_load_si256( (__m256i *) &mask_vector_y[3]);
			cy1=_mm256_load_si256( (__m256i *) &mask_vector_y[4]);
			cy2=_mm256_load_si256( (__m256i *) &mask_vector_y[5]);
			cy3=_mm256_load_si256( (__m256i *) &mask_vector_y[6]);
			cy4=_mm256_setzero_si256();
			cy5=_mm256_setzero_si256();
			cy6=_mm256_setzero_si256();


			cy0_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[10]);
			cy1_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[11]);
			cy2_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[12]);
			cy3_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[13]);
			cy4_sh1=_mm256_setzero_si256();
			cy5_sh1=_mm256_setzero_si256();
			cy6_sh1=_mm256_setzero_si256();

		}
		else if (row==1){
			r0=_mm256_loadu_si256( (__m256i *) &frame1[0][col2]);
			r1=_mm256_loadu_si256( (__m256i *) &frame1[1][col2]);
			r2=_mm256_loadu_si256( (__m256i *) &frame1[2][col2]);
			r3=_mm256_loadu_si256( (__m256i *) &frame1[3][col2]);
			r4=_mm256_loadu_si256( (__m256i *) &frame1[4][col2]);
			r5=_mm256_setzero_si256();
			r6=_mm256_setzero_si256();


			cy0=_mm256_load_si256( (__m256i *) &mask_vector_y[2]);
			cy1=_mm256_load_si256( (__m256i *) &mask_vector_y[3]);
			cy2=_mm256_load_si256( (__m256i *) &mask_vector_y[4]);
			cy3=_mm256_load_si256( (__m256i *) &mask_vector_y[5]);
			cy4=_mm256_load_si256( (__m256i *) &mask_vector_y[6]);
			cy5=_mm256_setzero_si256();
			cy6=_mm256_setzero_si256();


			cy0_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[9]);
			cy1_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[10]);
			cy2_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[11]);
			cy3_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[12]);
			cy4_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[13]);
			cy5_sh1=_mm256_setzero_si256();
			cy6_sh1=_mm256_setzero_si256();

		}
		else if (row==2){
			r0=_mm256_loadu_si256( (__m256i *) &frame1[0][col2]);
			r1=_mm256_loadu_si256( (__m256i *) &frame1[1][col2]);
			r2=_mm256_loadu_si256( (__m256i *) &frame1[2][col2]);
			r3=_mm256_loadu_si256( (__m256i *) &frame1[3][col2]);
			r4=_mm256_loadu_si256( (__m256i *) &frame1[4][col2]);
			r5=_mm256_loadu_si256( (__m256i *) &frame1[5][col2]);
			r6=_mm256_setzero_si256();


			cy0=_mm256_load_si256( (__m256i *) &mask_vector_y[1]);
			cy1=_mm256_load_si256( (__m256i *) &mask_vector_y[2]);
			cy2=_mm256_load_si256( (__m256i *) &mask_vector_y[3]);
			cy3=_mm256_load_si256( (__m256i *) &mask_vector_y[4]);
			cy4=_mm256_load_si256( (__m256i *) &mask_vector_y[5]);
			cy5=_mm256_load_si256( (__m256i *) &mask_vector_y[6]);
			cy6=_mm256_setzero_si256();

			cy0_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[8]);
			cy1_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[9]);
			cy2_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[10]);
			cy3_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[11]);
			cy4_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[12]);
			cy5_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[13]);
			cy6_sh1=_mm256_setzero_si256();

		}

		else if (row==N-3){
			r0=_mm256_setzero_si256();
			r1=_mm256_loadu_si256( (__m256i *) &frame1[N-6][col2]);
			r2=_mm256_loadu_si256( (__m256i *) &frame1[N-5][col2]);
			r3=_mm256_loadu_si256( (__m256i *) &frame1[N-4][col2]);
			r4=_mm256_loadu_si256( (__m256i *) &frame1[N-3][col2]);
			r5=_mm256_loadu_si256( (__m256i *) &frame1[N-2][col2]);
			r6=_mm256_loadu_si256( (__m256i *) &frame1[N-1][col2]);

			cy0=_mm256_setzero_si256();
			cy1=_mm256_load_si256( (__m256i *) &mask_vector_y[0]);
			cy2=_mm256_load_si256( (__m256i *) &mask_vector_y[1]);
			cy3=_mm256_load_si256( (__m256i *) &mask_vector_y[2]);
			cy4=_mm256_load_si256( (__m256i *) &mask_vector_y[3]);
			cy5=_mm256_load_si256( (__m256i *) &mask_vector_y[4]);
			cy6=_mm256_load_si256( (__m256i *) &mask_vector_y[5]);

			cy0_sh1=_mm256_setzero_si256();
			cy1_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[7]);
			cy2_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[8]);
			cy3_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[9]);
			cy4_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[10]);
			cy5_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[11]);
			cy6_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[12]);
		}
		else if (row==N-2){
			r0=_mm256_setzero_si256();
			r1=_mm256_setzero_si256();
			r2=_mm256_loadu_si256( (__m256i *) &frame1[N-5][col2]);
			r3=_mm256_loadu_si256( (__m256i *) &frame1[N-4][col2]);
			r4=_mm256_loadu_si256( (__m256i *) &frame1[N-3][col2]);
			r5=_mm256_loadu_si256( (__m256i *) &frame1[N-2][col2]);
			r6=_mm256_loadu_si256( (__m256i *) &frame1[N-1][col2]);

			cy0=_mm256_setzero_si256();
			cy1=_mm256_setzero_si256();
			cy2=_mm256_load_si256( (__m256i *) &mask_vector_y[0]);
			cy3=_mm256_load_si256( (__m256i *) &mask_vector_y[1]);
			cy4=_mm256_load_si256( (__m256i *) &mask_vector_y[2]);
			cy5=_mm256_load_si256( (__m256i *) &mask_vector_y[3]);
			cy6=_mm256_load_si256( (__m256i *) &mask_vector_y[4]);

			cy0_sh1=_mm256_setzero_si256();
			cy1_sh1=_mm256_setzero_si256();
			cy2_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[7]);
			cy3_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[8]);
			cy4_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[9]);
			cy5_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[10]);
			cy6_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[11]);
		}
		else if (row==N-1){
			r0=_mm256_setzero_si256();
			r1=_mm256_setzero_si256();
			r2=_mm256_setzero_si256();
			r3=_mm256_loadu_si256( (__m256i *) &frame1[N-4][col2]);
			r4=_mm256_loadu_si256( (__m256i *) &frame1[N-3][col2]);
			r5=_mm256_loadu_si256( (__m256i *) &frame1[N-2][col2]);
			r6=_mm256_loadu_si256( (__m256i *) &frame1[N-1][col2]);

			cy0=_mm256_setzero_si256();
			cy1=_mm256_setzero_si256();
			cy2=_mm256_setzero_si256();
			cy3=_mm256_load_si256( (__m256i *) &mask_vector_y[0]);
			cy4=_mm256_load_si256( (__m256i *) &mask_vector_y[1]);
			cy5=_mm256_load_si256( (__m256i *) &mask_vector_y[2]);
			cy6=_mm256_load_si256( (__m256i *) &mask_vector_y[3]);

			cy0_sh1=_mm256_setzero_si256();
			cy1_sh1=_mm256_setzero_si256();
			cy2_sh1=_mm256_setzero_si256();
			cy3_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[7]);
			cy4_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[8]);
			cy5_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[9]);
			cy6_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[10]);
		}
		else {
			printf("\nsomething went wrong");
			exit(EXIT_FAILURE);
		}
	}


	/*
	 * The above load operations load outside of the array bounds; these elements are filled with zeros just after
	 */
	reminder_mask=_mm256_load_si256( (__m256i *) &reminder_mask_9x9[REMINDER_ITERATIONS_Y-1][0]);

	//AND r0-r4 with reminder_mask
	r0=_mm256_and_si256(r0,reminder_mask);
	r1=_mm256_and_si256(r1,reminder_mask);
	r2=_mm256_and_si256(r2,reminder_mask);
	r3=_mm256_and_si256(r3,reminder_mask);
	r4=_mm256_and_si256(r4,reminder_mask);
	r5=_mm256_and_si256(r5,reminder_mask);
	r6=_mm256_and_si256(r6,reminder_mask);


//--------------------y filter begins--------------------
	//multiply by the mask
		m0=_mm256_maddubs_epi16(r0,cy0);
		m1=_mm256_maddubs_epi16(r1,cy1);
		m2=_mm256_maddubs_epi16(r2,cy2);
		m3=_mm256_maddubs_epi16(r3,cy3);
		m4=_mm256_maddubs_epi16(r4,cy4);
		m5=_mm256_maddubs_epi16(r5,cy5);
		m6=_mm256_maddubs_epi16(r6,cy6);

		//discard odd
__m256i m0_even=_mm256_and_si256(m0,mask_even_16);
__m256i	m1_even=_mm256_and_si256(m1,mask_even_16);
__m256i	m2_even=_mm256_and_si256(m2,mask_even_16);
__m256i	m3_even=_mm256_and_si256(m3,mask_even_16);
__m256i	m4_even=_mm256_and_si256(m4,mask_even_16);
__m256i	m5_even=_mm256_and_si256(m5,mask_even_16);
__m256i	m6_even=_mm256_and_si256(m6,mask_even_16);

//make from 16-bit to 32bit
  m0_even=_mm256_madd_epi16(m0_even,ones);
  m1_even=_mm256_madd_epi16(m1_even,ones);
  m2_even=_mm256_madd_epi16(m2_even,ones);
  m3_even=_mm256_madd_epi16(m3_even,ones);
  m4_even=_mm256_madd_epi16(m4_even,ones);
  m5_even=_mm256_madd_epi16(m5_even,ones);
  m6_even=_mm256_madd_epi16(m6_even,ones);

		//vertical 32-bit add
		m0_even=_mm256_add_epi32(m0_even,m1_even);
		m0_even=_mm256_add_epi32(m0_even,m2_even);
		m0_even=_mm256_add_epi32(m0_even,m3_even);
		m0_even=_mm256_add_epi32(m0_even,m4_even);
		m0_even=_mm256_add_epi32(m0_even,m5_even);
		m0_even=_mm256_add_epi32(m0_even,m6_even);

		//discard even
__m256i	m0_odd=_mm256_and_si256(m0,mask_odd_16);
__m256i	m1_odd=_mm256_and_si256(m1,mask_odd_16);
__m256i	m2_odd=_mm256_and_si256(m2,mask_odd_16);
__m256i	m3_odd=_mm256_and_si256(m3,mask_odd_16);
__m256i	m4_odd=_mm256_and_si256(m4,mask_odd_16);
__m256i	m5_odd=_mm256_and_si256(m5,mask_odd_16);
__m256i	m6_odd=_mm256_and_si256(m6,mask_odd_16);

//make from 16-bit to 32bit
		m0_odd=_mm256_madd_epi16(m0_odd,ones);
		m1_odd=_mm256_madd_epi16(m1_odd,ones);
		m2_odd=_mm256_madd_epi16(m2_odd,ones);
		m3_odd=_mm256_madd_epi16(m3_odd,ones);
		m4_odd=_mm256_madd_epi16(m4_odd,ones);
		m5_odd=_mm256_madd_epi16(m5_odd,ones);
		m6_odd=_mm256_madd_epi16(m6_odd,ones);

		//vertical 32-bit add
		m0_odd=_mm256_add_epi32(m0_odd,m1_odd);
		m0_odd=_mm256_add_epi32(m0_odd,m2_odd);
		m0_odd=_mm256_add_epi32(m0_odd,m3_odd);
		m0_odd=_mm256_add_epi32(m0_odd,m4_odd);
		m0_odd=_mm256_add_epi32(m0_odd,m5_odd);
		m0_odd=_mm256_add_epi32(m0_odd,m6_odd);

		m0_even=division_32(division_case,m0_even,f);//even results
		m0_odd=division_32(division_case,m0_odd,f);//even results

		m0_odd=_mm256_slli_si256(m0_odd,2); //shift two position right
		even=_mm256_add_epi8(m0_even,m0_odd); //add the odd with the even

		//-----------------multiply by the second mask----------
		m0=_mm256_maddubs_epi16(r0,cy0_sh1);
		m1=_mm256_maddubs_epi16(r1,cy1_sh1);
		m2=_mm256_maddubs_epi16(r2,cy2_sh1);
		m3=_mm256_maddubs_epi16(r3,cy3_sh1);
		m4=_mm256_maddubs_epi16(r4,cy4_sh1);
		m5=_mm256_maddubs_epi16(r5,cy5_sh1);
		m6=_mm256_maddubs_epi16(r6,cy6_sh1);


		m0_even=_mm256_and_si256(m0,mask_even_16);
		m1_even=_mm256_and_si256(m1,mask_even_16);
		m2_even=_mm256_and_si256(m2,mask_even_16);
		m3_even=_mm256_and_si256(m3,mask_even_16);
		m4_even=_mm256_and_si256(m4,mask_even_16);
		m5_even=_mm256_and_si256(m5,mask_even_16);
		m6_even=_mm256_and_si256(m6,mask_even_16);

		//make from 16-bit to 32bit
		  m0_even=_mm256_madd_epi16(m0_even,ones);
		  m1_even=_mm256_madd_epi16(m1_even,ones);
		  m2_even=_mm256_madd_epi16(m2_even,ones);
		  m3_even=_mm256_madd_epi16(m3_even,ones);
		  m4_even=_mm256_madd_epi16(m4_even,ones);
		  m5_even=_mm256_madd_epi16(m5_even,ones);
		  m6_even=_mm256_madd_epi16(m6_even,ones);

		//vertical 32-bit add
		m0_even=_mm256_add_epi32(m0_even,m1_even);
		m0_even=_mm256_add_epi32(m0_even,m2_even);
		m0_even=_mm256_add_epi32(m0_even,m3_even);
		m0_even=_mm256_add_epi32(m0_even,m4_even);
		m0_even=_mm256_add_epi32(m0_even,m5_even);
		m0_even=_mm256_add_epi32(m0_even,m6_even);

		 m0_odd=_mm256_and_si256(m0,mask_odd_16);
		 m1_odd=_mm256_and_si256(m1,mask_odd_16);
		 m2_odd=_mm256_and_si256(m2,mask_odd_16);
		 m3_odd=_mm256_and_si256(m3,mask_odd_16);
		 m4_odd=_mm256_and_si256(m4,mask_odd_16);
		 m5_odd=_mm256_and_si256(m5,mask_odd_16);
		 m6_odd=_mm256_and_si256(m6,mask_odd_16);

		 //make from 16-bit to 32bit
			m0_odd=_mm256_madd_epi16(m0_odd,ones);
			m1_odd=_mm256_madd_epi16(m1_odd,ones);
			m2_odd=_mm256_madd_epi16(m2_odd,ones);
			m3_odd=_mm256_madd_epi16(m3_odd,ones);
			m4_odd=_mm256_madd_epi16(m4_odd,ones);
			m5_odd=_mm256_madd_epi16(m5_odd,ones);
			m6_odd=_mm256_madd_epi16(m6_odd,ones);


		//vertical 32-bit add
		m0_odd=_mm256_add_epi32(m0_odd,m1_odd);
		m0_odd=_mm256_add_epi32(m0_odd,m2_odd);
		m0_odd=_mm256_add_epi32(m0_odd,m3_odd);
		m0_odd=_mm256_add_epi32(m0_odd,m4_odd);
		m0_odd=_mm256_add_epi32(m0_odd,m5_odd);
		m0_odd=_mm256_add_epi32(m0_odd,m6_odd);

		m0_even=division_32(division_case,m0_even,f);//even results
		m0_odd=division_32(division_case,m0_odd,f);//even results

		m0_odd=_mm256_slli_si256(m0_odd,2); //shift two position right
		odd=_mm256_add_epi8(m0_even,m0_odd); //add the odd with the even


		//MERGE AND pack to one register
		odd=_mm256_slli_si256(odd,1); //shift one position right
		r0=_mm256_add_epi8(even,odd); //add the odd with the even


	_mm256_storeu_si256( (__m256i *) &temp[M-REMINDER_ITERATIONS_Y],r0 );

	for (int gg=M;gg<M+32+6;gg++)
	    temp[gg]=0;
//y filter ends - r0 has now the data to be processed by x filter
}


}




int loop_reminder_7x7_32_blur_X(unsigned char **frame1,unsigned char **filt,unsigned char *temp,const unsigned int N,const unsigned int M,const unsigned int row, const unsigned int col,const unsigned int REMINDER_ITERATIONS_X,const unsigned int division_case,const signed char mask_vector_x[][32], const __m256i f, const unsigned short int divisor_xy,signed char *kernel_x){

	const __m256i cx0=_mm256_load_si256( (__m256i *) &mask_vector_x[0]);
__m256i r0,m0,m1,output1,output2,output3,output4;


const __m256i mask_even_32  = _mm256_set_epi32(0,0xffffffff,0,0xffffffff,0,0xffffffff,0,0xffffffff);
const __m256i ones=_mm256_set1_epi16(1);



	if (REMINDER_ITERATIONS_X==0){
		return 0;}

		//1st col iteration

	  r0=_mm256_loadu_si256( (__m256i *) &temp[col-3]);

		//multiply by the mask
		m0=_mm256_maddubs_epi16(r0,cx0);

		//make 16-bit values 32-bit (horizontal add)
		m0=_mm256_madd_epi16(m0,ones);

		//last horizontal add
		m1=_mm256_srli_si256(m0,4);//shift by one integer
		m0=_mm256_add_epi32(m0,m1);//add
		output1=_mm256_and_si256(m0,mask_even_32);//keep only 0,2,4,6 and discard 1,3,5,7


  			//2nd col iteration
		        r0=_mm256_loadu_si256( (__m256i *) &temp[col-2]);

			//multiply by the mask
			m0=_mm256_maddubs_epi16(r0,cx0);

			//make 16-bit values 32-bit (horizontal add)
			m0=_mm256_madd_epi16(m0,ones);

			//last horizontal add
			m1=_mm256_srli_si256(m0,4);//shift by one integer
			m0=_mm256_add_epi32(m0,m1);//add
			output2=_mm256_and_si256(m0,mask_even_32);//keep only 0,2,4,6 and discard 1,3,5,7


  			//3rd col iteration
		        r0=_mm256_loadu_si256( (__m256i *) &temp[col-1]);

			//multiply by the mask
			m0=_mm256_maddubs_epi16(r0,cx0);

			//make 16-bit values 32-bit (horizontal add)
			m0=_mm256_madd_epi16(m0,ones);

			//last horizontal add
			m1=_mm256_srli_si256(m0,4);//shift by one integer
			m0=_mm256_add_epi32(m0,m1);//add
			output3=_mm256_and_si256(m0,mask_even_32);//keep only 0,2,4,6 and discard 1,3,5,7


  			//4th col iteration
		        r0=_mm256_loadu_si256( (__m256i *) &temp[col]);

			//multiply by the mask
			m0=_mm256_maddubs_epi16(r0,cx0);

			//make 16-bit values 32-bit (horizontal add)
			m0=_mm256_madd_epi16(m0,ones);

			//last horizontal add
			m1=_mm256_srli_si256(m0,4);//shift by one integer
			m0=_mm256_add_epi32(m0,m1);//add
			output4=_mm256_and_si256(m0,mask_even_32);//keep only 0,2,4,6 and discard 1,3,5,7

  			//5th col iteration
		        r0=_mm256_loadu_si256( (__m256i *) &temp[col+1]);

			//multiply by the mask
			m0=_mm256_maddubs_epi16(r0,cx0);

			//make 16-bit values 32-bit (horizontal add)
			m0=_mm256_madd_epi16(m0,ones);

			//last horizontal add
			m1=_mm256_srli_si256(m0,4);//shift by one integer
			m0=_mm256_add_epi32(m0,m1);//add
			m0=_mm256_and_si256(m0,mask_even_32);//keep only 0,2,4,6 and discard 1,3,5,7

				//MERGE output1
			m0=_mm256_slli_si256(m0,4);
				output1=_mm256_add_epi32(output1,m0);

  			//6th col iteration
		        r0=_mm256_loadu_si256( (__m256i *) &temp[col+2]);

			//multiply by the mask
			m0=_mm256_maddubs_epi16(r0,cx0);

			//make 16-bit values 32-bit (horizontal add)
			m0=_mm256_madd_epi16(m0,ones);

			//last horizontal add
			m1=_mm256_srli_si256(m0,4);//shift by one integer
			m0=_mm256_add_epi32(m0,m1);//add
			m0=_mm256_and_si256(m0,mask_even_32);//keep only 0,2,4,6 and discard 1,3,5,7

				//MERGE output1
			m0=_mm256_slli_si256(m0,4);
				output2=_mm256_add_epi32(output2,m0);

  			//7th col iteration
		        r0=_mm256_loadu_si256( (__m256i *) &temp[col+3]);

			//multiply by the mask
			m0=_mm256_maddubs_epi16(r0,cx0);

			//make 16-bit values 32-bit (horizontal add)
			m0=_mm256_madd_epi16(m0,ones);

			//last horizontal add
			m1=_mm256_srli_si256(m0,4);//shift by one integer
			m0=_mm256_add_epi32(m0,m1);//add
			m0=_mm256_and_si256(m0,mask_even_32);//keep only 0,2,4,6 and discard 1,3,5,7

				//MERGE output1
			m0=_mm256_slli_si256(m0,4);
				output3=_mm256_add_epi32(output3,m0);

  			//8th col iteration
		        r0=_mm256_loadu_si256( (__m256i *) &temp[col+4]);

			//multiply by the mask
			m0=_mm256_maddubs_epi16(r0,cx0);

			//make 16-bit values 32-bit (horizontal add)
			m0=_mm256_madd_epi16(m0,ones);

			//last horizontal add
			m1=_mm256_srli_si256(m0,4);//shift by one integer
			m0=_mm256_add_epi32(m0,m1);//add
			m0=_mm256_and_si256(m0,mask_even_32);//keep only 0,2,4,6 and discard 1,3,5,7

				//MERGE output1
			m0=_mm256_slli_si256(m0,4);
				output4=_mm256_add_epi32(output4,m0);


				output1 = division_32(division_case,output1,f);
				output2 = division_32(division_case,output2,f);
				output3 = division_32(division_case,output3,f);
				output4 = division_32(division_case,output4,f);

			output2=_mm256_slli_si256(output2,1);
			output3=_mm256_slli_si256(output3,2);
			output4=_mm256_slli_si256(output4,3);

				output1 = _mm256_add_epi8(output1,output2);
				output1 = _mm256_add_epi8(output1,output3);
				output1 = _mm256_add_epi8(output1,output4);


	switch (REMINDER_ITERATIONS_X){
	case 1:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		  break;
	case 2:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		  break;
	case 3:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		  break;
	case 4:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output1,3);
		  break;
	case 5:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output1,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output1,4);
		  break;
	case 6:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output1,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output1,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output1,5);
		  break;
	case 7:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output1,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output1,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output1,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output1,6);
		  break;
	case 8:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output1,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output1,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output1,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output1,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output1,7);
		  break;
	case 9:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output1,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output1,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output1,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output1,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output1,7);
		filt[row][col+8] = (unsigned char) _mm256_extract_epi8(output1,8);
		  break;
	case 10:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output1,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output1,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output1,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output1,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output1,7);
		filt[row][col+8] = (unsigned char) _mm256_extract_epi8(output1,8);
		filt[row][col+9] = (unsigned char) _mm256_extract_epi8(output1,9);
		  break;
	case 11:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output1,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output1,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output1,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output1,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output1,7);
		filt[row][col+8] = (unsigned char) _mm256_extract_epi8(output1,8);
		filt[row][col+9] = (unsigned char) _mm256_extract_epi8(output1,9);
		filt[row][col+10] = (unsigned char) _mm256_extract_epi8(output1,10);
		  break;
	case 12:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output1,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output1,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output1,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output1,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output1,7);
		filt[row][col+8] = (unsigned char) _mm256_extract_epi8(output1,8);
		filt[row][col+9] = (unsigned char) _mm256_extract_epi8(output1,9);
		filt[row][col+10] = (unsigned char) _mm256_extract_epi8(output1,10);
		filt[row][col+11] = (unsigned char) _mm256_extract_epi8(output1,11);
		  break;
	case 13:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output1,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output1,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output1,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output1,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output1,7);
		filt[row][col+8] = (unsigned char) _mm256_extract_epi8(output1,8);
		filt[row][col+9] = (unsigned char) _mm256_extract_epi8(output1,9);
		filt[row][col+10] = (unsigned char) _mm256_extract_epi8(output1,10);
		filt[row][col+11] = (unsigned char) _mm256_extract_epi8(output1,11);
		filt[row][col+12] = (unsigned char) _mm256_extract_epi8(output1,12);
		  break;
	case 14:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output1,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output1,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output1,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output1,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output1,7);
		filt[row][col+8] = (unsigned char) _mm256_extract_epi8(output1,8);
		filt[row][col+9] = (unsigned char) _mm256_extract_epi8(output1,9);
		filt[row][col+10] = (unsigned char) _mm256_extract_epi8(output1,10);
		filt[row][col+11] = (unsigned char) _mm256_extract_epi8(output1,11);
		filt[row][col+12] = (unsigned char) _mm256_extract_epi8(output1,12);
		filt[row][col+13] = (unsigned char) _mm256_extract_epi8(output1,13);
		  break;
	case 15:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output1,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output1,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output1,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output1,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output1,7);
		filt[row][col+8] = (unsigned char) _mm256_extract_epi8(output1,8);
		filt[row][col+9] = (unsigned char) _mm256_extract_epi8(output1,9);
		filt[row][col+10] = (unsigned char) _mm256_extract_epi8(output1,10);
		filt[row][col+11] = (unsigned char) _mm256_extract_epi8(output1,11);
		filt[row][col+12] = (unsigned char) _mm256_extract_epi8(output1,12);
		filt[row][col+13] = (unsigned char) _mm256_extract_epi8(output1,13);
		filt[row][col+14] = (unsigned char) _mm256_extract_epi8(output1,14);
		  break;
	case 16:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
		  break;
	case 17:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
		  break;
	case 18:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
		  break;
	case 19:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
		  break;
	case 20:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output1,19);
		  break;
	case 21:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output1,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output1,20);
		  break;
	case 22:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output1,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output1,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output1,21);
		break;
	case 23:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output1,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output1,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output1,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output1,22);
		  break;
	case 24:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output1,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output1,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output1,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output1,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output1,23);
		  break;
	case 25:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output1,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output1,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output1,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output1,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output1,23);
	filt[row][col+24] = (unsigned char) _mm256_extract_epi8(output1,24);
		  break;
	case 26:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output1,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output1,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output1,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output1,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output1,23);
	filt[row][col+24] = (unsigned char) _mm256_extract_epi8(output1,24);
	filt[row][col+25] = (unsigned char) _mm256_extract_epi8(output1,25);
		  break;
	case 27:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output1,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output1,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output1,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output1,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output1,23);
	filt[row][col+24] = (unsigned char) _mm256_extract_epi8(output1,24);
	filt[row][col+25] = (unsigned char) _mm256_extract_epi8(output1,25);
	filt[row][col+26] = (unsigned char) _mm256_extract_epi8(output1,26);
		  break;
	case 28:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output1,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output1,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output1,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output1,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output1,23);
	filt[row][col+24] = (unsigned char) _mm256_extract_epi8(output1,24);
	filt[row][col+25] = (unsigned char) _mm256_extract_epi8(output1,25);
	filt[row][col+26] = (unsigned char) _mm256_extract_epi8(output1,26);
	filt[row][col+27] = (unsigned char) _mm256_extract_epi8(output1,27);
		  break;
	case 29:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output1,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output1,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output1,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output1,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output1,23);
	filt[row][col+24] = (unsigned char) _mm256_extract_epi8(output1,24);
	filt[row][col+25] = (unsigned char) _mm256_extract_epi8(output1,25);
	filt[row][col+26] = (unsigned char) _mm256_extract_epi8(output1,26);
	filt[row][col+27] = (unsigned char) _mm256_extract_epi8(output1,27);
	filt[row][col+28] = (unsigned char) _mm256_extract_epi8(output1,28);
		  break;
	case 30:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output1,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output1,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output1,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output1,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output1,23);
	filt[row][col+24] = (unsigned char) _mm256_extract_epi8(output1,24);
	filt[row][col+25] = (unsigned char) _mm256_extract_epi8(output1,25);
	filt[row][col+26] = (unsigned char) _mm256_extract_epi8(output1,26);
	filt[row][col+27] = (unsigned char) _mm256_extract_epi8(output1,27);
	filt[row][col+28] = (unsigned char) _mm256_extract_epi8(output1,28);
	filt[row][col+29] = (unsigned char) _mm256_extract_epi8(output1,29);
		  break;
	case 31:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output1,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output1,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output1,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output1,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output1,23);
	filt[row][col+24] = (unsigned char) _mm256_extract_epi8(output1,24);
	filt[row][col+25] = (unsigned char) _mm256_extract_epi8(output1,25);
	filt[row][col+26] = (unsigned char) _mm256_extract_epi8(output1,26);
	filt[row][col+27] = (unsigned char) _mm256_extract_epi8(output1,27);
	filt[row][col+28] = (unsigned char) _mm256_extract_epi8(output1,28);
	filt[row][col+29] = (unsigned char) _mm256_extract_epi8(output1,29);

	int newPixel = 0;

	newPixel += temp[col+30-4] * kernel_x[0];
	newPixel += temp[col+30-3] * kernel_x[1];
	newPixel += temp[col+30-2] * kernel_x[2];
	newPixel += temp[col+30-1] * kernel_x[3];
	newPixel += temp[col+30-0] * kernel_x[4];

	filt[row][col+30] = (unsigned char) (newPixel / divisor_xy);

		  break;
	default:
		printf("\nsomething went wrong");
		exit(EXIT_FAILURE);
 	}

	return 0;

}







void Gaussian_Blur_optimized_5x5_16_seperable(unsigned char **frame1,unsigned char **filt,const unsigned int M, const unsigned int N, signed char *kernel_y, signed char *kernel_x, const unsigned short int divisor_xy){


	const signed char fx0=kernel_x[0];
	const signed char fx1=kernel_x[1];
	const signed char fx2=kernel_x[2];
	const signed char fx3=kernel_x[3];
	const signed char fx4=kernel_x[4];

	const signed char fy0=kernel_y[0];
	const signed char fy1=kernel_y[1];
	const signed char fy2=kernel_y[2];
	const signed char fy3=kernel_y[3];
	const signed char fy4=kernel_y[4];



	const signed char mask_vector_x[1][32] __attribute__((aligned(64))) ={
			{fx0,fx1,fx2,fx3,fx4,0,fx0,fx1,fx2,fx3,fx4,0,fx0,fx1,fx2,fx3,fx4,0,fx0,fx1,fx2,fx3,fx4,0,fx0,fx1,fx2,fx3,fx4,0,0,0},
	};

	const signed char mask_vector_y[10][32] __attribute__((aligned(64))) ={
		{fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0},
		{fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0},
		{fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0},
		{fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0},
		{fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0},

		{0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0,0,fy0},
		{0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1,0,fy1},
		{0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2,0,fy2},
		{0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3,0,fy3},
		{0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4,0,fy4},
	};


		const __m256i cx0=_mm256_load_si256( (__m256i *) &mask_vector_x[0]);



		const __m256i cy0=_mm256_load_si256( (__m256i *) &mask_vector_y[0]);
		const __m256i cy1=_mm256_load_si256( (__m256i *) &mask_vector_y[1]);
		const __m256i cy2=_mm256_load_si256( (__m256i *) &mask_vector_y[2]);
		const __m256i cy3=_mm256_load_si256( (__m256i *) &mask_vector_y[3]);
		const __m256i cy4=_mm256_load_si256( (__m256i *) &mask_vector_y[4]);


		const __m256i cy0_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[5]);
		const __m256i cy1_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[6]);
		const __m256i cy2_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[7]);
		const __m256i cy3_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[8]);
		const __m256i cy4_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[9]);

		const __m256i mask_prelude2  = _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0);
		const __m256i mask_prelude1  = _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0);




		const unsigned int REMINDER_ITERATIONS_X_mask =  M-((((M-32)/30)*30)+30); //M-(last_col_value+30)
		const unsigned int REMINDER_ITERATIONS_Y_mask =  M-((((M-32)/32)*32)+32);

		//printf("\nREMINDER_ITERATIONSX,Y=%u %u",REMINDER_ITERATIONS_X_mask,REMINDER_ITERATIONS_Y_mask);


		const unsigned int division_case=prepare_for_division(divisor_xy); //determine which is the division case (A, B or C)
		const __m256i f = _mm256_load_si256( (__m256i *) &f_vector[0]);// initialize the division vector

		const __m256i mask2  = _mm256_set_epi16(0,0,0,0,0,0,0,0,0,65535,0,0,0,0,0,0);
		const __m256i mask3  = _mm256_set_epi16(0,0,0,0,0,0,0,65535,0,0,0,0,0,0,0,0);
		const __m256i output_mask      = _mm256_set_epi16(0,0,0,65535,0,0,65535,0,0,65535,0,0,65535,0,0,65535);

		//printf("\n%d ",division_case);

#pragma omp parallel
{

unsigned int row,col;
__m256i rr0,r0,r1,r2,r3,r4,m0,m1,m2,m3,m4,output_even,output_odd;
__m256i even,odd;

unsigned char temp[M+32+3] __attribute__((aligned(64)));//temporal storage of the output of y filter.

/*---------------------- Gaussian Blur ---------------------------------*/

    #pragma omp for schedule(static)
	for (row = 1; row <= N-2; row++) {

		if (row==1){//in this case I calculate filt[0][:] and filt[1][:] too. No extra loads or multiplications are required for this

			prelude_5x5_16_Ymask_0_new(0, M, frame1, temp,mask_vector_y, division_case, f);
			loop_reminder_5x5_16_blur_Y(frame1,filt,temp,N,M,0, REMINDER_ITERATIONS_Y_mask, division_case, mask_vector_y, f, divisor_xy);
			prelude_5x5_16_Xmask_new(frame1,filt,temp,N,M,0, mask_vector_x,  division_case,f,REMINDER_ITERATIONS_X_mask,mask_vector_y,divisor_xy,kernel_x);

			prelude_5x5_16_Ymask_1_new(0, M, frame1, temp,mask_vector_y, division_case, f);
			loop_reminder_5x5_16_blur_Y(frame1,filt,temp,N,M,1, REMINDER_ITERATIONS_Y_mask, division_case, mask_vector_y, f, divisor_xy);
			prelude_5x5_16_Xmask_new(frame1,filt,temp,N,M,1, mask_vector_x,  division_case,f,REMINDER_ITERATIONS_X_mask,mask_vector_y,divisor_xy,kernel_x);


		}

		else if (row==N-2){//in this case I calculate filt[N-2][:] and filt[N-1][:] too. Below row0 refers to row=N-2 and row1 refers to row=N-1

			prelude_5x5_16_Ymask_1_new(N-4, M, frame1, temp,mask_vector_y, division_case, f);
			loop_reminder_5x5_16_blur_Y(frame1,filt,temp,N,M,N-2, REMINDER_ITERATIONS_Y_mask, division_case, mask_vector_y, f, divisor_xy);
			prelude_5x5_16_Xmask_new(frame1,filt,temp,N,M,N-2, mask_vector_x,  division_case,f,REMINDER_ITERATIONS_X_mask,mask_vector_y,divisor_xy,kernel_x);

			prelude_5x5_16_Ymask_0_new(N-3, M, frame1, temp,mask_vector_y, division_case, f);
			loop_reminder_5x5_16_blur_Y(frame1,filt,temp,N,M,N-1, REMINDER_ITERATIONS_Y_mask, division_case, mask_vector_y, f, divisor_xy);
			prelude_5x5_16_Xmask_new(frame1,filt,temp,N,M,N-1, mask_vector_x,  division_case,f,REMINDER_ITERATIONS_X_mask,mask_vector_y,divisor_xy,kernel_x);

					}
		  else { //main loop


			  for (col = 0; col <= M-32; col+=32){

						//load the 5 rows
						r0=_mm256_load_si256( (__m256i *) &frame1[row-2][col]);
						r1=_mm256_load_si256( (__m256i *) &frame1[row-1][col]);
						r2=_mm256_load_si256( (__m256i *) &frame1[row][col]);
						r3=_mm256_load_si256( (__m256i *) &frame1[row+1][col]);
						r4=_mm256_load_si256( (__m256i *) &frame1[row+2][col]);

				  //--------------------y filter begins--------------------

					//multiply with the mask
						m0=_mm256_maddubs_epi16(r0,cy0);
						m1=_mm256_maddubs_epi16(r1,cy1);
						m2=_mm256_maddubs_epi16(r2,cy2);
						m3=_mm256_maddubs_epi16(r3,cy3);
						m4=_mm256_maddubs_epi16(r4,cy4);

						//vertical add
						m0=_mm256_add_epi16(m0,m1);
						m0=_mm256_add_epi16(m0,m2);
						m0=_mm256_add_epi16(m0,m3);
						m0=_mm256_add_epi16(m0,m4);

						even=division(division_case,m0,f);//even results

						//multiply with the mask
						m0=_mm256_maddubs_epi16(r0,cy0_sh1);
						m1=_mm256_maddubs_epi16(r1,cy1_sh1);
						m2=_mm256_maddubs_epi16(r2,cy2_sh1);
						m3=_mm256_maddubs_epi16(r3,cy3_sh1);
						m4=_mm256_maddubs_epi16(r4,cy4_sh1);

						//vertical add
						m0=_mm256_add_epi16(m0,m1);
						m0=_mm256_add_epi16(m0,m2);
						m0=_mm256_add_epi16(m0,m3);
						m0=_mm256_add_epi16(m0,m4);

						odd=division(division_case,m0,f);//odd results

						//pack to one register
						odd=_mm256_slli_si256(odd,1); //shift one position right
						r0=_mm256_add_epi8(even,odd); //add the odd with the even
					//y filter ends - r0 has now the data to be processed by x filter

					_mm256_store_si256( (__m256i *) &temp[col],r0 );

			  }
		loop_reminder_5x5_16_blur_Y(frame1,filt,temp,N,M,row, REMINDER_ITERATIONS_Y_mask, division_case, mask_vector_y, f, divisor_xy);


		  for (col = 0; col <= M-32; col+=30){

			  if (col==0){

		  			//1st col iteration

			  			//multiply by the mask
		  		        rr0=_mm256_load_si256( (__m256i *) &temp[0]);
						r0=insert_two_zeros_front(rr0,mask_prelude2);

		  				//multiply with the mask
		  				m0=_mm256_maddubs_epi16(r0,cx0);

		  				//1st hadd
		  				m1=_mm256_srli_si256(m0,2);
		  				m1=_mm256_add_epi16(m0,m1);

		  				//2nd hadd
		  				m2=_mm256_srli_si256(m0,4);
		  				m2=_mm256_add_epi16(m1,m2);

		  				//after shifts the 15th element is lost as it cannot be propagated to the 16th position (AVX registers are managed as two seperate SSE registers)
		  				m4=_mm256_and_si256(m0,mask3);
		  				m4=_mm256_permute2f128_si256(m4,m4,1);
		  				m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions
		  				m2=_mm256_add_epi16(m2,m4);

		  				//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
		  				output_even=_mm256_and_si256(m2,output_mask);

			  			//2nd col iteration

						r0=insert_one_zeros_front(rr0,mask_prelude1);

		  				//multiply with the mask
		  				m0=_mm256_maddubs_epi16(r0,cx0);

		  				//1st hadd
		  				m1=_mm256_srli_si256(m0,2);
		  				m1=_mm256_add_epi16(m0,m1);

		  				//2nd hadd
		  				m2=_mm256_srli_si256(m0,4);
		  				m2=_mm256_add_epi16(m1,m2);

		  				//after shifts the 15th element is lost as it cannot be propagated to the 16th position (AVX registers are managed as two seperate SSE registers)
		  				m4=_mm256_and_si256(m0,mask3);
		  				m4=_mm256_permute2f128_si256(m4,m4,1);
		  				m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions
		  				m2=_mm256_add_epi16(m2,m4);

		  				//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
		  				output_odd=_mm256_and_si256(m2,output_mask);

			  			//3rd col iteration

		  				//multiply with the mask
		  				m0=_mm256_maddubs_epi16(rr0,cx0);

		  				//1st hadd
		  				m1=_mm256_srli_si256(m0,2);
		  				m1=_mm256_add_epi16(m0,m1);

		  				//2nd hadd
		  				m2=_mm256_srli_si256(m0,4);
		  				m2=_mm256_add_epi16(m1,m2);

		  				//after shifts the 15th element is lost as it cannot be propagated to the 16th position (AVX registers are managed as two seperate SSE registers)
		  				m4=_mm256_and_si256(m0,mask3);
		  				m4=_mm256_permute2f128_si256(m4,m4,1);
		  				m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions
		  				m2=_mm256_add_epi16(m2,m4);

		  				//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
		  				m2=_mm256_and_si256(m2,output_mask);

		  				m2=_mm256_slli_si256(m2,2);
		  				output_even=_mm256_add_epi16(output_even,m2);

			  			//4th col iteration

			  			//multiply by the mask
		  		        r0=_mm256_loadu_si256( (__m256i *) &temp[col+1]);

		  				//multiply with the mask
		  				m0=_mm256_maddubs_epi16(r0,cx0);

		  				//1st hadd
		  				m1=_mm256_srli_si256(m0,2);
		  				m1=_mm256_add_epi16(m0,m1);

		  				//2nd hadd
		  				m2=_mm256_srli_si256(m0,4);
		  				m2=_mm256_add_epi16(m1,m2);

		  				//after shifts the 15th element is lost as it cannot be propagated to the 16th position (AVX registers are managed as two seperate SSE registers)
		  				m4=_mm256_and_si256(m0,mask3);
		  				m4=_mm256_permute2f128_si256(m4,m4,1);
		  				m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions
		  				m2=_mm256_add_epi16(m2,m4);

		  				//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
		  				m2=_mm256_and_si256(m2,output_mask);

		  				m2=_mm256_slli_si256(m2,2);
		  				output_odd=_mm256_add_epi16(output_odd,m2);

			  			//5th col iteration

			  			//multiply by the mask
		  		        r0=_mm256_loadu_si256( (__m256i *) &temp[col+2]);

		  				//multiply with the mask
		  				m0=_mm256_maddubs_epi16(r0,cx0);

		  				//1st hadd
		  				m1=_mm256_srli_si256(m0,2);
		  				m1=_mm256_add_epi16(m0,m1);

		  				//2nd hadd
		  				m2=_mm256_srli_si256(m0,4);
		  				m2=_mm256_add_epi16(m1,m2);

		  				//after shifts the 15th element is lost as it cannot be propagated to the 16th position (AVX registers are managed as two seperate SSE registers)
		  				m4=_mm256_and_si256(m0,mask3);
		  				m4=_mm256_permute2f128_si256(m4,m4,1);
		  				m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions
		  				m2=_mm256_add_epi16(m2,m4);

		  				//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
		  				m2=_mm256_and_si256(m2,output_mask);

		  				m3=_mm256_slli_si256(m2,4);
		  				m4=_mm256_and_si256(m2,mask2);
		  				m4=_mm256_permute2f128_si256(m4,m4,1);
		  				m4=_mm256_srli_si256(m4,12);//shift 6 short int positions or 12 char positions
		  				m2=_mm256_add_epi16(m3,m4);

		  				output_even=_mm256_add_epi16(output_even,m2);

			  			//6th col iteration

			  			//multiply by the mask
		  		        r0=_mm256_loadu_si256( (__m256i *) &temp[col+3]);

		  				//multiply with the mask
		  				m0=_mm256_maddubs_epi16(r0,cx0);

		  				//1st hadd
		  				m1=_mm256_srli_si256(m0,2);
		  				m1=_mm256_add_epi16(m0,m1);

		  				//2nd hadd
		  				m2=_mm256_srli_si256(m0,4);
		  				m2=_mm256_add_epi16(m1,m2);

		  				//after shifts the 15th element is lost as it cannot be propagated to the 16th position (AVX registers are managed as two seperate SSE registers)
		  				m4=_mm256_and_si256(m0,mask3);
		  				m4=_mm256_permute2f128_si256(m4,m4,1);
		  				m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions
		  				m2=_mm256_add_epi16(m2,m4);

		  				//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
		  				m2=_mm256_and_si256(m2,output_mask);

		  				m3=_mm256_slli_si256(m2,4);
		  				m4=_mm256_and_si256(m2,mask2);
		  				m4=_mm256_permute2f128_si256(m4,m4,1);
		  				m4=_mm256_srli_si256(m4,12);//shift 6 short int positions or 12 char positions
		  				m2=_mm256_add_epi16(m3,m4);

		  				output_odd=_mm256_add_epi16(output_odd,m2);

		  				//now division follows
		  				output_even=division(division_case,output_even,f);
		  				output_odd=division(division_case,output_odd,f);

		  				//shift odd 1 position and add to even
		  				output_odd = _mm256_slli_si256(output_odd,1);
		  				output_even = _mm256_add_epi8(output_even,output_odd);

		  				_mm256_store_si256( (__m256i *) &filt[row][0],output_even );

			  		  	}
			  else {

					// col iteration computes output pixels of   0,6,12,18,24
					// col+1 iteration computes output pixels of 1,7,13,19,25
					// col+2 iteration computes output pixels of 2,8,14,20,26
					// col+3 iteration computes output pixels of 3,9,15,21,27
					// col+4 iteration computes output pixels of 4,10,16,22,28
					// col+5 iteration computes output pixels of 5,11,17,23,29
					//afterwards, col becomes 30 and repeat the above process

			  			//1st col iteration

			  			//multiply by the mask
		  		        r0=_mm256_loadu_si256( (__m256i *) &temp[col-2]);

		  				//multiply with the mask
		  				m0=_mm256_maddubs_epi16(r0,cx0);

		  				//1st hadd
		  				m1=_mm256_srli_si256(m0,2);
		  				m1=_mm256_add_epi16(m0,m1);

		  				//2nd hadd
		  				m2=_mm256_srli_si256(m0,4);
		  				m2=_mm256_add_epi16(m1,m2);

		  				//after shifts the 15th element is lost as it cannot be propagated to the 16th position (AVX registers are managed as two seperate SSE registers)
		  				m4=_mm256_and_si256(m0,mask3);
		  				m4=_mm256_permute2f128_si256(m4,m4,1);
		  				m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions
		  				m2=_mm256_add_epi16(m2,m4);

		  				//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
		  				output_even=_mm256_and_si256(m2,output_mask);

			  			//2nd col iteration

			  			//multiply by the mask
		  		        r0=_mm256_loadu_si256( (__m256i *) &temp[col-1]);

		  				//multiply with the mask
		  				m0=_mm256_maddubs_epi16(r0,cx0);

		  				//1st hadd
		  				m1=_mm256_srli_si256(m0,2);
		  				m1=_mm256_add_epi16(m0,m1);

		  				//2nd hadd
		  				m2=_mm256_srli_si256(m0,4);
		  				m2=_mm256_add_epi16(m1,m2);

		  				//after shifts the 15th element is lost as it cannot be propagated to the 16th position (AVX registers are managed as two seperate SSE registers)
		  				m4=_mm256_and_si256(m0,mask3);
		  				m4=_mm256_permute2f128_si256(m4,m4,1);
		  				m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions
		  				m2=_mm256_add_epi16(m2,m4);

		  				//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
		  				output_odd=_mm256_and_si256(m2,output_mask);

			  			//3rd col iteration

			  			//multiply by the mask
		  		        r0=_mm256_loadu_si256( (__m256i *) &temp[col+0]);

		  				//multiply with the mask
		  				m0=_mm256_maddubs_epi16(r0,cx0);

		  				//1st hadd
		  				m1=_mm256_srli_si256(m0,2);
		  				m1=_mm256_add_epi16(m0,m1);

		  				//2nd hadd
		  				m2=_mm256_srli_si256(m0,4);
		  				m2=_mm256_add_epi16(m1,m2);

		  				//after shifts the 15th element is lost as it cannot be propagated to the 16th position (AVX registers are managed as two seperate SSE registers)
		  				m4=_mm256_and_si256(m0,mask3);
		  				m4=_mm256_permute2f128_si256(m4,m4,1);
		  				m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions
		  				m2=_mm256_add_epi16(m2,m4);

		  				//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
		  				m2=_mm256_and_si256(m2,output_mask);

		  				m2=_mm256_slli_si256(m2,2);
		  				output_even=_mm256_add_epi16(output_even,m2);

			  			//4th col iteration

			  			//multiply by the mask
		  		        r0=_mm256_loadu_si256( (__m256i *) &temp[col+1]);

		  				//multiply with the mask
		  				m0=_mm256_maddubs_epi16(r0,cx0);

		  				//1st hadd
		  				m1=_mm256_srli_si256(m0,2);
		  				m1=_mm256_add_epi16(m0,m1);

		  				//2nd hadd
		  				m2=_mm256_srli_si256(m0,4);
		  				m2=_mm256_add_epi16(m1,m2);

		  				//after shifts the 15th element is lost as it cannot be propagated to the 16th position (AVX registers are managed as two seperate SSE registers)
		  				m4=_mm256_and_si256(m0,mask3);
		  				m4=_mm256_permute2f128_si256(m4,m4,1);
		  				m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions
		  				m2=_mm256_add_epi16(m2,m4);

		  				//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
		  				m2=_mm256_and_si256(m2,output_mask);

		  				m2=_mm256_slli_si256(m2,2);
		  				output_odd=_mm256_add_epi16(output_odd,m2);

			  			//5th col iteration

			  			//multiply by the mask
		  		        r0=_mm256_loadu_si256( (__m256i *) &temp[col+2]);

		  				//multiply with the mask
		  				m0=_mm256_maddubs_epi16(r0,cx0);

		  				//1st hadd
		  				m1=_mm256_srli_si256(m0,2);
		  				m1=_mm256_add_epi16(m0,m1);

		  				//2nd hadd
		  				m2=_mm256_srli_si256(m0,4);
		  				m2=_mm256_add_epi16(m1,m2);

		  				//after shifts the 15th element is lost as it cannot be propagated to the 16th position (AVX registers are managed as two seperate SSE registers)
		  				m4=_mm256_and_si256(m0,mask3);
		  				m4=_mm256_permute2f128_si256(m4,m4,1);
		  				m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions
		  				m2=_mm256_add_epi16(m2,m4);

		  				//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
		  				m2=_mm256_and_si256(m2,output_mask);

		  				m3=_mm256_slli_si256(m2,4);
		  				m4=_mm256_and_si256(m2,mask2);
		  				m4=_mm256_permute2f128_si256(m4,m4,1);
		  				m4=_mm256_srli_si256(m4,12);//shift 6 short int positions or 12 char positions
		  				m2=_mm256_add_epi16(m3,m4);

		  				output_even=_mm256_add_epi16(output_even,m2);

			  			//6th col iteration

			  			//multiply by the mask
		  		        r0=_mm256_loadu_si256( (__m256i *) &temp[col+3]);

		  				//multiply with the mask
		  				m0=_mm256_maddubs_epi16(r0,cx0);

		  				//1st hadd
		  				m1=_mm256_srli_si256(m0,2);
		  				m1=_mm256_add_epi16(m0,m1);

		  				//2nd hadd
		  				m2=_mm256_srli_si256(m0,4);
		  				m2=_mm256_add_epi16(m1,m2);

		  				//after shifts the 15th element is lost as it cannot be propagated to the 16th position (AVX registers are managed as two seperate SSE registers)
		  				m4=_mm256_and_si256(m0,mask3);
		  				m4=_mm256_permute2f128_si256(m4,m4,1);
		  				m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions
		  				m2=_mm256_add_epi16(m2,m4);

		  				//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
		  				m2=_mm256_and_si256(m2,output_mask);

		  				m3=_mm256_slli_si256(m2,4);
		  				m4=_mm256_and_si256(m2,mask2);
		  				m4=_mm256_permute2f128_si256(m4,m4,1);
		  				m4=_mm256_srli_si256(m4,12);//shift 6 short int positions or 12 char positions
		  				m2=_mm256_add_epi16(m3,m4);

		  				output_odd=_mm256_add_epi16(output_odd,m2);

		  				//now division follows
		  				output_even=division(division_case,output_even,f);
		  				output_odd=division(division_case,output_odd,f);

		  				//shift odd 1 position and add to even
		  				output_odd = _mm256_slli_si256(output_odd,1);
		  				output_even = _mm256_add_epi8(output_even,output_odd);

		  				_mm256_storeu_si256( (__m256i *) &filt[row][col],output_even );

			  		 }

			}

		loop_reminder_5x5_16_blur_X(frame1,filt,temp,N,M,row, col, REMINDER_ITERATIONS_X_mask, division_case, mask_vector_x, f, divisor_xy,kernel_x);

			}
	}

	}//end of parallel


	}



//this function is computing the row=1 and row=N-2 only
//for row=1 input prelude_5x5_Ymask_3(0,...
//for row=N-2 input prelude_5x5_Ymask_3(N-4,...
void prelude_5x5_16_Ymask_1_new(const int row, const unsigned int M, unsigned char **frame1, unsigned char *temp, const signed char mask_vector_y[][32],  const unsigned int division_case, const __m256i f){

	__m256i r0,r1,r2,r3,m0,m1,m2,m3,even,odd;
	//__m256i ones=_mm256_set1_epi16(1);
	 __m256i cy0,cy1,cy2,cy3,cy0_sh1,cy1_sh1,cy2_sh1,cy3_sh1;

	if (row!=0){ //if row=N-4
	  cy0=_mm256_load_si256( (__m256i *) &mask_vector_y[0]);
	  cy1=_mm256_load_si256( (__m256i *) &mask_vector_y[1]);
	  cy2=_mm256_load_si256( (__m256i *) &mask_vector_y[2]);
	  cy3=_mm256_load_si256( (__m256i *) &mask_vector_y[3]);

	  cy0_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[5]);
	  cy1_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[6]);
	  cy2_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[7]);
	  cy3_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[8]);

	}
	else {
		  cy0=_mm256_load_si256( (__m256i *) &mask_vector_y[1]);
		  cy1=_mm256_load_si256( (__m256i *) &mask_vector_y[2]);
		  cy2=_mm256_load_si256( (__m256i *) &mask_vector_y[3]);
		  cy3=_mm256_load_si256( (__m256i *) &mask_vector_y[4]);

		  cy0_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[6]);
		  cy1_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[7]);
		  cy2_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[8]);
		  cy3_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[9]);

	}

	 for (unsigned int col = 0; col <= M-32; col+=32){

	  //--------------------y filter begins--------------------
	//load the 9 rows
	r0=_mm256_load_si256( (__m256i *) &frame1[row][col]);
	r1=_mm256_load_si256( (__m256i *) &frame1[row+1][col]);
	r2=_mm256_load_si256( (__m256i *) &frame1[row+2][col]);
	r3=_mm256_load_si256( (__m256i *) &frame1[row+3][col]);


		//multiply with the mask
			m0=_mm256_maddubs_epi16(r0,cy0);
			m1=_mm256_maddubs_epi16(r1,cy1);
			m2=_mm256_maddubs_epi16(r2,cy2);
			m3=_mm256_maddubs_epi16(r3,cy3);

			//vertical add
			m0=_mm256_add_epi16(m0,m1);
			m0=_mm256_add_epi16(m0,m2);
			m0=_mm256_add_epi16(m0,m3);

			even=division(division_case,m0,f);//even results

			//multiply with the mask
			m0=_mm256_maddubs_epi16(r0,cy0_sh1);
			m1=_mm256_maddubs_epi16(r1,cy1_sh1);
			m2=_mm256_maddubs_epi16(r2,cy2_sh1);
			m3=_mm256_maddubs_epi16(r3,cy3_sh1);

			//vertical add
			m0=_mm256_add_epi16(m0,m1);
			m0=_mm256_add_epi16(m0,m2);
			m0=_mm256_add_epi16(m0,m3);

			odd=division(division_case,m0,f);//odd results

			//pack to one register
			odd=_mm256_slli_si256(odd,1); //shift one position right
			r0 = _mm256_add_epi8(even,odd); //add the odd with the even
			_mm256_store_si256( (__m256i *) &temp[col],r0 );
	  }
		//y filter ends - r0 has now the data to be processed by x filter

}

//this function is computing the row=0 and row=N-1 only
//for row=0 input prelude_5x5_Ymask_3(0,...
//for row=N-1 input prelude_5x5_Ymask_3(N-3,...
void prelude_5x5_16_Ymask_0_new(const int row, const unsigned int M, unsigned char **frame1, unsigned char *temp, const signed char mask_vector_y[][32],  const unsigned int division_case, const __m256i f){

	__m256i r0,r1,r2,m0,m1,m2,even,odd;
	//__m256i ones=_mm256_set1_epi16(1);
	 __m256i cy0,cy1,cy2,cy0_sh1,cy1_sh1,cy2_sh1;

	if (row!=0){ //if row=N-6
	  cy0=_mm256_load_si256( (__m256i *) &mask_vector_y[0]);
	  cy1=_mm256_load_si256( (__m256i *) &mask_vector_y[1]);
	  cy2=_mm256_load_si256( (__m256i *) &mask_vector_y[2]);

	  cy0_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[5]);
	  cy1_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[6]);
	  cy2_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[7]);
	}
	else {
		  cy0=_mm256_load_si256( (__m256i *) &mask_vector_y[2]);
		  cy1=_mm256_load_si256( (__m256i *) &mask_vector_y[3]);
		  cy2=_mm256_load_si256( (__m256i *) &mask_vector_y[4]);

		  cy0_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[7]);
		  cy1_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[8]);
		  cy2_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[9]);

	}

	 for (unsigned int col = 0; col <= M-32; col+=32){

	  //--------------------y filter begins--------------------
	//load the 9 rows
	r0=_mm256_load_si256( (__m256i *) &frame1[row][col]);
	r1=_mm256_load_si256( (__m256i *) &frame1[row+1][col]);
	r2=_mm256_load_si256( (__m256i *) &frame1[row+2][col]);

		//multiply with the mask
			m0=_mm256_maddubs_epi16(r0,cy0);
			m1=_mm256_maddubs_epi16(r1,cy1);
			m2=_mm256_maddubs_epi16(r2,cy2);

			//vertical add
			m0=_mm256_add_epi16(m0,m1);
			m0=_mm256_add_epi16(m0,m2);

			even=division(division_case,m0,f);//even results

			//multiply with the mask
			m0=_mm256_maddubs_epi16(r0,cy0_sh1);
			m1=_mm256_maddubs_epi16(r1,cy1_sh1);
			m2=_mm256_maddubs_epi16(r2,cy2_sh1);

			//vertical add
			m0=_mm256_add_epi16(m0,m1);
			m0=_mm256_add_epi16(m0,m2);

			odd=division(division_case,m0,f);//odd results

			//pack to one register
			odd=_mm256_slli_si256(odd,1); //shift one position right
			r0 = _mm256_add_epi8(even,odd); //add the odd with the even
			_mm256_store_si256( (__m256i *) &temp[col],r0 );
	  }
		//y filter ends - r0 has now the data to be processed by x filter

}



void prelude_5x5_16_Xmask_new(unsigned char **frame1,unsigned char **filt,unsigned char *temp, const unsigned int N,const unsigned int M,const int row, const signed char mask_vector_x[][32],  const unsigned int division_case, const __m256i f,const unsigned int REMINDER_ITERATIONS_X,const signed char mask_vector_y[][32],const unsigned short int divisor_xy,signed char *kernel_x){

	const __m256i cx0=_mm256_load_si256( (__m256i *) &mask_vector_x[0]);

	const __m256i mask_prelude2  = _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0);
		const __m256i mask_prelude1  = _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0);


		const __m256i mask2  = _mm256_set_epi16(0,0,0,0,0,0,0,0,0,65535,0,0,0,0,0,0);
		const __m256i mask3  = _mm256_set_epi16(0,0,0,0,0,0,0,65535,0,0,0,0,0,0,0,0);
		const __m256i output_mask      = _mm256_set_epi16(0,0,0,65535,0,0,65535,0,0,65535,0,0,65535,0,0,65535);

		__m256i rr0,r0,m0,m1,m2,m3,m4,output_even,output_odd;

unsigned int col;

	  for (col = 0; col <= M-32; col+=30){

		  if (col==0){

	  			//1st col iteration

		  			//multiply by the mask
	  		        rr0=_mm256_load_si256( (__m256i *) &temp[0]);
					r0=insert_two_zeros_front(rr0,mask_prelude2);

	  				//multiply with the mask
	  				m0=_mm256_maddubs_epi16(r0,cx0);

	  				//1st hadd
	  				m1=_mm256_srli_si256(m0,2);
	  				m1=_mm256_add_epi16(m0,m1);

	  				//2nd hadd
	  				m2=_mm256_srli_si256(m0,4);
	  				m2=_mm256_add_epi16(m1,m2);

	  				//after shifts the 15th element is lost as it cannot be propagated to the 16th position (AVX registers are managed as two seperate SSE registers)
	  				m4=_mm256_and_si256(m0,mask3);
	  				m4=_mm256_permute2f128_si256(m4,m4,1);
	  				m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions
	  				m2=_mm256_add_epi16(m2,m4);

	  				//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
	  				output_even=_mm256_and_si256(m2,output_mask);

		  			//2nd col iteration

					r0=insert_one_zeros_front(rr0,mask_prelude1);

	  				//multiply with the mask
	  				m0=_mm256_maddubs_epi16(r0,cx0);

	  				//1st hadd
	  				m1=_mm256_srli_si256(m0,2);
	  				m1=_mm256_add_epi16(m0,m1);

	  				//2nd hadd
	  				m2=_mm256_srli_si256(m0,4);
	  				m2=_mm256_add_epi16(m1,m2);

	  				//after shifts the 15th element is lost as it cannot be propagated to the 16th position (AVX registers are managed as two seperate SSE registers)
	  				m4=_mm256_and_si256(m0,mask3);
	  				m4=_mm256_permute2f128_si256(m4,m4,1);
	  				m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions
	  				m2=_mm256_add_epi16(m2,m4);

	  				//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
	  				output_odd=_mm256_and_si256(m2,output_mask);

		  			//3rd col iteration

	  				//multiply with the mask
	  				m0=_mm256_maddubs_epi16(rr0,cx0);

	  				//1st hadd
	  				m1=_mm256_srli_si256(m0,2);
	  				m1=_mm256_add_epi16(m0,m1);

	  				//2nd hadd
	  				m2=_mm256_srli_si256(m0,4);
	  				m2=_mm256_add_epi16(m1,m2);

	  				//after shifts the 15th element is lost as it cannot be propagated to the 16th position (AVX registers are managed as two seperate SSE registers)
	  				m4=_mm256_and_si256(m0,mask3);
	  				m4=_mm256_permute2f128_si256(m4,m4,1);
	  				m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions
	  				m2=_mm256_add_epi16(m2,m4);

	  				//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
	  				m2=_mm256_and_si256(m2,output_mask);

	  				m2=_mm256_slli_si256(m2,2);
	  				output_even=_mm256_add_epi16(output_even,m2);

		  			//4th col iteration

		  			//multiply by the mask
	  		        r0=_mm256_loadu_si256( (__m256i *) &temp[col+1]);

	  				//multiply with the mask
	  				m0=_mm256_maddubs_epi16(r0,cx0);

	  				//1st hadd
	  				m1=_mm256_srli_si256(m0,2);
	  				m1=_mm256_add_epi16(m0,m1);

	  				//2nd hadd
	  				m2=_mm256_srli_si256(m0,4);
	  				m2=_mm256_add_epi16(m1,m2);

	  				//after shifts the 15th element is lost as it cannot be propagated to the 16th position (AVX registers are managed as two seperate SSE registers)
	  				m4=_mm256_and_si256(m0,mask3);
	  				m4=_mm256_permute2f128_si256(m4,m4,1);
	  				m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions
	  				m2=_mm256_add_epi16(m2,m4);

	  				//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
	  				m2=_mm256_and_si256(m2,output_mask);

	  				m2=_mm256_slli_si256(m2,2);
	  				output_odd=_mm256_add_epi16(output_odd,m2);

		  			//5th col iteration

		  			//multiply by the mask
	  		        r0=_mm256_loadu_si256( (__m256i *) &temp[col+2]);

	  				//multiply with the mask
	  				m0=_mm256_maddubs_epi16(r0,cx0);

	  				//1st hadd
	  				m1=_mm256_srli_si256(m0,2);
	  				m1=_mm256_add_epi16(m0,m1);

	  				//2nd hadd
	  				m2=_mm256_srli_si256(m0,4);
	  				m2=_mm256_add_epi16(m1,m2);

	  				//after shifts the 15th element is lost as it cannot be propagated to the 16th position (AVX registers are managed as two seperate SSE registers)
	  				m4=_mm256_and_si256(m0,mask3);
	  				m4=_mm256_permute2f128_si256(m4,m4,1);
	  				m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions
	  				m2=_mm256_add_epi16(m2,m4);

	  				//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
	  				m2=_mm256_and_si256(m2,output_mask);

	  				m3=_mm256_slli_si256(m2,4);
	  				m4=_mm256_and_si256(m2,mask2);
	  				m4=_mm256_permute2f128_si256(m4,m4,1);
	  				m4=_mm256_srli_si256(m4,12);//shift 6 short int positions or 12 char positions
	  				m2=_mm256_add_epi16(m3,m4);

	  				output_even=_mm256_add_epi16(output_even,m2);

		  			//6th col iteration

		  			//multiply by the mask
	  		        r0=_mm256_loadu_si256( (__m256i *) &temp[col+3]);

	  				//multiply with the mask
	  				m0=_mm256_maddubs_epi16(r0,cx0);

	  				//1st hadd
	  				m1=_mm256_srli_si256(m0,2);
	  				m1=_mm256_add_epi16(m0,m1);

	  				//2nd hadd
	  				m2=_mm256_srli_si256(m0,4);
	  				m2=_mm256_add_epi16(m1,m2);

	  				//after shifts the 15th element is lost as it cannot be propagated to the 16th position (AVX registers are managed as two seperate SSE registers)
	  				m4=_mm256_and_si256(m0,mask3);
	  				m4=_mm256_permute2f128_si256(m4,m4,1);
	  				m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions
	  				m2=_mm256_add_epi16(m2,m4);

	  				//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
	  				m2=_mm256_and_si256(m2,output_mask);

	  				m3=_mm256_slli_si256(m2,4);
	  				m4=_mm256_and_si256(m2,mask2);
	  				m4=_mm256_permute2f128_si256(m4,m4,1);
	  				m4=_mm256_srli_si256(m4,12);//shift 6 short int positions or 12 char positions
	  				m2=_mm256_add_epi16(m3,m4);

	  				output_odd=_mm256_add_epi16(output_odd,m2);

	  				//now division follows
	  				output_even=division(division_case,output_even,f);
	  				output_odd=division(division_case,output_odd,f);

	  				//shift odd 1 position and add to even
	  				output_odd = _mm256_slli_si256(output_odd,1);
	  				output_even = _mm256_add_epi8(output_even,output_odd);

	  				_mm256_store_si256( (__m256i *) &filt[row][0],output_even );

		  		  	}
		  else {

				// col iteration computes output pixels of   0,6,12,18,24
				// col+1 iteration computes output pixels of 1,7,13,19,25
				// col+2 iteration computes output pixels of 2,8,14,20,26
				// col+3 iteration computes output pixels of 3,9,15,21,27
				// col+4 iteration computes output pixels of 4,10,16,22,28
				// col+5 iteration computes output pixels of 5,11,17,23,29
				//afterwards, col becomes 30 and repeat the above process

		  			//1st col iteration

		  			//multiply by the mask
	  		        r0=_mm256_loadu_si256( (__m256i *) &temp[col-2]);

	  				//multiply with the mask
	  				m0=_mm256_maddubs_epi16(r0,cx0);

	  				//1st hadd
	  				m1=_mm256_srli_si256(m0,2);
	  				m1=_mm256_add_epi16(m0,m1);

	  				//2nd hadd
	  				m2=_mm256_srli_si256(m0,4);
	  				m2=_mm256_add_epi16(m1,m2);

	  				//after shifts the 15th element is lost as it cannot be propagated to the 16th position (AVX registers are managed as two seperate SSE registers)
	  				m4=_mm256_and_si256(m0,mask3);
	  				m4=_mm256_permute2f128_si256(m4,m4,1);
	  				m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions
	  				m2=_mm256_add_epi16(m2,m4);

	  				//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
	  				output_even=_mm256_and_si256(m2,output_mask);

		  			//2nd col iteration

		  			//multiply by the mask
	  		        r0=_mm256_loadu_si256( (__m256i *) &temp[col-1]);

	  				//multiply with the mask
	  				m0=_mm256_maddubs_epi16(r0,cx0);

	  				//1st hadd
	  				m1=_mm256_srli_si256(m0,2);
	  				m1=_mm256_add_epi16(m0,m1);

	  				//2nd hadd
	  				m2=_mm256_srli_si256(m0,4);
	  				m2=_mm256_add_epi16(m1,m2);

	  				//after shifts the 15th element is lost as it cannot be propagated to the 16th position (AVX registers are managed as two seperate SSE registers)
	  				m4=_mm256_and_si256(m0,mask3);
	  				m4=_mm256_permute2f128_si256(m4,m4,1);
	  				m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions
	  				m2=_mm256_add_epi16(m2,m4);

	  				//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
	  				output_odd=_mm256_and_si256(m2,output_mask);

		  			//3rd col iteration

		  			//multiply by the mask
	  		        r0=_mm256_loadu_si256( (__m256i *) &temp[col+0]);

	  				//multiply with the mask
	  				m0=_mm256_maddubs_epi16(r0,cx0);

	  				//1st hadd
	  				m1=_mm256_srli_si256(m0,2);
	  				m1=_mm256_add_epi16(m0,m1);

	  				//2nd hadd
	  				m2=_mm256_srli_si256(m0,4);
	  				m2=_mm256_add_epi16(m1,m2);

	  				//after shifts the 15th element is lost as it cannot be propagated to the 16th position (AVX registers are managed as two seperate SSE registers)
	  				m4=_mm256_and_si256(m0,mask3);
	  				m4=_mm256_permute2f128_si256(m4,m4,1);
	  				m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions
	  				m2=_mm256_add_epi16(m2,m4);

	  				//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
	  				m2=_mm256_and_si256(m2,output_mask);

	  				m2=_mm256_slli_si256(m2,2);
	  				output_even=_mm256_add_epi16(output_even,m2);

		  			//4th col iteration

		  			//multiply by the mask
	  		        r0=_mm256_loadu_si256( (__m256i *) &temp[col+1]);

	  				//multiply with the mask
	  				m0=_mm256_maddubs_epi16(r0,cx0);

	  				//1st hadd
	  				m1=_mm256_srli_si256(m0,2);
	  				m1=_mm256_add_epi16(m0,m1);

	  				//2nd hadd
	  				m2=_mm256_srli_si256(m0,4);
	  				m2=_mm256_add_epi16(m1,m2);

	  				//after shifts the 15th element is lost as it cannot be propagated to the 16th position (AVX registers are managed as two seperate SSE registers)
	  				m4=_mm256_and_si256(m0,mask3);
	  				m4=_mm256_permute2f128_si256(m4,m4,1);
	  				m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions
	  				m2=_mm256_add_epi16(m2,m4);

	  				//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
	  				m2=_mm256_and_si256(m2,output_mask);

	  				m2=_mm256_slli_si256(m2,2);
	  				output_odd=_mm256_add_epi16(output_odd,m2);

		  			//5th col iteration

		  			//multiply by the mask
	  		        r0=_mm256_loadu_si256( (__m256i *) &temp[col+2]);

	  				//multiply with the mask
	  				m0=_mm256_maddubs_epi16(r0,cx0);

	  				//1st hadd
	  				m1=_mm256_srli_si256(m0,2);
	  				m1=_mm256_add_epi16(m0,m1);

	  				//2nd hadd
	  				m2=_mm256_srli_si256(m0,4);
	  				m2=_mm256_add_epi16(m1,m2);

	  				//after shifts the 15th element is lost as it cannot be propagated to the 16th position (AVX registers are managed as two seperate SSE registers)
	  				m4=_mm256_and_si256(m0,mask3);
	  				m4=_mm256_permute2f128_si256(m4,m4,1);
	  				m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions
	  				m2=_mm256_add_epi16(m2,m4);

	  				//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
	  				m2=_mm256_and_si256(m2,output_mask);

	  				m3=_mm256_slli_si256(m2,4);
	  				m4=_mm256_and_si256(m2,mask2);
	  				m4=_mm256_permute2f128_si256(m4,m4,1);
	  				m4=_mm256_srli_si256(m4,12);//shift 6 short int positions or 12 char positions
	  				m2=_mm256_add_epi16(m3,m4);

	  				output_even=_mm256_add_epi16(output_even,m2);

		  			//6th col iteration

		  			//multiply by the mask
	  		        r0=_mm256_loadu_si256( (__m256i *) &temp[col+3]);

	  				//multiply with the mask
	  				m0=_mm256_maddubs_epi16(r0,cx0);

	  				//1st hadd
	  				m1=_mm256_srli_si256(m0,2);
	  				m1=_mm256_add_epi16(m0,m1);

	  				//2nd hadd
	  				m2=_mm256_srli_si256(m0,4);
	  				m2=_mm256_add_epi16(m1,m2);

	  				//after shifts the 15th element is lost as it cannot be propagated to the 16th position (AVX registers are managed as two seperate SSE registers)
	  				m4=_mm256_and_si256(m0,mask3);
	  				m4=_mm256_permute2f128_si256(m4,m4,1);
	  				m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions
	  				m2=_mm256_add_epi16(m2,m4);

	  				//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
	  				m2=_mm256_and_si256(m2,output_mask);

	  				m3=_mm256_slli_si256(m2,4);
	  				m4=_mm256_and_si256(m2,mask2);
	  				m4=_mm256_permute2f128_si256(m4,m4,1);
	  				m4=_mm256_srli_si256(m4,12);//shift 6 short int positions or 12 char positions
	  				m2=_mm256_add_epi16(m3,m4);

	  				output_odd=_mm256_add_epi16(output_odd,m2);

	  				//now division follows
	  				output_even=division(division_case,output_even,f);
	  				output_odd=division(division_case,output_odd,f);

	  				//shift odd 1 position and add to even
	  				output_odd = _mm256_slli_si256(output_odd,1);
	  				output_even = _mm256_add_epi8(output_even,output_odd);

	  				_mm256_storeu_si256( (__m256i *) &filt[row][col],output_even );

		  		 }

		}

		loop_reminder_5x5_16_blur_X(frame1,filt,temp,N,M,row, col, REMINDER_ITERATIONS_X, division_case, mask_vector_x, f, divisor_xy,kernel_x);

}




void loop_reminder_5x5_16_blur_Y(unsigned char **frame1,unsigned char **filt,unsigned char *temp,const unsigned int N,const unsigned int M,const unsigned int row, const unsigned int REMINDER_ITERATIONS_Y,const unsigned int division_case,const signed char mask_vector_y[][32], const __m256i f, const unsigned short int divisor_xy){

	__m256i r0,r1,r2,r3,r4,m0,m1,m2,m3,m4,even,odd,reminder_mask;


 if (REMINDER_ITERATIONS_Y==0){//no need for computing the y mask
		_mm256_store_si256( (__m256i *) &temp[M],_mm256_setzero_si256() );
        for (unsigned int g=M+32;g<M+32+3;g++)
        	temp[g]=0;
	}
 else {
		__m256i cy0,cy1,cy2,cy3,cy4,cy0_sh1,cy1_sh1,cy2_sh1,cy3_sh1,cy4_sh1;

	unsigned int col2=M-REMINDER_ITERATIONS_Y;//this is the col index to load the elements for the y mask

	if ((row>1) && (row<N-2)){//main case

		r0=_mm256_loadu_si256( (__m256i *) &frame1[row-2][col2]);
		r1=_mm256_loadu_si256( (__m256i *) &frame1[row-1][col2]);
		r2=_mm256_loadu_si256( (__m256i *) &frame1[row-0][col2]);
		r3=_mm256_loadu_si256( (__m256i *) &frame1[row+1][col2]);
		r4=_mm256_loadu_si256( (__m256i *) &frame1[row+2][col2]);


		cy0=_mm256_load_si256( (__m256i *) &mask_vector_y[0]);
		cy1=_mm256_load_si256( (__m256i *) &mask_vector_y[1]);
		cy2=_mm256_load_si256( (__m256i *) &mask_vector_y[2]);
		cy3=_mm256_load_si256( (__m256i *) &mask_vector_y[3]);
		cy4=_mm256_load_si256( (__m256i *) &mask_vector_y[4]);


		cy0_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[5]);
		cy1_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[6]);
		cy2_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[7]);
		cy3_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[8]);
		cy4_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[9]);

	}
	else {
		if (row==0){
			r0=_mm256_loadu_si256( (__m256i *) &frame1[0][col2]);
			r1=_mm256_loadu_si256( (__m256i *) &frame1[1][col2]);
			r2=_mm256_loadu_si256( (__m256i *) &frame1[2][col2]);
			r3=_mm256_setzero_si256();
			r4=_mm256_setzero_si256();

			cy0=_mm256_load_si256( (__m256i *) &mask_vector_y[2]);
			cy1=_mm256_load_si256( (__m256i *) &mask_vector_y[3]);
			cy2=_mm256_load_si256( (__m256i *) &mask_vector_y[4]);
			cy3=_mm256_setzero_si256();
			cy4=_mm256_setzero_si256();

			cy0_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[7]);
			cy1_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[8]);
			cy2_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[9]);
			cy3_sh1=_mm256_setzero_si256();
			cy4_sh1=_mm256_setzero_si256();

		}
		else if (row==1){
			r0=_mm256_loadu_si256( (__m256i *) &frame1[0][col2]);
			r1=_mm256_loadu_si256( (__m256i *) &frame1[1][col2]);
			r2=_mm256_loadu_si256( (__m256i *) &frame1[2][col2]);
			r3=_mm256_loadu_si256( (__m256i *) &frame1[3][col2]);
			r4=_mm256_setzero_si256();

			cy0=_mm256_load_si256( (__m256i *) &mask_vector_y[1]);
			cy1=_mm256_load_si256( (__m256i *) &mask_vector_y[2]);
			cy2=_mm256_load_si256( (__m256i *) &mask_vector_y[3]);
			cy3=_mm256_load_si256( (__m256i *) &mask_vector_y[4]);
			cy4=_mm256_setzero_si256();

			cy0_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[6]);
			cy1_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[7]);
			cy2_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[8]);
			cy3_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[9]);
			cy4_sh1=_mm256_setzero_si256();
		}

		else if (row==N-2){
			r0=_mm256_setzero_si256();
			r1=_mm256_loadu_si256( (__m256i *) &frame1[N-4][col2]);
			r2=_mm256_loadu_si256( (__m256i *) &frame1[N-3][col2]);
			r3=_mm256_loadu_si256( (__m256i *) &frame1[N-2][col2]);
			r4=_mm256_loadu_si256( (__m256i *) &frame1[N-1][col2]);

			cy0=_mm256_setzero_si256();
			cy1=_mm256_load_si256( (__m256i *) &mask_vector_y[0]);
			cy2=_mm256_load_si256( (__m256i *) &mask_vector_y[1]);
			cy3=_mm256_load_si256( (__m256i *) &mask_vector_y[2]);
			cy4=_mm256_load_si256( (__m256i *) &mask_vector_y[3]);

			cy0_sh1=_mm256_setzero_si256();
			cy1_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[5]);
			cy2_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[6]);
			cy3_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[7]);
			cy4_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[8]);
		}
		else if (row==N-1){
			r0=_mm256_setzero_si256();
			r1=_mm256_setzero_si256();
			r2=_mm256_loadu_si256( (__m256i *) &frame1[N-3][col2]);
			r3=_mm256_loadu_si256( (__m256i *) &frame1[N-2][col2]);
			r4=_mm256_loadu_si256( (__m256i *) &frame1[N-1][col2]);

			cy0=_mm256_setzero_si256();
			cy1=_mm256_setzero_si256();
			cy2=_mm256_load_si256( (__m256i *) &mask_vector_y[0]);
			cy3=_mm256_load_si256( (__m256i *) &mask_vector_y[1]);
			cy4=_mm256_load_si256( (__m256i *) &mask_vector_y[2]);

			cy0_sh1=_mm256_setzero_si256();
			cy1_sh1=_mm256_setzero_si256();
			cy2_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[5]);
			cy3_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[6]);
			cy4_sh1=_mm256_load_si256( (__m256i *) &mask_vector_y[7]);
		}
		else {
			printf("\nsomething went wrong");
			exit(EXIT_FAILURE);
		}
	}


	/*
	 * The above load operations load outside of the array bounds; these elements are filled with zeros just after
	 */
	reminder_mask=_mm256_load_si256( (__m256i *) &reminder_msk1[REMINDER_ITERATIONS_Y-1][0]);

	//AND r0-r4 with reminder_mask
	r0=_mm256_and_si256(r0,reminder_mask);
	r1=_mm256_and_si256(r1,reminder_mask);
	r2=_mm256_and_si256(r2,reminder_mask);
	r3=_mm256_and_si256(r3,reminder_mask);
	r4=_mm256_and_si256(r4,reminder_mask);


//--------------------y filter begins--------------------

//multiply with the mask
	m0=_mm256_maddubs_epi16(r0,cy0);
	m1=_mm256_maddubs_epi16(r1,cy1);
	m2=_mm256_maddubs_epi16(r2,cy2);
	m3=_mm256_maddubs_epi16(r3,cy3);
	m4=_mm256_maddubs_epi16(r4,cy4);

	//vertical add
	m0=_mm256_add_epi16(m0,m1);
	m0=_mm256_add_epi16(m0,m2);
	m0=_mm256_add_epi16(m0,m3);
	m0=_mm256_add_epi16(m0,m4);

	even=division(division_case,m0,f);//even results

	//multiply with the mask
	m0=_mm256_maddubs_epi16(r0,cy0_sh1);
	m1=_mm256_maddubs_epi16(r1,cy1_sh1);
	m2=_mm256_maddubs_epi16(r2,cy2_sh1);
	m3=_mm256_maddubs_epi16(r3,cy3_sh1);
	m4=_mm256_maddubs_epi16(r4,cy4_sh1);

	//vertical add
	m0=_mm256_add_epi16(m0,m1);
	m0=_mm256_add_epi16(m0,m2);
	m0=_mm256_add_epi16(m0,m3);
	m0=_mm256_add_epi16(m0,m4);

	odd=division(division_case,m0,f);//odd results

	//pack to one register
	odd=_mm256_slli_si256(odd,1); //shift one position right
	r0=_mm256_add_epi8(even,odd); //add the odd with the even


	_mm256_storeu_si256( (__m256i *) &temp[M-REMINDER_ITERATIONS_Y],r0 );

	for (int gg=M;gg<M+32+3;gg++)
	    temp[gg]=0;
//y filter ends - r0 has now the data to be processed by x filter
}


}




void loop_reminder_5x5_16_blur_X(unsigned char **frame1,unsigned char **filt,unsigned char *temp,const unsigned int N,const unsigned int M,const unsigned int row, const unsigned int col,const unsigned int REMINDER_ITERATIONS_X,const unsigned int division_case,const signed char mask_vector_x[][32], const __m256i f, const unsigned short int divisor_xy,signed char *kernel_x){

	const __m256i cx0=_mm256_load_si256( (__m256i *) &mask_vector_x[0]);

	const __m256i mask2  = _mm256_set_epi16(0,0,0,0,0,0,0,0,0,65535,0,0,0,0,0,0);
	const __m256i mask3  = _mm256_set_epi16(0,0,0,0,0,0,0,65535,0,0,0,0,0,0,0,0);
	const __m256i output_mask      = _mm256_set_epi16(0,0,0,65535,0,0,65535,0,0,65535,0,0,65535,0,0,65535);


	__m256i r0,m0,m1,m2,m3,m4,output1,output_even,output_odd;

		//1st col iteration

			//multiply by the mask
	        r0=_mm256_loadu_si256( (__m256i *) &temp[col-2]);

			//multiply with the mask
			m0=_mm256_maddubs_epi16(r0,cx0);

			//1st hadd
			m1=_mm256_srli_si256(m0,2);
			m1=_mm256_add_epi16(m0,m1);

			//2nd hadd
			m2=_mm256_srli_si256(m0,4);
			m2=_mm256_add_epi16(m1,m2);

			//after shifts the 15th element is lost as it cannot be propagated to the 16th position (AVX registers are managed as two seperate SSE registers)
			m4=_mm256_and_si256(m0,mask3);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions
			m2=_mm256_add_epi16(m2,m4);

			//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
			output_even=_mm256_and_si256(m2,output_mask);

			//2nd col iteration

			//multiply by the mask
	        r0=_mm256_loadu_si256( (__m256i *) &temp[col-1]);

			//multiply with the mask
			m0=_mm256_maddubs_epi16(r0,cx0);

			//1st hadd
			m1=_mm256_srli_si256(m0,2);
			m1=_mm256_add_epi16(m0,m1);

			//2nd hadd
			m2=_mm256_srli_si256(m0,4);
			m2=_mm256_add_epi16(m1,m2);

			//after shifts the 15th element is lost as it cannot be propagated to the 16th position (AVX registers are managed as two seperate SSE registers)
			m4=_mm256_and_si256(m0,mask3);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions
			m2=_mm256_add_epi16(m2,m4);

			//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
			output_odd=_mm256_and_si256(m2,output_mask);

			//3rd col iteration

			//multiply by the mask
	        r0=_mm256_loadu_si256( (__m256i *) &temp[col+0]);

			//multiply with the mask
			m0=_mm256_maddubs_epi16(r0,cx0);

			//1st hadd
			m1=_mm256_srli_si256(m0,2);
			m1=_mm256_add_epi16(m0,m1);

			//2nd hadd
			m2=_mm256_srli_si256(m0,4);
			m2=_mm256_add_epi16(m1,m2);

			//after shifts the 15th element is lost as it cannot be propagated to the 16th position (AVX registers are managed as two seperate SSE registers)
			m4=_mm256_and_si256(m0,mask3);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions
			m2=_mm256_add_epi16(m2,m4);

			//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
			m2=_mm256_and_si256(m2,output_mask);

			m2=_mm256_slli_si256(m2,2);
			output_even=_mm256_add_epi16(output_even,m2);

			//4th col iteration

			//multiply by the mask
	        r0=_mm256_loadu_si256( (__m256i *) &temp[col+1]);

			//multiply with the mask
			m0=_mm256_maddubs_epi16(r0,cx0);

			//1st hadd
			m1=_mm256_srli_si256(m0,2);
			m1=_mm256_add_epi16(m0,m1);

			//2nd hadd
			m2=_mm256_srli_si256(m0,4);
			m2=_mm256_add_epi16(m1,m2);

			//after shifts the 15th element is lost as it cannot be propagated to the 16th position (AVX registers are managed as two seperate SSE registers)
			m4=_mm256_and_si256(m0,mask3);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions
			m2=_mm256_add_epi16(m2,m4);

			//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
			m2=_mm256_and_si256(m2,output_mask);

			m2=_mm256_slli_si256(m2,2);
			output_odd=_mm256_add_epi16(output_odd,m2);

			//5th col iteration

			//multiply by the mask
	        r0=_mm256_loadu_si256( (__m256i *) &temp[col+2]);

			//multiply with the mask
			m0=_mm256_maddubs_epi16(r0,cx0);

			//1st hadd
			m1=_mm256_srli_si256(m0,2);
			m1=_mm256_add_epi16(m0,m1);

			//2nd hadd
			m2=_mm256_srli_si256(m0,4);
			m2=_mm256_add_epi16(m1,m2);

			//after shifts the 15th element is lost as it cannot be propagated to the 16th position (AVX registers are managed as two seperate SSE registers)
			m4=_mm256_and_si256(m0,mask3);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions
			m2=_mm256_add_epi16(m2,m4);

			//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
			m2=_mm256_and_si256(m2,output_mask);

			m3=_mm256_slli_si256(m2,4);
			m4=_mm256_and_si256(m2,mask2);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_srli_si256(m4,12);//shift 6 short int positions or 12 char positions
			m2=_mm256_add_epi16(m3,m4);

			output_even=_mm256_add_epi16(output_even,m2);

			//6th col iteration

			//multiply by the mask
	        r0=_mm256_loadu_si256( (__m256i *) &temp[col+3]);

			//multiply with the mask
			m0=_mm256_maddubs_epi16(r0,cx0);

			//1st hadd
			m1=_mm256_srli_si256(m0,2);
			m1=_mm256_add_epi16(m0,m1);

			//2nd hadd
			m2=_mm256_srli_si256(m0,4);
			m2=_mm256_add_epi16(m1,m2);

			//after shifts the 15th element is lost as it cannot be propagated to the 16th position (AVX registers are managed as two seperate SSE registers)
			m4=_mm256_and_si256(m0,mask3);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_slli_si256(m4,12);//shift 6 short int positions or 12 char positions
			m2=_mm256_add_epi16(m2,m4);

			//m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12. keep only those, discard others
			m2=_mm256_and_si256(m2,output_mask);

			m3=_mm256_slli_si256(m2,4);
			m4=_mm256_and_si256(m2,mask2);
			m4=_mm256_permute2f128_si256(m4,m4,1);
			m4=_mm256_srli_si256(m4,12);//shift 6 short int positions or 12 char positions
			m2=_mm256_add_epi16(m3,m4);

			output_odd=_mm256_add_epi16(output_odd,m2);

			//now division follows
			output_even=division(division_case,output_even,f);
			output_odd=division(division_case,output_odd,f);

			//shift odd 1 position and add to even
			output_odd = _mm256_slli_si256(output_odd,1);
			output1 = _mm256_add_epi8(output_even,output_odd);


	switch (REMINDER_ITERATIONS_X){
	case 1:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		  break;
	case 2:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		  break;
	case 3:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		  break;
	case 4:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output1,3);
		  break;
	case 5:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output1,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output1,4);
		  break;
	case 6:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output1,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output1,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output1,5);
		  break;
	case 7:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output1,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output1,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output1,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output1,6);
		  break;
	case 8:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output1,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output1,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output1,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output1,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output1,7);
		  break;
	case 9:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output1,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output1,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output1,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output1,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output1,7);
		filt[row][col+8] = (unsigned char) _mm256_extract_epi8(output1,8);
		  break;
	case 10:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output1,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output1,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output1,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output1,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output1,7);
		filt[row][col+8] = (unsigned char) _mm256_extract_epi8(output1,8);
		filt[row][col+9] = (unsigned char) _mm256_extract_epi8(output1,9);
		  break;
	case 11:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output1,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output1,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output1,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output1,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output1,7);
		filt[row][col+8] = (unsigned char) _mm256_extract_epi8(output1,8);
		filt[row][col+9] = (unsigned char) _mm256_extract_epi8(output1,9);
		filt[row][col+10] = (unsigned char) _mm256_extract_epi8(output1,10);
		  break;
	case 12:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output1,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output1,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output1,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output1,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output1,7);
		filt[row][col+8] = (unsigned char) _mm256_extract_epi8(output1,8);
		filt[row][col+9] = (unsigned char) _mm256_extract_epi8(output1,9);
		filt[row][col+10] = (unsigned char) _mm256_extract_epi8(output1,10);
		filt[row][col+11] = (unsigned char) _mm256_extract_epi8(output1,11);
		  break;
	case 13:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output1,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output1,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output1,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output1,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output1,7);
		filt[row][col+8] = (unsigned char) _mm256_extract_epi8(output1,8);
		filt[row][col+9] = (unsigned char) _mm256_extract_epi8(output1,9);
		filt[row][col+10] = (unsigned char) _mm256_extract_epi8(output1,10);
		filt[row][col+11] = (unsigned char) _mm256_extract_epi8(output1,11);
		filt[row][col+12] = (unsigned char) _mm256_extract_epi8(output1,12);
		  break;
	case 14:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output1,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output1,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output1,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output1,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output1,7);
		filt[row][col+8] = (unsigned char) _mm256_extract_epi8(output1,8);
		filt[row][col+9] = (unsigned char) _mm256_extract_epi8(output1,9);
		filt[row][col+10] = (unsigned char) _mm256_extract_epi8(output1,10);
		filt[row][col+11] = (unsigned char) _mm256_extract_epi8(output1,11);
		filt[row][col+12] = (unsigned char) _mm256_extract_epi8(output1,12);
		filt[row][col+13] = (unsigned char) _mm256_extract_epi8(output1,13);
		  break;
	case 15:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output1,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output1,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output1,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output1,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output1,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output1,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output1,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output1,7);
		filt[row][col+8] = (unsigned char) _mm256_extract_epi8(output1,8);
		filt[row][col+9] = (unsigned char) _mm256_extract_epi8(output1,9);
		filt[row][col+10] = (unsigned char) _mm256_extract_epi8(output1,10);
		filt[row][col+11] = (unsigned char) _mm256_extract_epi8(output1,11);
		filt[row][col+12] = (unsigned char) _mm256_extract_epi8(output1,12);
		filt[row][col+13] = (unsigned char) _mm256_extract_epi8(output1,13);
		filt[row][col+14] = (unsigned char) _mm256_extract_epi8(output1,14);
		  break;
	case 16:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
		  break;
	case 17:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
		  break;
	case 18:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
		  break;
	case 19:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
		  break;
	case 20:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output1,19);
		  break;
	case 21:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output1,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output1,20);
		  break;
	case 22:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output1,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output1,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output1,21);
		break;
	case 23:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output1,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output1,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output1,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output1,22);
		  break;
	case 24:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output1,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output1,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output1,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output1,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output1,23);
		  break;
	case 25:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output1,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output1,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output1,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output1,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output1,23);
	filt[row][col+24] = (unsigned char) _mm256_extract_epi8(output1,24);
		  break;
	case 26:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output1,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output1,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output1,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output1,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output1,23);
	filt[row][col+24] = (unsigned char) _mm256_extract_epi8(output1,24);
	filt[row][col+25] = (unsigned char) _mm256_extract_epi8(output1,25);
		  break;
	case 27:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output1,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output1,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output1,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output1,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output1,23);
	filt[row][col+24] = (unsigned char) _mm256_extract_epi8(output1,24);
	filt[row][col+25] = (unsigned char) _mm256_extract_epi8(output1,25);
	filt[row][col+26] = (unsigned char) _mm256_extract_epi8(output1,26);
		  break;
	case 28:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output1,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output1,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output1,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output1,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output1,23);
	filt[row][col+24] = (unsigned char) _mm256_extract_epi8(output1,24);
	filt[row][col+25] = (unsigned char) _mm256_extract_epi8(output1,25);
	filt[row][col+26] = (unsigned char) _mm256_extract_epi8(output1,26);
	filt[row][col+27] = (unsigned char) _mm256_extract_epi8(output1,27);
		  break;
	case 29:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output1,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output1,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output1,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output1,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output1,23);
	filt[row][col+24] = (unsigned char) _mm256_extract_epi8(output1,24);
	filt[row][col+25] = (unsigned char) _mm256_extract_epi8(output1,25);
	filt[row][col+26] = (unsigned char) _mm256_extract_epi8(output1,26);
	filt[row][col+27] = (unsigned char) _mm256_extract_epi8(output1,27);
	filt[row][col+28] = (unsigned char) _mm256_extract_epi8(output1,28);
		  break;
	case 30:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output1,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output1,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output1,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output1,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output1,23);
	filt[row][col+24] = (unsigned char) _mm256_extract_epi8(output1,24);
	filt[row][col+25] = (unsigned char) _mm256_extract_epi8(output1,25);
	filt[row][col+26] = (unsigned char) _mm256_extract_epi8(output1,26);
	filt[row][col+27] = (unsigned char) _mm256_extract_epi8(output1,27);
	filt[row][col+28] = (unsigned char) _mm256_extract_epi8(output1,28);
	filt[row][col+29] = (unsigned char) _mm256_extract_epi8(output1,29);
		  break;
	case 31:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output1, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output1,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output1,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output1,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output1,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output1,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output1,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output1,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output1,23);
	filt[row][col+24] = (unsigned char) _mm256_extract_epi8(output1,24);
	filt[row][col+25] = (unsigned char) _mm256_extract_epi8(output1,25);
	filt[row][col+26] = (unsigned char) _mm256_extract_epi8(output1,26);
	filt[row][col+27] = (unsigned char) _mm256_extract_epi8(output1,27);
	filt[row][col+28] = (unsigned char) _mm256_extract_epi8(output1,28);
	filt[row][col+29] = (unsigned char) _mm256_extract_epi8(output1,29);

	int newPixel = 0;

	newPixel += temp[col+30-2] * kernel_x[0];
	newPixel += temp[col+30-1] * kernel_x[1];
	newPixel += temp[col+30-0] * kernel_x[2];

	filt[row][col+30] = (unsigned char) (newPixel / divisor_xy);

		  break;
	default:
		printf("\nsomething went wrong");
		exit(EXIT_FAILURE);
 	}


}




void Gaussian_Blur_3x3_16_more_load(unsigned char **frame1,unsigned char **filt,const unsigned int M, const unsigned int N, const unsigned short int divisor, signed char **filter){

const signed char f00=filter[0][0];
const signed char f01=filter[0][1];
const signed char f02=filter[0][2];

const signed char f10=filter[1][0];
const signed char f11=filter[1][1];
const signed char f12=filter[1][2];


const __m256i c0=_mm256_set_epi8(0,f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00);
const __m256i c1=_mm256_set_epi8(0,f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10);



	//const __m256i mask3  = _mm256_set_epi16(0,0,0,0,0,0,0,65535,0,0,0,0,0,0,0,0);
	const __m256i mask_prelude  = _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0);

	const __m256i output_mask      = _mm256_set_epi16(0,65535,0,65535,0,65535,0,65535,0,65535,0,65535,0,65535,0,65535);
	//const __m256i output_mask_sh1  = _mm256_set_epi16(0,0,65535,0,65535,0,65535,0,65535,0,65535,0,65535,0,65535,0);



   //REMINDER_ITERATIONS value ranges from 2 to 33
	const unsigned int REMINDER_ITERATIONS =  (M-((((M-32-2)/32)*32)+32)); //M-(last_col_value+32)
	//printf("\n%d",REMINDER_ITERATIONS);


	const unsigned int division_case=prepare_for_division(divisor); //determine the division case (A, B or C)
	const __m256i f = _mm256_load_si256( (__m256i *) &f_vector[0]);// initialize the division vector

//printf("\n%d %d",division_case,b);


#pragma omp parallel
{

unsigned int row,col;
register __m256i r0,r1,r2,m0,m1,m2,rr0,rr1,rr2;
//__m256i output_row0_even,output_row1_even,output_row0_odd,output_row1_odd; //these are for processing row #0 and #1 only.
__m256i output_even,output_odd;
//__m256i m0_prelude_row0,m0_prelude_row1;


/*---------------------- Gaussian Blur ---------------------------------*/

    #pragma omp for schedule(static)
	for (row = 0; row < N; row++) {

	if (row==0){//special case compute filt[0:1][:]

		  for (col = 0; col <= M-32-2; col+=32){

			  if (col==0){

					//load the 2 rows only
					r0=_mm256_load_si256( (__m256i *) &frame1[0][0]);
					r1=_mm256_load_si256( (__m256i *) &frame1[1][0]);

		  			//START - extra code needed for prelude
			  					m0=_mm256_slli_si256(r0,1);//shift 1 elements left - equivalent to filling with one zero inthe beginning
			  					m1=_mm256_slli_si256(r1,1);//shift 1 elements left - equivalent to filling with one zero inthe beginning

			  					//preserve the element being lost because of the shift above
			  					r0=_mm256_and_si256(r0,mask_prelude);
			  					r0=_mm256_permute2f128_si256(r0,r0,1);
			  					r0=_mm256_srli_si256(r0,15);//shift 15 elements
			  					r0=_mm256_add_epi16(m0,r0);

			  					r1=_mm256_and_si256(r1,mask_prelude);
			  					r1=_mm256_permute2f128_si256(r1,r1,1);
			  					r1=_mm256_srli_si256(r1,15);//shift 15 elements
			  					r1=_mm256_add_epi16(m1,r1);

					//multiply with the mask
					m0=_mm256_maddubs_epi16(r0,c1);
					m1=_mm256_maddubs_epi16(r1,c0);

					//vertical add
					m0=_mm256_add_epi16(m0,m1);

					//hozizontal additions
					m1=_mm256_srli_si256(m0,2);
					m2=_mm256_add_epi16(m1,m0);

					//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others. later on they will be stored into 0,4,8,12,16,20,24,28 positions (8bit register).
					output_even=_mm256_and_si256(m2,output_mask);

					//2ND col iteration

					//load the 3 rows
					r0=_mm256_load_si256( (__m256i *) &frame1[0][0]);
					r1=_mm256_load_si256( (__m256i *) &frame1[1][0]);

					//multiply with the mask
					m0=_mm256_maddubs_epi16(r0,c1);
					m1=_mm256_maddubs_epi16(r1,c0);

					//vertical add
					m0=_mm256_add_epi16(m0,m1);

					//hozizontal additions
					m1=_mm256_srli_si256(m0,2);
					m2=_mm256_add_epi16(m1,m0);

					//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others. later on they will be stored into 0,4,8,12,16,20,24,28 positions (8bit register).
					output_odd=_mm256_and_si256(m2,output_mask);


					//3rd col iteration

					//load the 3 rows
					r0=_mm256_loadu_si256( (__m256i *) &frame1[0][1]);
					r1=_mm256_loadu_si256( (__m256i *) &frame1[1][1]);

					//multiply with the mask
					m0=_mm256_maddubs_epi16(r0,c1);
					m1=_mm256_maddubs_epi16(r1,c0);

					//vertical add
					m0=_mm256_add_epi16(m0,m1);

					//hozizontal additions
					m1=_mm256_srli_si256(m0,2);
					m2=_mm256_add_epi16(m1,m0);

					//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others. later on they will be stored into 0,4,8,12,16,20,24,28 positions (8bit register).
					m2=_mm256_and_si256(m2,output_mask);
					m2=_mm256_slli_si256(m2,2);
					output_even=_mm256_add_epi16(output_even,m2);

					//4th col iteration

					//load the 3 rows
					r0=_mm256_loadu_si256( (__m256i *) &frame1[0][2]);
					r1=_mm256_loadu_si256( (__m256i *) &frame1[1][2]);

					//multiply with the mask
					m0=_mm256_maddubs_epi16(r0,c1);
					m1=_mm256_maddubs_epi16(r1,c0);

					//vertical add
					m0=_mm256_add_epi16(m0,m1);

					//hozizontal additions
					m1=_mm256_srli_si256(m0,2);
					m2=_mm256_add_epi16(m1,m0);

					//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others. later on they will be stored into 0,4,8,12,16,20,24,28 positions (8bit register).
					m2=_mm256_and_si256(m2,output_mask);
					m2=_mm256_slli_si256(m2,2);
					output_odd=_mm256_add_epi16(output_odd,m2);


					//now division follows
					output_even=division(division_case,output_even,f);
					output_odd=division(division_case,output_odd,f);

					//shift odd 1 position and add to even
					output_odd = _mm256_slli_si256(output_odd,1);
					output_even = _mm256_add_epi8(output_even,output_odd);


					_mm256_store_si256( (__m256i *) &filt[0][0],output_even );
			  			//END - extra code needed for prelude
			  		}
			  else {


					//col iteration computes output pixels of    1,5,9,13,17,21,25,29
					// col+1 iteration computes output pixels of 2,6,10,14,18,22,26,30
					// col+2 iteration computes output pixels of 3,7,11,15,19,23,27
					// col+3 iteration computes output pixels of 4,8,12,16,20,24,28
					//afterwards, col becomes 30 and repeat the above process

			//1st col iteration

			//load the 3 rows
			r0=_mm256_loadu_si256( (__m256i *) &frame1[0][col-1]);
			r1=_mm256_loadu_si256( (__m256i *) &frame1[1][col-1]);

			//multiply with the mask
			m0=_mm256_maddubs_epi16(r0,c1);
			m1=_mm256_maddubs_epi16(r1,c0);

			//vertical add
			m0=_mm256_add_epi16(m0,m1);

			//hozizontal additions
			m1=_mm256_srli_si256(m0,2);
			m2=_mm256_add_epi16(m1,m0);

			//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others. later on they will be stored into 0,4,8,12,16,20,24,28 positions (8bit register).
			output_even=_mm256_and_si256(m2,output_mask);

			//2ND col iteration

			//load the 3 rows
			r0=_mm256_load_si256( (__m256i *) &frame1[0][col]);//these loads are always aligned
			r1=_mm256_load_si256( (__m256i *) &frame1[1][col]);

			//multiply with the mask
			m0=_mm256_maddubs_epi16(r0,c1);
			m1=_mm256_maddubs_epi16(r1,c0);

			//vertical add
			m0=_mm256_add_epi16(m0,m1);

			//hozizontal additions
			m1=_mm256_srli_si256(m0,2);
			m2=_mm256_add_epi16(m1,m0);

			//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others. later on they will be stored into 0,4,8,12,16,20,24,28 positions (8bit register).
			output_odd=_mm256_and_si256(m2,output_mask);


			//3rd col iteration

			//load the 3 rows
			r0=_mm256_loadu_si256( (__m256i *) &frame1[0][col+1]);
			r1=_mm256_loadu_si256( (__m256i *) &frame1[1][col+1]);

			//multiply with the mask
			m0=_mm256_maddubs_epi16(r0,c1);
			m1=_mm256_maddubs_epi16(r1,c0);

			//vertical add
			m0=_mm256_add_epi16(m0,m1);

			//hozizontal additions
			m1=_mm256_srli_si256(m0,2);
			m2=_mm256_add_epi16(m1,m0);

			//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others. later on they will be stored into 0,4,8,12,16,20,24,28 positions (8bit register).
			m2=_mm256_and_si256(m2,output_mask);
			m2=_mm256_slli_si256(m2,2);
			output_even=_mm256_add_epi16(output_even,m2);

			//4th col iteration

			//load the 3 rows
			r0=_mm256_loadu_si256( (__m256i *) &frame1[0][col+2]);
			r1=_mm256_loadu_si256( (__m256i *) &frame1[1][col+2]);

			//multiply with the mask
			m0=_mm256_maddubs_epi16(r0,c1);
			m1=_mm256_maddubs_epi16(r1,c0);

			//vertical add
			m0=_mm256_add_epi16(m0,m1);

			//hozizontal additions
			m1=_mm256_srli_si256(m0,2);
			m2=_mm256_add_epi16(m1,m0);

			//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others. later on they will be stored into 0,4,8,12,16,20,24,28 positions (8bit register).
			m2=_mm256_and_si256(m2,output_mask);
			m2=_mm256_slli_si256(m2,2);
			output_odd=_mm256_add_epi16(output_odd,m2);


			//now division follows
			output_even=division(division_case,output_even,f);
			output_odd=division(division_case,output_odd,f);

			//shift odd 1 position and add to even
			output_odd = _mm256_slli_si256(output_odd,1);
			output_even = _mm256_add_epi8(output_even,output_odd);


			_mm256_store_si256( (__m256i *) &filt[0][col],output_even );

			  }

			}
		  loop_reminder_3x3_new_first_last_rows(frame1,filt,M,N,0,col,REMINDER_ITERATIONS,division_case,divisor,filter,c0,c1,f);

	}

	else if (row==N-1){//special case compute filt[N-2:N-1][:]
		  for (col = 0; col <= M-32-2; col+=32){

			  if (col==0){

					//load the 3 rows
					r0=_mm256_load_si256( (__m256i *) &frame1[N-2][0]);
					r1=_mm256_load_si256( (__m256i *) &frame1[N-1][0]);

		  			//START - extra code needed for prelude
			  					m0=_mm256_slli_si256(r0,1);//shift 1 elements left - equivalent to filling with one zero inthe beginning
			  					m1=_mm256_slli_si256(r1,1);//shift 1 elements left - equivalent to filling with one zero inthe beginning

			  					//preserve the element being lost because of the shift above
			  					r0=_mm256_and_si256(r0,mask_prelude);
			  					r0=_mm256_permute2f128_si256(r0,r0,1);
			  					r0=_mm256_srli_si256(r0,15);//shift 15 elements
			  					r0=_mm256_add_epi16(m0,r0);

			  					r1=_mm256_and_si256(r1,mask_prelude);
			  					r1=_mm256_permute2f128_si256(r1,r1,1);
			  					r1=_mm256_srli_si256(r1,15);//shift 15 elements
			  					r1=_mm256_add_epi16(m1,r1);

					//multiply with the mask
					m0=_mm256_maddubs_epi16(r0,c0);
					m1=_mm256_maddubs_epi16(r1,c1);

					//vertical add
					m0=_mm256_add_epi16(m0,m1);

					//hozizontal additions
					m1=_mm256_srli_si256(m0,2);
					m2=_mm256_add_epi16(m1,m0);

					//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others. later on they will be stored into 0,4,8,12,16,20,24,28 positions (8bit register).
					output_even=_mm256_and_si256(m2,output_mask);

					//2ND col iteration

					//load the 3 rows
					r0=_mm256_load_si256( (__m256i *) &frame1[N-2][0]);
					r1=_mm256_load_si256( (__m256i *) &frame1[N-1][0]);

					//multiply with the mask
					m0=_mm256_maddubs_epi16(r0,c0);
					m1=_mm256_maddubs_epi16(r1,c1);

					//vertical add
					m0=_mm256_add_epi16(m0,m1);

					//hozizontal additions
					m1=_mm256_srli_si256(m0,2);
					m2=_mm256_add_epi16(m1,m0);

					//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others. later on they will be stored into 0,4,8,12,16,20,24,28 positions (8bit register).
					output_odd=_mm256_and_si256(m2,output_mask);


					//3rd col iteration

					//load the 3 rows
					r0=_mm256_loadu_si256( (__m256i *) &frame1[N-2][1]);
					r1=_mm256_loadu_si256( (__m256i *) &frame1[N-1][1]);

					//multiply with the mask
					m0=_mm256_maddubs_epi16(r0,c0);
					m1=_mm256_maddubs_epi16(r1,c1);

					//vertical add
					m0=_mm256_add_epi16(m0,m1);

					//hozizontal additions
					m1=_mm256_srli_si256(m0,2);
					m2=_mm256_add_epi16(m1,m0);

					//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others. later on they will be stored into 0,4,8,12,16,20,24,28 positions (8bit register).
					m2=_mm256_and_si256(m2,output_mask);
					m2=_mm256_slli_si256(m2,2);
					output_even=_mm256_add_epi16(output_even,m2);

					//4th col iteration

					//load the 3 rows
					r0=_mm256_loadu_si256( (__m256i *) &frame1[N-2][2]);
					r1=_mm256_loadu_si256( (__m256i *) &frame1[N-1][2]);

					//multiply with the mask
					m0=_mm256_maddubs_epi16(r0,c0);
					m1=_mm256_maddubs_epi16(r1,c1);

					//vertical add
					m0=_mm256_add_epi16(m0,m1);

					//hozizontal additions
					m1=_mm256_srli_si256(m0,2);
					m2=_mm256_add_epi16(m1,m0);

					//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others. later on they will be stored into 0,4,8,12,16,20,24,28 positions (8bit register).
					m2=_mm256_and_si256(m2,output_mask);
					m2=_mm256_slli_si256(m2,2);
					output_odd=_mm256_add_epi16(output_odd,m2);


					//now division follows
					output_even=division(division_case,output_even,f);
					output_odd=division(division_case,output_odd,f);

					//shift odd 1 position and add to even
					output_odd = _mm256_slli_si256(output_odd,1);
					output_even = _mm256_add_epi8(output_even,output_odd);


					_mm256_store_si256( (__m256i *) &filt[N-1][0],output_even );
			  			//END - extra code needed for prelude
			  		}
			  else {


					//col iteration computes output pixels of    1,5,9,13,17,21,25,29
					// col+1 iteration computes output pixels of 2,6,10,14,18,22,26,30
					// col+2 iteration computes output pixels of 3,7,11,15,19,23,27
					// col+3 iteration computes output pixels of 4,8,12,16,20,24,28
					//afterwards, col becomes 30 and repeat the above process

			//1st col iteration

			//load the 3 rows
			r0=_mm256_loadu_si256( (__m256i *) &frame1[N-2][col-1]);
			r1=_mm256_loadu_si256( (__m256i *) &frame1[N-1][col-1]);

			//multiply with the mask
			m0=_mm256_maddubs_epi16(r0,c0);
			m1=_mm256_maddubs_epi16(r1,c1);

			//vertical add
			m0=_mm256_add_epi16(m0,m1);

			//hozizontal additions
			m1=_mm256_srli_si256(m0,2);
			m2=_mm256_add_epi16(m1,m0);

			//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others. later on they will be stored into 0,4,8,12,16,20,24,28 positions (8bit register).
			output_even=_mm256_and_si256(m2,output_mask);

			//2ND col iteration

			//load the 3 rows
			r0=_mm256_load_si256( (__m256i *) &frame1[N-2][col]);//these loads are always aligned
			r1=_mm256_load_si256( (__m256i *) &frame1[N-1][col]);

			//multiply with the mask
			m0=_mm256_maddubs_epi16(r0,c0);
			m1=_mm256_maddubs_epi16(r1,c1);

			//vertical add
			m0=_mm256_add_epi16(m0,m1);

			//hozizontal additions
			m1=_mm256_srli_si256(m0,2);
			m2=_mm256_add_epi16(m1,m0);

			//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others. later on they will be stored into 0,4,8,12,16,20,24,28 positions (8bit register).
			output_odd=_mm256_and_si256(m2,output_mask);


			//3rd col iteration

			//load the 3 rows
			r0=_mm256_loadu_si256( (__m256i *) &frame1[N-2][col+1]);
			r1=_mm256_loadu_si256( (__m256i *) &frame1[N-1][col+1]);

			//multiply with the mask
			m0=_mm256_maddubs_epi16(r0,c0);
			m1=_mm256_maddubs_epi16(r1,c1);

			//vertical add
			m0=_mm256_add_epi16(m0,m1);

			//hozizontal additions
			m1=_mm256_srli_si256(m0,2);
			m2=_mm256_add_epi16(m1,m0);

			//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others. later on they will be stored into 0,4,8,12,16,20,24,28 positions (8bit register).
			m2=_mm256_and_si256(m2,output_mask);
			m2=_mm256_slli_si256(m2,2);
			output_even=_mm256_add_epi16(output_even,m2);

			//4th col iteration

			//load the 3 rows
			r0=_mm256_loadu_si256( (__m256i *) &frame1[N-2][col+2]);
			r1=_mm256_loadu_si256( (__m256i *) &frame1[N-1][col+2]);

			//multiply with the mask
			m0=_mm256_maddubs_epi16(r0,c0);
			m1=_mm256_maddubs_epi16(r1,c1);

			//vertical add
			m0=_mm256_add_epi16(m0,m1);

			//hozizontal additions
			m1=_mm256_srli_si256(m0,2);
			m2=_mm256_add_epi16(m1,m0);

			//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others. later on they will be stored into 0,4,8,12,16,20,24,28 positions (8bit register).
			m2=_mm256_and_si256(m2,output_mask);
			m2=_mm256_slli_si256(m2,2);
			output_odd=_mm256_add_epi16(output_odd,m2);


			//now division follows
			output_even=division(division_case,output_even,f);
			output_odd=division(division_case,output_odd,f);

			//shift odd 1 position and add to even
			output_odd = _mm256_slli_si256(output_odd,1);
			output_even = _mm256_add_epi8(output_even,output_odd);


			_mm256_store_si256( (__m256i *) &filt[N-1][col],output_even );

			  }

			}
		  loop_reminder_3x3_new_first_last_rows(frame1,filt,M,N,N-1,col,REMINDER_ITERATIONS,division_case,divisor,filter,c0,c1,f);

	}

	else {//main loop

	  for (col = 0; col <= M-32-2; col+=32){

		  if (col==0){

				//load the 3 rows
				rr0=_mm256_load_si256( (__m256i *) &frame1[row-1][0]);
				rr1=_mm256_load_si256( (__m256i *) &frame1[row][0]);
				rr2=_mm256_load_si256( (__m256i *) &frame1[row+1][0]);

	  			//START - extra code needed for prelude
		  					m0=_mm256_slli_si256(rr0,1);//shift 1 elements left - equivalent to filling with one zero inthe beginning
		  					m1=_mm256_slli_si256(rr1,1);//shift 1 elements left - equivalent to filling with one zero inthe beginning
		  					m2=_mm256_slli_si256(rr2,1);//shift 1 elements left - equivalent to filling with one zero inthe beginning

		  					//preserve the element being lost because of the shift above
		  					r0=_mm256_and_si256(rr0,mask_prelude);
		  					r0=_mm256_permute2f128_si256(r0,r0,1);
		  					r0=_mm256_srli_si256(r0,15);//shift 15 elements
		  					r0=_mm256_add_epi16(m0,r0);

		  					r1=_mm256_and_si256(rr1,mask_prelude);
		  					r1=_mm256_permute2f128_si256(r1,r1,1);
		  					r1=_mm256_srli_si256(r1,15);//shift 15 elements
		  					r1=_mm256_add_epi16(m1,r1);

		  					r2=_mm256_and_si256(rr2,mask_prelude);
		  					r2=_mm256_permute2f128_si256(r2,r2,1);
		  					r2=_mm256_srli_si256(r2,15);//shift 15 elements
		  					r2=_mm256_add_epi16(m2,r2);

				//multiply with the mask
				m0=_mm256_maddubs_epi16(r0,c0);
				m1=_mm256_maddubs_epi16(r1,c1);
				m2=_mm256_maddubs_epi16(r2,c0);

				//vertical add
				m0=_mm256_add_epi16(m0,m1);
				m0=_mm256_add_epi16(m0,m2);

				//hozizontal additions
				m1=_mm256_srli_si256(m0,2);
				m2=_mm256_add_epi16(m1,m0);

				//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others. later on they will be stored into 0,4,8,12,16,20,24,28 positions (8bit register).
				output_even=_mm256_and_si256(m2,output_mask);

				//2ND col iteration

				//load the 3 rows
				//r0=_mm256_load_si256( (__m256i *) &frame1[row-1][0]);
				//r1=_mm256_load_si256( (__m256i *) &frame1[row][0]);
				//r2=_mm256_load_si256( (__m256i *) &frame1[row+1][0]);

				//multiply with the mask
				m0=_mm256_maddubs_epi16(rr0,c0);
				m1=_mm256_maddubs_epi16(rr1,c1);
				m2=_mm256_maddubs_epi16(rr2,c0);

				//vertical add
				m0=_mm256_add_epi16(m0,m1);
				m0=_mm256_add_epi16(m0,m2);

				//hozizontal additions
				m1=_mm256_srli_si256(m0,2);
				m2=_mm256_add_epi16(m1,m0);

				//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others. later on they will be stored into 0,4,8,12,16,20,24,28 positions (8bit register).
				output_odd=_mm256_and_si256(m2,output_mask);


				//3rd col iteration

				//load the 3 rows
				r0=_mm256_loadu_si256( (__m256i *) &frame1[row-1][1]);
				r1=_mm256_loadu_si256( (__m256i *) &frame1[row][1]);
				r2=_mm256_loadu_si256( (__m256i *) &frame1[row+1][1]);

				//multiply with the mask
				m0=_mm256_maddubs_epi16(r0,c0);
				m1=_mm256_maddubs_epi16(r1,c1);
				m2=_mm256_maddubs_epi16(r2,c0);

				//vertical add
				m0=_mm256_add_epi16(m0,m1);
				m0=_mm256_add_epi16(m0,m2);

				//hozizontal additions
				m1=_mm256_srli_si256(m0,2);
				m2=_mm256_add_epi16(m1,m0);

				//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others. later on they will be stored into 0,4,8,12,16,20,24,28 positions (8bit register).
				m2=_mm256_and_si256(m2,output_mask);
				m2=_mm256_slli_si256(m2,2);
				output_even=_mm256_add_epi16(output_even,m2);

				//4th col iteration

				//load the 3 rows
				r0=_mm256_loadu_si256( (__m256i *) &frame1[row-1][2]);
				r1=_mm256_loadu_si256( (__m256i *) &frame1[row][2]);
				r2=_mm256_loadu_si256( (__m256i *) &frame1[row+1][2]);

				//multiply with the mask
				m0=_mm256_maddubs_epi16(r0,c0);
				m1=_mm256_maddubs_epi16(r1,c1);
				m2=_mm256_maddubs_epi16(r2,c0);

				//vertical add
				m0=_mm256_add_epi16(m0,m1);
				m0=_mm256_add_epi16(m0,m2);

				//hozizontal additions
				m1=_mm256_srli_si256(m0,2);
				m2=_mm256_add_epi16(m1,m0);

				//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others. later on they will be stored into 0,4,8,12,16,20,24,28 positions (8bit register).
				m2=_mm256_and_si256(m2,output_mask);
				m2=_mm256_slli_si256(m2,2);
				output_odd=_mm256_add_epi16(output_odd,m2);


				//now division follows
				output_even=division(division_case,output_even,f);
				output_odd=division(division_case,output_odd,f);

				//shift odd 1 position and add to even
				output_odd = _mm256_slli_si256(output_odd,1);
				output_even = _mm256_add_epi8(output_even,output_odd);


				_mm256_store_si256( (__m256i *) &filt[row][0],output_even );
		  			//END - extra code needed for prelude
		  		}
		  else {


				//col iteration computes output pixels of    1,5,9,13,17,21,25,29
				// col+1 iteration computes output pixels of 2,6,10,14,18,22,26,30
				// col+2 iteration computes output pixels of 3,7,11,15,19,23,27
				// col+3 iteration computes output pixels of 4,8,12,16,20,24,28
				//afterwards, col becomes 30 and repeat the above process

		//1st col iteration

		//load the 3 rows
		r0=_mm256_loadu_si256( (__m256i *) &frame1[row-1][col-1]);
		r1=_mm256_loadu_si256( (__m256i *) &frame1[row][col-1]);
		r2=_mm256_loadu_si256( (__m256i *) &frame1[row+1][col-1]);

		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,c0);
		m1=_mm256_maddubs_epi16(r1,c1);
		m2=_mm256_maddubs_epi16(r2,c0);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m0=_mm256_add_epi16(m0,m2);

		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others. later on they will be stored into 0,4,8,12,16,20,24,28 positions (8bit register).
		output_even=_mm256_and_si256(m2,output_mask);

		//2ND col iteration

		//load the 3 rows
		r0=_mm256_load_si256( (__m256i *) &frame1[row-1][col]);//these loads are always aligned
		r1=_mm256_load_si256( (__m256i *) &frame1[row][col]);
		r2=_mm256_load_si256( (__m256i *) &frame1[row+1][col]);

		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,c0);
		m1=_mm256_maddubs_epi16(r1,c1);
		m2=_mm256_maddubs_epi16(r2,c0);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m0=_mm256_add_epi16(m0,m2);

		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others. later on they will be stored into 0,4,8,12,16,20,24,28 positions (8bit register).
		output_odd=_mm256_and_si256(m2,output_mask);


		//3rd col iteration

		//load the 3 rows
		r0=_mm256_loadu_si256( (__m256i *) &frame1[row-1][col+1]);
		r1=_mm256_loadu_si256( (__m256i *) &frame1[row][col+1]);
		r2=_mm256_loadu_si256( (__m256i *) &frame1[row+1][col+1]);

		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,c0);
		m1=_mm256_maddubs_epi16(r1,c1);
		m2=_mm256_maddubs_epi16(r2,c0);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m0=_mm256_add_epi16(m0,m2);

		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others. later on they will be stored into 0,4,8,12,16,20,24,28 positions (8bit register).
		m2=_mm256_and_si256(m2,output_mask);
		m2=_mm256_slli_si256(m2,2);
		output_even=_mm256_add_epi16(output_even,m2);

		//4th col iteration

		//load the 3 rows
		r0=_mm256_loadu_si256( (__m256i *) &frame1[row-1][col+2]);
		r1=_mm256_loadu_si256( (__m256i *) &frame1[row][col+2]);
		r2=_mm256_loadu_si256( (__m256i *) &frame1[row+1][col+2]);

		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,c0);
		m1=_mm256_maddubs_epi16(r1,c1);
		m2=_mm256_maddubs_epi16(r2,c0);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m0=_mm256_add_epi16(m0,m2);

		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others. later on they will be stored into 0,4,8,12,16,20,24,28 positions (8bit register).
		m2=_mm256_and_si256(m2,output_mask);
		m2=_mm256_slli_si256(m2,2);
		output_odd=_mm256_add_epi16(output_odd,m2);


		//now division follows
		output_even=division(division_case,output_even,f);
		output_odd=division(division_case,output_odd,f);

		//shift odd 1 position and add to even
		output_odd = _mm256_slli_si256(output_odd,1);
		output_even = _mm256_add_epi8(output_even,output_odd);


		_mm256_store_si256( (__m256i *) &filt[row][col],output_even );

		  }

		}
	  loop_reminder_3x3_new(frame1,filt,M,N,row,col,REMINDER_ITERATIONS,division_case,divisor,filter,c0,c1,f);

	}

}

}//end of parallel


}





void Gaussian_Blur_3x3_16_ineff_more_LS(unsigned char **frame1,unsigned char **filt,const unsigned int M, const unsigned int N, const unsigned short int divisor, signed char **filter){

const signed char f00=filter[0][0];
const signed char f01=filter[0][1];
const signed char f02=filter[0][2];

const signed char f10=filter[1][0];
const signed char f11=filter[1][1];
const signed char f12=filter[1][2];


	const __m256i c0=_mm256_set_epi8(0,f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00);
	const __m256i c1=_mm256_set_epi8(0,f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10);



	//const __m256i mask3  = _mm256_set_epi16(0,0,0,0,0,0,0,65535,0,0,0,0,0,0,0,0);
	const __m256i mask_prelude  = _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0);

	const __m256i output_mask      = _mm256_set_epi16(0,65535,0,65535,0,65535,0,65535,0,65535,0,65535,0,65535,0,65535);
	//const __m256i output_mask_sh1  = _mm256_set_epi16(0,0,65535,0,65535,0,65535,0,65535,0,65535,0,65535,0,65535,0);




	//const unsigned int REMINDER_ITERATIONS =  (M-((((M-32)/32)*32)+32)); //M-(last_col_value+30)
	//printf("\n%d",REMINDER_ITERATIONS);


	const unsigned int division_case=prepare_for_division(divisor); //determine which is the division case (A, B or C)
	const __m256i f = _mm256_load_si256( (__m256i *) &f_vector[0]);// initialize the division vector

//printf("\n%d %d",division_case,b);


#pragma omp parallel
{

unsigned int row,col;
register __m256i r0,r1,r2,m0,m1,m2;
//__m256i output_row0_even,output_row1_even,output_row0_odd,output_row1_odd; //these are for processing row #0 and #1 only.
__m256i output_even;
//__m256i m0_prelude_row0,m0_prelude_row1;


/*---------------------- Gaussian Blur ---------------------------------*/

    #pragma omp for schedule(static)
	for (row = 1; row < N-1; row++) {

	if (row==1){//special case compute filt[0:1][:]

	;
	}

	else if (row==N-2){//special case compute filt[N-2:N-1][:]
		;
	}

	else {//main loop

	  for (col = 0; col <= M-32; col+=32){

		  if (col==0){

				//load the 3 rows
				r0=_mm256_load_si256( (__m256i *) &frame1[row-1][0]);
				r1=_mm256_load_si256( (__m256i *) &frame1[row][0]);
				r2=_mm256_load_si256( (__m256i *) &frame1[row+1][0]);

	  			//START - extra code needed for prelude
		  					m0=_mm256_slli_si256(r0,1);//shift 1 elements left - equivalent to filling with one zero inthe beginning
		  					m1=_mm256_slli_si256(r1,1);//shift 1 elements left - equivalent to filling with one zero inthe beginning
		  					m2=_mm256_slli_si256(r2,1);//shift 1 elements left - equivalent to filling with one zero inthe beginning

		  					//preserve the element being lost because of the shift above
		  					r0=_mm256_and_si256(r0,mask_prelude);
		  					r0=_mm256_permute2f128_si256(r0,r0,1);
		  					r0=_mm256_srli_si256(r0,15);//shift 15 elements
		  					r0=_mm256_add_epi16(m0,r0);

		  					r1=_mm256_and_si256(r1,mask_prelude);
		  					r1=_mm256_permute2f128_si256(r1,r1,1);
		  					r1=_mm256_srli_si256(r1,15);//shift 15 elements
		  					r1=_mm256_add_epi16(m1,r1);

		  					r2=_mm256_and_si256(r2,mask_prelude);
		  					r2=_mm256_permute2f128_si256(r2,r2,1);
		  					r2=_mm256_srli_si256(r2,15);//shift 15 elements
		  					r2=_mm256_add_epi16(m2,r2);

		  					//multiply with the mask
		  					m0=_mm256_maddubs_epi16(r0,c0);
		  					m1=_mm256_maddubs_epi16(r1,c1);
		  					m2=_mm256_maddubs_epi16(r2,c0);

		  					//vertical add
		  					m0=_mm256_add_epi16(m0,m1);
		  					m0=_mm256_add_epi16(m0,m2);

		  					//hozizontal additions
		  					m1=_mm256_srli_si256(m0,2);
		  					m2=_mm256_add_epi16(m1,m0);

		  					//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others. later on they will be stored into 0,4,8,12,16,20,24,28 positions (8bit register).
		  					output_even=_mm256_and_si256(m2,output_mask);

		  					//divide and store the results that are ready so far
		  					output_even=division(division_case,output_even,f);
		  					filt[row][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		  					filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output_even,4);
		  					filt[row][col+8] = (unsigned char) _mm256_extract_epi8(output_even,8);
		  					filt[row][col+12] = (unsigned char) _mm256_extract_epi8(output_even,12);
		  					filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
		  					filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output_even,20);
		  					filt[row][col+24] = (unsigned char) _mm256_extract_epi8(output_even,24);
		  					filt[row][col+28] = (unsigned char) _mm256_extract_epi8(output_even,28);



		  					//2ND col iteration

		  					//load the 3 rows
		  					r0=_mm256_loadu_si256( (__m256i *) &frame1[row-1][0]);
		  					r1=_mm256_loadu_si256( (__m256i *) &frame1[row][0]);
		  					r2=_mm256_loadu_si256( (__m256i *) &frame1[row+1][0]);

		  					//multiply with the mask
		  					m0=_mm256_maddubs_epi16(r0,c0);
		  					m1=_mm256_maddubs_epi16(r1,c1);
		  					m2=_mm256_maddubs_epi16(r2,c0);

		  					//vertical add
		  					m0=_mm256_add_epi16(m0,m1);
		  					m0=_mm256_add_epi16(m0,m2);

		  					//hozizontal additions
		  					m1=_mm256_srli_si256(m0,2);
		  					m2=_mm256_add_epi16(m1,m0);

		  					//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others. later on they will be stored into 0,4,8,12,16,20,24,28 positions (8bit register).
		  					m2=_mm256_and_si256(m2,output_mask);

		  					output_even=division(division_case,m2,f);
		  					filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output_even,0);
		  					filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output_even,4);
		  					filt[row][col+9] = (unsigned char) _mm256_extract_epi8(output_even,8);
		  					filt[row][col+13] = (unsigned char) _mm256_extract_epi8(output_even,12);
		  					filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output_even,16);
		  					filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output_even,20);
		  					filt[row][col+25] = (unsigned char) _mm256_extract_epi8(output_even,24);
		  					filt[row][col+29] = (unsigned char) _mm256_extract_epi8(output_even,28);

		  					//3rd col iteration

		  					//load the 3 rows
		  					r0=_mm256_loadu_si256( (__m256i *) &frame1[row-1][1]);
		  					r1=_mm256_loadu_si256( (__m256i *) &frame1[row][1]);
		  					r2=_mm256_loadu_si256( (__m256i *) &frame1[row+1][1]);

		  					//multiply with the mask
		  					m0=_mm256_maddubs_epi16(r0,c0);
		  					m1=_mm256_maddubs_epi16(r1,c1);
		  					m2=_mm256_maddubs_epi16(r2,c0);

		  					//vertical add
		  					m0=_mm256_add_epi16(m0,m1);
		  					m0=_mm256_add_epi16(m0,m2);

		  					//hozizontal additions
		  					m1=_mm256_srli_si256(m0,2);
		  					m2=_mm256_add_epi16(m1,m0);

		  					//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others. later on they will be stored into 0,4,8,12,16,20,24,28 positions (8bit register).
		  					m2=_mm256_and_si256(m2,output_mask);

		  					output_even=division(division_case,m2,f);
		  					filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output_even,0);
		  					filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output_even,4);
		  					filt[row][col+10] = (unsigned char) _mm256_extract_epi8(output_even,8);
		  					filt[row][col+14] = (unsigned char) _mm256_extract_epi8(output_even,12);
		  					filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output_even,16);
		  					filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output_even,20);
		  					filt[row][col+26] = (unsigned char) _mm256_extract_epi8(output_even,24);
		  					filt[row][col+30] = (unsigned char) _mm256_extract_epi8(output_even,28);

		  					//4th col iteration

		  					//load the 3 rows
		  					r0=_mm256_loadu_si256( (__m256i *) &frame1[row-1][2]);
		  					r1=_mm256_loadu_si256( (__m256i *) &frame1[row][2]);
		  					r2=_mm256_loadu_si256( (__m256i *) &frame1[row+1][2]);

		  					//multiply with the mask
		  					m0=_mm256_maddubs_epi16(r0,c0);
		  					m1=_mm256_maddubs_epi16(r1,c1);
		  					m2=_mm256_maddubs_epi16(r2,c0);

		  					//vertical add
		  					m0=_mm256_add_epi16(m0,m1);
		  					m0=_mm256_add_epi16(m0,m2);

		  					//hozizontal additions
		  					m1=_mm256_srli_si256(m0,2);
		  					m2=_mm256_add_epi16(m1,m0);

		  					//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others. later on they will be stored into 0,4,8,12,16,20,24,28 positions (8bit register).
		  					m2=_mm256_and_si256(m2,output_mask);

		  					output_even=division(division_case,m2,f);
		  					filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output_even,0);
		  					filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output_even,4);
		  					filt[row][col+11] = (unsigned char) _mm256_extract_epi8(output_even,8);
		  					filt[row][col+15] = (unsigned char) _mm256_extract_epi8(output_even,12);
		  					filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output_even,16);
		  					filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output_even,20);
		  					filt[row][col+27] = (unsigned char) _mm256_extract_epi8(output_even,24);
		  					filt[row][col+31] = (unsigned char) _mm256_extract_epi8(output_even,28);
		  		}
		  else {


				//col iteration computes output pixels of    1,5,9,13,17,21,25,29
				// col+1 iteration computes output pixels of 2,6,10,14,18,22,26,30
				// col+2 iteration computes output pixels of 3,7,11,15,19,23,27
				// col+3 iteration computes output pixels of 4,8,12,16,20,24,28
				//afterwards, col becomes 30 and repeat the above process

		//1st col iteration

		//load the 3 rows
		r0=_mm256_loadu_si256( (__m256i *) &frame1[row-1][col-1]);
		r1=_mm256_loadu_si256( (__m256i *) &frame1[row][col-1]);
		r2=_mm256_loadu_si256( (__m256i *) &frame1[row+1][col-1]);

		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,c0);
		m1=_mm256_maddubs_epi16(r1,c1);
		m2=_mm256_maddubs_epi16(r2,c0);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m0=_mm256_add_epi16(m0,m2);

		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others. later on they will be stored into 0,4,8,12,16,20,24,28 positions (8bit register).
		output_even=_mm256_and_si256(m2,output_mask);

		//divide and store the results that are ready so far
		output_even=division(division_case,output_even,f);
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output_even,4);
		filt[row][col+8] = (unsigned char) _mm256_extract_epi8(output_even,8);
		filt[row][col+12] = (unsigned char) _mm256_extract_epi8(output_even,12);
		filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
		filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output_even,20);
		filt[row][col+24] = (unsigned char) _mm256_extract_epi8(output_even,24);
		filt[row][col+28] = (unsigned char) _mm256_extract_epi8(output_even,28);



		//2ND col iteration

		//load the 3 rows
		r0=_mm256_loadu_si256( (__m256i *) &frame1[row-1][col]);
		r1=_mm256_loadu_si256( (__m256i *) &frame1[row][col]);
		r2=_mm256_loadu_si256( (__m256i *) &frame1[row+1][col]);

		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,c0);
		m1=_mm256_maddubs_epi16(r1,c1);
		m2=_mm256_maddubs_epi16(r2,c0);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m0=_mm256_add_epi16(m0,m2);

		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others. later on they will be stored into 0,4,8,12,16,20,24,28 positions (8bit register).
		m2=_mm256_and_si256(m2,output_mask);

		output_even=division(division_case,m2,f);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output_even,4);
		filt[row][col+9] = (unsigned char) _mm256_extract_epi8(output_even,8);
		filt[row][col+13] = (unsigned char) _mm256_extract_epi8(output_even,12);
		filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output_even,16);
		filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output_even,20);
		filt[row][col+25] = (unsigned char) _mm256_extract_epi8(output_even,24);
		filt[row][col+29] = (unsigned char) _mm256_extract_epi8(output_even,28);

		//3rd col iteration

		//load the 3 rows
		r0=_mm256_loadu_si256( (__m256i *) &frame1[row-1][col+1]);
		r1=_mm256_loadu_si256( (__m256i *) &frame1[row][col+1]);
		r2=_mm256_loadu_si256( (__m256i *) &frame1[row+1][col+1]);

		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,c0);
		m1=_mm256_maddubs_epi16(r1,c1);
		m2=_mm256_maddubs_epi16(r2,c0);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m0=_mm256_add_epi16(m0,m2);

		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others. later on they will be stored into 0,4,8,12,16,20,24,28 positions (8bit register).
		m2=_mm256_and_si256(m2,output_mask);

		output_even=division(division_case,m2,f);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output_even,4);
		filt[row][col+10] = (unsigned char) _mm256_extract_epi8(output_even,8);
		filt[row][col+14] = (unsigned char) _mm256_extract_epi8(output_even,12);
		filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output_even,16);
		filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output_even,20);
		filt[row][col+26] = (unsigned char) _mm256_extract_epi8(output_even,24);
		filt[row][col+30] = (unsigned char) _mm256_extract_epi8(output_even,28);

		//4th col iteration

		//load the 3 rows
		r0=_mm256_loadu_si256( (__m256i *) &frame1[row-1][col+2]);
		r1=_mm256_loadu_si256( (__m256i *) &frame1[row][col+2]);
		r2=_mm256_loadu_si256( (__m256i *) &frame1[row+1][col+2]);

		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,c0);
		m1=_mm256_maddubs_epi16(r1,c1);
		m2=_mm256_maddubs_epi16(r2,c0);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m0=_mm256_add_epi16(m0,m2);

		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others. later on they will be stored into 0,4,8,12,16,20,24,28 positions (8bit register).
		m2=_mm256_and_si256(m2,output_mask);

		output_even=division(division_case,m2,f);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output_even,4);
		filt[row][col+11] = (unsigned char) _mm256_extract_epi8(output_even,8);
		filt[row][col+15] = (unsigned char) _mm256_extract_epi8(output_even,12);
		filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output_even,16);
		filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output_even,20);
		filt[row][col+27] = (unsigned char) _mm256_extract_epi8(output_even,24);
		filt[row][col+31] = (unsigned char) _mm256_extract_epi8(output_even,28);


		  }

		}
	 // loop_reminder_3x3(frame1,filt,M,N,row,col,REMINDER_ITERATIONS,division_case,divisor,filter,c0,c1,c0_sh1,c1_sh1,c0_sh2,c1_sh2,c0_sh3,c1_sh3,f);

	}

}

}//end of parallel


}




void Gaussian_Blur_3x3_16_ineff_float_div(unsigned char **frame1,unsigned char **filt,const unsigned int M, const unsigned int N, const unsigned short int divisor, signed char **filter){

const signed char f00=filter[0][0];
const signed char f01=filter[0][1];
const signed char f02=filter[0][2];

const signed char f10=filter[1][0];
const signed char f11=filter[1][1];
const signed char f12=filter[1][2];


	const __m256i c0=_mm256_set_epi8(0,f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00,0,f02,f01,f00);
	const __m256i c1=_mm256_set_epi8(0,f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10,0,f12,f11,f10);



	//const __m256i mask3  = _mm256_set_epi16(0,0,0,0,0,0,0,65535,0,0,0,0,0,0,0,0);
	const __m256i mask_prelude  = _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0);

	const __m256i output_mask      = _mm256_set_epi16(0,65535,0,65535,0,65535,0,65535,0,65535,0,65535,0,65535,0,65535);
	//const __m256i output_mask_sh1  = _mm256_set_epi16(0,0,65535,0,65535,0,65535,0,65535,0,65535,0,65535,0,65535,0);




	//const unsigned int REMINDER_ITERATIONS =  (M-((((M-32)/32)*32)+32)); //M-(last_col_value+30)
	//printf("\n%d",REMINDER_ITERATIONS);


	//const unsigned int division_case=prepare_for_division(divisor); //determine which is the division case (A, B or C)
	//const __m256i f = _mm256_load_si256( (__m256i *) &f_vector[0]);// initialize the division vector
//print_vector_16(f);
//printf("\n%d %d",division_case,b);



#pragma omp parallel
{

unsigned int row,col;
register __m256i r0,r1,r2,m0,m1,m2;
//__m256i output_row0_even,output_row1_even,output_row0_odd,output_row1_odd; //these are for processing row #0 and #1 only.
__m256i output_even,output_odd;
//__m256i m0_prelude_row0,m0_prelude_row1;


/*---------------------- Gaussian Blur ---------------------------------*/

    #pragma omp for schedule(static)
	for (row = 1; row < N-1; row++) {

	if (row==1){//special case compute filt[0:1][:]

	;
	}

	else if (row==N-2){//special case compute filt[N-2:N-1][:]
		;
	}

	else {//main loop

	  for (col = 0; col <= M-32; col+=32){

		  if (col==0){

				//load the 3 rows
				r0=_mm256_load_si256( (__m256i *) &frame1[row-1][0]);
				r1=_mm256_load_si256( (__m256i *) &frame1[row][0]);
				r2=_mm256_load_si256( (__m256i *) &frame1[row+1][0]);

	  			//START - extra code needed for prelude
		  					m0=_mm256_slli_si256(r0,1);//shift 1 elements left - equivalent to filling with one zero inthe beginning
		  					m1=_mm256_slli_si256(r1,1);//shift 1 elements left - equivalent to filling with one zero inthe beginning
		  					m2=_mm256_slli_si256(r2,1);//shift 1 elements left - equivalent to filling with one zero inthe beginning

		  					//preserve the element being lost because of the shift above
		  					r0=_mm256_and_si256(r0,mask_prelude);
		  					r0=_mm256_permute2f128_si256(r0,r0,1);
		  					r0=_mm256_srli_si256(r0,15);//shift 15 elements
		  					r0=_mm256_add_epi16(m0,r0);

		  					r1=_mm256_and_si256(r1,mask_prelude);
		  					r1=_mm256_permute2f128_si256(r1,r1,1);
		  					r1=_mm256_srli_si256(r1,15);//shift 15 elements
		  					r1=_mm256_add_epi16(m1,r1);

		  					r2=_mm256_and_si256(r2,mask_prelude);
		  					r2=_mm256_permute2f128_si256(r2,r2,1);
		  					r2=_mm256_srli_si256(r2,15);//shift 15 elements
		  					r2=_mm256_add_epi16(m2,r2);

				//multiply with the mask
				m0=_mm256_maddubs_epi16(r0,c0);
				m1=_mm256_maddubs_epi16(r1,c1);
				m2=_mm256_maddubs_epi16(r2,c0);

				//vertical add
				m0=_mm256_add_epi16(m0,m1);
				m0=_mm256_add_epi16(m0,m2);

				//hozizontal additions
				m1=_mm256_srli_si256(m0,2);
				m2=_mm256_add_epi16(m1,m0);

				//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others. later on they will be stored into 0,4,8,12,16,20,24,28 positions (8bit register).
				output_even=_mm256_and_si256(m2,output_mask);

				//2ND col iteration

				//load the 3 rows
				r0=_mm256_load_si256( (__m256i *) &frame1[row-1][0]);
				r1=_mm256_load_si256( (__m256i *) &frame1[row][0]);
				r2=_mm256_load_si256( (__m256i *) &frame1[row+1][0]);

				//multiply with the mask
				m0=_mm256_maddubs_epi16(r0,c0);
				m1=_mm256_maddubs_epi16(r1,c1);
				m2=_mm256_maddubs_epi16(r2,c0);

				//vertical add
				m0=_mm256_add_epi16(m0,m1);
				m0=_mm256_add_epi16(m0,m2);

				//hozizontal additions
				m1=_mm256_srli_si256(m0,2);
				m2=_mm256_add_epi16(m1,m0);

				//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others. later on they will be stored into 0,4,8,12,16,20,24,28 positions (8bit register).
				output_odd=_mm256_and_si256(m2,output_mask);


				//3rd col iteration

				//load the 3 rows
				r0=_mm256_loadu_si256( (__m256i *) &frame1[row-1][1]);
				r1=_mm256_loadu_si256( (__m256i *) &frame1[row][1]);
				r2=_mm256_loadu_si256( (__m256i *) &frame1[row+1][1]);

				//multiply with the mask
				m0=_mm256_maddubs_epi16(r0,c0);
				m1=_mm256_maddubs_epi16(r1,c1);
				m2=_mm256_maddubs_epi16(r2,c0);

				//vertical add
				m0=_mm256_add_epi16(m0,m1);
				m0=_mm256_add_epi16(m0,m2);

				//hozizontal additions
				m1=_mm256_srli_si256(m0,2);
				m2=_mm256_add_epi16(m1,m0);

				//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others. later on they will be stored into 0,4,8,12,16,20,24,28 positions (8bit register).
				m2=_mm256_and_si256(m2,output_mask);
				m2=_mm256_slli_si256(m2,2);
				output_even=_mm256_add_epi16(output_even,m2);

				//4th col iteration

				//load the 3 rows
				r0=_mm256_loadu_si256( (__m256i *) &frame1[row-1][2]);
				r1=_mm256_loadu_si256( (__m256i *) &frame1[row][2]);
				r2=_mm256_loadu_si256( (__m256i *) &frame1[row+1][2]);

				//multiply with the mask
				m0=_mm256_maddubs_epi16(r0,c0);
				m1=_mm256_maddubs_epi16(r1,c1);
				m2=_mm256_maddubs_epi16(r2,c0);

				//vertical add
				m0=_mm256_add_epi16(m0,m1);
				m0=_mm256_add_epi16(m0,m2);

				//hozizontal additions
				m1=_mm256_srli_si256(m0,2);
				m2=_mm256_add_epi16(m1,m0);

				//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others. later on they will be stored into 0,4,8,12,16,20,24,28 positions (8bit register).
				m2=_mm256_and_si256(m2,output_mask);
				m2=_mm256_slli_si256(m2,2);
				output_odd=_mm256_add_epi16(output_odd,m2);


				//now division follows
				output_even=float_div(output_even);
				output_odd=float_div(output_odd);

				//shift odd 1 position and add to even
				output_odd = _mm256_slli_si256(output_odd,1);
				output_even = _mm256_add_epi8(output_even,output_odd);


				_mm256_store_si256( (__m256i *) &filt[row][0],output_even );
		  			//END - extra code needed for prelude
		  		}
		  else {


				//col iteration computes output pixels of    1,5,9,13,17,21,25,29
				// col+1 iteration computes output pixels of 2,6,10,14,18,22,26,30
				// col+2 iteration computes output pixels of 3,7,11,15,19,23,27
				// col+3 iteration computes output pixels of 4,8,12,16,20,24,28
				//afterwards, col becomes 30 and repeat the above process

		//1st col iteration

		//load the 3 rows
		r0=_mm256_loadu_si256( (__m256i *) &frame1[row-1][col-1]);
		r1=_mm256_loadu_si256( (__m256i *) &frame1[row][col-1]);
		r2=_mm256_loadu_si256( (__m256i *) &frame1[row+1][col-1]);

		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,c0);
		m1=_mm256_maddubs_epi16(r1,c1);
		m2=_mm256_maddubs_epi16(r2,c0);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m0=_mm256_add_epi16(m0,m2);

		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others. later on they will be stored into 0,4,8,12,16,20,24,28 positions (8bit register).
		output_even=_mm256_and_si256(m2,output_mask);

		//2ND col iteration

		//load the 3 rows
		r0=_mm256_loadu_si256( (__m256i *) &frame1[row-1][col]);
		r1=_mm256_loadu_si256( (__m256i *) &frame1[row][col]);
		r2=_mm256_loadu_si256( (__m256i *) &frame1[row+1][col]);

		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,c0);
		m1=_mm256_maddubs_epi16(r1,c1);
		m2=_mm256_maddubs_epi16(r2,c0);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m0=_mm256_add_epi16(m0,m2);

		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others. later on they will be stored into 0,4,8,12,16,20,24,28 positions (8bit register).
		output_odd=_mm256_and_si256(m2,output_mask);


		//3rd col iteration

		//load the 3 rows
		r0=_mm256_loadu_si256( (__m256i *) &frame1[row-1][col+1]);
		r1=_mm256_loadu_si256( (__m256i *) &frame1[row][col+1]);
		r2=_mm256_loadu_si256( (__m256i *) &frame1[row+1][col+1]);

		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,c0);
		m1=_mm256_maddubs_epi16(r1,c1);
		m2=_mm256_maddubs_epi16(r2,c0);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m0=_mm256_add_epi16(m0,m2);

		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others. later on they will be stored into 0,4,8,12,16,20,24,28 positions (8bit register).
		m2=_mm256_and_si256(m2,output_mask);
		m2=_mm256_slli_si256(m2,2);
		output_even=_mm256_add_epi16(output_even,m2);

		//4th col iteration

		//load the 3 rows
		r0=_mm256_loadu_si256( (__m256i *) &frame1[row-1][col+2]);
		r1=_mm256_loadu_si256( (__m256i *) &frame1[row][col+2]);
		r2=_mm256_loadu_si256( (__m256i *) &frame1[row+1][col+2]);

		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,c0);
		m1=_mm256_maddubs_epi16(r1,c1);
		m2=_mm256_maddubs_epi16(r2,c0);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m0=_mm256_add_epi16(m0,m2);

		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others. later on they will be stored into 0,4,8,12,16,20,24,28 positions (8bit register).
		m2=_mm256_and_si256(m2,output_mask);
		m2=_mm256_slli_si256(m2,2);
		output_odd=_mm256_add_epi16(output_odd,m2);

		//now division follows
		output_even=float_div(output_even);
		output_odd=float_div(output_odd);

		//shift odd 1 position and add to even
		output_odd = _mm256_slli_si256(output_odd,1);
		output_even = _mm256_add_epi8(output_even,output_odd);


		_mm256_store_si256( (__m256i *) &filt[row][col],output_even );

		  }

		}
	 // loop_reminder_3x3(frame1,filt,M,N,row,col,REMINDER_ITERATIONS,division_case,divisor,filter,c0,c1,c0_sh1,c1_sh1,c0_sh2,c1_sh2,c0_sh3,c1_sh3,f);

	}

}

}//end of parallel


}


__m256i float_div(__m256i a ){

	__m256 div=_mm256_set_ps(17.0,17.0,17.0,17.0,17.0,17.0,17.0,17.0);
	__m128i ah = _mm256_extractf128_si256(a, 0);
	__m128i al = _mm256_extractf128_si256(a, 1);

	//make 32 int
	__m256i bh =_mm256_cvtepi16_epi32(ah);
	__m256i bl =_mm256_cvtepi16_epi32(al);

	//make float
	__m256 ch=_mm256_cvtepi32_ps(bh);
	__m256 cl=_mm256_cvtepi32_ps(bl);

	//div
	ch=_mm256_div_ps(ch,div);
	cl=_mm256_div_ps(cl,div);

	//make 32 int
	bh=_mm256_cvtps_epi32(ch);
	bl=_mm256_cvtps_epi32(cl);


	bl=_mm256_slli_si256(bl,2);
	return _mm256_add_epi8(bh,bl);

}


int loop_reminder_3x3_new(unsigned char **frame1,unsigned char **filt,const unsigned int M, const unsigned int N,const unsigned int row, const unsigned int col,const unsigned int REMINDER_ITERATIONS,const unsigned int division_case,const unsigned short int divisor,signed char **filter,const __m256i c0,const __m256i c1,const __m256i f){

	register	__m256i r0,r1,r2,m0,m1,m2,output_even,output_odd;


	const __m256i output_mask      = _mm256_set_epi16(0,65535,0,65535,0,65535,0,65535,0,65535,0,65535,0,65535,0,65535);

	 __m256i reminder_mask1;



	//1st col iteration

		//load the 3 rows
		r0=_mm256_loadu_si256( (__m256i *) &frame1[row-1][col-1]);
		r1=_mm256_loadu_si256( (__m256i *) &frame1[row][col-1]);
		r2=_mm256_loadu_si256( (__m256i *) &frame1[row+1][col-1]);

		//AND r0-r3 with reminder_mask
		reminder_mask1=_mm256_load_si256( (__m256i *) &reminder_msk1_3x3[REMINDER_ITERATIONS-1][0]);
		r0=_mm256_and_si256(r0,reminder_mask1);
		r1=_mm256_and_si256(r1,reminder_mask1);
		r2=_mm256_and_si256(r2,reminder_mask1);

		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,c0);
		m1=_mm256_maddubs_epi16(r1,c1);
		m2=_mm256_maddubs_epi16(r2,c0);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m0=_mm256_add_epi16(m0,m2);

		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others. later on they will be stored into 0,4,8,12,16,20,24,28 positions (8bit register).
		output_even=_mm256_and_si256(m2,output_mask);

		//2ND col iteration

		//load the 3 rows
		r0=_mm256_loadu_si256( (__m256i *) &frame1[row-1][col]);
		r1=_mm256_loadu_si256( (__m256i *) &frame1[row][col]);
		r2=_mm256_loadu_si256( (__m256i *) &frame1[row+1][col]);

		//AND r0-r3 with reminder_mask
		reminder_mask1=_mm256_load_si256( (__m256i *) &reminder_msk1_3x3[REMINDER_ITERATIONS-2][0]);
		r0=_mm256_and_si256(r0,reminder_mask1);
		r1=_mm256_and_si256(r1,reminder_mask1);
		r2=_mm256_and_si256(r2,reminder_mask1);

		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,c0);
		m1=_mm256_maddubs_epi16(r1,c1);
		m2=_mm256_maddubs_epi16(r2,c0);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m0=_mm256_add_epi16(m0,m2);

		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others. later on they will be stored into 0,4,8,12,16,20,24,28 positions (8bit register).
		output_odd=_mm256_and_si256(m2,output_mask);



if (REMINDER_ITERATIONS>2){//this prevents  reminder_msk1_3x3[REMINDER_ITERATIONS-3][0]) access -1
	//3rd col iteration
	//load the 3 rows
		r0=_mm256_loadu_si256( (__m256i *) &frame1[row-1][col+1]);
		r1=_mm256_loadu_si256( (__m256i *) &frame1[row][col+1]);
		r2=_mm256_loadu_si256( (__m256i *) &frame1[row+1][col+1]);

		//AND r0-r3 with reminder_mask
		reminder_mask1=_mm256_load_si256( (__m256i *) &reminder_msk1_3x3[REMINDER_ITERATIONS-3][0]);
		r0=_mm256_and_si256(r0,reminder_mask1);
		r1=_mm256_and_si256(r1,reminder_mask1);
		r2=_mm256_and_si256(r2,reminder_mask1);

		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,c0);
		m1=_mm256_maddubs_epi16(r1,c1);
		m2=_mm256_maddubs_epi16(r2,c0);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m0=_mm256_add_epi16(m0,m2);

		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others. later on they will be stored into 0,4,8,12,16,20,24,28 positions (8bit register).
		m2=_mm256_and_si256(m2,output_mask);
		m2=_mm256_slli_si256(m2,2);
		output_even=_mm256_add_epi16(output_even,m2);
}


if (REMINDER_ITERATIONS>3){
		//4th col iteration

		//load the 3 rows
		r0=_mm256_loadu_si256( (__m256i *) &frame1[row-1][col+2]);
		r1=_mm256_loadu_si256( (__m256i *) &frame1[row][col+2]);
		r2=_mm256_loadu_si256( (__m256i *) &frame1[row+1][col+2]);

		//AND r0-r3 with reminder_mask
		reminder_mask1=_mm256_load_si256( (__m256i *) &reminder_msk1_3x3[REMINDER_ITERATIONS-4][0]);
		r0=_mm256_and_si256(r0,reminder_mask1);
		r1=_mm256_and_si256(r1,reminder_mask1);
		r2=_mm256_and_si256(r2,reminder_mask1);

		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,c0);
		m1=_mm256_maddubs_epi16(r1,c1);
		m2=_mm256_maddubs_epi16(r2,c0);

		//vertical add
		m0=_mm256_add_epi16(m0,m1);
		m0=_mm256_add_epi16(m0,m2);

		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others. later on they will be stored into 0,4,8,12,16,20,24,28 positions (8bit register).
		m2=_mm256_and_si256(m2,output_mask);
		m2=_mm256_slli_si256(m2,2);
		output_odd=_mm256_add_epi16(output_odd,m2);
}

		//now division follows
		output_even=division(division_case,output_even,f);
		output_odd=division(division_case,output_odd,f);

		//shift odd 1 position and add to even
		output_odd = _mm256_slli_si256(output_odd,1);
		output_even = _mm256_add_epi8(output_even,output_odd);



 	switch (REMINDER_ITERATIONS){

	case 2:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		  break;
	case 3:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		  break;
	case 4:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output_even,3);
		  break;
	case 5:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output_even,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output_even,4);
		  break;
	case 6:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output_even,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output_even,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output_even,5);
		  break;
	case 7:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output_even,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output_even,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output_even,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output_even,6);
		  break;
	case 8:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output_even,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output_even,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output_even,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output_even,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output_even,7);
		  break;
	case 9:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output_even,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output_even,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output_even,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output_even,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output_even,7);
		filt[row][col+8] = (unsigned char) _mm256_extract_epi8(output_even,8);
		  break;
	case 10:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output_even,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output_even,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output_even,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output_even,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output_even,7);
		filt[row][col+8] = (unsigned char) _mm256_extract_epi8(output_even,8);
		filt[row][col+9] = (unsigned char) _mm256_extract_epi8(output_even,9);
		  break;
	case 11:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output_even,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output_even,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output_even,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output_even,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output_even,7);
		filt[row][col+8] = (unsigned char) _mm256_extract_epi8(output_even,8);
		filt[row][col+9] = (unsigned char) _mm256_extract_epi8(output_even,9);
		filt[row][col+10] = (unsigned char) _mm256_extract_epi8(output_even,10);
		  break;
	case 12:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output_even,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output_even,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output_even,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output_even,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output_even,7);
		filt[row][col+8] = (unsigned char) _mm256_extract_epi8(output_even,8);
		filt[row][col+9] = (unsigned char) _mm256_extract_epi8(output_even,9);
		filt[row][col+10] = (unsigned char) _mm256_extract_epi8(output_even,10);
		filt[row][col+11] = (unsigned char) _mm256_extract_epi8(output_even,11);
		  break;
	case 13:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output_even,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output_even,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output_even,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output_even,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output_even,7);
		filt[row][col+8] = (unsigned char) _mm256_extract_epi8(output_even,8);
		filt[row][col+9] = (unsigned char) _mm256_extract_epi8(output_even,9);
		filt[row][col+10] = (unsigned char) _mm256_extract_epi8(output_even,10);
		filt[row][col+11] = (unsigned char) _mm256_extract_epi8(output_even,11);
		filt[row][col+12] = (unsigned char) _mm256_extract_epi8(output_even,12);
		  break;
	case 14:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output_even,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output_even,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output_even,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output_even,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output_even,7);
		filt[row][col+8] = (unsigned char) _mm256_extract_epi8(output_even,8);
		filt[row][col+9] = (unsigned char) _mm256_extract_epi8(output_even,9);
		filt[row][col+10] = (unsigned char) _mm256_extract_epi8(output_even,10);
		filt[row][col+11] = (unsigned char) _mm256_extract_epi8(output_even,11);
		filt[row][col+12] = (unsigned char) _mm256_extract_epi8(output_even,12);
		filt[row][col+13] = (unsigned char) _mm256_extract_epi8(output_even,13);
		  break;
	case 15:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output_even,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output_even,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output_even,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output_even,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output_even,7);
		filt[row][col+8] = (unsigned char) _mm256_extract_epi8(output_even,8);
		filt[row][col+9] = (unsigned char) _mm256_extract_epi8(output_even,9);
		filt[row][col+10] = (unsigned char) _mm256_extract_epi8(output_even,10);
		filt[row][col+11] = (unsigned char) _mm256_extract_epi8(output_even,11);
		filt[row][col+12] = (unsigned char) _mm256_extract_epi8(output_even,12);
		filt[row][col+13] = (unsigned char) _mm256_extract_epi8(output_even,13);
		filt[row][col+14] = (unsigned char) _mm256_extract_epi8(output_even,14);
		  break;
	case 16:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
		  break;
	case 17:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
		  break;
	case 18:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
		  break;
	case 19:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
		  break;
	case 20:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output_even,19);
		  break;
	case 21:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output_even,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output_even,20);
		  break;
	case 22:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output_even,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output_even,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output_even,21);
		break;
	case 23:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output_even,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output_even,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output_even,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output_even,22);
		  break;
	case 24:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output_even,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output_even,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output_even,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output_even,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output_even,23);
		  break;
	case 25:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output_even,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output_even,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output_even,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output_even,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output_even,23);
	filt[row][col+24] = (unsigned char) _mm256_extract_epi8(output_even,24);
		  break;
	case 26:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output_even,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output_even,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output_even,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output_even,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output_even,23);
	filt[row][col+24] = (unsigned char) _mm256_extract_epi8(output_even,24);
	filt[row][col+25] = (unsigned char) _mm256_extract_epi8(output_even,25);
		  break;
	case 27:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output_even,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output_even,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output_even,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output_even,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output_even,23);
	filt[row][col+24] = (unsigned char) _mm256_extract_epi8(output_even,24);
	filt[row][col+25] = (unsigned char) _mm256_extract_epi8(output_even,25);
	filt[row][col+26] = (unsigned char) _mm256_extract_epi8(output_even,26);
		  break;
	case 28:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output_even,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output_even,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output_even,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output_even,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output_even,23);
	filt[row][col+24] = (unsigned char) _mm256_extract_epi8(output_even,24);
	filt[row][col+25] = (unsigned char) _mm256_extract_epi8(output_even,25);
	filt[row][col+26] = (unsigned char) _mm256_extract_epi8(output_even,26);
	filt[row][col+27] = (unsigned char) _mm256_extract_epi8(output_even,27);
		  break;
	case 29:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output_even,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output_even,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output_even,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output_even,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output_even,23);
	filt[row][col+24] = (unsigned char) _mm256_extract_epi8(output_even,24);
	filt[row][col+25] = (unsigned char) _mm256_extract_epi8(output_even,25);
	filt[row][col+26] = (unsigned char) _mm256_extract_epi8(output_even,26);
	filt[row][col+27] = (unsigned char) _mm256_extract_epi8(output_even,27);
	filt[row][col+28] = (unsigned char) _mm256_extract_epi8(output_even,28);
		  break;
	case 30:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output_even,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output_even,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output_even,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output_even,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output_even,23);
	filt[row][col+24] = (unsigned char) _mm256_extract_epi8(output_even,24);
	filt[row][col+25] = (unsigned char) _mm256_extract_epi8(output_even,25);
	filt[row][col+26] = (unsigned char) _mm256_extract_epi8(output_even,26);
	filt[row][col+27] = (unsigned char) _mm256_extract_epi8(output_even,27);
	filt[row][col+28] = (unsigned char) _mm256_extract_epi8(output_even,28);
	filt[row][col+29] = (unsigned char) _mm256_extract_epi8(output_even,29);

		  break;

	case 31:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output_even,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output_even,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output_even,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output_even,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output_even,23);
	filt[row][col+24] = (unsigned char) _mm256_extract_epi8(output_even,24);
	filt[row][col+25] = (unsigned char) _mm256_extract_epi8(output_even,25);
	filt[row][col+26] = (unsigned char) _mm256_extract_epi8(output_even,26);
	filt[row][col+27] = (unsigned char) _mm256_extract_epi8(output_even,27);
	filt[row][col+28] = (unsigned char) _mm256_extract_epi8(output_even,28);
	filt[row][col+29] = (unsigned char) _mm256_extract_epi8(output_even,29);
	filt[row][col+30] = (unsigned char) _mm256_extract_epi8(output_even,30);

		  break;
	case 32:
		_mm256_storeu_si256( (__m256i *) &filt[row][col],output_even );

		  break;
	case 33:
		_mm256_storeu_si256( (__m256i *) &filt[row][col],output_even );

		//the filt[row][col+32] is computed unvectorized
		int newPixel = 0;
		newPixel += frame1[row-1][M-1-1] * filter[0][0];
		newPixel += frame1[row-1][M-1] * filter[0][1];

		newPixel += frame1[row][M-1-1] * filter[1][0];
		newPixel += frame1[row][M-1] * filter[1][1];

		newPixel += frame1[row+1][M-1-1] * filter[2][0];
		newPixel += frame1[row+1][M-1] * filter[2][1];

		filt[row][M-1] = (unsigned char) (newPixel / divisor);
		  break;
	default:
		printf("\nsomething went wrong");
		return -1;
 	}

return 0;


}


//this routine works only for row=0 and row=N-1
int loop_reminder_3x3_new_first_last_rows(unsigned char **frame1,unsigned char **filt,const unsigned int M, const unsigned int N,const unsigned int row, const unsigned int col,const unsigned int REMINDER_ITERATIONS,const unsigned int division_case,const unsigned short int divisor,signed char **filter,const __m256i c0,const __m256i c1,const __m256i f){

	register	__m256i r0,r1,m0,m1,m2,output_even,output_odd;


	const __m256i output_mask      = _mm256_set_epi16(0,65535,0,65535,0,65535,0,65535,0,65535,0,65535,0,65535,0,65535);

	 __m256i reminder_mask1;

	//1st col iteration
	if (row==0){
		r0=_mm256_loadu_si256( (__m256i *) &frame1[0][col-1]);
		r1=_mm256_loadu_si256( (__m256i *) &frame1[1][col-1]);
		reminder_mask1=_mm256_load_si256( (__m256i *) &reminder_msk1_3x3[REMINDER_ITERATIONS-1][0]);
		r0=_mm256_and_si256(r0,reminder_mask1);
		r1=_mm256_and_si256(r1,reminder_mask1);

		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,c1);
		m1=_mm256_maddubs_epi16(r1,c0);
	}
	else {//if row=N-1
		r0=_mm256_loadu_si256( (__m256i *) &frame1[N-2][col-1]);
		r1=_mm256_loadu_si256( (__m256i *) &frame1[N-1][col-1]);
		reminder_mask1=_mm256_load_si256( (__m256i *) &reminder_msk1_3x3[REMINDER_ITERATIONS-1][0]);
		r0=_mm256_and_si256(r0,reminder_mask1);
		r1=_mm256_and_si256(r1,reminder_mask1);

		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,c0);
		m1=_mm256_maddubs_epi16(r1,c1);
	}

		//vertical add
		m0=_mm256_add_epi16(m0,m1);

		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others. later on they will be stored into 0,4,8,12,16,20,24,28 positions (8bit register).
		output_even=_mm256_and_si256(m2,output_mask);

		//2ND col iteration
		if (row==0){
			r0=_mm256_loadu_si256( (__m256i *) &frame1[0][col]);
			r1=_mm256_loadu_si256( (__m256i *) &frame1[1][col]);
			reminder_mask1=_mm256_load_si256( (__m256i *) &reminder_msk1_3x3[REMINDER_ITERATIONS-2][0]);
			r0=_mm256_and_si256(r0,reminder_mask1);
			r1=_mm256_and_si256(r1,reminder_mask1);

			//multiply with the mask
			m0=_mm256_maddubs_epi16(r0,c1);
			m1=_mm256_maddubs_epi16(r1,c0);
		}
		else {//if row=N-1
			r0=_mm256_loadu_si256( (__m256i *) &frame1[N-2][col]);
			r1=_mm256_loadu_si256( (__m256i *) &frame1[N-1][col]);
			reminder_mask1=_mm256_load_si256( (__m256i *) &reminder_msk1_3x3[REMINDER_ITERATIONS-2][0]);
			r0=_mm256_and_si256(r0,reminder_mask1);
			r1=_mm256_and_si256(r1,reminder_mask1);

			//multiply with the mask
			m0=_mm256_maddubs_epi16(r0,c0);
			m1=_mm256_maddubs_epi16(r1,c1);
		}


		//vertical add
		m0=_mm256_add_epi16(m0,m1);

		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others. later on they will be stored into 0,4,8,12,16,20,24,28 positions (8bit register).
		output_odd=_mm256_and_si256(m2,output_mask);



if (REMINDER_ITERATIONS>2){//this prevents  reminder_msk1_3x3[REMINDER_ITERATIONS-3][0]) access -1
	//3rd col iteration
	if (row==0){
		r0=_mm256_loadu_si256( (__m256i *) &frame1[0][col+1]);
		r1=_mm256_loadu_si256( (__m256i *) &frame1[1][col+1]);
		reminder_mask1=_mm256_load_si256( (__m256i *) &reminder_msk1_3x3[REMINDER_ITERATIONS-3][0]);
		r0=_mm256_and_si256(r0,reminder_mask1);
		r1=_mm256_and_si256(r1,reminder_mask1);

		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,c1);
		m1=_mm256_maddubs_epi16(r1,c0);
	}
	else {//if row=N-1
		r0=_mm256_loadu_si256( (__m256i *) &frame1[N-2][col+1]);
		r1=_mm256_loadu_si256( (__m256i *) &frame1[N-1][col+1]);
		reminder_mask1=_mm256_load_si256( (__m256i *) &reminder_msk1_3x3[REMINDER_ITERATIONS-3][0]);
		r0=_mm256_and_si256(r0,reminder_mask1);
		r1=_mm256_and_si256(r1,reminder_mask1);

		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,c0);
		m1=_mm256_maddubs_epi16(r1,c1);
	}


		//vertical add
		m0=_mm256_add_epi16(m0,m1);

		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others. later on they will be stored into 0,4,8,12,16,20,24,28 positions (8bit register).
		m2=_mm256_and_si256(m2,output_mask);
		m2=_mm256_slli_si256(m2,2);
		output_even=_mm256_add_epi16(output_even,m2);
}


if (REMINDER_ITERATIONS>3){
		//4th col iteration

	if (row==0){
		r0=_mm256_loadu_si256( (__m256i *) &frame1[0][col+2]);
		r1=_mm256_loadu_si256( (__m256i *) &frame1[1][col+2]);
		reminder_mask1=_mm256_load_si256( (__m256i *) &reminder_msk1_3x3[REMINDER_ITERATIONS-4][0]);
		r0=_mm256_and_si256(r0,reminder_mask1);
		r1=_mm256_and_si256(r1,reminder_mask1);

		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,c1);
		m1=_mm256_maddubs_epi16(r1,c0);
	}
	else {//if row=N-1
		r0=_mm256_loadu_si256( (__m256i *) &frame1[N-2][col+2]);
		r1=_mm256_loadu_si256( (__m256i *) &frame1[N-1][col+2]);
		reminder_mask1=_mm256_load_si256( (__m256i *) &reminder_msk1_3x3[REMINDER_ITERATIONS-4][0]);
		r0=_mm256_and_si256(r0,reminder_mask1);
		r1=_mm256_and_si256(r1,reminder_mask1);

		//multiply with the mask
		m0=_mm256_maddubs_epi16(r0,c0);
		m1=_mm256_maddubs_epi16(r1,c1);
	}

		//vertical add
		m0=_mm256_add_epi16(m0,m1);

		//hozizontal additions
		m1=_mm256_srli_si256(m0,2);
		m2=_mm256_add_epi16(m1,m0);

		//m2 has 16 16bit values now. the results I need are in positions 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others. later on they will be stored into 0,4,8,12,16,20,24,28 positions (8bit register).
		m2=_mm256_and_si256(m2,output_mask);
		m2=_mm256_slli_si256(m2,2);
		output_odd=_mm256_add_epi16(output_odd,m2);
}

		//now division follows
		output_even=division(division_case,output_even,f);
		output_odd=division(division_case,output_odd,f);

		//shift odd 1 position and add to even
		output_odd = _mm256_slli_si256(output_odd,1);
		output_even = _mm256_add_epi8(output_even,output_odd);



 	switch (REMINDER_ITERATIONS){

	case 2:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		  break;
	case 3:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		  break;
	case 4:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output_even,3);
		  break;
	case 5:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output_even,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output_even,4);
		  break;
	case 6:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output_even,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output_even,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output_even,5);
		  break;
	case 7:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output_even,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output_even,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output_even,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output_even,6);
		  break;
	case 8:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output_even,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output_even,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output_even,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output_even,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output_even,7);
		  break;
	case 9:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output_even,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output_even,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output_even,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output_even,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output_even,7);
		filt[row][col+8] = (unsigned char) _mm256_extract_epi8(output_even,8);
		  break;
	case 10:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output_even,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output_even,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output_even,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output_even,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output_even,7);
		filt[row][col+8] = (unsigned char) _mm256_extract_epi8(output_even,8);
		filt[row][col+9] = (unsigned char) _mm256_extract_epi8(output_even,9);
		  break;
	case 11:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output_even,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output_even,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output_even,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output_even,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output_even,7);
		filt[row][col+8] = (unsigned char) _mm256_extract_epi8(output_even,8);
		filt[row][col+9] = (unsigned char) _mm256_extract_epi8(output_even,9);
		filt[row][col+10] = (unsigned char) _mm256_extract_epi8(output_even,10);
		  break;
	case 12:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output_even,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output_even,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output_even,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output_even,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output_even,7);
		filt[row][col+8] = (unsigned char) _mm256_extract_epi8(output_even,8);
		filt[row][col+9] = (unsigned char) _mm256_extract_epi8(output_even,9);
		filt[row][col+10] = (unsigned char) _mm256_extract_epi8(output_even,10);
		filt[row][col+11] = (unsigned char) _mm256_extract_epi8(output_even,11);
		  break;
	case 13:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output_even,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output_even,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output_even,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output_even,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output_even,7);
		filt[row][col+8] = (unsigned char) _mm256_extract_epi8(output_even,8);
		filt[row][col+9] = (unsigned char) _mm256_extract_epi8(output_even,9);
		filt[row][col+10] = (unsigned char) _mm256_extract_epi8(output_even,10);
		filt[row][col+11] = (unsigned char) _mm256_extract_epi8(output_even,11);
		filt[row][col+12] = (unsigned char) _mm256_extract_epi8(output_even,12);
		  break;
	case 14:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output_even,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output_even,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output_even,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output_even,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output_even,7);
		filt[row][col+8] = (unsigned char) _mm256_extract_epi8(output_even,8);
		filt[row][col+9] = (unsigned char) _mm256_extract_epi8(output_even,9);
		filt[row][col+10] = (unsigned char) _mm256_extract_epi8(output_even,10);
		filt[row][col+11] = (unsigned char) _mm256_extract_epi8(output_even,11);
		filt[row][col+12] = (unsigned char) _mm256_extract_epi8(output_even,12);
		filt[row][col+13] = (unsigned char) _mm256_extract_epi8(output_even,13);
		  break;
	case 15:
		filt[row][col] = (unsigned char) _mm256_extract_epi8(output_even,0);
		filt[row][col+1] = (unsigned char) _mm256_extract_epi8(output_even,1);
		filt[row][col+2] = (unsigned char) _mm256_extract_epi8(output_even,2);
		filt[row][col+3] = (unsigned char) _mm256_extract_epi8(output_even,3);
		filt[row][col+4] = (unsigned char) _mm256_extract_epi8(output_even,4);
		filt[row][col+5] = (unsigned char) _mm256_extract_epi8(output_even,5);
		filt[row][col+6] = (unsigned char) _mm256_extract_epi8(output_even,6);
		filt[row][col+7] = (unsigned char) _mm256_extract_epi8(output_even,7);
		filt[row][col+8] = (unsigned char) _mm256_extract_epi8(output_even,8);
		filt[row][col+9] = (unsigned char) _mm256_extract_epi8(output_even,9);
		filt[row][col+10] = (unsigned char) _mm256_extract_epi8(output_even,10);
		filt[row][col+11] = (unsigned char) _mm256_extract_epi8(output_even,11);
		filt[row][col+12] = (unsigned char) _mm256_extract_epi8(output_even,12);
		filt[row][col+13] = (unsigned char) _mm256_extract_epi8(output_even,13);
		filt[row][col+14] = (unsigned char) _mm256_extract_epi8(output_even,14);
		  break;
	case 16:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
		  break;
	case 17:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
		  break;
	case 18:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
		  break;
	case 19:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
		  break;
	case 20:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output_even,19);
		  break;
	case 21:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output_even,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output_even,20);
		  break;
	case 22:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output_even,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output_even,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output_even,21);
		break;
	case 23:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output_even,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output_even,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output_even,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output_even,22);
		  break;
	case 24:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output_even,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output_even,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output_even,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output_even,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output_even,23);
		  break;
	case 25:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output_even,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output_even,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output_even,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output_even,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output_even,23);
	filt[row][col+24] = (unsigned char) _mm256_extract_epi8(output_even,24);
		  break;
	case 26:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output_even,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output_even,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output_even,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output_even,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output_even,23);
	filt[row][col+24] = (unsigned char) _mm256_extract_epi8(output_even,24);
	filt[row][col+25] = (unsigned char) _mm256_extract_epi8(output_even,25);
		  break;
	case 27:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output_even,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output_even,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output_even,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output_even,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output_even,23);
	filt[row][col+24] = (unsigned char) _mm256_extract_epi8(output_even,24);
	filt[row][col+25] = (unsigned char) _mm256_extract_epi8(output_even,25);
	filt[row][col+26] = (unsigned char) _mm256_extract_epi8(output_even,26);
		  break;
	case 28:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output_even,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output_even,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output_even,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output_even,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output_even,23);
	filt[row][col+24] = (unsigned char) _mm256_extract_epi8(output_even,24);
	filt[row][col+25] = (unsigned char) _mm256_extract_epi8(output_even,25);
	filt[row][col+26] = (unsigned char) _mm256_extract_epi8(output_even,26);
	filt[row][col+27] = (unsigned char) _mm256_extract_epi8(output_even,27);
		  break;
	case 29:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output_even,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output_even,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output_even,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output_even,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output_even,23);
	filt[row][col+24] = (unsigned char) _mm256_extract_epi8(output_even,24);
	filt[row][col+25] = (unsigned char) _mm256_extract_epi8(output_even,25);
	filt[row][col+26] = (unsigned char) _mm256_extract_epi8(output_even,26);
	filt[row][col+27] = (unsigned char) _mm256_extract_epi8(output_even,27);
	filt[row][col+28] = (unsigned char) _mm256_extract_epi8(output_even,28);
		  break;
	case 30:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output_even,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output_even,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output_even,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output_even,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output_even,23);
	filt[row][col+24] = (unsigned char) _mm256_extract_epi8(output_even,24);
	filt[row][col+25] = (unsigned char) _mm256_extract_epi8(output_even,25);
	filt[row][col+26] = (unsigned char) _mm256_extract_epi8(output_even,26);
	filt[row][col+27] = (unsigned char) _mm256_extract_epi8(output_even,27);
	filt[row][col+28] = (unsigned char) _mm256_extract_epi8(output_even,28);
	filt[row][col+29] = (unsigned char) _mm256_extract_epi8(output_even,29);

		  break;

	case 31:
  	_mm_storeu_si128( (__m128i *) &filt[row][col],_mm256_extractf128_si256(output_even, 0)); //store low 128bit - 16pixels
	filt[row][col+16] = (unsigned char) _mm256_extract_epi8(output_even,16);
	filt[row][col+17] = (unsigned char) _mm256_extract_epi8(output_even,17);
	filt[row][col+18] = (unsigned char) _mm256_extract_epi8(output_even,18);
	filt[row][col+19] = (unsigned char) _mm256_extract_epi8(output_even,19);
	filt[row][col+20] = (unsigned char) _mm256_extract_epi8(output_even,20);
	filt[row][col+21] = (unsigned char) _mm256_extract_epi8(output_even,21);
	filt[row][col+22] = (unsigned char) _mm256_extract_epi8(output_even,22);
	filt[row][col+23] = (unsigned char) _mm256_extract_epi8(output_even,23);
	filt[row][col+24] = (unsigned char) _mm256_extract_epi8(output_even,24);
	filt[row][col+25] = (unsigned char) _mm256_extract_epi8(output_even,25);
	filt[row][col+26] = (unsigned char) _mm256_extract_epi8(output_even,26);
	filt[row][col+27] = (unsigned char) _mm256_extract_epi8(output_even,27);
	filt[row][col+28] = (unsigned char) _mm256_extract_epi8(output_even,28);
	filt[row][col+29] = (unsigned char) _mm256_extract_epi8(output_even,29);
	filt[row][col+30] = (unsigned char) _mm256_extract_epi8(output_even,30);

		  break;
	case 32:
		_mm256_storeu_si256( (__m256i *) &filt[row][col],output_even );

		  break;
	case 33:
		_mm256_storeu_si256( (__m256i *) &filt[row][col],output_even );

		//the filt[row][col+32] is computed unvectorized
		int newPixel = 0;

		if (row==N-1){
		newPixel += frame1[row-1][M-1-1] * filter[0][0];
		newPixel += frame1[row-1][M-1] * filter[0][1];}

		newPixel += frame1[row][M-1-1] * filter[1][0];
		newPixel += frame1[row][M-1] * filter[1][1];

		if (row==0){
		newPixel += frame1[row+1][M-1-1] * filter[2][0];
		newPixel += frame1[row+1][M-1] * filter[2][1];}

		filt[row][M-1] = (unsigned char) (newPixel / divisor);
		  break;
	default:
		printf("\nsomething went wrong");
		return -1;
 	}

return 0;


}


